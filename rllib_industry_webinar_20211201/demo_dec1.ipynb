{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a886feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third party imports.\n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import os\n",
    "from starlette.requests import Request\n",
    "import time\n",
    "\n",
    "# Ray imports.\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray import serve\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f88e1",
   "metadata": {},
   "source": [
    "### Running on Anyscale\n",
    "\n",
    "Let's connect to an existing 1GPU/16CPUs cluster via `ray.init(address=...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "155a8549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m(anyscale +0.1s)\u001b[0m Loaded Anyscale authentication token from ~/.anyscale/credentials.json\n",
      "\u001b[1m\u001b[36m(anyscale +0.1s)\u001b[0m Loaded Anyscale authentication token from ~/.anyscale/credentials.json\n",
      "\u001b[1m\u001b[36m(anyscale +0.9s)\u001b[0m .anyscale.yaml found in project_dir. Directory is attached to a project.\n",
      "\u001b[1m\u001b[36m(anyscale +1.7s)\u001b[0m Using project (name: cuj_rllib, project_dir: /Users/sven/Dropbox/Projects/anyscale_projects/cuj-rl-in-production, id: prj_84JWkW5F1TqLJwhSqLDadyML).\n",
      "\u001b[1m\u001b[36m(anyscale +3.1s)\u001b[0m cluster cluster-12 is currently running, the cluster will not be restarted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'pip' or 'conda' field was specified in the runtime env, so it may take some time to install the environment before Ray connects.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=runtime_env)\u001b[0m 2021-12-01 09:17:12,040\tINFO conda.py:219 -- Setting up conda environment with {'_ray_commit': '72fdf3be605e4aadfd9a8c3f01dd7843c01cb8fd', 'env_vars': {'RAY_SERVE_ROOT_URL': 'https://serve-session-qkbdqbk6fq939rimgf52smke.i.anyscaleuserdata.com'}, 'excludes': ['.git', '__pycache__', 'venv', '/Users/sven/Dropbox/Projects/anyscale_projects/cuj-rl-in-production/.anyscale.yaml', '/Users/sven/Dropbox/Projects/anyscale_projects/cuj-rl-in-production/session-default.yaml'], 'pip': ['ray[rllib]', 'jupyter', 'tblib', 'fastapi', 'uvicorn', 'anyscale', 'requests', 'torch', 'tensorflow', 'gsutil'], 'uris': ['gcs://_ray_pkg_9d.zip']}\n",
      "\u001b[2m\u001b[36m(pid=runtime_env)\u001b[0m 2021-12-01 09:17:13,162\tINFO conda.py:243 -- Finished setting up runtime environment at /tmp/ray/session_2021-12-01_04-27-46_331452_161/runtime_resources/conda/ray-1fdfbc69e9e1c069ec07b30674483e16d97bdd20\n",
      "\u001b[1m\u001b[36m(anyscale +28.8s)\u001b[0m Connected to cluster-12, see: https://console.anyscale.com/projects/prj_84JWkW5F1TqLJwhSqLDadyML/clusters/ses_QKBDQBK6fq939rimGf52Smke\n",
      "\u001b[1m\u001b[36m(anyscale +28.8s)\u001b[0m URL for head node of cluster: https://session-qkbdqbk6fq939rimgf52smke.i.anyscaleuserdata.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnyscaleClientContext(dashboard_url='https://session-qkbdqbk6fq939rimgf52smke.i.anyscaleuserdata.com/auth/?token=688034c9-c779-4257-b833-e7dc6d33a430&redirect_to=dashboard', python_version='3.8.5', ray_version='1.8.0', ray_commit='72fdf3be605e4aadfd9a8c3f01dd7843c01cb8fd', protocol_version='2021-09-22', _num_clients=1, _context_to_restore=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(\n",
    "    # Connecting to an existing (and running) cluster (\"cluster-12\" in my account).\n",
    "    address=\"anyscale://cluster-12\",\n",
    "\n",
    "    # This will upload this directory to Anyscale so that the code can be run on cluster.\n",
    "    project_dir=\".\",\n",
    "    \n",
    "    #cloud=\"anyscale_default_cloud\",\n",
    "    \n",
    "    # Our Python dependencies, e.g. tensorflow\n",
    "    # (make sure everything is available on the cluster).\n",
    "    runtime_env={\"pip\": \"./requirements.txt\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379fdb3d",
   "metadata": {},
   "source": [
    "### Coding/defining our \"problem\" via an RL environment.\n",
    "\n",
    "We will use the following (adversarial) multi-agent environment throughout this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568a228",
   "metadata": {},
   "source": [
    "<img src=\"img/environment.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "215530cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15beff0b6fac4adfb1386ef6027bfbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent1's x/y position=[2, 2]\n",
      "Agent2's x/y position=[7, 7]\n",
      "Env timesteps=4\n"
     ]
    }
   ],
   "source": [
    "# Let's code our multi-agent environment.\n",
    "\n",
    "class MultiAgentArena(MultiAgentEnv):\n",
    "    def __init__(self, config=None):\n",
    "        config = config or {}\n",
    "        # Dimensions of the grid.\n",
    "        self.width = config.get(\"width\", 10)\n",
    "        self.height = config.get(\"height\", 10)\n",
    "\n",
    "        # End an episode after this many timesteps.\n",
    "        self.timestep_limit = config.get(\"ts\", 100)\n",
    "\n",
    "        self.observation_space = MultiDiscrete([self.width * self.height,\n",
    "                                                self.width * self.height])\n",
    "        # 0=up, 1=right, 2=down, 3=left.\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        # Reset env.\n",
    "        self.reset()\n",
    "\n",
    "        # For rendering.\n",
    "        self.out = None\n",
    "        if config.get(\"render\"):\n",
    "            self.out = Output()\n",
    "            display.display(self.out)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Returns initial observation of next(!) episode.\"\"\"\n",
    "        # Row-major coords.\n",
    "        self.agent1_pos = [0, 0]  # upper left corner\n",
    "        self.agent2_pos = [self.height - 1, self.width - 1]  # lower bottom corner\n",
    "\n",
    "        # Accumulated rewards in this episode.\n",
    "        self.agent1_R = 0.0\n",
    "        self.agent2_R = 0.0\n",
    "\n",
    "        # Reset agent1's visited fields.\n",
    "        self.agent1_visited_fields = set([tuple(self.agent1_pos)])\n",
    "\n",
    "        # How many timesteps have we done in this episode.\n",
    "        self.timesteps = 0\n",
    "\n",
    "        # Did we have a collision in recent step?\n",
    "        self.collision = False\n",
    "        # How many collisions in total have we had in this episode?\n",
    "        self.num_collisions = 0\n",
    "\n",
    "        # Return the initial observation in the new episode.\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: dict):\n",
    "        \"\"\"\n",
    "        Returns (next observation, rewards, dones, infos) after having taken the given actions.\n",
    "        \n",
    "        e.g.\n",
    "        `action={\"agent1\": action_for_agent1, \"agent2\": action_for_agent2}`\n",
    "        \"\"\"\n",
    "        \n",
    "        # increase our time steps counter by 1.\n",
    "        self.timesteps += 1\n",
    "        # An episode is \"done\" when we reach the time step limit.\n",
    "        is_done = self.timesteps >= self.timestep_limit\n",
    "\n",
    "        # Agent2 always moves first.\n",
    "        # events = [collision|agent1_new_field]\n",
    "        events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        events |= self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "\n",
    "        # Useful for rendering.\n",
    "        self.collision = \"collision\" in events\n",
    "        if self.collision is True:\n",
    "            self.num_collisions += 1\n",
    "            \n",
    "        # Get observations (based on new agent positions).\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Determine rewards based on the collected events:\n",
    "        r1 = -1.0 if \"collision\" in events else 1.0 if \"agent1_new_field\" in events else -0.5\n",
    "        r2 = 1.0 if \"collision\" in events else -0.1\n",
    "\n",
    "        self.agent1_R += r1\n",
    "        self.agent2_R += r2\n",
    "        \n",
    "        rewards = {\n",
    "            \"agent1\": r1,\n",
    "            \"agent2\": r2,\n",
    "        }\n",
    "\n",
    "        # Generate a `done` dict (per-agent and total).\n",
    "        dones = {\n",
    "            \"agent1\": is_done,\n",
    "            \"agent2\": is_done,\n",
    "            # special `__all__` key indicates that the episode is done for all agents.\n",
    "            \"__all__\": is_done,\n",
    "        }\n",
    "\n",
    "        return obs, rewards, dones, {}  # <- info dict (not needed here).\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns obs dict (agent name to discrete-pos tuple) using each\n",
    "        agent's current x/y-positions.\n",
    "        \"\"\"\n",
    "        ag1_discrete_pos = self.agent1_pos[0] * self.width + \\\n",
    "            (self.agent1_pos[1] % self.width)\n",
    "        ag2_discrete_pos = self.agent2_pos[0] * self.width + \\\n",
    "            (self.agent2_pos[1] % self.width)\n",
    "        return {\n",
    "            \"agent1\": np.array([ag1_discrete_pos, ag2_discrete_pos]),\n",
    "            \"agent2\": np.array([ag2_discrete_pos, ag1_discrete_pos]),\n",
    "        }\n",
    "\n",
    "    def _move(self, coords, action, is_agent1):\n",
    "        \"\"\"\n",
    "        Moves an agent (agent1 iff is_agent1=True, else agent2) from `coords` (x/y) using the\n",
    "        given action (0=up, 1=right, etc..) and returns a resulting events dict:\n",
    "        Agent1: \"new\" when entering a new field. \"bumped\" when having been bumped into by agent2.\n",
    "        Agent2: \"bumped\" when bumping into agent1 (agent1 then gets -1.0).\n",
    "        \"\"\"\n",
    "        orig_coords = coords[:]\n",
    "        # Change the row: 0=up (-1), 2=down (+1)\n",
    "        coords[0] += -1 if action == 0 else 1 if action == 2 else 0\n",
    "        # Change the column: 1=right (+1), 3=left (-1)\n",
    "        coords[1] += 1 if action == 1 else -1 if action == 3 else 0\n",
    "\n",
    "        # Solve collisions.\n",
    "        # Make sure, we don't end up on the other agent's position.\n",
    "        # If yes, don't move (we are blocked).\n",
    "        if (is_agent1 and coords == self.agent2_pos) or (not is_agent1 and coords == self.agent1_pos):\n",
    "            coords[0], coords[1] = orig_coords\n",
    "            # Agent2 blocked agent1 (agent1 tried to run into agent2)\n",
    "            # OR Agent2 bumped into agent1 (agent2 tried to run into agent1)\n",
    "            return {\"collision\"}\n",
    "\n",
    "        # No agent blocking -> check walls.\n",
    "        if coords[0] < 0:\n",
    "            coords[0] = 0\n",
    "        elif coords[0] >= self.height:\n",
    "            coords[0] = self.height - 1\n",
    "        if coords[1] < 0:\n",
    "            coords[1] = 0\n",
    "        elif coords[1] >= self.width:\n",
    "            coords[1] = self.width - 1\n",
    "\n",
    "        # If agent1 -> \"new\" if new tile covered.\n",
    "        if is_agent1 and not tuple(coords) in self.agent1_visited_fields:\n",
    "            self.agent1_visited_fields.add(tuple(coords))\n",
    "            return {\"agent1_new_field\"}\n",
    "        # No new tile for agent1.\n",
    "        return set()\n",
    "\n",
    "    def render(self, mode=None):\n",
    "\n",
    "        if self.out is not None:\n",
    "            self.out.clear_output(wait=True)\n",
    "\n",
    "        print(\"_\" * (self.width + 2))\n",
    "        for r in range(self.height):\n",
    "            print(\"|\", end=\"\")\n",
    "            for c in range(self.width):\n",
    "                field = r * self.width + c % self.width\n",
    "                if self.agent1_pos == [r, c]:\n",
    "                    print(\"1\", end=\"\")\n",
    "                elif self.agent2_pos == [r, c]:\n",
    "                    print(\"2\", end=\"\")\n",
    "                elif (r, c) in self.agent1_visited_fields:\n",
    "                    print(\".\", end=\"\")\n",
    "                else:\n",
    "                    print(\" \", end=\"\")\n",
    "            print(\"|\")\n",
    "        print(\"‾\" * (self.width + 2))\n",
    "        print(f\"{'!!Collision!!' if self.collision else ''}\")\n",
    "        print(\"R1={: .1f}\".format(self.agent1_R))\n",
    "        print(\"R2={: .1f} ({} collisions)\".format(self.agent2_R, self.num_collisions))\n",
    "        print()\n",
    "        time.sleep(0.25)\n",
    "\n",
    "\n",
    "env = MultiAgentArena(config={\"render\": True})\n",
    "obs = env.reset()\n",
    "\n",
    "with env.out:\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves right, Agent2 moves left.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 1, \"agent2\": 3})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves right, Agent2 moves left.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 1, \"agent2\": 3})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "\n",
    "print(\"Agent1's x/y position={}\".format(env.agent1_pos))\n",
    "print(\"Agent2's x/y position={}\".format(env.agent2_pos))\n",
    "print(\"Env timesteps={}\".format(env.timesteps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256665b3",
   "metadata": {},
   "source": [
    "### Configuring our Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd0b9aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_CFG = {\n",
    "    # Using our environment class defined above.\n",
    "    \"env\": MultiAgentArena,\n",
    "    # Use `framework=torch` here for PyTorch.\n",
    "    \"framework\": \"tf\",\n",
    "\n",
    "    # Run on 1 GPU on the \"learner\".\n",
    "    \"num_gpus\": 1,\n",
    "    # Use 15 ray-parallelized environment workers,\n",
    "    # which collect samples to learn from. Each worker gets assigned\n",
    "    # 1 CPU.\n",
    "    \"num_workers\": 15,\n",
    "    # Each of the 15 workers has 10 environment copies (\"vectorization\")\n",
    "    # for faster (batched) forward passes.\n",
    "    \"num_envs_per_worker\": 10,\n",
    "\n",
    "    # Multi-agent setup: 2 policies.\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\"policy1\", \"policy2\"},\n",
    "        \"policy_mapping_fn\": lambda agent_id: \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c2ba3",
   "metadata": {},
   "source": [
    "### Training our 2 Policies (agent1 and agent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d65afb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:24:41 (running for 00:00:00.14)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 4.7/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 PENDING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+-------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc   |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+-------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | PENDING  |       |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+-------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m 2021-12-01 09:24:47,812\tINFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m 2021-12-01 09:24:47,813\tWARNING ppo.py:143 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=15 num_envs_per_worker=10 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 26.\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m 2021-12-01 09:24:47,813\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m 2021-12-01 09:24:47,813\tINFO trainer.py:770 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m 2021-12-01 09:24:59,196\tWARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m 2021-12-01 09:25:04,123\tWARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:25:09 (running for 00:00:28.45)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m 2021-12-01 09:25:09,715\tWARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m 2021-12-01 09:25:09,716\tINFO trainable.py:110 -- Trainable.setup took 21.905 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m 2021-12-01 09:25:09,716\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m 2021-12-01 09:25:09,814\tWARNING deprecation.py:38 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:25:10 (running for 00:00:29.47)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:25:15 (running for 00:00:34.47)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:25:20 (running for 00:00:39.49)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:25:25 (running for 00:00:44.49)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 15600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-25-29\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: .nan\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: .nan\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.352578043937683\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.03468821570277214\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.06490864604711533\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 14.65290355682373\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.024883203208446503\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 14.710874557495117\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.3761183023452759\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.01018455158919096\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.0198676697909832\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 0.5552526712417603\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.2040128856897354\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 0.5730834007263184\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 15600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 15600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 7800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 7800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 1\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 33.84285714285714\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.317857142857145\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 19.408130884170532\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 19.408130884170532\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 19.408130884170532\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 423.863\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18402.159\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 121232.542\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 64.339\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 8580.757\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 909.011\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 7.935\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379529\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 7800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 1\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:25:31 (running for 00:00:49.91)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      1 |          19.4081 | 7800 |      nan |                  nan |                  nan |                nan |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:25:36 (running for 00:00:54.91)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      1 |          19.4081 | 7800 |      nan |                  nan |                  nan |                nan |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:25:41 (running for 00:00:59.93)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      1 |          19.4081 | 7800 |      nan |                  nan |                  nan |                nan |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:25:46 (running for 00:01:04.93)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      1 |          19.4081 | 7800 |      nan |                  nan |                  nan |                nan |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 31200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-25-48\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 12.000000000000012\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: -5.1979999999999915\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -30.00000000000007\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 150\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 150\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.3058615922927856\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.021461952477693558\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.028986014425754547\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 12.187546730041504\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.08370912075042725\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 12.212239265441895\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.346630573272705\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.011997955851256847\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.02863633818924427\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 0.8352543711662292\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.1101209819316864\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 0.8614911437034607\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 31200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 31200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 15600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 15600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 2\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 39.021428571428565\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.396428571428572\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 22.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -1.1999999999999937\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 3.4966666666666666\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -8.694666666666647\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -20.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6159058828202507\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.3817797464037698\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 7.362540714324468\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.419015581645663\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 38.79487133026123\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 19.3867404460907\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 38.79487133026123\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 422.946\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18442.088\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 221789.957\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 35.168\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 767.261\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 10166.033\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 7.56\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379548\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 15600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 2\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:25:51 (running for 00:01:10.55)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-5.1979999999999915 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369baa3a0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      2 |          38.7949 | 15600 |   -5.198 |                   12 |                  -30 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:25:56 (running for 00:01:15.56)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-5.1979999999999915 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369baa3a0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      2 |          38.7949 | 15600 |   -5.198 |                   12 |                  -30 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:26:01 (running for 00:01:20.57)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-5.1979999999999915 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369baa3a0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      2 |          38.7949 | 15600 |   -5.198 |                   12 |                  -30 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:26:06 (running for 00:01:25.57)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-5.1979999999999915 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369baa3a0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      2 |          38.7949 | 15600 |   -5.198 |                   12 |                  -30 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 46800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-26-08\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 11.700000000000017\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: -5.039999999999991\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -25.500000000000032\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 150\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.273499608039856\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.01224368717521429\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.040747806429862976\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 13.092193603515625\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.1883929818868637\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 13.130492210388184\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.3168257474899292\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.013803876005113125\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.031341176480054855\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 0.8136160373687744\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.20239371061325073\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 0.8421964645385742\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 46800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 46800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 23400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 23400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 3\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 39.25714285714286\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 19.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -2.2999999999999834\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 3.585\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -8.624999999999982\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -15.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6078206925165087\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.3791245960053942\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 7.298472495306105\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.378433863321941\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 58.21427774429321\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 19.419406414031982\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 58.21427774429321\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 422.166\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18476.145\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 309552.494\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 25.198\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 586.403\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 13301.44\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 7.283\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379568\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 23400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 3\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:26:12 (running for 00:01:30.99)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-5.039999999999991 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369bb2e50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      3 |          58.2143 | 23400 |    -5.04 |                 11.7 |                -25.5 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:26:17 (running for 00:01:36.00)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-5.039999999999991 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369bb2e50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      3 |          58.2143 | 23400 |    -5.04 |                 11.7 |                -25.5 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:26:22 (running for 00:01:41.01)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-5.039999999999991 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369bb2e50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      3 |          58.2143 | 23400 |    -5.04 |                 11.7 |                -25.5 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:26:27 (running for 00:01:46.02)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-5.039999999999991 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369bb2e50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      3 |          58.2143 | 23400 |    -5.04 |                 11.7 |                -25.5 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 62400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-26-27\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 18.599999999999937\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: -2.183999999999988\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -18.899999999999984\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 150\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 300\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.289932131767273\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.016491953283548355\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.030901003628969193\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 9.473957061767578\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.43975281715393066\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 9.501559257507324\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.2796396017074585\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.014932800084352493\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.01970311440527439\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 1.4031100273132324\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.33258169889450073\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 1.419826626777649\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 62400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 62400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 31200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 31200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 4\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 35.43703703703704\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 27.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 7.6000000000000085\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 4.67\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -6.853999999999988\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -15.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.625742983780029\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.39853791110633474\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 7.281195337882642\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.549221403861161\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 77.30685639381409\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 19.092578649520874\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 77.30685639381409\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 424.184\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18388.267\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 382085.242\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 20.414\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 524.581\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 14869.022\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 7.439\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379587\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 31200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 4\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:26:32 (running for 00:01:51.24)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-2.183999999999988 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369b94430>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      4 |          77.3069 | 31200 |   -2.184 |                 18.6 |                -18.9 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:26:37 (running for 00:01:56.24)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-2.183999999999988 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369b94430>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      4 |          77.3069 | 31200 |   -2.184 |                 18.6 |                -18.9 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:26:42 (running for 00:02:01.26)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-2.183999999999988 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369b94430>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      4 |          77.3069 | 31200 |   -2.184 |                 18.6 |                -18.9 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 78000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-26-46\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 18.599999999999937\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: -2.0009999999999866\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -17.399999999999984\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 300\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.2455532550811768\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.01769167371094227\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.043734852224588394\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 10.493243217468262\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.33148348331451416\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 10.533439636230469\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.240049958229065\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.016685953363776207\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.03764614462852478\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 1.5265305042266846\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.1994437724351883\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 1.5608394145965576\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 78000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 78000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 39000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 39000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 37.29642857142857\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 27.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 2.0999999999999965\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 5.04\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -7.040999999999988\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -11.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6227875440314627\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.40234891991866256\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 7.214690851822993\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.542001313570014\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 96.50926899909973\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 19.202412605285645\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 96.50926899909973\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 424.535\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18373.063\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 443578.11\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 17.584\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 496.206\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 15719.264\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 7.243\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379606\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 39000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:26:47 (running for 00:02:06.42)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-2.0009999999999866 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368b1db80>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      5 |          96.5093 | 39000 |   -2.001 |                 18.6 |                -17.4 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:26:52 (running for 00:02:11.44)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-2.0009999999999866 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368b1db80>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      5 |          96.5093 | 39000 |   -2.001 |                 18.6 |                -17.4 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:26:57 (running for 00:02:16.44)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-2.0009999999999866 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368b1db80>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      5 |          96.5093 | 39000 |   -2.001 |                 18.6 |                -17.4 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:27:02 (running for 00:02:21.45)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=-2.0009999999999866 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368b1db80>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      5 |          96.5093 | 39000 |   -2.001 |                 18.6 |                -17.4 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 93600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-27-06\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 23.099999999999962\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 1.5220000000000091\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -16.79999999999999\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 150\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 450\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.2165671586990356\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.017917027696967125\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.032206691801548004\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 11.530242919921875\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.4442717730998993\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 11.558867454528809\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.1902633905410767\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.017297208309173584\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.03190551698207855\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 1.627307415008545\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.4129865765571594\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 1.655753493309021\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 93600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 93600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 46800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 46800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 6\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 39.574999999999996\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 30.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 8.700000000000005\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 5.963333333333333\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -4.4413333333333265\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -14.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6216539234652939\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.39179038188962767\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 7.015504369847439\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.4782417508613825\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 116.27884483337402\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 19.769575834274292\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 116.27884483337402\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 422.068\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18480.42\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 504080.356\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 15.474\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 478.959\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 16285.336\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 7.14\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379626\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 46800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 6\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:27:08 (running for 00:02:27.26)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=1.5220000000000091 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369be8550>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      6 |          116.279 | 46800 |    1.522 |                 23.1 |                -16.8 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:27:13 (running for 00:02:32.34)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=1.5220000000000091 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369be8550>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      6 |          116.279 | 46800 |    1.522 |                 23.1 |                -16.8 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:27:18 (running for 00:02:37.35)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=1.5220000000000091 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369be8550>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      6 |          116.279 | 46800 |    1.522 |                 23.1 |                -16.8 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:27:23 (running for 00:02:42.36)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=1.5220000000000091 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369be8550>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      6 |          116.279 | 46800 |    1.522 |                 23.1 |                -16.8 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 109200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-27-25\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 23.099999999999962\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 1.8660000000000079\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -14.099999999999977\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 450\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.2160497903823853\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.015280854888260365\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.03276867792010307\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 14.585691452026367\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.12178187817335129\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 14.615403175354004\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.120141863822937\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.01760859228670597\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.03267073258757591\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 1.7231041193008423\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.12034871429204941\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 1.7522531747817993\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 109200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 109200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 54600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 54600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 7\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 35.407407407407405\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 30.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 6.499999999999997\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 6.41\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -4.5439999999999925\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -14.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6198810693174124\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.3955132664202122\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 6.949160274225303\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.462103606032108\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 135.21978569030762\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 18.940940856933594\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 135.21978569030762\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 422.985\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18440.349\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 557592.945\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 13.989\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 464.587\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 16789.098\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 7.214\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379645\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 54600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 7\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:27:29 (running for 00:02:48.27)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=1.8660000000000079 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369be8b80>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      7 |           135.22 | 54600 |    1.866 |                 23.1 |                -14.1 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:27:34 (running for 00:02:53.28)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=1.8660000000000079 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369be8b80>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      7 |           135.22 | 54600 |    1.866 |                 23.1 |                -14.1 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:27:39 (running for 00:02:58.28)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=1.8660000000000079 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369be8b80>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      7 |           135.22 | 54600 |    1.866 |                 23.1 |                -14.1 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:27:44 (running for 00:03:03.30)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=1.8660000000000079 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369be8b80>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      7 |           135.22 | 54600 |    1.866 |                 23.1 |                -14.1 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 124800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-27-45\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 27.29999999999993\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 4.014000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -15.899999999999988\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 150\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.1697142124176025\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.01568864844739437\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.02855854481458664\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 12.848346710205078\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.38770440220832825\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 12.873766899108887\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.0657033920288086\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.015300621278584003\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.02559126541018486\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 1.8759502172470093\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.3678937554359436\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 1.8984814882278442\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 124800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 124800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 62400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 62400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 8\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 39.34285714285715\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 33.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 8.700000000000012\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 8.36\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -4.345999999999991\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -13.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6172929164602889\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.3899915422276437\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 6.844291641271945\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.4379192949961315\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 154.67544102668762\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 19.455655336380005\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 154.67544102668762\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 422.683\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18453.555\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 602473.13\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 12.947\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 456.429\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 17089.201\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 7.034\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379665\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 62400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 8\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:27:50 (running for 00:03:08.80)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=4.014000000000002 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368b1d700>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      8 |          154.675 | 62400 |    4.014 |                 27.3 |                -15.9 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:27:55 (running for 00:03:13.86)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=4.014000000000002 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368b1d700>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      8 |          154.675 | 62400 |    4.014 |                 27.3 |                -15.9 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:28:00 (running for 00:03:18.86)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=4.014000000000002 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368b1d700>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      8 |          154.675 | 62400 |    4.014 |                 27.3 |                -15.9 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 140400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-28-03\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 25.199999999999896\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 4.2810000000000015\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -15.899999999999988\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.1397846937179565\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.018616892397403717\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.03548170253634453\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 16.317535400390625\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.05439038574695587\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 16.349294662475586\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.0021177530288696\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.014687897637486458\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.021212873980402946\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 2.277317762374878\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.1966138780117035\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 2.29559326171875\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 140400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 140400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 70200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 70200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 9\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 35.276923076923076\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 33.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 8.700000000000012\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 8.66\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -4.378999999999992\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -13.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6166142525432781\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.39373216011541345\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 6.805252571471876\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.4207107146009275\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 173.3804132938385\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 18.70497226715088\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 173.3804132938385\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 423.851\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18402.699\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 645506.712\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 12.084\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 449.883\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 17337.858\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 6.952\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379683\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 70200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 9\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:28:05 (running for 00:03:24.56)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=4.2810000000000015 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369b94d30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      9 |           173.38 | 70200 |    4.281 |                 25.2 |                -15.9 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:28:10 (running for 00:03:29.56)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=4.2810000000000015 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369b94d30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      9 |           173.38 | 70200 |    4.281 |                 25.2 |                -15.9 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:28:15 (running for 00:03:34.58)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=4.2810000000000015 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369b94d30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      9 |           173.38 | 70200 |    4.281 |                 25.2 |                -15.9 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:28:20 (running for 00:03:39.58)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=4.2810000000000015 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369b94d30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |      9 |           173.38 | 70200 |    4.281 |                 25.2 |                -15.9 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 156000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-28-22\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 33.29999999999992\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 7.78199999999999\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -13.49999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 150\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 750\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.0662401914596558\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.016846507787704468\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.02887602336704731\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 13.417784690856934\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.3685062527656555\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 13.443290710449219\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.954621434211731\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.015207123011350632\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.02868635766208172\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 2.0109498500823975\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.35226261615753174\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 2.036594867706299\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 156000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 156000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 78000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 78000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 10\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 30.48888888888889\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 36.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 9.800000000000013\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 12.37\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -4.587999999999989\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -11.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6207112539905199\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.38814791791040926\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 6.648912570145522\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.415876660679519\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 191.82028245925903\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 18.439869165420532\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 191.82028245925903\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 425.729\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18321.522\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 680178.617\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 11.468\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 445.904\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 17492.569\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 6.961\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379702\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 78000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 10\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:28:26 (running for 00:03:45.15)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=7.78199999999999 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368b25040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     10 |           191.82 | 78000 |    7.782 |                 33.3 |                -13.5 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:28:31 (running for 00:03:50.15)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=7.78199999999999 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368b25040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     10 |           191.82 | 78000 |    7.782 |                 33.3 |                -13.5 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:28:36 (running for 00:03:55.16)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=7.78199999999999 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368b25040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     10 |           191.82 | 78000 |    7.782 |                 33.3 |                -13.5 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:28:41 (running for 00:04:00.17)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=7.78199999999999 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368b25040>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     10 |           191.82 | 78000 |    7.782 |                 33.3 |                -13.5 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 171600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-28-41\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 29.099999999999923\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 7.202999999999994\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -13.49999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 750\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 1.0344570875167847\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.019009849056601524\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.03449263796210289\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 23.014543533325195\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.021803703159093857\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 23.04523468017578\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.915333092212677\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.013815188780426979\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.027714861556887627\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 2.823155403137207\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.19087649881839752\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 2.84810733795166\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 171600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 171600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 85800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 85800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 11\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 37.737037037037034\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 36.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 9.800000000000013\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 11.725\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -4.52199999999999\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -11.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6212229920897968\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.3876070143393957\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 6.624511793799225\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.396989119487626\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 211.14561367034912\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 19.325331211090088\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 211.14561367034912\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 425.608\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18326.736\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 1394864.532\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 5.592\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 405.02\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 19258.308\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 6.777\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379721\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 85800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 11\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:28:46 (running for 00:04:05.47)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=7.202999999999994 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368b25d30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     11 |          211.146 | 85800 |    7.203 |                 29.1 |                -13.5 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:28:51 (running for 00:04:10.48)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=7.202999999999994 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368b25d30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     11 |          211.146 | 85800 |    7.203 |                 29.1 |                -13.5 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:28:56 (running for 00:04:15.49)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=7.202999999999994 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368b25d30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     11 |          211.146 | 85800 |    7.203 |                 29.1 |                -13.5 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 187200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-29-00\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 37.499999999999915\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 12.037999999999975\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -17.999999999999986\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 150\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 900\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.9818256497383118\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.017568103969097137\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.02485743910074234\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 15.730594635009766\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.2226598709821701\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 15.751938819885254\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.8797468543052673\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.01357285212725401\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.017291557043790817\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 2.1474294662475586\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.2691691219806671\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 2.1620066165924072\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 187200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 187200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 93600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 93600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 12\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 35.55185185185185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 47.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 12.000000000000009\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 17.586666666666666\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -5.548666666666655\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -8.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6195299530029297\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.38458638509114595\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 6.566298853556315\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.395380732218425\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 229.9449965953827\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 18.79938292503357\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 229.9449965953827\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 426.531\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18287.077\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 1417774.469\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 5.502\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 405.482\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 19236.371\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 6.738\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379740\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 93600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 12\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:29:02 (running for 00:04:21.36)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=12.037999999999975 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368adcb80>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     12 |          229.945 | 93600 |   12.038 |                 37.5 |                  -18 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:29:07 (running for 00:04:26.42)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=12.037999999999975 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368adcb80>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     12 |          229.945 | 93600 |   12.038 |                 37.5 |                  -18 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:29:12 (running for 00:04:31.43)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=12.037999999999975 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368adcb80>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     12 |          229.945 | 93600 |   12.038 |                 37.5 |                  -18 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:29:17 (running for 00:04:36.44)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=12.037999999999975 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368adcb80>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     12 |          229.945 | 93600 |   12.038 |                 37.5 |                  -18 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 202800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-29-20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 34.79999999999993\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 11.816999999999977\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -11.999999999999979\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 900\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.9300974011421204\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.016453851014375687\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.03121008351445198\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 30.792354583740234\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.014796935021877289\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 30.820274353027344\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.808943510055542\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.013253547251224518\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.0191953144967556\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 4.916837215423584\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.0745464563369751\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 4.933381080627441\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 202800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 202800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 101400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 101400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 13\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 39.017857142857146\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 39.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 12.000000000000009\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 17.395\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -5.577999999999988\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -3.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6211639404296876\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.3843962860107423\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 6.565148735046388\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.383883018493655\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 249.3457591533661\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 19.4007625579834\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 249.3457591533661\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 426.672\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18281.037\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 1357943.35\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 5.744\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 406.457\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 19190.209\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 6.682\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379760\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 101400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 13\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:29:23 (running for 00:04:41.80)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=11.816999999999977 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368adce50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     13 |          249.346 | 101400 |   11.817 |                 34.8 |                  -12 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:29:28 (running for 00:04:46.82)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=11.816999999999977 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368adce50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     13 |          249.346 | 101400 |   11.817 |                 34.8 |                  -12 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:29:33 (running for 00:04:51.82)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=11.816999999999977 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368adce50>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     13 |          249.346 | 101400 |   11.817 |                 34.8 |                  -12 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 218400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-29-37\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 31.499999999999915\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 17.83799999999995\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -9.000000000000025\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 150\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 1050\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.8869412541389465\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.016777392476797104\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.021161433309316635\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 27.317121505737305\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.39148521423339844\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 27.33492660522461\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.7680550217628479\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.013181091286242008\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.030184056609869003\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 4.505331993103027\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.1828087419271469\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 4.532879829406738\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 218400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 218400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 109200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 109200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 14\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 27.467999999999996\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.4\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 41.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 52.699999999999996\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 21.04\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -3.2019999999999884\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -35.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6208115862448248\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.38523187798673175\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 6.548791178078429\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.3903016315838705\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 267.13405179977417\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 17.78829264640808\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 267.13405179977417\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 429.138\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18175.954\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 1382737.582\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 5.641\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 407.144\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 19157.826\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 6.55\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379777\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 109200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 14\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:29:39 (running for 00:04:57.73)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=17.83799999999995 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369c753a0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     14 |          267.134 | 109200 |   17.838 |                 31.5 |                   -9 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:29:44 (running for 00:05:02.73)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=17.83799999999995 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369c753a0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     14 |          267.134 | 109200 |   17.838 |                 31.5 |                   -9 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:29:49 (running for 00:05:07.74)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=17.83799999999995 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369c753a0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     14 |          267.134 | 109200 |   17.838 |                 31.5 |                   -9 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:29:54 (running for 00:05:12.75)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=17.83799999999995 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369c753a0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     14 |          267.134 | 109200 |   17.838 |                 31.5 |                   -9 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 234000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-29-56\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 31.499999999999915\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 18.314999999999948\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -3.2999999999999794\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 1050\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.8837244510650635\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.01737598329782486\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.032258354127407074\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 32.259437561035156\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.01831134222447872\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 32.2882194519043\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.7463847398757935\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.012381501495838165\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.020321723073720932\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 6.877186298370361\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.044027335941791534\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 6.895031452178955\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 234000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 234000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 117000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 117000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 33.46666666666667\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 41.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 10.900000000000016\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 22.265\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -3.9499999999999886\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -1.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6224843209663047\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.3843351973754715\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 6.554650213819801\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.375173324093078\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 285.8171637058258\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 18.683111906051636\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 285.8171637058258\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 429.828\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18146.804\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 1410786.353\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 5.529\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 409.959\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 19026.316\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 6.552\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379796\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 117000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:29:59 (running for 00:05:18.40)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=18.314999999999948 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368ae95e0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     15 |          285.817 | 117000 |   18.315 |                 31.5 |                 -3.3 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:30:04 (running for 00:05:23.40)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=18.314999999999948 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368ae95e0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     15 |          285.817 | 117000 |   18.315 |                 31.5 |                 -3.3 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:30:09 (running for 00:05:28.42)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=18.314999999999948 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368ae95e0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     15 |          285.817 | 117000 |   18.315 |                 31.5 |                 -3.3 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:30:14 (running for 00:05:33.42)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=18.314999999999948 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368ae95e0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     15 |          285.817 | 117000 |   18.315 |                 31.5 |                 -3.3 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 249600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-30-15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 40.79999999999992\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 20.341999999999935\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -5.999999999999973\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 150\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 1200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.8673596978187561\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.018653573468327522\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.024872321635484695\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 29.003780364990234\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.31559160351753235\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 29.024921417236328\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.6934888362884521\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.013836504891514778\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.023446468636393547\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 4.235317230224609\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.14754225313663483\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 4.2559967041015625\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 249600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 249600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 124800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 124800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 16\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 35.86071428571429\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 43.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 39.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 21.886666666666667\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -1.5446666666666589\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -21.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6191501335031084\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.38303372954406373\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 6.4056215213746635\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.3582439331018055\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 305.0411307811737\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 19.2239670753479\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 305.0411307811737\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 431.068\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18094.592\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 1405519.352\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 5.55\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 410.648\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 18994.353\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 6.484\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379815\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 124800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 16\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:30:20 (running for 00:05:38.77)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=20.341999999999935 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369b944c0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     16 |          305.041 | 124800 |   20.342 |                 40.8 |                   -6 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:30:25 (running for 00:05:43.77)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=20.341999999999935 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369b944c0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     16 |          305.041 | 124800 |   20.342 |                 40.8 |                   -6 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:30:30 (running for 00:05:48.79)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.2/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=20.341999999999935 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4369b944c0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     16 |          305.041 | 124800 |   20.342 |                 40.8 |                   -6 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 265200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-30-34\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 40.79999999999992\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 21.71999999999993\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -5.999999999999973\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 1200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.83233243227005\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.01508010271936655\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.025293180719017982\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 25.627256393432617\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: -0.013681064359843731\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 25.6495361328125\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.6998674869537354\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.014383983798325062\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.032037097960710526\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 2.7635464668273926\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: -0.061789270490407944\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 2.7927067279815674\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 265200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 265200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 132600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 132600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 17\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 35.81481481481482\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 43.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 36.19999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 23.745\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -2.0249999999999924\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -12.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6201360835319236\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.3819917287288452\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 6.421479199971613\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.34307687651782\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 323.9289309978485\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 18.887800216674805\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 323.9289309978485\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 431.055\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18095.148\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 1401227.148\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 5.567\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 411.875\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 18937.789\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 6.363\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379834\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 132600\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 17\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:30:35 (running for 00:05:54.63)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=21.71999999999993 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368ad59d0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     17 |          323.929 | 132600 |    21.72 |                 40.8 |                   -6 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:30:40 (running for 00:05:59.65)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=21.71999999999993 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368ad59d0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     17 |          323.929 | 132600 |    21.72 |                 40.8 |                   -6 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:30:45 (running for 00:06:04.65)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=21.71999999999993 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368ad59d0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     17 |          323.929 | 132600 |    21.72 |                 40.8 |                   -6 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:30:50 (running for 00:06:09.67)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=21.71999999999993 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368ad59d0>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     17 |          323.929 | 132600 |    21.72 |                 40.8 |                   -6 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 280800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-30-53\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 36.2999999999999\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 18.68999999999994\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -11.999999999999982\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 150\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 1350\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.786988377571106\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.01656203716993332\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.017600398510694504\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 32.3912239074707\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.3021905720233917\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 32.405513763427734\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.6534068584442139\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.012366734445095062\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.02205418050289154\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 2.211688280105591\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.25379469990730286\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 2.231269121170044\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 280800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 280800\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 140400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 140400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 18\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 35.2074074074074\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 43.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 15.299999999999976\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 24.07\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -5.379999999999989\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -3.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6212044508590344\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.3820818626596511\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 6.3950249072454515\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.358739799979911\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 342.8647744655609\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 18.935843467712402\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 342.8647744655609\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 432.328\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18041.838\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 1372781.148\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 5.682\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 411.876\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 18937.757\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 6.39\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379853\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 140400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 18\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:30:56 (running for 00:06:15.64)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=18.68999999999994 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368ae9790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     18 |          342.865 | 140400 |    18.69 |                 36.3 |                  -12 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:31:02 (running for 00:06:20.73)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=18.68999999999994 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368ae9790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     18 |          342.865 | 140400 |    18.69 |                 36.3 |                  -12 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:31:07 (running for 00:06:25.73)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=18.68999999999994 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368ae9790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     18 |          342.865 | 140400 |    18.69 |                 36.3 |                  -12 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:31:12 (running for 00:06:30.75)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=18.68999999999994 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368ae9790>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     18 |          342.865 | 140400 |    18.69 |                 36.3 |                  -12 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 296400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-31-12\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 36.2999999999999\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 19.100999999999942\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -11.999999999999982\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 1350\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.742710530757904\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.013212763704359531\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.02212326042354107\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 32.14974594116211\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.03039086051285267\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 32.169227600097656\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.6061703562736511\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.011865952983498573\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.021910836920142174\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 8.572741508483887\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: -0.19280065596103668\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 8.592279434204102\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 296400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 296400\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 148200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 148200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 19\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 36.51111111111111\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 43.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 12.00000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 24.14\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -5.03899999999999\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -2.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6216930605176927\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.3805742955894837\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 6.392881659077226\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.341562746428631\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 361.85974168777466\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 18.994967222213745\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 361.85974168777466\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 431.552\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18074.294\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 1376078.201\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 5.668\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 413.126\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 18880.445\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 6.459\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379872\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 148200\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 19\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:31:17 (running for 00:06:36.70)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=19.100999999999942 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f436db7ed30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     19 |           361.86 | 148200 |   19.101 |                 36.3 |                  -12 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:31:22 (running for 00:06:41.72)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=19.100999999999942 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f436db7ed30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     19 |           361.86 | 148200 |   19.101 |                 36.3 |                  -12 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:31:28 (running for 00:06:46.73)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 16.0/16 CPUs, 1.0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=19.100999999999942 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f436db7ed30>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status   | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | RUNNING  | 172.31.43.185:58807 |     19 |           361.86 | 148200 |   19.101 |                 36.3 |                  -12 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+----------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result for PPO_MultiAgentArena_9155f_00000:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   agent_timesteps_total: 312000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   date: 2021-12-01_09-31-32\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   done: true\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_len_mean: 100.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_max: 41.0999999999999\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_mean: 22.011999999999933\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episode_reward_min: -12.000000000000012\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_this_iter: 150\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   episodes_total: 1500\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   experiment_id: 9b88d1fc0f0740009943392898e1ec20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   hostname: ip-172-31-43-185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy1:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.6885514259338379\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.015364629216492176\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.02216554805636406\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 36.78342056274414\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.32827505469322205\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 36.802513122558594\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m       policy2:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_kl_coeff: 0.20000000298023224\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           cur_lr: 4.999999873689376e-05\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy: 0.5919711589813232\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           kl: 0.012098466977477074\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           model: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           policy_loss: -0.03333766758441925\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           total_loss: 6.231944561004639\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_explained_var: 0.022511431947350502\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m           vf_loss: 6.2628631591796875\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m         train: null\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_sampled: 312000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_agent_steps_trained: 312000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_sampled: 156000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained: 156000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     num_steps_trained_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   iterations_since_restore: 20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   node_ip: 172.31.43.185\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   num_healthy_workers: 15\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     cpu_util_percent: 38.67407407407407\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     ram_util_percent: 9.400000000000002\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   pid: 58807\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_max:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 47.5\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: 68.10000000000007\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_mean:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: 23.85\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -1.8379999999999916\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   policy_reward_min:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy1: -57.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     policy2: -9.99999999999998\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_action_processing_ms: 0.6194094683243759\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_env_wait_ms: 0.3812628375617427\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_inference_ms: 6.362390945708182\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     mean_raw_obs_processing_ms: 5.361323458927777\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_since_restore: 381.09946274757385\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_this_iter_s: 19.239721059799194\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   time_total_s: 381.09946274757385\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_throughput: 429.732\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     learn_time_ms: 18150.831\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_throughput: 1401323.179\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     load_time_ms: 5.566\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_throughput: 412.265\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     sample_time_ms: 18919.853\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m     update_time_ms: 6.306\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timestamp: 1638379892\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   timesteps_total: 156000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   training_iteration: 20\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   trial_id: 9155f_00000\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current time: 2021-12-01 09:31:32 (running for 00:06:51.08)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Memory usage on this node: 11.3/119.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/74.36 GiB heap, 0.0/35.86 GiB objects (0.0/1.0 accelerator_type:M60)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Current best trial: 9155f_00000 with episode_reward_mean=22.011999999999933 and parameters={'num_workers': 15, 'num_envs_per_worker': 10, 'create_env_on_driver': False, 'rollout_fragment_length': 26, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={}), 'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': <function <lambda> at 0x7f4368ad5b80>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Result logdir: /home/ray/ray_results/CUJ-RL\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m Number of trials: 1/1 (1 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+------------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | Trial name                      | status     | loc                 |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m |---------------------------------+------------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m | PPO_MultiAgentArena_9155f_00000 | TERMINATED | 172.31.43.185:58807 |     20 |          381.099 | 156000 |   22.012 |                 41.1 |                  -12 |                100 |\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m +---------------------------------+------------+---------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=None)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m [2021-12-01 09:31:32,370 E 58909 59316] raylet_client.cc:159: IOError: Broken pipe [RayletClient] Failed to disconnect from raylet.\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m 2021-12-01 09:31:32,371\tERROR worker.py:425 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"python/ray/_raylet.pyx\", line 692, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"python/ray/_raylet.pyx\", line 521, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"python/ray/_raylet.pyx\", line 558, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"python/ray/_raylet.pyx\", line 565, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"python/ray/_raylet.pyx\", line 569, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"python/ray/_raylet.pyx\", line 519, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"/tmp/ray/session_2021-12-01_04-27-46_331452_161/runtime_resources/conda/ray-1fdfbc69e9e1c069ec07b30674483e16d97bdd20/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 576, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"/tmp/ray/session_2021-12-01_04-27-46_331452_161/runtime_resources/conda/ray-1fdfbc69e9e1c069ec07b30674483e16d97bdd20/lib/python3.8/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"/tmp/ray/session_2021-12-01_04-27-46_331452_161/runtime_resources/conda/ray-1fdfbc69e9e1c069ec07b30674483e16d97bdd20/lib/python3.8/site-packages/ray/actor.py\", line 1047, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"/tmp/ray/session_2021-12-01_04-27-46_331452_161/runtime_resources/conda/ray-1fdfbc69e9e1c069ec07b30674483e16d97bdd20/lib/python3.8/site-packages/ray/actor.py\", line 1109, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     ray.worker.disconnect()\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"/tmp/ray/session_2021-12-01_04-27-46_331452_161/runtime_resources/conda/ray-1fdfbc69e9e1c069ec07b30674483e16d97bdd20/lib/python3.8/site-packages/ray/worker.py\", line 1493, in disconnect\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     worker.import_thread.join_import_thread()\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"/tmp/ray/session_2021-12-01_04-27-46_331452_161/runtime_resources/conda/ray-1fdfbc69e9e1c069ec07b30674483e16d97bdd20/lib/python3.8/site-packages/ray/_private/import_thread.py\", line 50, in join_import_thread\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     self.t.join()\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"/tmp/ray/session_2021-12-01_04-27-46_331452_161/runtime_resources/conda/ray-1fdfbc69e9e1c069ec07b30674483e16d97bdd20/lib/python3.8/threading.py\", line 1011, in join\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     self._wait_for_tstate_lock()\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"/tmp/ray/session_2021-12-01_04-27-46_331452_161/runtime_resources/conda/ray-1fdfbc69e9e1c069ec07b30674483e16d97bdd20/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     elif lock.acquire(block, timeout):\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   File \"/tmp/ray/session_2021-12-01_04-27-46_331452_161/runtime_resources/conda/ray-1fdfbc69e9e1c069ec07b30674483e16d97bdd20/lib/python3.8/site-packages/ray/worker.py\", line 422, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m 2021-12-01 09:31:32,483\tINFO tune.py:630 -- Total run time: 411.34 seconds (411.02 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint:  /home/ray/ray_results/CUJ-RL/PPO_MultiAgentArena_9155f_00000_0_2021-12-01_09-24-41/checkpoint_000020/checkpoint-20\n"
     ]
    }
   ],
   "source": [
    "results = tune.run(\n",
    "    # RLlib Trainer class (we use the \"PPO\" algorithm today).\n",
    "    PPOTrainer,\n",
    "    # Give our experiment a name (we will find results/checkpoints\n",
    "    # under this name on the server's `~ray_results/` dir).\n",
    "    name=f\"CUJ-RL\",\n",
    "    # The RLlib config (defined in a cell above).\n",
    "    config=TRAINER_CFG,\n",
    "    # Take a snapshot every 2 iterations.\n",
    "    checkpoint_freq=2,\n",
    "    # Plus one at the very end of training.\n",
    "    checkpoint_at_end=True,\n",
    "    # Run for exactly 30 training iterations.\n",
    "    stop={\"training_iteration\": 20},\n",
    "    # Define what we are comparing for, when we search for the\n",
    "    # \"best\" checkpoint at the end.\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\")\n",
    "\n",
    "print(\"Best checkpoint: \", results.best_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf894c0",
   "metadata": {},
   "source": [
    "### Restoring from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf5db17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes, checkpoint files are on local machine ('Downloads' folder)\n"
     ]
    }
   ],
   "source": [
    "local_checkpoint = \"/Users/sven/Downloads/checkpoint-20-2\"\n",
    "\n",
    "if os.path.isfile(local_checkpoint):\n",
    "    print(\"yes, checkpoint files are on local machine ('Downloads' folder)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a7f5a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 18:34:51,615\tWARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "2021-12-01 18:34:54,804\tWARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
      "Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# We'll restore the trained PPOTrainer locally on this laptop here and have it run\n",
    "# through a new environment to demonstrate it has learnt useful policies for our agents:\n",
    "\n",
    "cpu_config = TRAINER_CFG.copy()\n",
    "cpu_config[\"num_gpus\"] = 0\n",
    "cpu_config[\"num_workers\"] = 0\n",
    "\n",
    "new_trainer = PPOTrainer(config=cpu_config)\n",
    "# Restore weights of the learnt policies via `restore()`.\n",
    "new_trainer.restore(local_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20548c9",
   "metadata": {},
   "source": [
    "### Running inference locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff7e0e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84186b74b5964851ba8ffa0e03f75874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = MultiAgentArena(config={\"render\": True})\n",
    "\n",
    "with env.out:\n",
    "\n",
    "    obs = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    while True:\n",
    "        a1 = new_trainer.compute_single_action(obs[\"agent1\"], policy_id=\"policy1\", explore=True)\n",
    "        a2 = new_trainer.compute_single_action(obs[\"agent2\"], policy_id=\"policy2\", explore=False)\n",
    "\n",
    "        obs, rewards, dones, _ = env.step({\"agent1\": a1, \"agent2\": a2})\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        if dones[\"agent1\"] is True:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d032e2",
   "metadata": {},
   "source": [
    "### Inference using Ray Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(route_prefix=\"/multi-agent-arena\")\n",
    "class ServeRLlibTrainer:\n",
    "\n",
    "    def __init__(self, config, checkpoint_path):\n",
    "        # Link to our trainer.\n",
    "        self.trainer = PPOTrainer(cpu_config)\n",
    "        self.trainer.restore(checkpoint_path)\n",
    "\n",
    "    async def __call__(self, request: Request):\n",
    "        json_input = await request.json()\n",
    "\n",
    "        # Compute and return the action for the given observation.\n",
    "        obs1 = json_input[\"observation_agent1\"]\n",
    "        obs2 = json_input[\"observation_agent2\"]\n",
    "        a1 = self.trainer.compute_single_action(obs1, policy_id=\"policy1\")\n",
    "        a2 = self.trainer.compute_single_action(obs2, policy_id=\"policy2\")\n",
    "\n",
    "        return {\"action\": {\"agent1\": int(a1), \"agent2\": int(a2)}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1cfbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = serve.start()\n",
    "ServeRLlibTrainer.deploy(cpu_config, results.best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1094f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
