{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa06051",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Recommender Systems\n",
    "## From Contextual Bandits to Slate-Q\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"images/youtube.png\" style=\"width: 230px;\"/> </td>\n",
    "    <td> <img src=\"images/dota2.jpg\" style=\"width: 213px;\"/> </td>\n",
    "    <td> <img src=\"images/forklifts.jpg\" style=\"width: 169px;\"/> </td>\n",
    "    <td> <img src=\"images/spotify.jpg\" style=\"width: 254px;\"/> </td>\n",
    "    <td> <img src=\"images/robots.jpg\" style=\"width: 252px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Overview\n",
    "“Reinforcement Learning for Recommender Systems, From Contextual Bandits to Slate-Q” is a tutorial for industry researchers, domain-experts, and ML-engineers, showcasing ...\n",
    "\n",
    "1) .. how you can use RLlib to build a recommender system **simulator** for your industry applications and run Bandit algorithms and the Slate-Q algorithm against this simulator.\n",
    "\n",
    "2) .. how RLlib's offline algorithms pose solutions in case you **don't have a simulator** of your problem environment at hand.\n",
    "\n",
    "We will further explore how to deploy trained models to production using Ray Serve.\n",
    "\n",
    "During the live-coding phases, we will using a recommender system simulating environment by google's RecSim and configure and run 2 RLlib algorithms against it. We'll also demonstrate how you may use offline RL as a solution for recommender systems and how to deploy a learned policy into production.\n",
    "\n",
    "RLlib offers industry-grade scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL?) before proceeding to RLlib (recommender system) environments, neural network models, offline RL, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "### Intended Audience\n",
    "* Python programmers who are interested in using RL to solve their specific industry decision making problems and who want to get started with RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "To get this very notebook up and running on your local machine, you can follow these steps here:\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Linux):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install cmake \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "$ pip install grpcio # <- extra install only on apple M1 mac\n",
    "$ # **Note:** In case you are getting a \"requires TensorFlow version >= 2.8\" error at some point in the notebook, try the following:\n",
    "$ pip uninstall -y tensorflow\n",
    "$ python -m pip install tensorflow-macos --no-cache-dir\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "$ pip install pywin32 # <- extra install only on Win10.\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials/production_rl_2022\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and RLlib?\n",
    "* How do recommender systems work? How do we build our own?\n",
    "* How do we train RLlib's different algorithms on a recommender system problem?\n",
    "* What's offline RL and how can I use it with RLlib?\n",
    "* How do I deploy an already trained policy into production using Ray Serve.\n",
    "\n",
    "\n",
    "### Tutorial Outline\n",
    "\n",
    "1. Reinforcement learning (RL) in a nutshell.\n",
    "1. How to formulate any problem as an RL-solvable one?\n",
    "1. Recommender systems - How they work.\n",
    "1. Why you should use RLlib.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. [Google RecSim - Build your own recom sys simulator.](#recsim)\n",
    "1. [Dissecting the \"long term satisfaction\" (LTE) environment.](#dissecting_lte)\n",
    "1. [Using a contextual Bandit algorithm with RLlib and starting our first training run on the LTE env.](#rllib)\n",
    "1. [What did the Bandit learn?](#bandit_results)\n",
    "1. [Intro to Slate-Q.](#slateq)\n",
    "1. [Starting a Slate-Q training run.](#slateq_experiment)\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. [Analyzing the results of the SlateQ run.](#slateq_results)\n",
    "1. [Intro to Offline RL - What if we don't have an environment?](#offline_rl)\n",
    "1. [BC (behavior cloning) and MARWIL: Quick how-to and setup instructions.](#bc_and_marwil)\n",
    "1. [Off policy evaluation (OPE) as a means to estimate how well an offline-RL trained policy will perform in production.](#ope)\n",
    "1. [Ray Serve example: How can we deploy a trained policy into our production environment?](#ray_serve)\n",
    "\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n",
    "\n",
    "<img src=\"images/unity3d_blog_post.png\" width=400>\n",
    "\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7e21f-f3de-4bad-a3a7-4bbd0b015559",
   "metadata": {},
   "source": [
    "# Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "930deb27-e739-4507-bc24-e39ded9caeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Let's get started with some basic imports.\n",
    "\n",
    "import ray  # .. of course\n",
    "from ray import serve\n",
    "from ray import tune\n",
    "\n",
    "from collections import OrderedDict\n",
    "import gym  # RL environments and action/observation spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "from pprint import pprint\n",
    "import re\n",
    "import recsim  # google's RecSim package.\n",
    "import requests\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "from scipy.stats import linregress, sem\n",
    "from starlette.requests import Request\n",
    "import tree  # dm_tree\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f010a6-7ee3-49b3-814a-2b9455c66c8f",
   "metadata": {},
   "source": [
    "<a id='recsim'></a>\n",
    "## Introducing google RecSim\n",
    "\n",
    "<img src=\"images/recsim_documentation.png\" width=600 style=\"float:right;\">\n",
    "\n",
    "<a href=\"https://github.com/google-research/recsim\">Google's RecSim package</a> offers a flexible way for you to <a href=\"https://github.com/google-research/recsim/blob/master/recsim/colab/RecSim_Developing_an_Environment.ipynb\">define the different building blocks of a recommender system</a>:\n",
    "\n",
    "\n",
    "- User model (how do users change their preferences when having been faced with, selected, and consumed certain items?).\n",
    "- Document model: Features of documents and how do documents get pre-selected/sampled.\n",
    "- Reward functions.\n",
    "\n",
    "RLlib comes with 3 off-the-shelf RecSim environments that are ready for training (with RLlib):\n",
    "* Long Term Satisfaction (<- the \"env\" we will use in this tutorial)\n",
    "* Interest Evolution\n",
    "* Interest Exploration\n",
    "\n",
    "<a id='dissecting_lte'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3363126-0f38-4f92-a031-1ea791b9a747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space = Discrete(10)\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in RecSim exapmle environment: \"Long Term Satisfaction\", ready to be trained by RLlib.\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n",
    "\n",
    "# Create a RecSim instance using the following config parameters (very similar to what we used above in our own recommender system env):\n",
    "lts_10_1_env = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 10,  # Discrete(10) -> int 0-9\n",
    "    \"slate_size\": 1,\n",
    "    # Set to False for re-using the same candidate doecuments each timestep.\n",
    "    \"resample_documents\": False,\n",
    "    # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "    # e.g. slate_size=2 and num_candidates=10 -> MultiDiscrete([10, 10]) -> Discrete(100)  # 10x10\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "})\n",
    "\n",
    "# What are our spaces?\n",
    "#print(f\"observation space = {lts_10_1_env.observation_space}\")\n",
    "print(f\"action space = {lts_10_1_env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eac27a-bc9a-47dd-b3f9-cffd3e590e3b",
   "metadata": {},
   "source": [
    "Let's make use of our knowledge on the gym.Env API and call our new environment's `reset()` and `step()` methods.\n",
    "First: `reset()` to receive the initial observation in a new episode/trajectory/session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "913d34a6-fac6-4436-b1d5-9292ebf88006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('user', array([], dtype=float32)),\n",
      "             ('doc',\n",
      "              {'0': array([0.5488135], dtype=float32),\n",
      "               '1': array([0.71518934], dtype=float32),\n",
      "               '2': array([0.60276335], dtype=float32),\n",
      "               '3': array([0.5448832], dtype=float32),\n",
      "               '4': array([0.4236548], dtype=float32),\n",
      "               '5': array([0.6458941], dtype=float32),\n",
      "               '6': array([0.4375872], dtype=float32),\n",
      "               '7': array([0.891773], dtype=float32),\n",
      "               '8': array([0.96366274], dtype=float32),\n",
      "               '9': array([0.3834415], dtype=float32)}),\n",
      "             ('response',\n",
      "              (OrderedDict([('click', 1),\n",
      "                            ('engagement',\n",
      "                             array(43.921787, dtype=float32))]),))])\n"
     ]
    }
   ],
   "source": [
    "# Start a new episode and look at initial observation.\n",
    "obs = lts_10_1_env.reset()\n",
    "pprint(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9857197-f79f-4f9d-8e3f-68eee20daf94",
   "metadata": {},
   "source": [
    "Now let's play RL agent ourselves and recommend some items (pick some actions) via the environment's `step()` method:\n",
    "\n",
    "**Task:** Execute the following cell a couple of times chosing different actions (from 0 - 9) to be sent into the environment's `step()` method. Each time, look at the returned next observation, reward, and `done` flag and write down what you find interesting about the dynamics and observations of this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f10b83e-993f-4c59-8e13-4e1074bfd7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('user', array([], dtype=float32)),\n",
      "             ('doc',\n",
      "              {'0': array([0.5488135], dtype=float32),\n",
      "               '1': array([0.71518934], dtype=float32),\n",
      "               '2': array([0.60276335], dtype=float32),\n",
      "               '3': array([0.5448832], dtype=float32),\n",
      "               '4': array([0.4236548], dtype=float32),\n",
      "               '5': array([0.6458941], dtype=float32),\n",
      "               '6': array([0.4375872], dtype=float32),\n",
      "               '7': array([0.891773], dtype=float32),\n",
      "               '8': array([0.96366274], dtype=float32),\n",
      "               '9': array([0.3834415], dtype=float32)}),\n",
      "             ('response',\n",
      "              ({'click': 1, 'engagement': array(6.931141, dtype=float32)},))])\n",
      "reward = 6.93; done = False\n"
     ]
    }
   ],
   "source": [
    "# Let's send our first action (1-slate back into the env) using the env's `step()` method.\n",
    "action = 0  # Discrete(10): 0-9 are all valid actions\n",
    "\n",
    "# This method returns 4 items:\n",
    "# - next observation (after having applied the action)\n",
    "# - reward (after having applied the action)\n",
    "# - `done` flag; if True, the episode is terminated and the environment needs to be `reset()` again.\n",
    "# - info dict (we'll ignore this)\n",
    "next_obs, reward, done, _ = lts_10_1_env.step(action)\n",
    "\n",
    "# Print out the next observation.\n",
    "# We expect the \"doc\" and \"user\" items to be the same as in the previous observation\n",
    "# b/c we set \"resample_documents\" to False.\n",
    "pprint(next_obs)\n",
    "# Print out rewards and the vlaue of the `done` flag.\n",
    "print(f\"reward = {reward:.2f}; done = {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c7982-b372-4fe9-806a-18a29101c094",
   "metadata": {},
   "source": [
    "<br />.<br />.<br />.<br />.<br />.\n",
    "<br />.<br />.<br />.<br />.<br />.\n",
    "<br />.<br />.<br />.<br />.<br />.\n",
    "<br />.<br />.<br />.<br />.<br />.\n",
    "<br />..<br />.<br />.<br />.<br />.\n",
    "\n",
    "\n",
    "### What have we learnt from experimenting with the environment?\n",
    "\n",
    "* User's state (if any) is hidden to agent (not part of observation).\n",
    "* Episodes seem to last at least n timesteps -> user seems to have some time budget to spend.\n",
    "* User always seems to click, no matter what we recommend.\n",
    "* Reward seems to be always identical to the \"engagement\" value (of the clicked item). These values range somewhere between 0.0 and 20.0+.\n",
    "* Weak suspicion: If we always recommend the item with the highest feature value, rewards seem to taper off over time - in most of the episodes.\n",
    "* Weak suspicion: If we always recommend the item with the lowest feature value, rewards seem to increase over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05210c-69ea-4c09-acf8-831fffca5f8c",
   "metadata": {},
   "source": [
    "### What the environment actually does under the hood\n",
    "\n",
    "Let's take a quick look at a pre-configured RecSim environment: \"Long Term Satisfaction\".\n",
    "\n",
    "<img src=\"images/long_term_satisfaction_env.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958ff84-f7d0-45c9-aa43-b807243b8452",
   "metadata": {},
   "source": [
    "Now that we know, that there is a double objective built into the env (a. sweetness -> engagement; b. sweetness -> unhappyness; unhappyness -> low engagement), let's make this effect a tiny bit stronger by slightly modifying the environment. As said above, the effect is very weak and almost not measurable, which is a problem on the env's side. We can use this following `gym.ObservationWrapper` class in the cell below to \"fix\" that problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "375a469e-aa46-4038-9df7-02cabccdad50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok; registered the string 'modified_lts' to be used in RLlib configs (see below)\n"
     ]
    }
   ],
   "source": [
    "# Modifying wrapper around the LTS (Long Term Satisfaction) env, allowing us to tweak the user model\n",
    "# (and this, reward behavior):\n",
    "\n",
    "class LTSWithStrongerDissatisfactionEffect(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Tweak incoming environment.\n",
    "        env.environment._user_model._user_sampler._state_parameters.update({\n",
    "            \"sensitivity\": 0.058,\n",
    "            \"time_budget\": 120,\n",
    "            \"choc_stddev\": 0.1,\n",
    "            \"kale_stddev\": 0.1,\n",
    "            #\"innovation_stddev\": 0.01,\n",
    "            #\"choc_mean\": 1.25,\n",
    "            #\"kale_mean\": 1.0,\n",
    "            #\"memory_discount\": 0.9,\n",
    "        })\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Adjust observation space.\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            self.observation_space.spaces[\"user\"] = gym.spaces.Box(0.0, 1.0, (1, ), dtype=np.float32)\n",
    "            for r in self.observation_space[\"response\"]:\n",
    "                if \"engagement\" in r.spaces:\n",
    "                    r.spaces[\"watch_time\"] = r.spaces[\"engagement\"]\n",
    "                    del r.spaces[\"engagement\"]\n",
    "                    break\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            observation[\"user\"] = np.array([self.env.environment._user_model._user_state.satisfaction])\n",
    "            for r in observation[\"response\"]:\n",
    "                if \"engagement\" in r:\n",
    "                    r[\"watch_time\"] = r[\"engagement\"]\n",
    "                    del r[\"engagement\"]\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Add the wrapping around \n",
    "tune.register_env(\"modified_lts\", lambda env_config: LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(env_config)))\n",
    "\n",
    "print(\"ok; registered the string 'modified_lts' to be used in RLlib configs (see below)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce752b-68a3-4a84-aada-97810039e4e8",
   "metadata": {},
   "source": [
    "Now that we have a stronger effect of the user's satisfaction value on the long-term rewards, we may be able to measure this effect reliably\n",
    "using the following utility code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b1f1ab8-6b08-47c7-9dfa-3fe9bd672c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005072077866955191\n"
     ]
    }
   ],
   "source": [
    "# This cell should help you with your own analysis of the two above \"suspicions\":\n",
    "# Always chosing the highest/lowest-valued action will lead to a decrease/increase in rewards over the course of an episode.\n",
    "modified_lts_10_1_env = LTSWithStrongerDissatisfactionEffect(lts_10_1_env)\n",
    "\n",
    "# Capture slopes of all trendlines over all episodes.\n",
    "slopes = []\n",
    "# Run 1000 episodes.\n",
    "for _ in range(1000):\n",
    "    obs = modified_lts_10_1_env.reset()  # Reset environment to get initial observation:\n",
    "\n",
    "    # Compute actions that pick doc with highest/lowest feature value.\n",
    "    action_sweetest = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    action_kaleiest = np.argmin([value for _, value in obs[\"doc\"].items()])\n",
    "\n",
    "    # Play one episode.\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        #action = action_sweetest\n",
    "        action = action_kaleiest\n",
    "        #action = np.random.choice([action_kaleiest, action_sweetest])\n",
    "\n",
    "        obs, reward, done, _ = modified_lts_10_1_env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Create linear model of rewards over time.\n",
    "    reward_linreg = linregress(np.array((range(len(rewards)))), np.array(rewards))\n",
    "    slopes.append(reward_linreg.slope)\n",
    "\n",
    "print(np.mean(slopes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be848212-87b8-4eb1-9353-74e09ae72310",
   "metadata": {},
   "source": [
    "## Measuring random baseline of our environment\n",
    "\n",
    "In the cells above, we created a new environment instance (`lts_10_1_env`). As we have seen above, in order to start \"walking\" through a recommender system episode, we need to perform `reset()` and then several `step()` calls (with different actions) until the returned `done` flag is True.\n",
    "\n",
    "Let's find out how well a randomly acting agent performs in this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that measures and outputs the random baseline reward.\n",
    "# This is the expected accumulated reward per episode, if we act randomly (recommend random items) at each time step.\n",
    "def measure_random_performance_for_env(env, episodes=1000, verbose=False):\n",
    "\n",
    "    # Reset the env.\n",
    "    env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Enter while loop (to step through the episode).\n",
    "    while num_episodes < episodes:\n",
    "        # Produce a random action.\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Check, whether the episde is done, if yes, reset and increase episode counter.\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            elif num_episodes % 100 == 0:\n",
    "                print(f\" {num_episodes} \", end=\"\")\n",
    "            elif num_episodes % 10 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "            num_episodes += 1\n",
    "            env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # Print out and return mean episode reward (and standard error of the mean).\n",
    "    env_mean_random_reward = np.mean(episode_rewards)\n",
    "\n",
    "    print(f\"\\n\\nMean episode reward when acting randomly: {env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
    "\n",
    "    return env_mean_random_reward, sem(episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e6e63e6-d030-4a45-af5b-ab88eaef3969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 ......... 100 ......... 200 ......... 300 ......... 400 ......... 500 ......... 600 ......... 700 ......... 800 ......... 900 .........\n",
      "\n",
      "Mean episode reward when acting randomly: 1157.82+/-0.36\n"
     ]
    }
   ],
   "source": [
    "# Let's create a somewhat tougher version of this with 20 candidates (instead of 10) and a slate-size of 2.\n",
    "# We'll also keep using our wrapper from above to strengthen the dissatisfaction effect on the engagement:\n",
    "lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config={\n",
    "    \"num_candidates\": 20,\n",
    "    \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "    \"resample_documents\": True,\n",
    "    # Convert to Discrete action space.\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "    # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "    \"wrap_for_bandits\": True,\n",
    "}))\n",
    "\n",
    "lts_20_2_env_mean_random_reward, _ = measure_random_performance_for_env(lts_20_2_env, episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
   "metadata": {},
   "source": [
    "# Plugging in RLlib\n",
    "<a id='rllib'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
   "metadata": {},
   "source": [
    "## Picking an RLlib algorithm (\"Trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_algorithms_bandits.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1b0b3-ec96-41c0-9d5b-93db1c5ce021",
   "metadata": {},
   "source": [
    "### Trying a \"Contextual n-armed Bandit\" on our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a26e094-9887-4fc6-88b6-d1448e931526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use one of the above algorithms, you may instantiate its associated Trainer class.\n",
    "# For example, to import a Bandit Trainer w/ Upper Confidence Bound (UCB) exploration, do:\n",
    "\n",
    "from ray.rllib.agents.bandit import BanditLinUCBTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0911f212-523e-4a75-846d-342dd2a681a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandit's default config is:\n",
      "{'_disable_action_flattening': False,\n",
      " '_disable_execution_plan_api': False,\n",
      " '_disable_preprocessor_api': False,\n",
      " '_fake_gpus': False,\n",
      " '_tf_policy_handles_more_than_one_loss': False,\n",
      " 'action_space': None,\n",
      " 'actions_in_input_normalized': False,\n",
      " 'always_attach_evaluation_results': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': False,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': -1,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'disable_env_checking': False,\n",
      " 'eager_max_retraces': 20,\n",
      " 'eager_tracing': False,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'env_task_fn': None,\n",
      " 'evaluation_config': {},\n",
      " 'evaluation_duration': 10,\n",
      " 'evaluation_duration_unit': 'episodes',\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': -1,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'evaluation_parallel_to_training': False,\n",
      " 'exploration_config': {'type': 'StochasticSampling'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'torch',\n",
      " 'gamma': 0.99,\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_config': {},\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'keep_per_episode_custom_metrics': False,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'lr': 0.0001,\n",
      " 'metrics_episode_collection_timeout_s': 180,\n",
      " 'metrics_num_episodes_for_smoothing': 100,\n",
      " 'metrics_smoothing_episodes': -1,\n",
      " 'min_iter_time_s': -1,\n",
      " 'min_sample_timesteps_per_reporting': None,\n",
      " 'min_time_s_per_reporting': None,\n",
      " 'min_train_timesteps_per_reporting': None,\n",
      " 'model': {'_disable_action_flattening': False,\n",
      "           '_disable_preprocessor_api': False,\n",
      "           '_time_major': False,\n",
      "           '_use_default_native_models': False,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': True,\n",
      "           'zero_mean': True},\n",
      " 'monitor': -1,\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_map_cache': None,\n",
      "                'policy_map_capacity': 100,\n",
      "                'policy_mapping_fn': None,\n",
      "                'replay_mode': 'independent'},\n",
      " 'no_done_at_end': False,\n",
      " 'normalize_actions': True,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_workers': 0,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'observation_space': None,\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_config': {},\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'record_env': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'rollout_fragment_length': 1,\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'seed': None,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'simple_optimizer': -1,\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 100,\n",
      " 'train_batch_size': 1}\n"
     ]
    }
   ],
   "source": [
    "# Configuration dicts for RLlib Trainers.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# E.g. Bandit algorithms:\n",
    "from ray.rllib.agents.bandit.bandit import DEFAULT_CONFIG as BANDIT_DEFAULT_CONFIG\n",
    "print(f\"Bandit's default config is:\")\n",
    "pprint(BANDIT_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9bd9775-f2bb-41d9-8ff6-20be9abd68db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 10:37:22,606\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-03-29 10:37:22,607\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "WARNING:ray.tune.utils.util:Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BanditLinUCBTrainer"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_config = {\n",
    "    \"env\": \"modified_lts\",\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 20,  # 20x19 = ~400 unique slates (arms)\n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "\n",
    "        # Bandit-specific flags:\n",
    "        \"convert_to_discrete_action_space\": True,\n",
    "        # Convert \"doc\" key into \"item\" key.\n",
    "        \"wrap_for_bandits\": True,\n",
    "        # Use consistent seeds for the environment ...\n",
    "        \"seed\": 0,\n",
    "    },\n",
    "    # ... and the Trainer itself.\n",
    "    \"seed\": 0,\n",
    "\n",
    "    # The following settings are affecting the reporting only:\n",
    "    # ---\n",
    "    # Generate a result dict every single time step.\n",
    "    \"timesteps_per_iteration\": 1,\n",
    "    # Report rewards as smoothed mean over this many episodes.\n",
    "    \"metrics_num_episodes_for_smoothing\": 200,\n",
    "}\n",
    "\n",
    "# Create the RLlib Trainer using above config.\n",
    "bandit_trainer = BanditLinUCBTrainer(config=bandit_config)\n",
    "bandit_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a22cc0-0efb-40be-85fe-720e62a7a419",
   "metadata": {},
   "source": [
    "#### Running a single training iteration, by calling the `.train()` method:\n",
    "\n",
    "One iteration for most algos involves:\n",
    "\n",
    "1. Sampling from the environment(s)\n",
    "1. Using the sampled data (observations, actions taken, rewards) to update the policy model (e.g. a neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddd18251-2a1a-4822-8744-ca6df4a14787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_timesteps_total': 1,\n",
      " 'custom_metrics': {},\n",
      " 'date': '2022-03-29_10-37-22',\n",
      " 'done': False,\n",
      " 'episode_len_mean': nan,\n",
      " 'episode_media': {},\n",
      " 'episode_reward_max': nan,\n",
      " 'episode_reward_mean': nan,\n",
      " 'episode_reward_min': nan,\n",
      " 'episodes_this_iter': 0,\n",
      " 'episodes_total': 0,\n",
      " 'experiment_id': 'b967f30529bf4b68aac1a9354e3c8196',\n",
      " 'hist_stats': {'episode_lengths': [], 'episode_reward': []},\n",
      " 'hostname': 'Svens-MacBook-Pro.local',\n",
      " 'info': {'learner': {'default_policy': {'learner_stats': {'update_latency': 0.0011608600616455078}}},\n",
      "          'num_agent_steps_sampled': 1,\n",
      "          'num_agent_steps_trained': 1,\n",
      "          'num_steps_sampled': 1,\n",
      "          'num_steps_trained': 1,\n",
      "          'num_steps_trained_this_iter': 1},\n",
      " 'iterations_since_restore': 1,\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_healthy_workers': 0,\n",
      " 'off_policy_estimator': {},\n",
      " 'perf': {'cpu_util_percent': 10.6, 'ram_util_percent': 61.2},\n",
      " 'pid': 5594,\n",
      " 'policy_reward_max': {},\n",
      " 'policy_reward_mean': {},\n",
      " 'policy_reward_min': {},\n",
      " 'sampler_perf': {},\n",
      " 'time_since_restore': 0.009541749954223633,\n",
      " 'time_this_iter_s': 0.009541749954223633,\n",
      " 'time_total_s': 0.009541749954223633,\n",
      " 'timers': {'learn_throughput': 650.582,\n",
      "            'learn_time_ms': 1.537,\n",
      "            'load_throughput': 3744.914,\n",
      "            'load_time_ms': 0.267,\n",
      "            'sample_throughput': 64.89,\n",
      "            'sample_time_ms': 15.411},\n",
      " 'timestamp': 1648543042,\n",
      " 'timesteps_since_restore': 1,\n",
      " 'timesteps_this_iter': 1,\n",
      " 'timesteps_total': 1,\n",
      " 'training_iteration': 1,\n",
      " 'trial_id': 'default',\n",
      " 'warmup_time': 0.0461580753326416}\n"
     ]
    }
   ],
   "source": [
    "# Perform single `.train()` call.\n",
    "result = bandit_trainer.train()\n",
    "# Erase config dict from result (for better overview).\n",
    "del result[\"config\"]\n",
    "# Print out training iteration results.\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d6f089b-e0cc-47de-af9b-dc05a71e102f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 .... 500 .... 1000 .... 1500 .... 2000 .... 2500 ...."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlwAAAG5CAYAAABbfeocAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAArmElEQVR4nO3de5hdV33f//d37jMaje6yLQkj2/h+CTaDwWDAhIJtoD/AgSdJ8zRJQ+ufc2l//bU8KX2SAiGhbUhKE2h/oU7rAKE1IQEKiQFDKFgGjLF8l++WjUGybEmju+Y+8/39sfeMjgfdfZZmJL1fj89z9l57n73XWT46+mjtddaOzESSJEnltMx2BSRJkk50Bi5JkqTCDFySJEmFGbgkSZIKM3BJkiQVZuCSJEkqzMAlSXNARHwnIv7pbNdDUhkGLknFRMSPImI0IpbOKL83IjIiVs9S1STpmDJwSSrtaeAXp1Yi4mKgZ/aqs09EtM3COSMi/O6VTjL+oZdU2l8Cv9yw/ivAZxp3iIjOiPjjiPhxRDwfEZ+MiO5626KI+LuI2BIR2+vlVQ2v/U5E/H5EfC8idkfEN2b2qDXse1VEbIiIfxMRzwF/EREtEfH+iFgfEQMR8fmIWFzv/+mI+Nf18sq6V+436/WzImJb/frDqeNHIuJ7wCBwZkS8OSIejYidEfFfgGhCW0uaowxckkr7AdAXEedHRCvwC8BnZ+zzH4FzgJcDLwNWAh+ot7UAfwG8FDgdGAL+y4zX/yPgnwDLgQ7gfQepz6nA4vp41wP/HHgn8AZgBbAd+K/1vrcBV9XLbwCeAl7fsH57Zk4eZh3/cX2++cBO4IvA7wJLgfXAaw9SZ0nHOQOXpGNhqpfrzcAjwMapDRERVEHk/83MbZm5G/j3VMGMzBzIzC9k5mC97SNUYafRX2Tm45k5BHyeKrgdyCTwwcwcqfe/AfidzNyQmSPAh4B315cbbwOurC8Bvh74KPuC0Rvq7Ydbx09l5kOZOQ5cCzyUmX+TmWPAnwDPHaoRJR2/jvn4BUknpb8E1gBnMONyIrCMakzX3VX2AqrLa60AEdED/GfgGmBRvX1+RLRm5kS93hhWBoHeg9RlS2YON6y/FPhSREw2lE0Ap2Tm+ojYSxXgXgf8PvDeiDiXKlB9/Ajq+JOG469oXM/MjIjG7ZJOMPZwSSouM5+hGjz/VqpLaY22Ul2CuzAzF9aPBZk5FZr+NXAu8KrM7GPfJb2jHfOUM9Z/AlzbcO6FmdmVmVO9cLcB7wY66rLbqMahLQLuO4I6Np53E/CSqZW6l+8lSDphGbgkHSvvBX42M/c2FtZjoP4c+M8RsRymB6hfXe8ynyqQ7agHs3+wyfX6JPCRiHhpfe5lEfGOhu23Ab9F1UMH8J16/bsNvVdHWsdbgAsj4rr60uW/oBpbJukEZeCSdExk5vrMXHuAzf8GeBL4QUTsAv6eqscIqvFN3VQ9YT8Avt7kqv0p8BXgGxGxuz7Hqxq230YVqKYC13epLoGuadjniOqYmVuB91D9WGAAOBv43ot8H5LmsMic2bsuSZKkZrKHS5IkqTADlyRJUmEGLkmSpMIMXJIkSYXN+YlPly5dmqtXr57takiSJB3S3XffvTUzl80sn/OBa/Xq1axde6BfkkuSJM0dEfHM/sq9pChJklSYgUuSJKkwA5ckSVJhBi5JkqTCDFySJEmFGbgkSZIKM3BJkiQVZuCSJEkqzMAlSZJUmIFLkiSpMAOXJElSYQYuSZKkwgxckiRJhRm4JEmSCjNwSdIcNjYxyaadQ+wZGZ/tqkh6EdpmuwKSdDKanEy2D47y/K4Rnt81zPO7hnlu1/AL1p/fNczA3lEyq9cs7Gln1aJuVi3sqZ4XdXPGsl6uOHMJHW3++1maywxcklTQ5GTyo4G9PLhxJw9s2Mm6jTvZsH2IzbuHGZvIn9p/aW8Hp/R1cUpfF5esWsgpfZ0sm9/JnuFxNmwf4ifbB3lyyx6+8/hmhscmgSqIvf2S03jXpau47PSFRMSxfpuSDsHAJUlNNDo+yffXb+WOpwZ4cMNOHty4k93D1eXAzrYWLljRx+VnLK5DVSen9nWxvK+LUxd0say387B7qjKTgb2jPLBhB1+691n+eu0GPvuDH7N6SQ/vvHQl77p0JS9dMq/kW5V0BCLzp/+FNZf09/fn2rVrZ7saknRAw2MT3Pb4Fr6+7jn+/pHn2T08TntrcP5pfVyyagGXrFzIxasWcPbyXtpay1z62z08xtfWPceX7tnID54eIBNe8dJFXHfZSt5+8QoW9LQXOa+kF4qIuzOz/6fKDVySdOR2D4/x7ce28PV1m/j2o1sYGptgQXc7b77gFK696FRe+7KldLW3zkrdnt0xxP++byNfumcjT2zeQ0drCz973nLeeelK3njeMjrbZqde0snAwCVJL9L2vaN885HnuXXdc9z+xFZGJyZZ2tvJ1ReewrUXncarzlxMe6EerKORmazbuIsv3ruBv73/WbbuGWVhTztvu/g0rrtsJZe+ZBEtLY73kprJwCVJR2Hz7mFufagKWXc8NcDEZLJyYTdXX3gq1158KpedvojW4yC0jE9McvsTW/nivRv5xkPPMTI+yaKedi4/YzGvOmMJrzpzMeef2mcAk16kAwUuB81LOukNj03w422DbNg+yMbtQ2zYPsSGHUM8M7CXh57dRSacuXQe//frz+Sai07l4pULjrtfAra1tvDG85bzxvOWs3t4jG889DzfXz/AnU8PcOtDzwPQ19XG5Wcs5vIzFtO/ejEXnNY3a5dFpRONPVySThqZyaadwzz63C4e2bSbR5/bzaObdvHU1r1MTO77LuxobWHlom5WLuzmlasXc+3Fp3L28t7jLmQdro07hvjh0wPc+dQ27nx6G09v3QtAa0vwsmW9XLiyj4tWLOCilQu4YEUfvZ3N/7f65GSye2ScnYNj7BgaZcfgGDuGxhgcGWcyYTKTzGxYhpaAzvZWutpb6Gprpau9lc62ln1l7fvKutpb6WprKfajBWmKlxQlnVQGR8d5/Pk9PLppF48+t5uHN+3i0U272DW8b8b2VYu6Oe/UPs4/bT4vW97LqkU9vGRRN0t7O0/qS2ubdw1zz4938PCzO1n37C7WbdzJ5t0jAETAGUvmceHKBVy0oo8LVyzgwhV9LJrXAVS9hTuHxtgxOMbOoX2PHYOj7Gpcb9hnx+AoO4fGmDwGfx21tcR0AOvuaGVhTzuLejpY0F09L5rXweKedhbN62DJvE4WzWtn8bwOFvV02Nunw2LgknRQu4bHeOTZXfxoYC8RQUdrC+2tLbS3Bu1tLXS0ttDRVpV1tLYwr7P6C2teRxvd7a3HNKCMTUwyODLBruGxekb2enb23cP8eGCQR5/bzY8G9k7P0D6vo5VzT53Peaf1cX79fO6p8+nrcqqEw7V51zAP1eFr3bM7WbdxFxt3DE1vX9TTzt7RCUbHJw94jAiY39nGgoaQs7Cng4Xd7SzsaZ8OPQt7ptY7mNfZSmsEBLRE1A8IgolMRsYnGB6bZHhsgpHx6rl6TDIyPsHI2CTD41XZvuVqv6HRCXYMjbF9sOpR214HvwP9tTivo7UOYnUwm9fB4rq+vZ1t9HZVz/O72qafl8zrpK+77YTtHZ1pfGKSsYkk+elGPFjciICutmP7PVKKY7gkAdVltQ3bh3hk0y4e3rSLh5+tnjdsHzr0iw+iu72VeZ2t9HS00dPRSk9HK/M6p5bbptcb92ttCQZHJxgcGWdwrH4enagfB17e3wztAO2twcqFVa/VO16+gvNP6+P8U/tYtaj7hPgin03L6wla33je8umy7XtHqxD27E5+vG2Q+Z1t9DWEp6nHwu4qXM3vapvz/x8mJpMdg6NsHxxl294xtu0dYdveKowN7Jkqr5afeH4P2wdHGRydOOgx21uDJfM6WdLbwdLefc9LezteUL60t5PF8zpm7TZNw2MT08Fzx+BY3Q77guj2vdX6zqHqeWi0Crmj4xOMTkwyOj75onopI6C3o43erjbmde4LrYt6OqbbaHEdeJfUbbe4t4P5ncdHoLWHSzqBjYxP8MTze3h4064qYD1bPU9dVpu6PHT+ij4uOK16vGx5LxEwNpGM1V+iY/W/WscmJhmdmGRkbOIFAWjvyAtD0d6RcYbGqufB0Qn2jo4zNDrB3pEJhsb2/5dTe2u8IKy9YLmzjZ72OrB1tDKvo5XujurL+JS+LpbP7+SUvi4W9bQfF1+8OrGMjFef7b0j4+weHmfPyDh7RqrLpQN7RhnYO8rW3SMM7B1lYM8IW/eMsnXPCCMH6A3s62pjYU8HPR3VGLTu9urPQVdHtdze2kJrC7S1tNASQWsLtLQErRG0tcT0cktLTP+CdnBknD11HfeMjrN3ZHy6vjvrXr6pW0XtT2dbywt6Hxd2d9DT2UpnWzVGrmNGL/iBcvWB/nhOZlXH3XW99tR12z08Ph12D3QD947WliqI9XawpLezCmTzOqow1tXO/Dq89Xa1cenpC4vPQ2cPl3SC2753dF+vVR2unty8h/H6n5zd7dVltbf/zIoqXK3o47xT59PTcWy/BiYmk6GxKqBNTCY97VWI8ubLOl5VoaOVxfU4tsORmewdnagDWBXCBvaMTq/vHBpjaGyCobFJhkbHeX73GIOj1WXQsYlkMpOJyYZHJpOTOf3nfaYImNfRxrzO1uneo3kdbaxa1MNFK9tZ1FNf3u1puKzb3cGiee1zZvza8NgE2/ZWPYxb94wwsKde3jvCtjrYDuwd5akte9i2d/89j2t/9x/Q2Ts778XAJR1nJieTn2wfnL4UONVr9ezO4el9ls/v5IIVfbzxvOXT4Wr1knlzYr6o1pao/rVZ4Jdu0vEiYt+fg2bf83KyDmATk1UwgxNjfFRXeysrFnazYmH3Ye0/PDYx3eO4e3iMPcPjLOyevXGbfuPpkCYmk8HRcSYn8X5sx9jw2ASPP7/7BeHq0ed2T3ettwSctayXV55RzZl0fv1YNr9zlmsuaba0tAQtBHOgU2pWTU0LMle+Dw1cJ5HM5OFNu7j9ia1s3jVSdVePjteXd6qu6qGxfc9TZaMT+67rr17SwxVnLeGKs5ZyxZlL5swH+XgxPjHJRD2HUCYk1bxCmcnQ6ASPzQhX67fsmR6E2tvZxnmnzue6y1ZO91qdc8r8OdHVL0k6OAPXCW738BjfXz/Atx/dzLcf28zzu6q5dHrrwcdTgzGnlhf2tNM1VdZeDUye2md8Mrn7me383f2buPmHPwHgnFN6ueLMKoC9+szFLOw5/DEMx6OxiUl2D49Pzye0a3iMXUPj7Bqu1w9YVr3mQINkZ1qxoIsLVvRxzUWnToerlyzqOe4vCUjSycrAdYIZm5jk/p/s4PYntvK9J7dy7092MDGZzO9s43XnLOWN5y7nDecuY/n8rqM+x/jEJA89u4vvrx/gjqcG+PzaDXz6jmeIgAtO6+M1Zy3hNWct5ZVnLD5uxulkJtv2jrJxxxAbtw+xcUd1e5eNO4Z4dscQ2/ZWP4s+1M+/W1uCBd3t9HVVP49f0N3OigXd9HW30dfVzrzOaiqEqOcRaolqMGtLBO2tLZy9vJfzT9s3iaQk6cTgtBDHuczkqa17+e4TW7n9ia384KkB9oyMEwGXrFzAlWcv5cqXLaN/9SLaC93SYnR8kvs37OCO9QN8f/1W7nlmB6MTk7S2BJesWjAdwF7x0kWzcvkrs/pV3PbBMZ7dT6DauH2QZ3cM/9R0BfM6Wlm5qBqguay3k77udvq62lnQ3Ta9PBWqpgJVT0er0xJI0knsqGeaj4ibgLcDmzPzorrsPcCHgPOByzNzbcP+lwD/DegDJoFXZuZwRHwdOI2qV+124Dcz8+DdBRi49mdgzwjffXIr3617saZ+nfaSxd1c+bJlvO7spbzmrCWzdnlveGyCu5/ZPh3A7t+wk4nJpKO1hUtPX8hrzlrKa162hJ9ZtfCwpgIYn7qMNzw2fTlv14z1ar6WsX1lM7bt76fSi+d1sHJhdb+8qfvmTT2vWtTNgm7ndJIkHZkXE7heD+wBPtMQuM6nClP/DXjfVOCKiDbgHuAfZ+b9EbEE2JGZExHRl5m7ovob7G+Av87Mzx2q4qUD1+/+7wf5/voBoqo/QXV5Z+rv2emyluoSUH2Hiaq8YXnqVhP1f7S3tkxPtNZ4q4fermpMVGdbdXPVzrZWOusbr3a2t0zf46uzrWV6QrnRiUnu+tG26V6shzftAqrJ8V77sqV1L9bSpv+0uFn2jIxz19PbuOOpKoA99OwuMqt5oU5b2EUm03PJZNbzyWT10+apwfuHMq+jlb56Juu+rvq5Xp/fNdUz1c6KhV2sqnutjvX8U5KkE99RT3yamWsiYvWMskfqg87c/S3AA5l5f73fQMNrdjWcswP2c6OlWXD64h52DI5N/2Js6tdjk1ndCarKo/lTZdVzvuB1k9PrsHd0nM27h9kzXM2cu2dk/KD3kTqYiOqc7a3BZacv4n1vOYcrz17GxSsXzIl5lQ6lt7ONN563fPqWIDsGR7nz6W3csX6ArXtG6pmSqwDbOnWvtJYqxHa3t1aBqXsqOLVNr/fVQaq3q+24aAdJ0smr2f/EPwfIiLgVWAZ8LjM/OrWxLr8c+BpVL9d+RcT1wPUAp59+epOr+ELXv/6sosefkpkMjk6wZ2R8+v5TUzddbXyeurnqyNjk9I1YM5NLT1/E5WcsZt5xMgj9YBb2dHD1hady9YWnznZVJEk6Jpr9t3cbcCXwSmAQ+FbdtfYtgMy8OiK6gP8J/Czwzf0dJDNvBG6E6pJik+s4KyKCeZ1tJ0RgkiRJR6bZP1vbAKzJzK2ZOQh8FbiscYfMHAa+DLyjyeeWJEmak5oduG4FLo6InnoA/RuAhyOiNyJOg+mB9W8DHm3yuSVJkuakQ17fioibgauApRGxAfggsA34BNU4rVsi4r7MvDozt0fEx4C7qMaOfzUzb4mIU4CvREQnVcj7NvDJIu9IkiRpjnHiU0mSpCY50LQQZaYelyRJ0jQDlyRJUmEGLkmSpMIMXJIkSYUZuCRJkgozcEmSJBVm4JIkSSrMwCVJklSYgUuSJKkwA5ckSVJhBi5JkqTCDFySJEmFGbgkSZIKM3BJkiQVZuCSJEkqzMAlSZJUmIFLkiSpMAOXJElSYQYuSZKkwgxckiRJhRm4JEmSCjNwSZIkFWbgkiRJKszAJUmSVJiBS5IkqTADlyRJUmEGLkmSpMIMXJIkSYUZuCRJkgozcEmSJBVm4JIkSSrMwCVJklSYgUuSJKkwA5ckSVJhBi5JkqTCDFySJEmFGbgkSZIKM3BJkiQVZuCSJEkqzMAlSZJUmIFLkiSpMAOXJElSYQYuSZKkwgxckiRJhRm4JEmSCjNwSZIkFWbgkiRJKszAJUmSVJiBS5IkqTADlyRJUmEGLkmSpMIMXJIkSYUZuCRJkgo7ZOCKiJsiYnNErGsoe09EPBQRkxHRP2P/SyLijnr7gxHRFRE9EXFLRDxal//HEm9GkiRpLjqcHq5PAdfMKFsHXAesaSyMiDbgs8ANmXkhcBUwVm/+48w8D7gUeG1EXHv01ZYkSTp+tB1qh8xcExGrZ5Q9AhARM3d/C/BAZt5f7zdQlw8C367LRiPiHmDVi6q5JEnScaLZY7jOATIibo2IeyLit2fuEBELgX8IfOtAB4mI6yNibUSs3bJlS5OrKEmSdGw1O3C1AVcCv1Q/vysi3jS1sb7keDPw8cx86kAHycwbM7M/M/uXLVvW5CpKkiQdW80OXBuANZm5NTMHga8ClzVsvxF4IjP/pMnnlSRJmrOaHbhuBS6uf5XYBrwBeBggIv4AWAD8yyafU5IkaU47nGkhbgbuAM6NiA0R8d6IeFdEbACuAG6JiFsBMnM78DHgLuA+4J7MvCUiVgG/A1wA3BMR90XEPy3zliRJkuaWw/mV4i8eYNOXDrD/Z6mmhmgs2wD81E8aJUmSTgbONC9JklSYgUuSJKkwA5ckSVJhBi5JkqTCDFySJEmFGbgkSZIKM3BJkiQVZuCSJEkqzMAlSZJUmIFLkiSpMAOXJElSYQYuSZKkwgxckiRJhRm4JEmSCjNwSZIkFWbgkiRJKszAJUmSVJiBS5IkqTADlyRJUmEGLkmSpMIMXJIkSYUZuCRJkgozcEmSJBVm4JIkSSrMwCVJklSYgUuSJKkwA5ckSVJhBi5JkqTCDFySJEmFGbgkSZIKM3BJkiQVZuCSJEkqzMAlSZJUmIFLkiSpMAOXJElSYQYuSZKkwgxckiRJhRm4JEmSCjNwSZIkFWbgkiRJKszAJUmSVJiBS5IkqTADlyRJUmEGLkmSpMIMXJIkSYUZuCRJkgozcEmSJBVm4JIkSSrMwCVJklSYgUuSJKkwA5ckSVJhBi5JkqTCDFySJEmFGbgkSZIKO2TgioibImJzRKxrKHtPRDwUEZMR0T9j/0si4o56+4MR0VWXfyQifhIRe5r/NiRJkuauw+nh+hRwzYyydcB1wJrGwohoAz4L3JCZFwJXAWP15r8FLn8RdZUkSToutR1qh8xcExGrZ5Q9AhARM3d/C/BAZt5f7zfQ8JofHOA1kiRJJ7Rmj+E6B8iIuDUi7omI3z6ag0TE9RGxNiLWbtmypclVlCRJOraaHbjagCuBX6qf3xURbzrSg2TmjZnZn5n9y5Yta3IVJUmSjq1mB64NwJrM3JqZg8BXgcuafA5JkqTjSrMD163AxRHRUw+gfwPwcJPPIUmSdFw5nGkhbgbuAM6NiA0R8d6IeFdEbACuAG6JiFsBMnM78DHgLuA+4J7MvKU+zkfr1/TUx/lQkXckSZI0x0RmznYdDqq/vz/Xrl0729WQJEk6pIi4OzP7Z5Y707wkSVJhBi5JkqTCDFySJEmFGbgkSZIKM3BJkiQVZuCSJEkqzMAlSZJUmIFLkiSpMAOXJElSYQYuSZKkwgxckiRJhRm4JEmSCjNwSZIkFWbgkiRJKszAJUmSVJiBS5IkqTADlyRJUmEGLkmSpMIMXJIkSYUZuCRJkgozcEmSJBVm4JIkSSrMwCVJklSYgUuSJKkwA5ckSVJhBi5JkqTCDFySJEmFGbgkSZIKM3BJkiQVZuCSJEkqzMAlSZJUmIFLkiSpMAOXJElSYQYuSZKkwgxckiRJhRm4JEmSCjNwSZIkFWbgkiRJKszAJUmSVJiBS5IkqTADlyRJUmEGLkmSpMIMXJIkSYUZuCRJkgozcEmSJBVm4JIkSSrMwCVJklSYgUuSJKkwA5ckSVJhBi5JkqTCDFySJEmFGbgkSZIKM3BJkiQVdsjAFRE3RcTmiFjXUPaeiHgoIiYjon/G/pdExB319gcjoqsuf0W9/mREfDwiovlvR5Ikae45nB6uTwHXzChbB1wHrGksjIg24LPADZl5IXAVMFZv/jPgnwFn14+Zx5QkSTohHTJwZeYaYNuMskcy87H97P4W4IHMvL/ebyAzJyLiNKAvM3+QmQl8Bnjni669JEnScaDZY7jOATIibo2IeyLit+vylcCGhv021GX7FRHXR8TaiFi7ZcuWJldRkiTp2GorcLwrgVcCg8C3IuJuYOeRHCQzbwRuBOjv788m11GSJOmYanYP1wZgTWZuzcxB4KvAZcBGYFXDfqvqMkmSpBNeswPXrcDFEdFTD6B/A/BwZm4CdkXEq+tfJ/4y8OUmn1uSJGlOOpxpIW4G7gDOjYgNEfHeiHhXRGwArgBuiYhbATJzO/Ax4C7gPuCezLylPtRvAP8deBJYD3yt2W9GkiRpLorqR4NzV39/f65du3a2qyFJknRIEXF3ZvbPLHemeUmSpMIMXJIkSYUZuCRJkgozcEmSJBVm4JIkSSrMwCVJklSYgUuSJKkwA5ckSVJhBi5JkqTCDFySJEmFGbgkSZIKM3BJkiQVZuCSJEkqzMAlSZJUmIFLkiSpMAOXJElSYQYuSZKkwgxckiRJhRm4JEmSCjNwSZIkFWbgkiRJKszAJUmSVJiBS5IkqTADlyRJUmEGLkmSpMIMXJIkSYUZuCRJkgozcEmSJBVm4JIkSSrMwCVJklSYgUuSJKkwA5ckSVJhBi5JkqTCDFySJEmFGbgkSZIKM3BJkiQVZuCSJEkqzMAlSZJUmIFLkiSpMAOXJElSYQYuSZKkwgxckiRJhRm4JEmSCjNwSZIkFWbgkiRJKszAJUmSVJiBS5IkqTADlyRJUmEGLkmSpMIMXJIkSYUZuCRJkgozcEmSJBVm4JIkSSrMwCVJklTYIQNXRNwUEZsjYl1D2Xsi4qGImIyI/oby1RExFBH31Y9PNmz7+Yh4oH7dHzb/rUiSJM1Nh9PD9Sngmhll64DrgDX72X99Zr68ftwAEBFLgD8C3pSZFwKnRsSbjr7akiRJx49DBq7MXANsm1H2SGY+dgTnORN4IjO31Ot/D/zcEbxekiTpuFViDNcZEXFvRNwWEa+ry54Ezq0vObYB7wRecqADRMT1EbE2ItZu2bLlQLtJkiQdF5oduDYBp2fmpcC/Av5XRPRl5nbg14G/Am4HfgRMHOggmXljZvZnZv+yZcuaXEVJkqRjq6mBKzNHMnOgXr4bWA+cU6//bWa+KjOvAB4DHm/muSVJkuaqpgauiFgWEa318pnA2cBT9fry+nkR8BvAf2/muSVJkuaqtkPtEBE3A1cBSyNiA/BBqkH0nwCWAbdExH2ZeTXweuDDETEGTAI3ZObUgPs/jYifqZc/nJn2cEmSpJNCZOZs1+Gg+vv7c+3atbNdDUmSpEOKiLszs39muTPNS5IkFWbgkiRJKszAJUmSVJiBS5IkqTADlyRJUmEGLkmSpMIMXJIkSYUZuCRJkgozcEmSJBVm4JIkSSrMwCVJklSYgUuSJKkwA5ckSVJhBi5JkqTCDFySJEmFGbgkSZIKM3BJkiQVZuCSJEkqzMAlSZJUmIFLkiSpMAOXJElSYQYuSZKkwgxckiRJhRm4JEmSCjNwSZIkFWbgkiRJKszAJUmSVJiBS5IkqTADlyRJUmEGLkmSpMIMXJIkSYUZuCRJkgozcEmSJBVm4JIkSSrMwCVJklSYgUuSJKkwA5ckSVJhBi5JkqTCDFySJEmFGbgkSZIKM3BJkiQVZuCSJEkqzMAlSZJUmIFLkiSpMAOXJElSYQYuSZKkwgxckiRJhRm4JEmSCjNwSZIkFWbgkiRJKszAJUmSVJiBS5IkqTADlyRJUmGHDFwRcVNEbI6IdQ1l74mIhyJiMiL6G8pXR8RQRNxXPz7ZsO0XI+LBiHggIr4eEUub/3YkSZLmnsPp4foUcM2MsnXAdcCa/ey/PjNfXj9uAIiINuBPgTdm5iXAA8BvHXWtJUmSjiOHDFyZuQbYNqPskcx87AjOE/VjXkQE0Ac8eyQVlSRJOl6VGMN1RkTcGxG3RcTrADJzDPh14EGqoHUB8D8OdICIuD4i1kbE2i1bthSooiRJ0rHT7MC1CTg9My8F/hXwvyKiLyLaqQLXpcAKqkuK//ZAB8nMGzOzPzP7ly1b1uQqSpIkHVtNDVyZOZKZA/Xy3cB64Bzg5XXZ+sxM4PPAa5p5bkmSpLmqqYErIpZFRGu9fCZwNvAUsBG4ICKmuqveDDzSzHNLkiTNVW2H2iEibgauApZGxAbgg1SD6D8BLANuiYj7MvNq4PXAhyNiDJgEbsjMbfVxfg9YU297BvjV5r8dSZKkuSeqK3xzV39/f65du3a2qyFJknRIEXF3ZvbPLHemeUmSpMIMXJIkSYUZuCRJkgozcEmSJBVm4JIkSSrskNNCzLrHHoOrrprtWkiSJB01e7gkSZIKm/s9XOeeC9/5zmzXQpIk6dAi9ltsD5ckSVJhBi5JkqTCDFySJEmFGbgkSZIKM3BJkiQVZuCSJEkqzMAlSZJUmIFLkiSpMAOXJElSYQYuSZKkwgxckiRJhRm4JEmSCjNwSZIkFWbgkiRJKszAJUmSVFhk5mzX4aAiYgvwTOHTLAW2Fj7HycY2bS7bs/ls0+ayPZvPNm2+Y9GmL83MZTML53zgOhYiYm1m9s92PU4ktmlz2Z7NZ5s2l+3ZfLZp881mm3pJUZIkqTADlyRJUmEGrsqNs12BE5Bt2ly2Z/PZps1lezafbdp8s9amjuGSJEkqzB4uSZKkwgxckiRJhZ30gSsiromIxyLiyYh4/2zX53gRET+KiAcj4r6IWFuXLY6Ib0bEE/Xzoro8IuLjdRs/EBGXzW7t54aIuCkiNkfEuoayI27DiPiVev8nIuJXZuO9zAUHaM8PRcTG+nN6X0S8tWHbv63b87GIuLqh3O+EWkS8JCK+HREPR8RDEfH/1OV+To/CQdrTz+lRioiuiPhhRNxft+nv1eVnRMSddfv8VUR01OWd9fqT9fbVDcfab1s3TWaetA+gFVgPnAl0APcDF8x2vY6HB/AjYOmMso8C76+X3w/8Yb38VuBrQACvBu6c7frPhQfweuAyYN3RtiGwGHiqfl5ULy+a7fc2h9rzQ8D79rPvBfWf907gjPp7oNXvhJ9qp9OAy+rl+cDjddv5OW1ue/o5Pfo2DaC3Xm4H7qw/e58HfqEu/yTw6/XybwCfrJd/Afirg7V1M+t6svdwXQ48mZlPZeYo8DngHbNcp+PZO4BP18ufBt7ZUP6ZrPwAWBgRp81C/eaUzFwDbJtRfKRteDXwzczclpnbgW8C1xSv/Bx0gPY8kHcAn8vMkcx8GniS6vvA74QGmbkpM++pl3cDjwAr8XN6VA7Sngfi5/QQ6s/annq1vX4k8LPA39TlMz+jU5/dvwHeFBHBgdu6aU72wLUS+EnD+gYO/uHXPgl8IyLujojr67JTMnNTvfwccEq9bDsfviNtQ9v20H6rvrx109SlL2zPI1ZfermUqgfBz+mLNKM9wc/pUYuI1oi4D9hMFebXAzsyc7zepbF9ptuu3r4TWMIxaNOTPXDp6F2ZmZcB1wK/GRGvb9yYVR+tc468CLZhU/wZcBbwcmAT8J9mtTbHqYjoBb4A/MvM3NW4zc/pkdtPe/o5fREycyIzXw6souqVOm92a7R/J3vg2gi8pGF9VV2mQ8jMjfXzZuBLVB/y56cuFdbPm+vdbefDd6RtaNseRGY+X38ZTwJ/zr5LBLbnYYqIdqpw8D8z84t1sZ/To7S/9vRz2hyZuQP4NnAF1eXstnpTY/tMt129fQEwwDFo05M9cN0FnF3/mqGDagDdV2a5TnNeRMyLiPlTy8BbgHVUbTf166NfAb5cL38F+OX6F0yvBnY2XI7QCx1pG94KvCUiFtWXId5Sl4npMDDlXVSfU6ja8xfqXyydAZwN/BC/E16gHtvyP4BHMvNjDZv8nB6FA7Wnn9OjFxHLImJhvdwNvJlqbNy3gXfXu838jE59dt8N/J+6l/ZAbd08x+qXBHP1QfWrmseprvn+zmzX53h4UP0y5v768dBUu1FdB/8W8ATw98DiujyA/1q38YNA/2y/h7nwAG6munwwRjVe4L1H04bAr1EN8HwS+Cez/b7mWHv+Zd1eD1B9oZ7WsP/v1O35GHBtQ7nfCfva4kqqy4UPAPfVj7f6OW16e/o5Pfo2vQS4t267dcAH6vIzqQLTk8BfA511eVe9/mS9/cxDtXWzHt7aR5IkqbCT/ZKiJElScQYuSZKkwgxckiRJhRm4JEmSCjNwSZIkFWbgknTEImJJRNxXP56LiI318p6I+P+aeJ5XR8TTDefaExGP1cufOcxj3BARv3yIffoj4uPNqfV+j//yiHhrqeNLmvucFkLSixIRHwL2ZOYfFzj27wEPZOYX6vXvAO/LzLUz9mvNzIlmn79ZIuJXqeak+q3Zrouk2WEPl6SmiYirIuLv6uUPRcSnI+L2iHgmIq6LiI9GxIMR8fX6FidExCsi4rb6Rui3zph1+01UE2vu71w/iog/jIh7gPdExD+LiLsi4v6I+EJE9DTU43318nfq1/wwIh6PiNcdoN431fs+FRH/ouGc/67uYftuRNw8ddwZ9XpPRKyr67Gmngn8w8DP1z1zP1/freGmuh73RsQ76tf+akR8uT73ExHxwbp8XkTcUh9zXUT8/Iv8XyXpGGs79C6SdNTOAt4IXADcAfxcZv52RHwJeFtE3AJ8AnhHZm6pg8RHgF+LiKXAWGbuPMjxB7K6iToRsSQz/7xe/gOqmeY/sZ/XtGXm5fUlvg8C/2A/+5xX13s+8FhE/BnVjYV/DvgZoB24B7h7P6/9AHB1Zm6MiIWZORoRH6Chhysi/j3VLUV+rb4tyQ8jYipYXg5cBAwCd9Vt9FLg2cx8W/36BQdpE0lzkIFLUklfy8yxiHgQaAW+Xpc/CKwGzqUKF9+sbjNHK9XteaC63943DnH8v2pYvqgOWguBXg58r76pGzDfXddhf27JzBFgJCI2A6cArwW+nJnDwHBE/O0BXvs94FMR8fmGc830FuD/augh6wJOr5e/mZkDABHxRarbwXwV+E8R8YfA32Xm7Qc4rqQ5ysAlqaQRgMycjIix3DdodJLq+yeAhzLziv289lrgY/spb7S3YflTwDsz8/56zNRVB6sTMMGBvwNHGpYPtt9PycwbIuJVwNuAuyPiFfvZLah6+x57QWH1upkDazMzH4+Iy6jun/cHEfGtzPzw4dZJ0uxzDJek2fQYsCwirgCIiPaIuDCq7q5LqG7ue7jmA5vqsWG/1PSaVj1X/zAiuiKiF3j7/naKiLMy887M/ACwBXgJsLuu35RbgX9ev08i4tKGbW+OiMUR0Q28E/heRKwABjPzs8AfAZc1+b1JKsweLkmzph7f9G7g4/W4pDbgT4Bu4N6GHrHD8e+AO6lCzp28MOA0o653RcRXgAeA56kui+5vfNkfRcTZVL1Y3wLuB34MvD8i7gP+A/D7VO/zgYhoAZ5mX4D7IfAFYBXw2cxcGxFX18edBMaAX2/me5NUntNCSJpzIuJ3gScz83OzXZdGEdGbmXvqX0CuAa7PzHuaePxfxekjpBOSPVyS5pzM/IPZrsMB3BgRF1ANcv90M8OWpBObPVySJEmFOWhekiSpMAOXJElSYQYuSZKkwgxckiRJhRm4JEmSCvv/AZ+G0pZVwzTOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train for n more iterations (timesteps) and collect n-arm rewards.\n",
    "rewards = []\n",
    "for i in range(3000):\n",
    "    # Run a single timestep in the environment and update\n",
    "    # the model immediately on the received reward.\n",
    "    result = bandit_trainer.train()\n",
    "    # Extract reward from results.\n",
    "    #rewards.extend(result[\"hist_stats\"][\"episode_reward\"]\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    if i % 500 == 0:\n",
    "        print(f\" {i} \", end=\"\")\n",
    "    elif i % 100 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "start_at = 0\n",
    "smoothing_win = 200\n",
    "x = list(range(start_at, len(rewards)))\n",
    "y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) for i in range(start_at, len(rewards))]\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=lts_20_2_env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77c3cf-226c-4cab-82c6-7d8a54bfe9ea",
   "metadata": {},
   "source": [
    "<a id='bandit_results'></a>\n",
    "### What does our trained Bandit actually recommend?\n",
    "\n",
    "The first method of the RLlib Trainer API we used above was `train()`.\n",
    "We'll now use another method of the Trainer, `compute_single_action(input_dict={})`.\n",
    "It takes a input_dict keyword arg, into which you may pass a single (unbatched!) observation to receive an action for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6eb62acb-0a16-4412-949a-4851201620bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action's feature value=0.9990183711051941; max-choc-feature=0.9990183711051941; \n",
      "action's feature value=0.9261587262153625; max-choc-feature=0.9261587262153625; \n",
      "action's feature value=0.8654089570045471; max-choc-feature=0.8654089570045471; \n",
      "action's feature value=0.983932375907898; max-choc-feature=0.983932375907898; \n",
      "action's feature value=0.9677650332450867; max-choc-feature=0.9677650332450867; \n",
      "action's feature value=0.9945515990257263; max-choc-feature=0.9945515990257263; \n",
      "action's feature value=0.9629301428794861; max-choc-feature=0.9629301428794861; \n",
      "action's feature value=0.9626579880714417; max-choc-feature=0.9626579880714417; \n",
      "action's feature value=0.9741053581237793; max-choc-feature=0.9741053581237793; \n",
      "action's feature value=0.9592612981796265; max-choc-feature=0.9592612981796265; \n",
      "action's feature value=0.8881962895393372; max-choc-feature=0.8881962895393372; \n",
      "action's feature value=0.9937254190444946; max-choc-feature=0.9937254190444946; \n",
      "action's feature value=0.9818119406700134; max-choc-feature=0.9818119406700134; \n",
      "action's feature value=0.95710289478302; max-choc-feature=0.95710289478302; \n",
      "action's feature value=0.9707748293876648; max-choc-feature=0.9707748293876648; \n",
      "action's feature value=0.9660489559173584; max-choc-feature=0.9660489559173584; \n",
      "action's feature value=0.9629757404327393; max-choc-feature=0.9629757404327393; \n",
      "action's feature value=0.9689820408821106; max-choc-feature=0.9689820408821106; \n",
      "action's feature value=0.9414763450622559; max-choc-feature=0.9414763450622559; \n",
      "action's feature value=0.9705929160118103; max-choc-feature=0.9705929160118103; \n",
      "action's feature value=0.9640102982521057; max-choc-feature=0.9640102982521057; \n",
      "action's feature value=0.9907165169715881; max-choc-feature=0.9907165169715881; \n",
      "action's feature value=0.9449115991592407; max-choc-feature=0.9449115991592407; \n",
      "action's feature value=0.9208205342292786; max-choc-feature=0.9208205342292786; \n",
      "action's feature value=0.9172598123550415; max-choc-feature=0.9172598123550415; \n",
      "action's feature value=0.9965361952781677; max-choc-feature=0.9965361952781677; \n",
      "action's feature value=0.959436297416687; max-choc-feature=0.959436297416687; \n",
      "action's feature value=0.9982270002365112; max-choc-feature=0.9982270002365112; \n",
      "action's feature value=0.9743911027908325; max-choc-feature=0.9743911027908325; \n",
      "action's feature value=0.9789937138557434; max-choc-feature=0.9789937138557434; \n",
      "action's feature value=0.9521772861480713; max-choc-feature=0.9521772861480713; \n",
      "action's feature value=0.9771995544433594; max-choc-feature=0.9771995544433594; \n",
      "action's feature value=0.986014187335968; max-choc-feature=0.986014187335968; \n",
      "action's feature value=0.890098512172699; max-choc-feature=0.890098512172699; \n",
      "action's feature value=0.9601969718933105; max-choc-feature=0.9601969718933105; \n",
      "action's feature value=0.9544320106506348; max-choc-feature=0.9544320106506348; \n",
      "action's feature value=0.974500298500061; max-choc-feature=0.974500298500061; \n",
      "action's feature value=0.9437193870544434; max-choc-feature=0.9437193870544434; \n",
      "action's feature value=0.9876242280006409; max-choc-feature=0.9876242280006409; \n",
      "action's feature value=0.929969847202301; max-choc-feature=0.929969847202301; \n",
      "action's feature value=0.9930447936058044; max-choc-feature=0.9930447936058044; \n",
      "action's feature value=0.8335474729537964; max-choc-feature=0.8335474729537964; \n",
      "action's feature value=0.8850065469741821; max-choc-feature=0.8850065469741821; \n",
      "action's feature value=0.9805171489715576; max-choc-feature=0.9805171489715576; \n",
      "action's feature value=0.9916070699691772; max-choc-feature=0.9916070699691772; \n",
      "action's feature value=0.9975301623344421; max-choc-feature=0.9975301623344421; \n",
      "action's feature value=0.741567075252533; max-choc-feature=0.741567075252533; \n",
      "action's feature value=0.9793862700462341; max-choc-feature=0.9793862700462341; \n",
      "action's feature value=0.9928807020187378; max-choc-feature=0.9928807020187378; \n",
      "action's feature value=0.8960515260696411; max-choc-feature=0.8960515260696411; \n",
      "action's feature value=0.9533839821815491; max-choc-feature=0.9533839821815491; \n",
      "action's feature value=0.9781012535095215; max-choc-feature=0.9781012535095215; \n",
      "action's feature value=0.993452250957489; max-choc-feature=0.993452250957489; \n",
      "action's feature value=0.9380106329917908; max-choc-feature=0.9380106329917908; \n",
      "action's feature value=0.9502686262130737; max-choc-feature=0.9502686262130737; \n",
      "action's feature value=0.9592873454093933; max-choc-feature=0.9592873454093933; \n",
      "action's feature value=0.9177380204200745; max-choc-feature=0.9177380204200745; \n",
      "action's feature value=0.9741724729537964; max-choc-feature=0.9741724729537964; \n",
      "action's feature value=0.9473077058792114; max-choc-feature=0.9473077058792114; \n",
      "action's feature value=0.9695368409156799; max-choc-feature=0.9695368409156799; \n",
      "action's feature value=0.9961513876914978; max-choc-feature=0.9961513876914978; \n",
      "action's feature value=0.9535254836082458; max-choc-feature=0.9535254836082458; \n",
      "action's feature value=0.9302289485931396; max-choc-feature=0.9302289485931396; \n",
      "action's feature value=0.992776095867157; max-choc-feature=0.992776095867157; \n",
      "action's feature value=0.8897455334663391; max-choc-feature=0.8897455334663391; \n",
      "action's feature value=0.9464811086654663; max-choc-feature=0.9464811086654663; \n",
      "action's feature value=0.9660783410072327; max-choc-feature=0.9660783410072327; \n",
      "action's feature value=0.9881228804588318; max-choc-feature=0.9881228804588318; \n",
      "action's feature value=0.9712324142456055; max-choc-feature=0.9712324142456055; \n",
      "action's feature value=0.8849026560783386; max-choc-feature=0.8849026560783386; \n",
      "action's feature value=0.9298375844955444; max-choc-feature=0.9298375844955444; \n",
      "action's feature value=0.9961418509483337; max-choc-feature=0.9961418509483337; \n",
      "action's feature value=0.9551523327827454; max-choc-feature=0.9551523327827454; \n",
      "action's feature value=0.9550685286521912; max-choc-feature=0.9550685286521912; \n",
      "action's feature value=0.9303013682365417; max-choc-feature=0.9303013682365417; \n",
      "action's feature value=0.9976799488067627; max-choc-feature=0.9976799488067627; \n",
      "action's feature value=0.953008770942688; max-choc-feature=0.953008770942688; \n",
      "action's feature value=0.9415672421455383; max-choc-feature=0.9415672421455383; \n",
      "action's feature value=0.993816614151001; max-choc-feature=0.993816614151001; \n",
      "action's feature value=0.9178575873374939; max-choc-feature=0.9178575873374939; \n",
      "action's feature value=0.9946078062057495; max-choc-feature=0.9946078062057495; \n",
      "action's feature value=0.9887571334838867; max-choc-feature=0.9887571334838867; \n",
      "action's feature value=0.9569783806800842; max-choc-feature=0.9569783806800842; \n",
      "action's feature value=0.9394294619560242; max-choc-feature=0.9394294619560242; \n",
      "action's feature value=0.9068728089332581; max-choc-feature=0.9068728089332581; \n",
      "action's feature value=0.8752159476280212; max-choc-feature=0.8752159476280212; \n",
      "action's feature value=0.9841798543930054; max-choc-feature=0.9841798543930054; \n",
      "action's feature value=0.9476582407951355; max-choc-feature=0.9476582407951355; \n",
      "action's feature value=0.9058099985122681; max-choc-feature=0.9058099985122681; \n",
      "action's feature value=0.9838818311691284; max-choc-feature=0.9838818311691284; \n",
      "action's feature value=0.9809923768043518; max-choc-feature=0.9809923768043518; \n",
      "action's feature value=0.9048473238945007; max-choc-feature=0.9048473238945007; \n",
      "action's feature value=0.9924371838569641; max-choc-feature=0.9924371838569641; \n",
      "action's feature value=0.9532560110092163; max-choc-feature=0.9532560110092163; \n",
      "action's feature value=0.9819613099098206; max-choc-feature=0.9819613099098206; \n",
      "action's feature value=0.9757139682769775; max-choc-feature=0.9757139682769775; \n",
      "action's feature value=0.9919356107711792; max-choc-feature=0.9919356107711792; \n",
      "action's feature value=0.8730562329292297; max-choc-feature=0.8730562329292297; \n",
      "action's feature value=0.8942882418632507; max-choc-feature=0.8942882418632507; \n",
      "action's feature value=0.9817694425582886; max-choc-feature=0.9817694425582886; \n",
      "action's feature value=0.9674445390701294; max-choc-feature=0.9674445390701294; \n",
      "action's feature value=0.9817304015159607; max-choc-feature=0.9817304015159607; \n",
      "action's feature value=0.9277258515357971; max-choc-feature=0.9277258515357971; \n",
      "action's feature value=0.9238638281822205; max-choc-feature=0.9238638281822205; \n",
      "action's feature value=0.9705137610435486; max-choc-feature=0.9705137610435486; \n",
      "action's feature value=0.9426661729812622; max-choc-feature=0.9426661729812622; \n",
      "action's feature value=0.9798500537872314; max-choc-feature=0.9798500537872314; \n",
      "action's feature value=0.9878652691841125; max-choc-feature=0.9878652691841125; \n",
      "action's feature value=0.9590775370597839; max-choc-feature=0.9590775370597839; \n",
      "action's feature value=0.9802720546722412; max-choc-feature=0.9802720546722412; \n",
      "action's feature value=0.9328365921974182; max-choc-feature=0.9328365921974182; \n",
      "action's feature value=0.8970288634300232; max-choc-feature=0.8970288634300232; \n",
      "action's feature value=0.9960023760795593; max-choc-feature=0.9960023760795593; \n",
      "action's feature value=0.998275876045227; max-choc-feature=0.998275876045227; \n",
      "action's feature value=0.9211506247520447; max-choc-feature=0.9211506247520447; \n",
      "action's feature value=0.9987455010414124; max-choc-feature=0.9987455010414124; \n",
      "action's feature value=0.9975762963294983; max-choc-feature=0.9975762963294983; \n",
      "action's feature value=0.9881392121315002; max-choc-feature=0.9881392121315002; \n",
      "action's feature value=0.9476602673530579; max-choc-feature=0.9476602673530579; \n",
      "action's feature value=0.9908797740936279; max-choc-feature=0.9908797740936279; \n"
     ]
    }
   ],
   "source": [
    "# Let's see what items our bandit recommends now that it has been trained and achieves good (>> random) rewards.\n",
    "obs = lts_20_2_env.reset()\n",
    "\n",
    "# Run a single episode.\n",
    "done = False\n",
    "while not done:\n",
    "    # Pass the single (unbatched) observation into the `compute_single_action` method of our Trainer.\n",
    "    # This is one way to perform inference on a learned policy.\n",
    "    action = bandit_trainer.compute_single_action(input_dict={\"obs\": obs})\n",
    "    feat_value_of_action = obs[\"item\"][action][0]\n",
    "    max_choc_feat = obs['item'][np.argmax(obs[\"item\"])][0]\n",
    "\n",
    "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
    "    print(f\"action's feature value={feat_value_of_action}; max-choc-feature={max_choc_feat}; \")\n",
    "\n",
    "    # Apply the computed action in the environment and continue.\n",
    "    obs, r, done, _ = lts_20_2_env.step(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9355c1b-f0f7-4690-a7fe-332b01a651c4",
   "metadata": {},
   "source": [
    "### Ok, Bandits want Chocolate! :)\n",
    "#### Why is that?\n",
    "\n",
    "<img src=\"images/contextual_bandit.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84291b69-050f-489a-b822-239294bb3a2e",
   "metadata": {},
   "source": [
    "### Recap: Advantages and Disatvantages of Bandits:\n",
    "#### Advantages:\n",
    "* Very fast\n",
    "* Very sample-efficient\n",
    "* Easy to understand learning process\n",
    "\n",
    "#### Disadvantages\n",
    "* Need immediate reward (not capable of solving long-horizon credit assignment problem)\n",
    "* Models user -> If > 1 user, must train separate bandit per user\n",
    "* Not able to handle components of MultiDiscrete action space separately (works only on flattened Discrete action space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a830b-cc5f-454c-b986-75c59d40df89",
   "metadata": {},
   "source": [
    "<a id='slateq'></a>\n",
    "### Switching to Slate-Q\n",
    "\n",
    "<img src=\"images/rllib_algorithms_slateq.png\" width=800>\n",
    "\n",
    "RLlib offers another algorithm - Slate-Q - designed for k-slate, long time horizon, and dynamic user recommendation problems. Let's take a quick look:\n",
    "\n",
    "<img src=\"images/slateq.png\" width=1000>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49c258f2-60db-4ed2-8c4f-8f83809d1d73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 10:57:45,811\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-03-29 10:57:45,812\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "WARNING:ray.tune.utils.util:Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SlateQTrainer"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.agents.slateq import SlateQTrainer\n",
    "\n",
    "slateq_config = {\n",
    "    \"env\": \"modified_lts\",\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 20,  # MultiDiscrete([20, 20]) -> no flattening necessary (see `convert_to_discrete_action_space=False` below)\n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "        \"wrap_for_bandits\": False,  # SlateQ != Bandit (will keep \"doc\" key, instead of \"items\")\n",
    "        \"convert_to_discrete_action_space\": False,  # SlateQ handles MultiDiscrete action spaces (slate recommendations).\n",
    "    },\n",
    "    # Setup exploratory behavior: Implemented as \"epsilon greedy\" strategy:\n",
    "    # Act randomly `e` percent of the time; `e` gets reduced from 1.0 to almost 0.0 over\n",
    "    # the course of `epsilon_timesteps`.\n",
    "    \"exploration_config\": {\n",
    "        #\"warmup_timesteps\": 20000,  # default\n",
    "        \"epsilon_timesteps\": 40000,  # default: 250000\n",
    "    },\n",
    "    #\"learning_starts\": 20000,  # default\n",
    "    \"target_network_update_freq\": 3200,\n",
    "\n",
    "    # Report rewards as smoothed mean over this many episodes.\n",
    "    \"metrics_num_episodes_for_smoothing\": 200,\n",
    "}\n",
    "\n",
    "# Instantiate the Trainer object using the exact same config as in our Bandit experiment above.\n",
    "slateq_trainer = SlateQTrainer(config=slateq_config)\n",
    "slateq_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {},
   "source": [
    "<a id='slateq_experiment'></a>\n",
    "Now that we have confirmed we have setup the Trainer correctly, let's call `train()` on it several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc47c75f-4f6f-4806-995e-80ec974cfd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=1; ts=20000: R(\"return\")=1158.07695414925\n",
      "Iteration=2; ts=21000: R(\"return\")=1157.8903765976938\n",
      "Iteration=3; ts=22000: R(\"return\")=1157.9466247850746\n",
      "Iteration=4; ts=23000: R(\"return\")=1158.0313753911357\n",
      "Iteration=5; ts=24000: R(\"return\")=1158.3381739243462\n",
      "Iteration=6; ts=25000: R(\"return\")=1158.2036990060517\n",
      "Iteration=7; ts=26000: R(\"return\")=1158.1490698625867\n",
      "Iteration=8; ts=27000: R(\"return\")=1158.3569562525197\n",
      "Iteration=9; ts=28000: R(\"return\")=1158.9155008888797\n",
      "Iteration=10; ts=29000: R(\"return\")=1158.6946349071636\n",
      "Iteration=11; ts=30000: R(\"return\")=1158.8493776685268\n",
      "Iteration=12; ts=31000: R(\"return\")=1158.8979546261978\n",
      "Iteration=13; ts=32000: R(\"return\")=1159.2062797615288\n",
      "Iteration=14; ts=33000: R(\"return\")=1159.5240533866827\n",
      "Iteration=15; ts=34000: R(\"return\")=1160.0113778021973\n",
      "Iteration=16; ts=35000: R(\"return\")=1160.3101639331983\n",
      "Iteration=17; ts=36000: R(\"return\")=1161.173887314875\n",
      "Iteration=18; ts=37000: R(\"return\")=1161.4775258193392\n",
      "Iteration=19; ts=38000: R(\"return\")=1161.9586539098368\n",
      "Iteration=20; ts=39000: R(\"return\")=1162.7013306278322\n",
      "Iteration=21; ts=40000: R(\"return\")=1163.4610604797351\n",
      "Iteration=22; ts=41000: R(\"return\")=1163.7485775623397\n",
      "Iteration=23; ts=42000: R(\"return\")=1164.4700077977272\n",
      "Iteration=24; ts=43000: R(\"return\")=1165.0411456469096\n",
      "Iteration=25; ts=44000: R(\"return\")=1165.4920204573275\n",
      "Iteration=26; ts=45000: R(\"return\")=1166.3248503203963\n",
      "Iteration=27; ts=46000: R(\"return\")=1166.75350410377\n",
      "Iteration=28; ts=47000: R(\"return\")=1167.0454024468106\n",
      "Iteration=29; ts=48000: R(\"return\")=1167.1485397727122\n",
      "Iteration=30; ts=49000: R(\"return\")=1167.6385325578665\n",
      "Iteration=31; ts=50000: R(\"return\")=1168.2054287860474\n",
      "Iteration=32; ts=51000: R(\"return\")=1168.464746776638\n",
      "Iteration=33; ts=52000: R(\"return\")=1168.4316474387704\n",
      "Iteration=34; ts=53000: R(\"return\")=1168.5857672637853\n",
      "Iteration=35; ts=54000: R(\"return\")=1168.7346509633915\n",
      "Iteration=36; ts=55000: R(\"return\")=1168.837427167555\n",
      "Iteration=37; ts=56000: R(\"return\")=1168.8441006016747\n",
      "Iteration=38; ts=57000: R(\"return\")=1169.0444349782301\n",
      "Iteration=39; ts=58000: R(\"return\")=1168.7395925460776\n",
      "Iteration=40; ts=59000: R(\"return\")=1168.7081169095527\n",
      "Iteration=41; ts=60000: R(\"return\")=1168.207580851124\n",
      "Iteration=42; ts=61000: R(\"return\")=1167.9579098703878\n",
      "Iteration=43; ts=62000: R(\"return\")=1167.7403834747713\n",
      "Iteration=44; ts=63000: R(\"return\")=1167.3520436961435\n",
      "Iteration=45; ts=64000: R(\"return\")=1166.929180993841\n",
      "Iteration=46; ts=65000: R(\"return\")=1166.7448109123989\n",
      "Iteration=47; ts=66000: R(\"return\")=1167.0207939926984\n",
      "Iteration=48; ts=67000: R(\"return\")=1166.2688530890973\n",
      "Iteration=49; ts=68000: R(\"return\")=1166.214961122136\n",
      "Iteration=50; ts=69000: R(\"return\")=1165.7623912601127\n",
      "Iteration=51; ts=70000: R(\"return\")=1165.6766810340357\n",
      "Iteration=52; ts=71000: R(\"return\")=1165.7518643738076\n",
      "Iteration=53; ts=72000: R(\"return\")=1165.9189814071897\n",
      "Iteration=54; ts=73000: R(\"return\")=1165.7419032445434\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mslateq_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mslateq_trainer\u001b[38;5;241m.\u001b[39miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; ts=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimesteps_total\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: R(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/trainable.py:342\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    341\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 342\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:1074\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step_ctx\u001b[38;5;241m.\u001b[39mshould_stop(step_attempt_results):\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m         step_attempt_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_attempt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# @ray.remote RolloutWorker failure.\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1077\u001b[0m         \u001b[38;5;66;03m# Try to recover w/o the failed worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:1155\u001b[0m, in \u001b[0;36mTrainer.step_attempt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;66;03m# No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluate_this_iter:\n\u001b[0;32m-> 1155\u001b[0m     step_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_plan_or_training_iteration_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# We have to evaluate in this training iteration.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;66;03m# No parallelism.\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:2174\u001b[0m, in \u001b[0;36mTrainer._exec_plan_or_training_iteration_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2172\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_iteration()\n\u001b[1;32m   2173\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2174\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_exec_impl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:779\u001b[0m, in \u001b[0;36mLocalIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_once()\n\u001b[0;32m--> 779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilt_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:807\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[1;32m    809\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:869\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[1;32m    871\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:869\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[1;32m    871\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:807\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[1;32m    809\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:869\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[1;32m    871\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:1108\u001b[0m, in \u001b[0;36mLocalIterator.union.<locals>.build_union\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_pull):\n\u001b[0;32m-> 1108\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[1;32m   1110\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:779\u001b[0m, in \u001b[0;36mLocalIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_once()\n\u001b[0;32m--> 779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilt_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:807\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[1;32m    809\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:807\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[1;32m    809\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:807\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[1;32m    809\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/execution/rollout_ops.py:137\u001b[0m, in \u001b[0;36mParallelRollouts.<locals>.sampler\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msampler\u001b[39m(_):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mworkers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:815\u001b[0m, in \u001b[0;36mRolloutWorker.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_start\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    809\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating sample batch of size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    811\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_fragment_length\n\u001b[1;32m    812\u001b[0m         )\n\u001b[1;32m    813\u001b[0m     )\n\u001b[0;32m--> 815\u001b[0m batches \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    816\u001b[0m steps_so_far \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    817\u001b[0m     batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcount\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount_steps_by \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39magent_steps()\n\u001b[1;32m    820\u001b[0m )\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# In truncate_episodes mode, never pull more than 1 batch per env.\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# This avoids over-running the target batch size.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py:116\u001b[0m, in \u001b[0;36mSamplerInput.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;129m@override\u001b[39m(InputReader)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleBatchType:\n\u001b[0;32m--> 116\u001b[0m     batches \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    117\u001b[0m     batches\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extra_batches())\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py:289\u001b[0m, in \u001b[0;36mSyncSampler.get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@override\u001b[39m(SamplerInput)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleBatchType:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 289\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env_runner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, RolloutMetrics):\n\u001b[1;32m    291\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_queue\u001b[38;5;241m.\u001b[39mput(item)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py:702\u001b[0m, in \u001b[0;36m_env_runner\u001b[0;34m(worker, base_env, extra_batch_callback, horizon, normalize_actions, clip_actions, multiple_episodes_in_batch, callbacks, perf_stats, soft_horizon, no_done_at_end, observation_fn, sample_collector, render)\u001b[0m\n\u001b[1;32m    700\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    701\u001b[0m \u001b[38;5;66;03m# types: Dict[PolicyID, Tuple[TensorStructType, StateBatch, dict]]\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43m_do_policy_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactive_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactive_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m perf_stats\u001b[38;5;241m.\u001b[39minference_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t2\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# Process results and update episode state.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py:1162\u001b[0m, in \u001b[0;36m_do_policy_eval\u001b[0;34m(to_eval, policies, sample_collector, active_episodes)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         policy: Policy \u001b[38;5;241m=\u001b[39m _get_or_raise(policies, policy_id)\n\u001b[1;32m   1161\u001b[0m     input_dict \u001b[38;5;241m=\u001b[39m sample_collector\u001b[38;5;241m.\u001b[39mget_inference_input_dict(policy_id)\n\u001b[0;32m-> 1162\u001b[0m     eval_results[policy_id] \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_actions_from_input_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_timestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mactive_episodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_actions_result\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1169\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputs of compute_actions():\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(summarize(eval_results))\n\u001b[1;32m   1171\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/policy/tf_policy.py:318\u001b[0m, in \u001b[0;36mTFPolicy.compute_actions_from_input_dict\u001b[0;34m(self, input_dict, explore, timestep, episodes, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m builder \u001b[38;5;241m=\u001b[39m TFRunBuilder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_session(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_actions_from_input_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    317\u001b[0m obs_batch \u001b[38;5;241m=\u001b[39m input_dict[SampleBatch\u001b[38;5;241m.\u001b[39mOBS]\n\u001b[0;32m--> 318\u001b[0m to_fetch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_compute_actions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# Execute session run to get action (and other fetches).\u001b[39;00m\n\u001b[1;32m    323\u001b[0m fetched \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mget(to_fetch)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/policy/tf_policy.py:1037\u001b[0m, in \u001b[0;36mTFPolicy._build_compute_actions\u001b[0;34m(self, builder, input_dict, obs_batch, state_batches, prev_action_batch, prev_reward_batch, episodes, explore, timestep)\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m input_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dict:\n\u001b[1;32m   1036\u001b[0m             \u001b[38;5;66;03m# Handle complex/nested spaces as well.\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m             \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_feed_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# For policies that inherit directly from TFPolicy.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m     builder\u001b[38;5;241m.\u001b[39madd_feed_dict({\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_input: input_dict[SampleBatch\u001b[38;5;241m.\u001b[39mOBS]})\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/tree/__init__.py:510\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    507\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 510\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/tree/__init__.py:510\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    507\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 510\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/policy/tf_policy.py:1038\u001b[0m, in \u001b[0;36mTFPolicy._build_compute_actions.<locals>.<lambda>\u001b[0;34m(k, v)\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m input_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dict:\n\u001b[1;32m   1036\u001b[0m             \u001b[38;5;66;03m# Handle complex/nested spaces as well.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m             tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m-> 1038\u001b[0m                 \u001b[38;5;28;01mlambda\u001b[39;00m k, v: \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_feed_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1039\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dict[key],\n\u001b[1;32m   1040\u001b[0m                 value,\n\u001b[1;32m   1041\u001b[0m             )\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# For policies that inherit directly from TFPolicy.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m     builder\u001b[38;5;241m.\u001b[39madd_feed_dict({\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_input: input_dict[SampleBatch\u001b[38;5;241m.\u001b[39mOBS]})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "for _ in range(40):\n",
    "    results = slateq_trainer.train()\n",
    "    print(f\"Iteration={slateq_trainer.iteration}; ts={results['timesteps_total']}: R(\\\"return\\\")={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86aecb-90ce-4be1-91a2-5c5391ab6adf",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 15 min break :)\n",
    "\n",
    "while Slate-Q is (hopefully) leaning\n",
    "\n",
    "------------------\n",
    "\n",
    "<a id='slateq_results'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34bc1113-bd5e-45f5-bd8e-93931b8cee0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action's feature value=0.7991585731506348 max-choc-feature=0.978618323802948\n",
      "action's feature value=0.9883738160133362 max-choc-feature=0.9883738160133362\n",
      "action's feature value=0.9767611026763916 max-choc-feature=0.9767611026763916\n",
      "action's feature value=0.8289400339126587 max-choc-feature=0.9292961955070496\n",
      "action's feature value=0.9527490139007568 max-choc-feature=0.9621885418891907\n",
      "action's feature value=0.9560836553573608 max-choc-feature=0.9560836553573608\n",
      "action's feature value=0.9988470077514648 max-choc-feature=0.9988470077514648\n",
      "action's feature value=0.9280812740325928 max-choc-feature=0.9755215048789978\n",
      "action's feature value=0.9443724155426025 max-choc-feature=0.9443724155426025\n",
      "action's feature value=0.8966712951660156 max-choc-feature=0.990338921546936\n",
      "action's feature value=0.9473705887794495 max-choc-feature=0.9527916312217712\n",
      "action's feature value=0.7653252482414246 max-choc-feature=0.961936354637146\n",
      "action's feature value=0.9413776993751526 max-choc-feature=0.9774951338768005\n",
      "action's feature value=0.8221177458763123 max-choc-feature=0.9818294048309326\n",
      "action's feature value=0.774047315120697 max-choc-feature=0.9065554738044739\n",
      "action's feature value=0.7774075865745544 max-choc-feature=0.7885454893112183\n",
      "action's feature value=0.9564056992530823 max-choc-feature=0.9594333171844482\n",
      "action's feature value=0.7567786574363708 max-choc-feature=0.9591665863990784\n",
      "action's feature value=0.9589827060699463 max-choc-feature=0.9589827060699463\n",
      "action's feature value=0.9453015327453613 max-choc-feature=0.9453015327453613\n",
      "action's feature value=0.9903450012207031 max-choc-feature=0.9903450012207031\n",
      "action's feature value=0.9249669313430786 max-choc-feature=0.9920112490653992\n",
      "action's feature value=0.9679655432701111 max-choc-feature=0.9944007992744446\n",
      "action's feature value=0.8967611789703369 max-choc-feature=0.96779465675354\n",
      "action's feature value=0.879234790802002 max-choc-feature=0.9241587519645691\n",
      "action's feature value=0.9488610029220581 max-choc-feature=0.9488610029220581\n",
      "action's feature value=0.8869608044624329 max-choc-feature=0.9627703428268433\n",
      "action's feature value=0.968286395072937 max-choc-feature=0.9805801510810852\n",
      "action's feature value=0.997962236404419 max-choc-feature=0.997962236404419\n",
      "action's feature value=0.9992780089378357 max-choc-feature=0.9992780089378357\n",
      "action's feature value=0.9587409496307373 max-choc-feature=0.9762256741523743\n",
      "action's feature value=0.9834262132644653 max-choc-feature=0.9834262132644653\n",
      "action's feature value=0.8224067091941833 max-choc-feature=0.857124924659729\n",
      "action's feature value=0.8427768349647522 max-choc-feature=0.95914226770401\n",
      "action's feature value=0.7734555602073669 max-choc-feature=0.8562761545181274\n",
      "action's feature value=0.987348735332489 max-choc-feature=0.987348735332489\n",
      "action's feature value=0.9308187365531921 max-choc-feature=0.9707314372062683\n",
      "action's feature value=0.9918903112411499 max-choc-feature=0.9918903112411499\n",
      "action's feature value=0.9325612187385559 max-choc-feature=0.9758838415145874\n",
      "action's feature value=0.7699671983718872 max-choc-feature=0.9543338418006897\n",
      "action's feature value=0.914863109588623 max-choc-feature=0.9323939681053162\n",
      "action's feature value=0.9195073843002319 max-choc-feature=0.9195073843002319\n",
      "action's feature value=0.923305332660675 max-choc-feature=0.9631972908973694\n",
      "action's feature value=0.8667885661125183 max-choc-feature=0.8667885661125183\n",
      "action's feature value=0.9804856777191162 max-choc-feature=0.9804856777191162\n",
      "action's feature value=0.8138801455497742 max-choc-feature=0.9031496644020081\n",
      "action's feature value=0.9738187193870544 max-choc-feature=0.9738187193870544\n",
      "action's feature value=0.9391608834266663 max-choc-feature=0.9998085498809814\n",
      "action's feature value=0.805263876914978 max-choc-feature=0.9384120106697083\n",
      "action's feature value=0.9536756873130798 max-choc-feature=0.9536756873130798\n",
      "action's feature value=0.9438508749008179 max-choc-feature=0.9958152770996094\n",
      "action's feature value=0.9692058563232422 max-choc-feature=0.9692058563232422\n",
      "action's feature value=0.9750946760177612 max-choc-feature=0.9750946760177612\n",
      "action's feature value=0.9670549035072327 max-choc-feature=0.9673377871513367\n",
      "action's feature value=0.9851086735725403 max-choc-feature=0.9851086735725403\n",
      "action's feature value=0.9561231732368469 max-choc-feature=0.9979940056800842\n",
      "action's feature value=0.9555683732032776 max-choc-feature=0.9555683732032776\n",
      "action's feature value=0.983853816986084 max-choc-feature=0.983853816986084\n",
      "action's feature value=0.8668609261512756 max-choc-feature=0.9279761910438538\n",
      "action's feature value=0.8882650136947632 max-choc-feature=0.9421847462654114\n",
      "action's feature value=0.992667019367218 max-choc-feature=0.992667019367218\n",
      "action's feature value=0.9163403511047363 max-choc-feature=0.9717630743980408\n",
      "action's feature value=0.7701281309127808 max-choc-feature=0.9792863130569458\n",
      "action's feature value=0.8451541066169739 max-choc-feature=0.9960712790489197\n",
      "action's feature value=0.9942330718040466 max-choc-feature=0.9942330718040466\n",
      "action's feature value=0.9824448823928833 max-choc-feature=0.9824448823928833\n",
      "action's feature value=0.9492799043655396 max-choc-feature=0.9492799043655396\n",
      "action's feature value=0.9440324902534485 max-choc-feature=0.9568705558776855\n",
      "action's feature value=0.9853785634040833 max-choc-feature=0.9853785634040833\n",
      "action's feature value=0.8986376523971558 max-choc-feature=0.9903685450553894\n",
      "action's feature value=0.944202721118927 max-choc-feature=0.9980227947235107\n",
      "action's feature value=0.8811882734298706 max-choc-feature=0.9376630783081055\n",
      "action's feature value=0.902131199836731 max-choc-feature=0.9834339618682861\n",
      "action's feature value=0.9832748770713806 max-choc-feature=0.9832748770713806\n",
      "action's feature value=0.9792702198028564 max-choc-feature=0.9804664850234985\n",
      "action's feature value=0.8754513263702393 max-choc-feature=0.9825738668441772\n",
      "action's feature value=0.9607664942741394 max-choc-feature=0.9983548521995544\n",
      "action's feature value=0.7697890400886536 max-choc-feature=0.7697890400886536\n",
      "action's feature value=0.9985265731811523 max-choc-feature=0.9985265731811523\n",
      "action's feature value=0.827313244342804 max-choc-feature=0.9906516671180725\n",
      "action's feature value=0.7780388593673706 max-choc-feature=0.9890884160995483\n",
      "action's feature value=0.9596956372261047 max-choc-feature=0.9605224132537842\n",
      "action's feature value=0.8180772066116333 max-choc-feature=0.9260265231132507\n",
      "action's feature value=0.8996517062187195 max-choc-feature=0.9829264879226685\n",
      "action's feature value=0.9454309940338135 max-choc-feature=0.9454309940338135\n",
      "action's feature value=0.9270205497741699 max-choc-feature=0.9270205497741699\n",
      "action's feature value=0.9585323333740234 max-choc-feature=0.9585323333740234\n",
      "action's feature value=0.9738933444023132 max-choc-feature=0.9738933444023132\n",
      "action's feature value=0.904982328414917 max-choc-feature=0.9793245196342468\n",
      "action's feature value=0.9559617638587952 max-choc-feature=0.9890880584716797\n",
      "action's feature value=0.10128425061702728 max-choc-feature=0.9670467972755432\n",
      "action's feature value=0.9301263689994812 max-choc-feature=0.9703752994537354\n",
      "action's feature value=0.9550641179084778 max-choc-feature=0.9550641179084778\n",
      "action's feature value=0.9574885368347168 max-choc-feature=0.9574885368347168\n",
      "action's feature value=0.7468338012695312 max-choc-feature=0.9229142665863037\n",
      "action's feature value=0.9832027554512024 max-choc-feature=0.9832027554512024\n",
      "action's feature value=0.9427794218063354 max-choc-feature=0.998198926448822\n",
      "action's feature value=0.9634699821472168 max-choc-feature=0.9642097353935242\n",
      "action's feature value=0.9004101753234863 max-choc-feature=0.9417421221733093\n",
      "action's feature value=0.9040508270263672 max-choc-feature=0.9040508270263672\n",
      "action's feature value=0.9788569808006287 max-choc-feature=0.9835554361343384\n",
      "action's feature value=0.9334558844566345 max-choc-feature=0.9747744202613831\n",
      "action's feature value=0.5955850481987 max-choc-feature=0.8408302664756775\n",
      "action's feature value=0.9682117700576782 max-choc-feature=0.9682117700576782\n",
      "action's feature value=0.9669559597969055 max-choc-feature=0.974723219871521\n",
      "action's feature value=0.7080749273300171 max-choc-feature=0.9895250797271729\n",
      "action's feature value=0.9818642139434814 max-choc-feature=0.9818642139434814\n",
      "action's feature value=0.987434983253479 max-choc-feature=0.987434983253479\n",
      "action's feature value=0.9674006104469299 max-choc-feature=0.9674006104469299\n",
      "action's feature value=0.9522157907485962 max-choc-feature=0.9522157907485962\n",
      "action's feature value=0.9103958606719971 max-choc-feature=0.9649279713630676\n",
      "action's feature value=0.9670055508613586 max-choc-feature=0.9949018955230713\n",
      "action's feature value=0.8948697447776794 max-choc-feature=0.9630938768386841\n",
      "action's feature value=0.9801590442657471 max-choc-feature=0.9801590442657471\n",
      "action's feature value=0.9809793829917908 max-choc-feature=0.9809793829917908\n",
      "action's feature value=0.9905391931533813 max-choc-feature=0.9905391931533813\n",
      "action's feature value=0.9747872352600098 max-choc-feature=0.9747872352600098\n",
      "action's feature value=0.9961004257202148 max-choc-feature=0.9961004257202148\n",
      "action's feature value=0.7503432035446167 max-choc-feature=0.9473085999488831\n",
      "action's feature value=0.8309087753295898 max-choc-feature=0.9615751504898071\n"
     ]
    }
   ],
   "source": [
    "lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config=slateq_config[\"env_config\"]))\n",
    "\n",
    "obs = lts_20_2_env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = slateq_trainer.compute_single_action(input_dict={\"obs\": obs})\n",
    "    feat_value_of_action = obs[\"doc\"][str(action[0])][0]\n",
    "    max_feat_action = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    max_choc_feat = obs['doc'][str(max_feat_action)][0]\n",
    "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
    "    print(f\"action's feature value={feat_value_of_action} max-choc-feature={max_choc_feat}\")\n",
    "    \n",
    "    obs, r, done, _ = lts_20_2_env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK!\n",
    "\n",
    "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using the SlateQ Policy and its NN models inside our Trainer object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2b62d74-6392-453a-b25f-f8cbc90009d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Policy right now is: SlateQTFPolicy\n",
      "Our Policy's observation space is: Dict(user:Box([0.], [1.], (1,), float32), doc:Dict(0:Box([0.], [1.], (1,), float32), 1:Box([0.], [1.], (1,), float32), 2:Box([0.], [1.], (1,), float32), 3:Box([0.], [1.], (1,), float32), 4:Box([0.], [1.], (1,), float32), 5:Box([0.], [1.], (1,), float32), 6:Box([0.], [1.], (1,), float32), 7:Box([0.], [1.], (1,), float32), 8:Box([0.], [1.], (1,), float32), 9:Box([0.], [1.], (1,), float32), 10:Box([0.], [1.], (1,), float32), 11:Box([0.], [1.], (1,), float32), 12:Box([0.], [1.], (1,), float32), 13:Box([0.], [1.], (1,), float32), 14:Box([0.], [1.], (1,), float32), 15:Box([0.], [1.], (1,), float32), 16:Box([0.], [1.], (1,), float32), 17:Box([0.], [1.], (1,), float32), 18:Box([0.], [1.], (1,), float32), 19:Box([0.], [1.], (1,), float32)), response:Tuple(Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), float32)), Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), float32))))\n",
      "\n",
      "Our Policy's action space is: MultiDiscrete([20 20])\n",
      "\n",
      "q_values_per_candidate=[[25.567028 25.959293 25.89232  25.177546 25.032394 25.270922 25.178589\n",
      "  25.380077 25.23824  25.733046 25.32251  25.40268  25.872583 25.169783\n",
      "  25.459785 25.280615 25.449635 25.241138 24.967045 25.36419 ]]\n"
     ]
    }
   ],
   "source": [
    "# To get the policy inside the Trainer, use `Trainer.get_policy([policy ID]=\"default_policy\")`:\n",
    "policy = slateq_trainer.get_policy()\n",
    "print(f\"Our Policy right now is: {policy}\")\n",
    "\n",
    "# To get to the model inside any policy, do:\n",
    "model = policy.model\n",
    "#print(f\"Our Policy's model is: {model}\")\n",
    "\n",
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\\n\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\\n\")\n",
    "\n",
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = lts_20_2_env.observation_space.sample()\n",
    "\n",
    "# tf-specific code: Use tf1.Session().\n",
    "sess = policy.get_session()\n",
    "\n",
    "# Get the action logits (as torch tensor).\n",
    "with sess.graph.as_default():\n",
    "    q_values_per_candidate = model.q_value_head([\n",
    "        np.expand_dims(obs[\"user\"], 0),\n",
    "        np.expand_dims(np.concatenate([value for value in obs[\"doc\"].values()]), 0),\n",
    "    ])\n",
    "print(f\"q_values_per_candidate={sess.run(q_values_per_candidate)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddf1390-bdea-412a-be31-ebbbc6f4ac72",
   "metadata": {},
   "source": [
    "#### !END: OPTIONAL HACK!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de603d14-f0cb-4363-a72b-8f147c094071",
   "metadata": {},
   "source": [
    "In order to release all resources from a Trainer, you can use a Trainer's `stop()` method.\n",
    "You should definitley run this cell as it frees resources that we'll need later in this tutorial, when we'll do parallel hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "737dca4f-942f-4fda-abcc-0052263a103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 11:14:56,681\tINFO services.py:1490 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "# In order to release resources that a Trainer uses, you can call its `stop()` method:\n",
    "slateq_trainer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce74b8-20ed-43c5-ad88-54a2dec32f71",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recap: Advantages and Disadvantages of SlateQ:\n",
    "#### Advantages:\n",
    "* Decomposes MultiDiscrete action space (better understanding of items inside a k-slate)\n",
    "* Handles long-horizon credit assignment better than bandits (Q-learning)\n",
    "* Handles > 1 user problems\n",
    "* Sample efficient (due to replay buffer + off-policy DQN-style learning)\n",
    "\n",
    "#### Disadvantages\n",
    "* Uses larger (deep) model(s): One Q-value NN head per candidate\n",
    "* Slower and heavier feel to it\n",
    "* Requires careful hyperparameter-tuning, e.g. exploration timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82057-6b4c-4075-bd32-93c3426a1700",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='offline_rl'></a>\n",
    "## Introduction to Offline RL\n",
    "\n",
    "<img src=\"images/offline_rl.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5114691-b74f-4b4e-8bdb-5df704bee067",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The logdir contains the following files:\n",
      "['README.py',\n",
      " 'output-2022-03-28_16-43-43_worker-2_0.json',\n",
      " '.ipynb_checkpoints']\n",
      "\n",
      "\n",
      "The JSON file with all sampled trajectories is:\n",
      "offline_rl/output-2022-03-28_16-43-43_worker-2_0.json\n"
     ]
    }
   ],
   "source": [
    "# The previous tune.run (the one we did before the break) produced \"historic data\" output.\n",
    "# We will use this output in the following as input to a newly initialized, untrained offline RL algorithm.\n",
    "\n",
    "# Let's take a look at the generated file(s) first:\n",
    "output_dir = \"offline_rl/\"\n",
    "\n",
    "# Here is what the best log directory contains:\n",
    "print(\"\\n\\nThe logdir contains the following files:\")\n",
    "all_output_files = os.listdir(os.path.dirname(output_dir + \"/\"))\n",
    "pprint(all_output_files)\n",
    "\n",
    "json_output_file = os.path.join(output_dir, [f for f in all_output_files if re.match(\"^.*worker.*\\.json$\", f)][0])\n",
    "print(\"\\n\\nThe JSON file with all sampled trajectories is:\")\n",
    "print(json_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4230f",
   "metadata": {},
   "source": [
    "### Using an (offline) input file with an offline RL algorithm.\n",
    "\n",
    "We will now pretend that we don't have a simulator for our problem (same recommender system problem as above) available, however, let's assume we possess a lot of pre-recorded, historic data from some legacy (non-RL) system.\n",
    "\n",
    "Assuming that this legacy system wrote some data into a JSON file (we'll simply use the same JSON file that our SlateQ algo produced above), how can we use this historic data to do RL either way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34a0aaea-b811-41e1-9e6c-532d9ce1b060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>obs</th>\n",
       "      <th>actions</th>\n",
       "      <th>prev_actions</th>\n",
       "      <th>rewards</th>\n",
       "      <th>prev_rewards</th>\n",
       "      <th>dones</th>\n",
       "      <th>eps_id</th>\n",
       "      <th>unroll_id</th>\n",
       "      <th>agent_index</th>\n",
       "      <th>t</th>\n",
       "      <th>action_dist_inputs</th>\n",
       "      <th>action_logp</th>\n",
       "      <th>action_prob</th>\n",
       "      <th>advantages</th>\n",
       "      <th>value_targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACsI0kAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[18, 2], [2, 15], [11, 14], [8, 16], [4, 18],...</td>\n",
       "      <td>[[18, 2], [18, 2], [2, 15], [11, 14], [8, 16],...</td>\n",
       "      <td>[23.29007911682129, 7.152186393737793, 2.76373...</td>\n",
       "      <td>[23.29007911682129, 23.29007911682129, 7.15218...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[542138261, 542138261, 542138261, 542138261, 5...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[[-0.006720410659909, 0.014437448233366, -0.00...</td>\n",
       "      <td>[-5.9771623611450195, -6.004640102386475, -5.9...</td>\n",
       "      <td>[0.0025360123254350004, 0.0024672772269690004,...</td>\n",
       "      <td>[932.8204345703125, 918.7171630859375, 920.772...</td>\n",
       "      <td>[932.8135986328125, 918.7106323242188, 920.766...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACs/0cAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[13, 3], [16, 17], [7, 17], [8, 7], [4, 5], [...</td>\n",
       "      <td>[[14, 1], [13, 3], [16, 17], [7, 17], [8, 7], ...</td>\n",
       "      <td>[5.713678359985352, 18.23175048828125, 6.07325...</td>\n",
       "      <td>[21.042491912841797, 5.713678359985352, 18.231...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[1118934910, 1118934910, 1118934910, 111893491...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3...</td>\n",
       "      <td>[[-0.003344316966831, -0.002333797048777, 0.00...</td>\n",
       "      <td>[-6.002190589904785, -5.994460582733154, -5.98...</td>\n",
       "      <td>[0.002473328262567, 0.0024925211910150004, 0.0...</td>\n",
       "      <td>[515.4043579101562, 514.835205078125, 501.6228...</td>\n",
       "      <td>[515.3938598632812, 514.8284301757812, 501.612...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACsX0kAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[3, 17], [6, 3], [15, 17], [11, 2], [0, 12], ...</td>\n",
       "      <td>[[10, 1], [3, 17], [6, 3], [15, 17], [11, 2], ...</td>\n",
       "      <td>[14.05547046661377, 20.706499099731445, 34.158...</td>\n",
       "      <td>[4.046912670135498, 14.05547046661377, 20.7064...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[1466202559, 1466202559, 1466202559, 146620255...</td>\n",
       "      <td>[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5...</td>\n",
       "      <td>[[-0.003262223675847, 0.008641279302537, -0.00...</td>\n",
       "      <td>[-5.993566513061523, -5.996161460876465, -5.98...</td>\n",
       "      <td>[0.002494750544428, 0.0024882853031150003, 0.0...</td>\n",
       "      <td>[221.78857421875, 209.83851623535156, 191.0375...</td>\n",
       "      <td>[221.7843475341797, 209.8271484375, 191.030960...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACsCkkAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[10, 19], [8, 3], [18, 8], [8, 7], [16, 2], [...</td>\n",
       "      <td>[[10, 19], [10, 19], [8, 3], [18, 8], [8, 7], ...</td>\n",
       "      <td>[34.26883316040039, 21.044775009155273, 8.3825...</td>\n",
       "      <td>[34.26883316040039, 34.26883316040039, 21.0447...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[1418911249, 1418911249, 1418911249, 141891124...</td>\n",
       "      <td>[12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[[-0.008724463172256001, 0.0040771835483610006...</td>\n",
       "      <td>[-5.995693683624268, -5.9909610748291025, -5.9...</td>\n",
       "      <td>[0.002489449456334, 0.002501259092241, 0.00250...</td>\n",
       "      <td>[658.6182861328125, 630.65478515625, 615.76763...</td>\n",
       "      <td>[658.6080322265625, 630.6456298828125, 615.758...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACsjkkAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[1, 8], [2, 19], [9, 1], [1, 16], [5, 2], [15...</td>\n",
       "      <td>[[6, 5], [1, 8], [2, 19], [9, 1], [1, 16], [5,...</td>\n",
       "      <td>[3.041858196258545, 8.18496322631836, 62.71932...</td>\n",
       "      <td>[3.607390880584717, 3.041858196258545, 8.18496...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[450621694, 450621694, 450621694, 450621694, 4...</td>\n",
       "      <td>[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3...</td>\n",
       "      <td>[[-0.002998510375618, -0.00399568863213, -0.00...</td>\n",
       "      <td>[-5.999508380889893, -5.996327877044678, -5.98...</td>\n",
       "      <td>[0.0024799711536610002, 0.0024878710974000004,...</td>\n",
       "      <td>[455.0074157714844, 456.52606201171875, 452.87...</td>\n",
       "      <td>[454.9999084472656, 456.5232849121094, 452.866...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                                                obs  \\\n",
       "0  SampleBatch  BCJNGGhAwFEAAAAAAACsI0kAAGGABZW1UQABAPMZjBJudW...   \n",
       "1  SampleBatch  BCJNGGhAwFEAAAAAAACs/0cAAGGABZW1UQABAPMZjBJudW...   \n",
       "2  SampleBatch  BCJNGGhAwFEAAAAAAACsX0kAAGGABZW1UQABAPMZjBJudW...   \n",
       "3  SampleBatch  BCJNGGhAwFEAAAAAAACsCkkAAGGABZW1UQABAPMZjBJudW...   \n",
       "4  SampleBatch  BCJNGGhAwFEAAAAAAACsjkkAAGGABZW1UQABAPMZjBJudW...   \n",
       "\n",
       "                                             actions  \\\n",
       "0  [[18, 2], [2, 15], [11, 14], [8, 16], [4, 18],...   \n",
       "1  [[13, 3], [16, 17], [7, 17], [8, 7], [4, 5], [...   \n",
       "2  [[3, 17], [6, 3], [15, 17], [11, 2], [0, 12], ...   \n",
       "3  [[10, 19], [8, 3], [18, 8], [8, 7], [16, 2], [...   \n",
       "4  [[1, 8], [2, 19], [9, 1], [1, 16], [5, 2], [15...   \n",
       "\n",
       "                                        prev_actions  \\\n",
       "0  [[18, 2], [18, 2], [2, 15], [11, 14], [8, 16],...   \n",
       "1  [[14, 1], [13, 3], [16, 17], [7, 17], [8, 7], ...   \n",
       "2  [[10, 1], [3, 17], [6, 3], [15, 17], [11, 2], ...   \n",
       "3  [[10, 19], [10, 19], [8, 3], [18, 8], [8, 7], ...   \n",
       "4  [[6, 5], [1, 8], [2, 19], [9, 1], [1, 16], [5,...   \n",
       "\n",
       "                                             rewards  \\\n",
       "0  [23.29007911682129, 7.152186393737793, 2.76373...   \n",
       "1  [5.713678359985352, 18.23175048828125, 6.07325...   \n",
       "2  [14.05547046661377, 20.706499099731445, 34.158...   \n",
       "3  [34.26883316040039, 21.044775009155273, 8.3825...   \n",
       "4  [3.041858196258545, 8.18496322631836, 62.71932...   \n",
       "\n",
       "                                        prev_rewards  \\\n",
       "0  [23.29007911682129, 23.29007911682129, 7.15218...   \n",
       "1  [21.042491912841797, 5.713678359985352, 18.231...   \n",
       "2  [4.046912670135498, 14.05547046661377, 20.7064...   \n",
       "3  [34.26883316040039, 34.26883316040039, 21.0447...   \n",
       "4  [3.607390880584717, 3.041858196258545, 8.18496...   \n",
       "\n",
       "                                               dones  \\\n",
       "0  [False, False, False, False, False, False, Fal...   \n",
       "1  [False, False, False, False, False, False, Fal...   \n",
       "2  [False, False, False, False, False, False, Fal...   \n",
       "3  [False, False, False, False, False, False, Fal...   \n",
       "4  [False, False, False, False, False, False, Fal...   \n",
       "\n",
       "                                              eps_id  \\\n",
       "0  [542138261, 542138261, 542138261, 542138261, 5...   \n",
       "1  [1118934910, 1118934910, 1118934910, 111893491...   \n",
       "2  [1466202559, 1466202559, 1466202559, 146620255...   \n",
       "3  [1418911249, 1418911249, 1418911249, 141891124...   \n",
       "4  [450621694, 450621694, 450621694, 450621694, 4...   \n",
       "\n",
       "                                           unroll_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "2  [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...   \n",
       "3  [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1...   \n",
       "4  [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1...   \n",
       "\n",
       "                                         agent_index  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                   t  \\\n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "1  [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3...   \n",
       "2  [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5...   \n",
       "3  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "4  [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3...   \n",
       "\n",
       "                                  action_dist_inputs  \\\n",
       "0  [[-0.006720410659909, 0.014437448233366, -0.00...   \n",
       "1  [[-0.003344316966831, -0.002333797048777, 0.00...   \n",
       "2  [[-0.003262223675847, 0.008641279302537, -0.00...   \n",
       "3  [[-0.008724463172256001, 0.0040771835483610006...   \n",
       "4  [[-0.002998510375618, -0.00399568863213, -0.00...   \n",
       "\n",
       "                                         action_logp  \\\n",
       "0  [-5.9771623611450195, -6.004640102386475, -5.9...   \n",
       "1  [-6.002190589904785, -5.994460582733154, -5.98...   \n",
       "2  [-5.993566513061523, -5.996161460876465, -5.98...   \n",
       "3  [-5.995693683624268, -5.9909610748291025, -5.9...   \n",
       "4  [-5.999508380889893, -5.996327877044678, -5.98...   \n",
       "\n",
       "                                         action_prob  \\\n",
       "0  [0.0025360123254350004, 0.0024672772269690004,...   \n",
       "1  [0.002473328262567, 0.0024925211910150004, 0.0...   \n",
       "2  [0.002494750544428, 0.0024882853031150003, 0.0...   \n",
       "3  [0.002489449456334, 0.002501259092241, 0.00250...   \n",
       "4  [0.0024799711536610002, 0.0024878710974000004,...   \n",
       "\n",
       "                                          advantages  \\\n",
       "0  [932.8204345703125, 918.7171630859375, 920.772...   \n",
       "1  [515.4043579101562, 514.835205078125, 501.6228...   \n",
       "2  [221.78857421875, 209.83851623535156, 191.0375...   \n",
       "3  [658.6182861328125, 630.65478515625, 615.76763...   \n",
       "4  [455.0074157714844, 456.52606201171875, 452.87...   \n",
       "\n",
       "                                       value_targets  \n",
       "0  [932.8135986328125, 918.7106323242188, 920.766...  \n",
       "1  [515.3938598632812, 514.8284301757812, 501.612...  \n",
       "2  [221.7843475341797, 209.8271484375, 191.030960...  \n",
       "3  [658.6080322265625, 630.6456298828125, 615.758...  \n",
       "4  [454.9999084472656, 456.5232849121094, 452.866...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the output file first:\n",
    "dataframe = pandas.read_json(json_output_file, lines=True)  # don't forget lines=True -> Each line in the json is one \"rollout\" of 4 timesteps.\n",
    "dataframe.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48768f8f-25ad-4fee-92d2-5f99f34295f9",
   "metadata": {},
   "source": [
    "<a id='bc_and_marwil'></a>\n",
    "### Picking an offline RL algorithm\n",
    "\n",
    "RLlib offers different offline specialized algorithms, such as Behavior Cloning (imitation learning), MARWIL, or CQL.\n",
    "\n",
    "<img src=\"images/rllib_algorithms_offline_rl.png\" width=800>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01e7b767-d11e-4b2d-bf86-bf88f06f8146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 11:15:09,118\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_timesteps_total': 200,\n",
      " 'custom_metrics': {},\n",
      " 'date': '2022-03-29_11-15-09',\n",
      " 'done': False,\n",
      " 'episode_len_mean': nan,\n",
      " 'episode_media': {},\n",
      " 'episode_reward_max': nan,\n",
      " 'episode_reward_mean': nan,\n",
      " 'episode_reward_min': nan,\n",
      " 'episodes_this_iter': 0,\n",
      " 'episodes_total': 0,\n",
      " 'experiment_id': 'd9316d0617834f928311d646a3408dfd',\n",
      " 'hist_stats': {'episode_lengths': [], 'episode_reward': []},\n",
      " 'hostname': 'Svens-MacBook-Pro.local',\n",
      " 'info': {'learner': {'default_policy': {'custom_metrics': {},\n",
      "                                         'learner_stats': {'allreduce_latency': 0.0,\n",
      "                                                           'grad_gnorm': array(0.9247925, dtype=float32),\n",
      "                                                           'policy_loss': 5.991230010986328,\n",
      "                                                           'total_loss': 5.991230010986328},\n",
      "                                         'model': {},\n",
      "                                         'num_agent_steps_trained': 2000}},\n",
      "          'num_agent_steps_sampled': 200,\n",
      "          'num_agent_steps_trained': 2000,\n",
      "          'num_steps_sampled': 200,\n",
      "          'num_steps_trained': 2000,\n",
      "          'num_steps_trained_this_iter': 2000},\n",
      " 'iterations_since_restore': 1,\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_healthy_workers': 0,\n",
      " 'off_policy_estimator': {'is': {'V_gain_est': 1.0134784277118198,\n",
      "                                 'V_prev': 640.7169325047403,\n",
      "                                 'V_step_IS': 647.3491802276093},\n",
      "                          'wis': {'V_gain_est': 1.0080572574146194,\n",
      "                                  'V_prev': 640.7169325047403,\n",
      "                                  'V_step_WIS': 646.051093706985}},\n",
      " 'perf': {'cpu_util_percent': 6.1, 'ram_util_percent': 65.9},\n",
      " 'pid': 5594,\n",
      " 'policy_reward_max': {},\n",
      " 'policy_reward_mean': {},\n",
      " 'policy_reward_min': {},\n",
      " 'sampler_perf': {},\n",
      " 'time_since_restore': 0.1676630973815918,\n",
      " 'time_this_iter_s': 0.1676630973815918,\n",
      " 'time_total_s': 0.1676630973815918,\n",
      " 'timers': {'learn_throughput': 43405.591,\n",
      "            'learn_time_ms': 46.077,\n",
      "            'sample_throughput': 14539.725,\n",
      "            'sample_time_ms': 137.554},\n",
      " 'timestamp': 1648545309,\n",
      " 'timesteps_since_restore': 2000,\n",
      " 'timesteps_this_iter': 2000,\n",
      " 'timesteps_total': 200,\n",
      " 'training_iteration': 1,\n",
      " 'trial_id': 'default',\n",
      " 'warmup_time': 0.12243199348449707}\n",
      "Parameter containing:\n",
      "tensor([[-1.1867e-04, -6.4889e-04,  4.3922e-04,  ..., -7.6761e-04,\n",
      "         -1.4275e-04,  1.6096e-03],\n",
      "        [ 1.2910e-03,  3.8758e-04, -3.5441e-04,  ..., -5.3761e-04,\n",
      "          2.2055e-04, -1.2105e-04],\n",
      "        [-3.2762e-04, -3.6650e-04, -9.3771e-05,  ..., -5.7043e-05,\n",
      "         -2.1970e-04, -6.3845e-04],\n",
      "        ...,\n",
      "        [-3.0905e-04,  1.6025e-04,  9.2990e-04,  ...,  4.6150e-04,\n",
      "         -4.4965e-04, -4.3836e-04],\n",
      "        [-8.8701e-04, -8.6069e-04,  1.0044e-04,  ...,  2.4873e-04,\n",
      "          4.2824e-04,  1.3847e-04],\n",
      "        [ 5.9810e-05, -3.3555e-04, -8.6056e-04,  ..., -6.4227e-04,\n",
      "         -9.1245e-04,  3.1956e-04]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Let's configure a new RLlib Trainer, one that's capable of reading the JSON input described\n",
    "# above and able to learn from this input.\n",
    "\n",
    "# For simplicity, we'll start with a behavioral cloning (BC) trainer:\n",
    "from ray.rllib.agents.marwil import BCTrainer\n",
    "\n",
    "offline_rl_env = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 20,\n",
    "    \"slate_size\": 2,\n",
    "    \"wrap_for_bandits\": False,  # SlateQ != Bandit\n",
    "    \"convert_to_discrete_action_space\": False,\n",
    "})\n",
    "\n",
    "\n",
    "# Configuring the BCTrainer:\n",
    "offline_rl_config = {\n",
    "    # Specify your offline RL algo's historic (JSON) inputs:\n",
    "    \"input\": [json_output_file],\n",
    "    \"actions_in_input_normalized\": True,\n",
    "    # Note: For non-offline RL algos, this is set to \"sampler\" by default.\n",
    "    #\"input\": \"sampler\",\n",
    "\n",
    "    # Since we don't have an environment and the obs/action-spaces are not defined in the JSON file,\n",
    "    # we need to provide these here manually.\n",
    "    \"env\": None,  # default\n",
    "    \"observation_space\": offline_rl_env.observation_space,\n",
    "    \"action_space\": offline_rl_env.action_space,\n",
    "\n",
    "    # Perform \"off-policy estimation\" (OPE) on train batches and report results.\n",
    "    \"input_evaluation\": [\"is\", \"wis\"],\n",
    "}\n",
    "\n",
    "# Create a behavior cloning (BC) Trainer from our config.\n",
    "bc_trainer = BCTrainer(config=offline_rl_config)\n",
    "bc_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d181dd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "688000 steps trained; reward = nan\n",
      "690000 steps trained; reward = nan\n",
      "692000 steps trained; reward = nan\n",
      "694000 steps trained; reward = nan\n",
      "696000 steps trained; reward = nan\n",
      "698000 steps trained; reward = nan\n",
      "700000 steps trained; reward = nan\n",
      "702000 steps trained; reward = nan\n",
      "704000 steps trained; reward = nan\n",
      "706000 steps trained; reward = nan\n"
     ]
    }
   ],
   "source": [
    "# Let's train our new behavioral cloning Trainer for some iterations:\n",
    "for _ in range(10):\n",
    "    results = bc_trainer.train()\n",
    "    print(f\"{results['info']['num_agent_steps_trained']} steps trained; reward = {results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3e48b13b-5b2d-4e75-9b50-c1a6b45c253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708000 steps trained; loss = 5.991226673126221\n",
      "710000 steps trained; loss = 5.991433143615723\n",
      "712000 steps trained; loss = 5.991380214691162\n",
      "714000 steps trained; loss = 5.991509914398193\n",
      "716000 steps trained; loss = 5.9915900230407715\n",
      "718000 steps trained; loss = 5.991552829742432\n",
      "720000 steps trained; loss = 5.9914445877075195\n",
      "722000 steps trained; loss = 5.991354465484619\n",
      "724000 steps trained; loss = 5.991418838500977\n",
      "726000 steps trained; loss = 5.991578102111816\n"
     ]
    }
   ],
   "source": [
    "# Oh no! What happened?\n",
    "# We don't have an environment! No way to measure rewards per episode.\n",
    "\n",
    "# For behavior cloning, simply looking at the loss as a measurement of progress would be a good idea:\n",
    "for _ in range(10):\n",
    "    results = bc_trainer.train()\n",
    "    print(f\"{results['info']['num_agent_steps_trained']} steps trained; loss = {results['info']['learner']['default_policy']['learner_stats']['total_loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb18e474-c5d0-45be-97e3-932482bc1cbb",
   "metadata": {},
   "source": [
    "<a id='ope'></a>\n",
    "### Off Policy Estimators\n",
    "Also: `results` still holds the last output of our non-evaluation BCTrainer.\n",
    "\n",
    "Extract off-policy estimator (OPE) results for the different methods:\n",
    "* 'is'=importance sampling\n",
    "<img src=\"images/ope.png\" width=400>\n",
    "\n",
    "* 'wis'=weighted importance sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db41f0d5-7388-42b3-bd49-e82789011d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS off-policy estimation: {'V_prev': 611.6802869584454, 'V_step_IS': 969.3242267673241, 'V_gain_est': 1.5304956863771924}\n",
      "WIS off-policy estimation: {'V_prev': 608.2245320277681, 'V_step_WIS': 843.0467949826555, 'V_gain_est': 1.3482511404139497}\n"
     ]
    }
   ],
   "source": [
    "ope_results = results['off_policy_estimator']\n",
    "print(f\"IS off-policy estimation: {ope_results['is']}\")\n",
    "print(f\"WIS off-policy estimation: {ope_results['wis']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd66c3-f07a-4795-84ea-6b232ba6a047",
   "metadata": {},
   "source": [
    "### Saving and restoring a trained Trainer.\n",
    "Currently, `bc_trainer` is in an already trained state.\n",
    "It holds optimized weights in its Q-value/Policy's models that allow it to act\n",
    "already somewhat smart in our environment when given an observation.\n",
    "\n",
    "However, if we closed this notebook right now, all the effort would have been for nothing.\n",
    "Let's therefore save the state of our trainer to disk for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "57eae1e4-3cc4-4282-9a83-bc374bdad978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer (at iteration 363 was saved in '/Users/sven/ray_results/BCTrainer_None_2022-03-29_11-15-08poelm_x7/checkpoint_000363/checkpoint-363'!\n",
      "The checkpoint directory contains the following files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.is_checkpoint', 'checkpoint-363', 'checkpoint-363.tune_metadata']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the `Trainer.save()` method to create a checkpoint.\n",
    "checkpoint_file = bc_trainer.save()\n",
    "print(f\"Trainer (at iteration {bc_trainer.iteration} was saved in '{checkpoint_file}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "os.listdir(os.path.dirname(checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e0ab-2c10-469a-97b1-4aadf1a1ec97",
   "metadata": {},
   "source": [
    "### Restoring and evaluating a Trainer\n",
    "In the following cell, we'll learn how to restore a saved Trainer from a checkpoint file.\n",
    "\n",
    "We'll also evaluate a completely new Trainer (should act more or less randomly) vs an already trained one (the one we just restored from the created checkpoint file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "74ceedb9-c225-46f2-ad1d-f902c81d3256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 11:40:10,171\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n",
      "2022-03-29 11:40:10,201\tINFO trainable.py:521 -- Restored on 127.0.0.1 from checkpoint: /Users/sven/ray_results/BCTrainer_None_2022-03-29_11-15-08poelm_x7/checkpoint_000363/checkpoint-363\n",
      "2022-03-29 11:40:10,202\tINFO trainable.py:530 -- Current state after restoring: {'_iteration': 363, '_timesteps_total': 726000, '_time_total': 62.90191435813904, '_episodes_total': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before restoring: Trainer is at iteration=0\n",
      "After restoring: Trainer is at iteration=363\n"
     ]
    }
   ],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_bc_trainer = BCTrainer(config=offline_rl_config)\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "print(f\"Before restoring: Trainer is at iteration={new_bc_trainer.iteration}\")\n",
    "new_bc_trainer.restore(checkpoint_file)\n",
    "print(f\"After restoring: Trainer is at iteration={new_bc_trainer.iteration}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147db25-4202-433e-aadb-c4fadbdfcbea",
   "metadata": {},
   "source": [
    "<a id='ray_serve'></a>\n",
    "### Deploying a trained policy via Ray Serve\n",
    "In the following cell, we'll learn how to use Ray Serve to deploy a trained policy, e.g. in your production for inference (computing actions; no further learning):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "adadb612-9578-46de-a9e8-67724045c801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m [2022-03-29 11:41:48,243 E 6220 220549] (gcs_server) core_worker_client.h:302: Push normal task\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m 2022-03-29 11:41:48,471\tINFO checkpoint_path.py:15 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "[2022-03-29 11:41:48,600 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m [2022-03-29 11:41:48,595 E 6220 220549] (gcs_server) core_worker_client.h:302: Push normal task\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m 2022-03-29 11:41:48,580\tINFO http_state.py:102 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:BZOVWe:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "[2022-03-29 11:41:48,764 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "[2022-03-29 11:41:48,766 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "[2022-03-29 11:41:48,767 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "[2022-03-29 11:41:48,768 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "2022-03-29 11:41:48,768\tINFO api.py:708 -- Started Serve instance in namespace '2c24f6a8-4421-4a05-8e98-7956fcdef316'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:41:48,742 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m INFO:     Started server process [6242]\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:41:48,762 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "[2022-03-29 11:41:48,775 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "2022-03-29 11:41:48,780\tINFO api.py:541 -- Updating deployment 'ServeModel'. component=serve deployment=ServeModel\n",
      "[2022-03-29 11:41:48,780 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(pid=gcs_server)\u001b[0m [2022-03-29 11:41:48,833 E 6220 220549] (gcs_server) core_worker_client.h:302: Push normal task\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m 2022-03-29 11:41:48,805\tINFO deployment_state.py:1198 -- Adding 1 replicas to deployment 'ServeModel'. component=serve deployment=ServeModel\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:41:48,780 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:41:48,782 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:41:48,782 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:41:48,783 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:41:48,784 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:41:48,784 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "[2022-03-29 11:41:49,785 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "[2022-03-29 11:41:50,790 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "[2022-03-29 11:41:51,795 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "[2022-03-29 11:41:52,800 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "[2022-03-29 11:41:53,804 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "[2022-03-29 11:41:54,806 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "[2022-03-29 11:41:55,812 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:41:56,554 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:41:56,554 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "[2022-03-29 11:41:56,816 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeModel pid=6243)\u001b[0m 2022-03-29 11:41:57,020\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(ServeModel pid=6243)\u001b[0m 2022-03-29 11:41:57,097\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(ServeModel pid=6243)\u001b[0m 2022-03-29 11:41:57,111\tINFO trainable.py:521 -- Restored on 127.0.0.1 from checkpoint: /Users/sven/ray_results/BCTrainer_None_2022-03-29_11-15-08poelm_x7/checkpoint_000363/checkpoint-363\n",
      "\u001b[2m\u001b[36m(ServeModel pid=6243)\u001b[0m 2022-03-29 11:41:57,111\tINFO trainable.py:530 -- Current state after restoring: {'_iteration': 363, '_timesteps_total': 726000, '_time_total': 62.90191435813904, '_episodes_total': 0}\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:41:57,227 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:41:57,324 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "[2022-03-29 11:41:57,820 E 5594 222707] core_worker_client.h:281: Skip queue true\n",
      "2022-03-29 11:41:57,822\tINFO api.py:556 -- Deployment 'ServeModel' is ready at `http://127.0.0.1:8000/long-term-satisfaction`. component=serve deployment=ServeModel\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:42:06,660 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:42:16,406 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:42:22,689 E 6242 222680] core_worker_client.h:281: Skip queue true\n"
     ]
    }
   ],
   "source": [
    "serve.start()\n",
    "\n",
    "@serve.deployment(route_prefix=\"/long-term-satisfaction\")\n",
    "class ServeModel:\n",
    "    def __init__(self, checkpoint_path) -> None:\n",
    "        self.trainer = BCTrainer(\n",
    "            config=offline_rl_config,\n",
    "        )\n",
    "        self.trainer.restore(checkpoint_path)\n",
    "\n",
    "    async def __call__(self, request: Request):\n",
    "        json_input = await request.json()\n",
    "        obs = json_input[\"observation\"]\n",
    "        # Translate obs back to np.arrays.\n",
    "        np_obs = OrderedDict(tree.map_structure(lambda s: np.array(s) if isinstance(s, list) else s, obs))\n",
    "        action = self.trainer.compute_single_action(np_obs, explore=False)\n",
    "        return {\"action\": action}\n",
    "\n",
    "\n",
    "ServeModel.deploy(checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1082340d-51f4-42a2-853a-508413a3d20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Sending observation OrderedDict([('user', []), ('doc', {'0': [0.6974287629127502], '1': [0.45354267954826355], '2': [0.7220556139945984], '3': [0.8663823008537292], '4': [0.9755215048789978], '5': [0.855803370475769], '6': [0.011714084073901176], '7': [0.359978049993515], '8': [0.729990541934967], '9': [0.17162968218326569], '10': [0.5210366249084473], '11': [0.054337989538908005], '12': [0.19999653100967407], '13': [0.01852179504930973], '14': [0.793697714805603], '15': [0.2239246815443039], '16': [0.3453516662120819], '17': [0.9280812740325928], '18': [0.704414427280426], '19': [0.031838931143283844]}), ('response', (OrderedDict([('click', 0), ('engagement', 47.201908111572266)]), OrderedDict([('click', 0), ('engagement', 88.50106811523438)])))])\n",
      "<- got {'action': [1, 2]}\n",
      "-> Sending observation OrderedDict([('user', []), ('doc', {'0': [0.1646941602230072], '1': [0.6214783787727356], '2': [0.5772286057472229], '3': [0.23789282143115997], '4': [0.9342139959335327], '5': [0.6139659285545349], '6': [0.5356327891349792], '7': [0.5899099707603455], '8': [0.7301220297813416], '9': [0.31194499135017395], '10': [0.39822107553482056], '11': [0.20984375476837158], '12': [0.18619300425052643], '13': [0.9443724155426025], '14': [0.739550769329071], '15': [0.49045881628990173], '16': [0.22741462290287018], '17': [0.2543564736843109], '18': [0.058029159903526306], '19': [0.43441662192344666]}), ('response', ({'click': 1, 'engagement': 31.114789962768555}, {'click': 0, 'engagement': 0.0}))])\n",
      "<- got {'action': [8, 9]}\n",
      "-> Sending observation OrderedDict([('user', []), ('doc', {'0': [0.3117958903312683], '1': [0.6963434815406799], '2': [0.37775182723999023], '3': [0.1796036809682846], '4': [0.024678727611899376], '5': [0.06724963337182999], '6': [0.6793927550315857], '7': [0.4536968469619751], '8': [0.5365791916847229], '9': [0.8966712951660156], '10': [0.990338921546936], '11': [0.21689698100090027], '12': [0.6630781888961792], '13': [0.2633223831653595], '14': [0.02065099962055683], '15': [0.7583786249160767], '16': [0.32001715898513794], '17': [0.38346388936042786], '18': [0.5883170962333679], '19': [0.8310484290122986]}), ('response', ({'click': 1, 'engagement': 4.622944355010986}, {'click': 0, 'engagement': 0.0}))])\n",
      "<- got {'action': [13, 9]}\n",
      "-> Sending observation OrderedDict([('user', []), ('doc', {'0': [0.6289818286895752], '1': [0.872650682926178], '2': [0.27354204654693604], '3': [0.7980468273162842], '4': [0.18563593924045563], '5': [0.9527916312217712], '6': [0.6874882578849792], '7': [0.21550767123699188], '8': [0.9473705887794495], '9': [0.7308558225631714], '10': [0.2539416551589966], '11': [0.21331197023391724], '12': [0.518200695514679], '13': [0.02566271834075451], '14': [0.20747007429599762], '15': [0.4246854782104492], '16': [0.3741699755191803], '17': [0.46357542276382446], '18': [0.27762871980667114], '19': [0.5867843627929688]}), ('response', ({'click': 1, 'engagement': 7.150041580200195}, {'click': 0, 'engagement': 0.0}))])\n",
      "<- got {'action': [13, 9]}\n",
      "-> Sending observation OrderedDict([('user', []), ('doc', {'0': [0.8638556003570557], '1': [0.11753185838460922], '2': [0.517379105091095], '3': [0.13206811249256134], '4': [0.7168596982955933], '5': [0.39605969190597534], '6': [0.5654212832450867], '7': [0.1832798421382904], '8': [0.14484776556491852], '9': [0.4880562722682953], '10': [0.35561272501945496], '11': [0.9404319524765015], '12': [0.7653252482414246], '13': [0.748663604259491], '14': [0.9037197232246399], '15': [0.08342243731021881], '16': [0.5521924495697021], '17': [0.5844760537147522], '18': [0.961936354637146], '19': [0.29214751720428467]}), ('response', ({'click': 1, 'engagement': 13.240273475646973}, {'click': 0, 'engagement': 0.0}))])\n",
      "<- got {'action': [13, 9]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:44:24,218 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:44:24,227 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:44:24,237 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:44:24,247 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:44:24,257 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:44:31,307 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:44:37,490 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:44:40,775 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:44:50,230 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:44:59,828 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:45:04,695 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:45:09,275 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:45:18,825 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:45:28,649 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:45:32,232 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:45:38,373 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:45:48,211 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:45:51,981 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:45:58,180 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:46:07,627 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:46:17,160 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:46:20,541 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:46:23,547 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:46:27,139 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:46:36,400 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:46:45,614 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:46:55,408 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:47:05,629 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:47:14,413 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:47:15,303 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:47:18,565 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:47:24,826 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:47:34,407 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:47:44,200 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:47:44,885 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:47:53,651 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:47:59,341 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:48:03,633 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:48:12,951 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:48:22,317 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:48:30,322 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:48:31,966 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:48:40,997 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:48:41,385 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:48:50,712 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:49:00,172 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:49:08,654 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:49:09,858 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:49:14,394 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:49:19,469 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:49:28,825 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:49:38,752 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:49:47,972 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:49:57,591 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:50:02,711 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:50:04,584 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:50:07,429 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:50:17,233 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:50:26,746 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:50:36,143 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:50:45,774 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:50:45,779 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:50:55,312 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:51:04,188 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:51:04,748 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:51:14,192 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:51:23,362 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:51:32,118 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:51:32,691 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:51:42,255 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:51:51,897 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:52:01,254 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:52:01,708 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:52:09,810 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:52:11,288 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:52:20,565 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:52:30,397 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:52:40,048 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:52:49,602 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:52:55,780 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:52:55,927 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:52:59,147 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:53:08,422 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:53:17,692 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:53:26,833 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:53:36,285 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:53:45,770 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:53:52,059 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:53:52,583 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:53:55,613 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:54:05,013 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:54:14,130 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:54:23,919 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:54:33,176 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:54:39,672 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:54:42,536 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:54:51,811 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:54:52,067 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:55:01,577 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:55:11,057 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:55:18,083 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:55:21,053 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:55:30,629 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:55:33,663 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:55:40,562 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:55:48,678 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:55:50,228 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:55:59,638 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:56:08,846 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:56:18,511 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:56:23,729 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:56:27,800 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:56:28,692 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:56:38,475 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:56:48,703 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:56:58,827 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:57:08,357 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:57:14,461 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:57:18,067 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:57:21,572 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:57:28,053 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:57:37,429 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:57:46,988 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:57:56,828 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:58:06,136 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:58:09,951 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:58:15,745 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:58:19,185 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:58:25,416 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:58:35,259 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:58:44,734 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:58:48,266 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:58:54,106 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:59:03,540 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:59:12,989 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:59:15,743 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:59:22,865 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:59:32,617 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:59:35,526 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:59:41,898 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 11:59:50,696 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 11:59:51,197 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 12:00:01,292 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 12:00:11,070 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 12:00:13,672 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 12:00:20,222 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 12:00:29,302 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 12:00:36,954 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 12:00:38,882 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 12:00:48,365 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 12:00:57,978 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 12:01:06,203 E 6242 222680] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 12:01:07,677 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 12:01:17,010 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(ServeController pid=6244)\u001b[0m [2022-03-29 12:01:26,389 E 6244 222685] core_worker_client.h:281: Skip queue true\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6242)\u001b[0m [2022-03-29 12:01:26,857 E 6242 222680] core_worker_client.h:281: Skip queue true\n"
     ]
    }
   ],
   "source": [
    "# Request 5 actions of an episode from served policy.\n",
    "obs = offline_rl_env.reset()\n",
    "\n",
    "for _ in range(5):\n",
    "    # Convert numpy arrays to lists (needed for transfer).\n",
    "    obs = tree.map_structure(lambda s: s.tolist() if isinstance(s, np.ndarray) else s, obs)\n",
    "\n",
    "    print(f\"-> Sending observation {obs}\")\n",
    "    resp = requests.get(\n",
    "        \"http://localhost:8000/long-term-satisfaction\", json={\"observation\": obs}\n",
    "    )\n",
    "    response_json = resp.json()\n",
    "    print(f\"<- got {response_json}\")\n",
    "    obs, _, _, _ = offline_rl_env.step(np.array(response_json[\"action\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {},
   "source": [
    "## Time for Q&A\n",
    "\n",
    "...\n",
    "\n",
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful.\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials/tree/main/production_rl_2022\">github repo of this tutorial</a>.\n",
    "- <a href=\"https://docs.ray.io/en/latest/rllib/index.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on Ray and its libraries.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac2bd7-8a01-4a89-a899-c5a33b7ada4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
