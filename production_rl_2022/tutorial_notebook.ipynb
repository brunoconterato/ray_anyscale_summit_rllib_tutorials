{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa06051",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Recommender Systems\n",
    "## From Contextual Bandits to Slate-Q\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"images/youtube.png\" style=\"width: 230px;\"/> </td>\n",
    "    <td> <img src=\"images/dota2.jpg\" style=\"width: 213px;\"/> </td>\n",
    "    <td> <img src=\"images/forklifts.jpg\" style=\"width: 169px;\"/> </td>\n",
    "    <td> <img src=\"images/spotify.jpg\" style=\"width: 254px;\"/> </td>\n",
    "    <td> <img src=\"images/robots.jpg\" style=\"width: 252px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Overview\n",
    "“Industry-grade, hands-on RL with Ray RLlib” is a tutorial for industry researchers, domain-experts, and ML-engineers, showcasing ...\n",
    "\n",
    "1) .. how you can use RLlib to build a recommender system simulator for your industry applications and run a slate-capable algorithm against this simulator.\n",
    "\n",
    "2) .. how RLlib's offline algorithms pose solutions in case you don't have a simulator of your problem environment at hand.\n",
    "\n",
    "We will further explore how to deploy one or more trained models to production using Ray Serve and how RLlib's bandit algorithms could be used to select the best model from some set of candidates for that purpose.\n",
    "\n",
    "During the live-coding phases, we will build a recommender system simulating environment with RLlib and google's RecSim, choose, configure, and run an RLlib algorithm, and experiment and tune hyperparameters with Ray Tune.\n",
    "\n",
    "RLlib offers industry-grade scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL?) before proceeding to RLlib (recommender system) environments, neural network models, offline RL, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "### Intended Audience\n",
    "* Python programmers who are interested in using RL to solve their specific industry decision making problems and who want to get started with RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "To get this very notebook up and running on your local machine, you can follow these steps here:\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Linux):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install cmake \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "$ pip install grpcio # <- extra install only on apple M1 mac\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "$ pip install pywin32 # <- extra install only on Win10.\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials/rl_conference_2022\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and RLlib?\n",
    "* How do recommender systems work? How do we build our own?\n",
    "* How do we train RLlib's different algorithms on a recommender system problem?\n",
    "* (Optional) What's offline RL and how can I use it with RLlib?\n",
    "\n",
    "\n",
    "\n",
    "### TODO: Tutorial Outline\n",
    "\n",
    "1. RL in a nutshell. How to formulate any problem as an RL-solvable one?\n",
    "1. Recommender systems - How they work\n",
    "\n",
    "(7min break)\n",
    "\n",
    "1. What are contextual bandits?\n",
    "1. How to use contextual Bandits with RLlib and start our first training run.\n",
    "1. What if the environment becomes more difficult? Intro to google's RecSim and RLlib's Slate-Q algorithm.\n",
    "1. Starting a Slate-Q training run using Ray Tune.\n",
    "\n",
    "(7min break)\n",
    "\n",
    "1. Intro to Offline RL.\n",
    "1. What if we don't have an environment? Pretending the output of our previous experiments is historic data with which we can train an offline RL agent.\n",
    "1. BC and MARWIL: Quick how-to and setup instructions.\n",
    "1. Off policy evaluation (OPE) as a means to estimate how well an offline-RL trained policy will perform in production.\n",
    "1. Ray Serve example: How can we deploy a trained policy into our production environment?\n",
    "\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n",
    "\n",
    "<img src=\"images/unity3d_blog_post.png\" width=400>\n",
    "\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7e21f-f3de-4bad-a3a7-4bbd0b015559",
   "metadata": {},
   "source": [
    "# Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c03743-56bd-432c-b921-1d4282fb05cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get started with some basic imports.\n",
    "\n",
    "import ray  # .. of course\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "from pprint import pprint\n",
    "import re\n",
    "import recsim  # google's recsim package.\n",
    "import requests\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "from scipy.stats import sem  # standard error of the mean\n",
    "import tree  # dm_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f010a6-7ee3-49b3-814a-2b9455c66c8f",
   "metadata": {},
   "source": [
    "## Introducing google RecSim\n",
    "\n",
    "<img src=\"images/recsim_documentation.png\" width=600 style=\"float:right;\">\n",
    "\n",
    "<a href=\"https://github.com/google-research/recsim\">Google's RecSim package</a> offers a flexible way for you to <a href=\"https://github.com/google-research/recsim/blob/master/recsim/colab/RecSim_Developing_an_Environment.ipynb\">define the different building blocks of a recommender system</a>:\n",
    "\n",
    "\n",
    "- User model (how do users change their preferences when having been faced with, selected, and consumed certain items?).\n",
    "- Document model: Features of documents and how do documents get pre-selected/sampled.\n",
    "- Reward functions.\n",
    "\n",
    "RLlib comes with 3 off-the-shelf RecSim environments that are ready for training (with RLlib):\n",
    "* Long Term Satisfaction (<- our first environment)\n",
    "* Interest Evolution (<- harder env, we'll work with later on)\n",
    "* Interest Exploration (<- not used today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "2f665f07-30e6-4b30-89da-1d9732122472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('user', array([], dtype=float32)),\n",
      "             ('doc',\n",
      "              {'0': array([0.5488135], dtype=float32),\n",
      "               '1': array([0.71518934], dtype=float32),\n",
      "               '2': array([0.60276335], dtype=float32),\n",
      "               '3': array([0.5448832], dtype=float32),\n",
      "               '4': array([0.4236548], dtype=float32),\n",
      "               '5': array([0.6458941], dtype=float32),\n",
      "               '6': array([0.4375872], dtype=float32),\n",
      "               '7': array([0.891773], dtype=float32),\n",
      "               '8': array([0.96366274], dtype=float32),\n",
      "               '9': array([0.3834415], dtype=float32)}),\n",
      "             ('response',\n",
      "              (OrderedDict([('click', 0),\n",
      "                            ('engagement',\n",
      "                             array(21.269724, dtype=float32))]),))])\n"
     ]
    }
   ],
   "source": [
    "# Import a built-in RecSim environment, ready to be trained by RLlib.\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n",
    "\n",
    "# Create a RecSim instance using the following config parameters (very similar to what we used above in our own recommender system env):\n",
    "lts_10_env = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 10,\n",
    "    \"slate_size\": 1,\n",
    "    \"resample_documents\": False,  # Set to False for re-using the same candidate doecuments each timestep.\n",
    "})\n",
    "# Start a new episode and look at initial observation.\n",
    "obs = lts_10_env.reset()\n",
    "pprint(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "829ae910-621f-4c28-8a5b-303e7eac2b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiDiscrete([10])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's send our first action (k-slate back into the env).\n",
    "\n",
    "# What was our action space again?\n",
    "lts_10_env.action_space\n",
    "#action = np.array([5])\n",
    "#print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "6a68ed30-a73e-4477-8c7d-c1305d04d05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('user', array([], dtype=float32)),\n",
      "             ('doc',\n",
      "              {'0': array([0.5488135], dtype=float32),\n",
      "               '1': array([0.71518934], dtype=float32),\n",
      "               '2': array([0.60276335], dtype=float32),\n",
      "               '3': array([0.5448832], dtype=float32),\n",
      "               '4': array([0.4236548], dtype=float32),\n",
      "               '5': array([0.6458941], dtype=float32),\n",
      "               '6': array([0.4375872], dtype=float32),\n",
      "               '7': array([0.891773], dtype=float32),\n",
      "               '8': array([0.96366274], dtype=float32),\n",
      "               '9': array([0.3834415], dtype=float32)}),\n",
      "             ('response',\n",
      "              ({'click': 1, 'engagement': array(19.73187, dtype=float32)},))])\n",
      "reward = 19.73; done = False\n"
     ]
    }
   ],
   "source": [
    "#user-satisfacton={lts_10_env.env.env.env._environment._user_model._user_state.satisfaction}\n",
    "action = np.array([9])\n",
    "next_obs, reward, done, _ = lts_10_env.step(action)\n",
    "pprint(next_obs)\n",
    "print(f\"reward = {reward:.2f}; done = {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "fa9c3a56-1196-4d1d-95f5-dc54076ce512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = lts_10_env.reset()\n",
    "action_high = np.array([np.argmax([value for _, value in obs[\"doc\"].items()])])\n",
    "action_low = np.array([np.argmin([value for _, value in obs[\"doc\"].items()])])\n",
    "\n",
    "done = False\n",
    "rewards = []\n",
    "satisfactions = []\n",
    "while not done:\n",
    "    obs, reward, done, _ = lts_10_env.step(action_high) #np.random.choice([action_low, action_high])]))\n",
    "    rewards.append(reward)\n",
    "    satisfactions.append(lts_10_env.env.env.env._environment._user_model._user_state.satisfaction)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot([np.mean(rewards[i:i+1]) for i in range(len(x))])  # [np.mean([r[max(i-10, 0):i] for r in all_rewards]) for i in range(20, len(x))]\n",
    "#plt.plot((np.array(satisfactions) - 0.5) * 10000)\n",
    "plt.title(\"smoothed rewards\")\n",
    "plt.xlabel(\"ts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c7982-b372-4fe9-806a-18a29101c094",
   "metadata": {},
   "source": [
    "#### What have we learnt from experimenting with the environment?\n",
    "\n",
    "* Episodes seem to last at least n timesteps (user seems to have some time budget to spend).\n",
    "* User always seems to click, no matter what we recommend.\n",
    "* Reward seems to be always identical to the \"engagement\" value (of the clicked item).\n",
    "* Weak suspicion: If we always recommend the item with the highest feature value, rewards seem to taper off over time - in most of the episodes.\n",
    "* Weak suspicion: If we always recommend the item with the lowest feature value, rewards seem to increase over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05210c-69ea-4c09-acf8-831fffca5f8c",
   "metadata": {},
   "source": [
    "### What the environment actually does under the hood\n",
    "\n",
    "Let's take a quick look at a pre-configured RecSim environment: \"Long Term Satisfaction\".\n",
    "\n",
    "<img src=\"images/long_term_satisfaction_env.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-sussex",
   "metadata": {},
   "source": [
    "## Measuring random baseline of our environment\n",
    "\n",
    "In the cells above, we created a new environment instance (`lts_10_env`). As we have seen above, in order to start \"walking\" through a recommender system episode, we need to perform `reset()` and then several `step()` calls (with different actions) until the returned `done` flag is True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(env, episodes=1000, verbose=False):\n",
    "\n",
    "    # 1) Reset the env.\n",
    "    obs = env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "\n",
    "    # 2) Enter an infinite while loop (to step through the episode).\n",
    "    while num_episodes < episodes:\n",
    "        # 3) Calculate agent's action, using random sampling via the environment's action space.\n",
    "        action = env.action_space.sample()\n",
    "        # action = trainer.compute_single_action([obs])\n",
    "\n",
    "        # 4) Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # 5) Check, whether the episde is done, if yes, break out of the while loop.\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            elif num_episodes % 100 == 0:\n",
    "                print(f\" {num_episodes} \", end=\"\")\n",
    "            elif num_episodes % 10 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "            num_episodes += 1\n",
    "            env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # 6) Print out mean episode reward!\n",
    "    env_mean_random_reward = np.mean(episode_rewards)\n",
    "    print(f\"\\n\\nMean episode reward when acting randomly: {env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
    "\n",
    "    return env_mean_random_reward, sem(episode_rewards)\n",
    "\n",
    "lts_10_env_mean_random_reward, lts_10_env_sem_random_reward = test_env(lts_10_env, episodes=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144037c-bde0-45e0-8cfb-104455892cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a somewhat tougher version of this with 50 candidates (instead of 10).\n",
    "lts_50_env = LongTermSatisfactionRecSimEnv(config={\n",
    "    \"num_candidates\": 50,\n",
    "    \"slate_size\": 1,\n",
    "    \"resample_documents\": True,\n",
    "})\n",
    "\n",
    "lts_50_env_mean_random_reward, lts_50_env_sem_random_reward = test_env(lts_50_env, episodes=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a3d658",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 7 min break :)\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
   "metadata": {},
   "source": [
    "# Plugging in RLlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd830b90-5762-4d22-8fa9-0abf0777a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new instance of Ray (when running this tutorial locally) or\n",
    "# connect to an already running one (when running this tutorial through Anyscale).\n",
    "\n",
    "ray.init()  # Hear the engine humming? ;)\n",
    "\n",
    "# In case you encounter the following error during our tutorial: `RuntimeError: Maybe you called ray.init twice by accident?`\n",
    "# Try: `ray.shutdown() + ray.init()` or `ray.init(ignore_reinit_error=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
   "metadata": {},
   "source": [
    "## Picking an RLlib algorithm (\"Trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_algorithms.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1b0b3-ec96-41c0-9d5b-93db1c5ce021",
   "metadata": {},
   "source": [
    "### What's a Bandit?\n",
    "<img src=\"images/simple_n_armed_bandit.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6282688-d983-4608-bac3-32beb555c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use one of the above algorithms, you may instantiate its associated Trainer class.\n",
    "# For example, to import a Bandit Trainer (w/ Upper Confidence Bound exploration), do:\n",
    "\n",
    "from ray.rllib.agents.bandit import BanditLinUCBTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4656220",
   "metadata": {},
   "source": [
    "### Trying a contextual Bandit on our environment\n",
    "<img src=\"images/contextual_bandit.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232b67b-1935-4628-bcba-66a0564fda6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_config = {\n",
    "    \"env\": LongTermSatisfactionRecSimEnv,\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 50,  # 50x49 ~2500 unique slates (arms)\n",
    "        \"slate_size\": 2,\n",
    "        # Bandit-specific flags:\n",
    "        \"convert_to_discrete_action_space\": True,\n",
    "        # Convert \"doc\" key into \"item\" key.\n",
    "        \"wrap_for_bandits\": True,\n",
    "    },\n",
    "    # Generate a result dict every single time step.\n",
    "    \"timesteps_per_iteration\": 1,\n",
    "}\n",
    "\n",
    "# Create the RLlib Trainer using above config.\n",
    "bandit_trainer = BanditLinUCBTrainer(config=bandit_config)\n",
    "bandit_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a22cc0-0efb-40be-85fe-720e62a7a419",
   "metadata": {},
   "source": [
    "#### Running a single training iteration, by calling the `.train()` method:\n",
    "\n",
    "One iteration for most algos involves:\n",
    "\n",
    "1. Sampling from the environment(s)\n",
    "1. Using the sampled data (observations, actions taken, rewards) to update the policy model (e.g. a neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd18251-2a1a-4822-8744-ca6df4a14787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform single `.train()` call.\n",
    "result = bandit_trainer.train()\n",
    "# Erase config dict from result (for better overview).\n",
    "del result[\"config\"]\n",
    "# Print out training iteration results.\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c635d69a-9b22-4b64-8b09-124f346c9ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for n more iterations (timesteps) and collect n-arm rewards.\n",
    "rewards = []\n",
    "for i in range(2000):\n",
    "    result = bandit_trainer.train()\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    if i % 500 == 0:\n",
    "        print(f\" {i} \", end=\"\")\n",
    "    elif i % 100 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "start_at = 0\n",
    "smoothing_win = 50\n",
    "x = list(range(start_at, len(rewards)))\n",
    "y = [np.nanmean(rewards[max(i - smoothing_win, 0):i]) for i in range(start_at, len(rewards))]\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=lts_50_env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528eab5f-548e-4bfc-9651-6865e1ee6573",
   "metadata": {},
   "source": [
    "## Trying Bandits on a tougher environment\n",
    "\n",
    "So far, we have trained a UCB Bandit against the LongTermSatisfaction environment, a quite simple problem to solve.\n",
    "\n",
    "Let's move to a more complex RecSim environmeent: The InterestEvolution env.\n",
    "\n",
    "<img src=\"images/interest_evolution_env.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0f901-bbce-4fe7-9700-7624eaf8263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import InterestEvolutionRecSimEnv\n",
    "\n",
    "# Update our env: Making things harder.\n",
    "# Leave the env_config the same as for the\n",
    "# LongTermEvolution env: 50 candidates, slate-size=2\n",
    "bandit_config.update({\n",
    "    \"env\": InterestEvolutionRecSimEnv,\n",
    "    \"env_config\": dict(bandit_config[\"env_config\"], **{\n",
    "        \"num_candidates\": 50,  # 50*49= ~2500 unique slates\n",
    "        \"slate_size\": 2,  # k=2\n",
    "    }),\n",
    "})\n",
    "\n",
    "interest_evolution_env_for_bandits = InterestEvolutionRecSimEnv(config=bandit_config[\"env_config\"])\n",
    "\n",
    "# Re-computing our random baseline.\n",
    "harder_env_mean_random_reward, _ = test_env(interest_evolution_env_for_bandits, episodes=200)\n",
    "\n",
    "# Create the RLlib Trainer using above config.\n",
    "bandit_trainer = BanditLinUCBTrainer(config=bandit_config)\n",
    "\n",
    "# Train for n iterations (timesteps) and collect n-arm rewards.\n",
    "rewards = []\n",
    "for i in range(3000):\n",
    "    result = bandit_trainer.train()\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    if i % 500 == 0:\n",
    "        print(f\" {i} \", end=\"\")\n",
    "    elif i % 100 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "start_at = 100\n",
    "smoothing_win = 500\n",
    "x = list(range(start_at, len(rewards)))\n",
    "y = [np.nanmean(rewards[max(i - smoothing_win, 0):i]) for i in range(start_at, len(rewards))]\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=harder_env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1fb307-76e5-4283-88e3-c96b6954910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tune.run(\"BanditLinUCB\", config=bandit_config, stop={\"timesteps_total\": 3000}, verbose=1, num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef4f68d-b99c-48ee-879f-8b258b7111ac",
   "metadata": {},
   "source": [
    "#### Well, that doesn't look so great anymore.\n",
    "\n",
    "Bandits are able to learn recommender-system envs, but are having a harder time when we increase the number of candidates to select from, the number of users, the slate size, and the episode/session length.\n",
    "\n",
    "<img src=\"images/rewards_bandit_on_50_2_ie_env.png\">\n",
    "\n",
    "Luckily, RLlib offers another algorithm - Slate-Q - designed for k-slate and long-time horizon (user journey) recommendation problems.\n",
    "\n",
    "### Switching to Slate-Q\n",
    "<img src=\"images/slateq.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897c2ee-d0c8-4d06-b580-3c5da033c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "# We use the SlateQ algorithm here b/c it is specialized in solving slate recommendation problems\n",
    "# and works well with RLlib's RecSim environment adapter.\n",
    "\n",
    "from ray.rllib.agents.slateq import SlateQTrainer\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import InterestEvolutionRecSimEnv\n",
    "\n",
    "\n",
    "slateq_config = {\n",
    "    \"env\": InterestEvolutionRecSimEnv,\n",
    "    # Use exact same env config as above for direct comparison ...\n",
    "    \"env_config\": dict(bandit_config[\"env_config\"],\n",
    "                       # ... but switch off bandit wrapping and use MultiDiscrete (slate) action space.\n",
    "                       **{\n",
    "                            \"wrap_for_bandits\": False,  # SlateQ != Bandit\n",
    "                            \"convert_to_discrete_action_space\": False,  # SlateQ handles MultiDiscrete action spaces (slate recommendations).\n",
    "                       }\n",
    "                      ),  \n",
    "    \"exploration_config\": {\n",
    "        \"warmup_timesteps\": 10000,\n",
    "        \"epsilon_timesteps\": 25000,\n",
    "    },\n",
    "    \"replay_buffer_config\": {\n",
    "        \"capacity\": 100000,\n",
    "    },\n",
    "    \"learning_starts\": 10000,\n",
    "    \"target_network_update_freq\": 3200,\n",
    "\n",
    "    \"metrics_num_episodes_for_smoothing\": 200,\n",
    "}\n",
    "\n",
    "# Instantiate the Trainer object using the exact same config as in our last (harder-to-solve env) Bandit experiment above.\n",
    "slateq_trainer = SlateQTrainer(config=slateq_config)\n",
    "slateq_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c94d4-6871-4d20-81af-3d4081f05f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a single training iteration.\n",
    "results = slateq_trainer.train()\n",
    "\n",
    "# Delete the config from the results for clarity.\n",
    "# Only the stats will remain, then.\n",
    "del results[\"config\"]\n",
    "# Pretty print the stats.\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {},
   "source": [
    "Now that we have confirmed we have setup the Trainer correctly, let's call `train()` on it several times (what about 10 times?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae724d-71cc-422b-96cb-3dc9faa2d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "for _ in range(10):\n",
    "    results = slateq_trainer.train()\n",
    "    print(f\"Iteration={slateq_trainer.iteration}: R(\\\"return\\\")={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK!\n",
    "\n",
    "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using the SlateQ Policy and its NN models inside our Trainer object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff679e8-74b4-4603-9d5c-4cc0c6ebe45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the policy inside the Trainer, use `Trainer.get_policy([policy ID]=\"default_policy\")`:\n",
    "policy = slateq_trainer.get_policy()\n",
    "print(f\"Our Policy right now is: {policy}\")\n",
    "\n",
    "# To get to the model inside any policy, do:\n",
    "model = policy.model\n",
    "#print(f\"Our Policy's model is: {model}\")\n",
    "\n",
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\\n\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\\n\")\n",
    "\n",
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = env.observation_space.sample()\n",
    "\n",
    "# tf-specific code: Use tf1.Session().\n",
    "sess = policy.get_session()\n",
    "\n",
    "# Get the action logits (as torch tensor).\n",
    "with sess.graph.as_default():\n",
    "    q_values_per_candidate = model.q_value_head([\n",
    "        np.expand_dims(obs[\"user\"], 0),\n",
    "        np.expand_dims(np.concatenate([value for value in obs[\"doc\"].values()]), 0),\n",
    "    ])\n",
    "print(f\"q_values_per_candidate={sess.run(q_values_per_candidate)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de603d14-f0cb-4363-a72b-8f147c094071",
   "metadata": {},
   "source": [
    "In order to release all resources from a Trainer, you can use a Trainer's `stop()` method.\n",
    "You should definitley run this cell as it frees resources that we'll need later in this tutorial, when we'll do parallel hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737dca4f-942f-4fda-abcc-0052263a103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to release resources that a Trainer uses, you can call its `stop()` method:\n",
    "slateq_trainer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c1e4c-cb02-4719-ac5a-0106172a6c6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Moving stuff to the professional level: RLlib in connection w/ Ray Tune\n",
    "\n",
    "Running any experiments through Ray Tune is the recommended way of doing things with RLlib. If you look at our\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/rllib/examples\">examples scripts folder</a>, you will see that almost all of the scripts use Ray Tune to run the particular RLlib workload demonstrated in each script.\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdacebb-d27f-4174-9002-35c5657f146c",
   "metadata": {
    "tags": []
   },
   "source": [
    "When setting up hyperparameter sweeps for Tune, we'll do this in our already familiar config dict.\n",
    "\n",
    "So let's take a quick look at our SlateQ algo's default config to understand, which hyperparameters we may want to play around with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b32582-52bd-4585-9009-2f877a0723a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration dicts and Ray Tune.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# SlateQ algorithm:\n",
    "from ray.rllib.agents.slateq import DEFAULT_CONFIG as SLATEQ_DEFAULT_CONFIG\n",
    "print(f\"SlateQ's default config is:\")\n",
    "pprint(SLATEQ_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded886cc-436e-46cd-8fea-d68af8b41236",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's do a very simple grid-search over two learning rates with tune.run().\n",
    "\n",
    "In particular, we will try the learning rates (\"lr\") 0.00025 and 0.001 using `tune.grid_search([...])`\n",
    "inside our config dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063991e-173b-49be-a4e7-467e2e18321a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plugging in Ray Tune.\n",
    "# Note that this is the recommended way to run any experiments with RLlib.\n",
    "# Reasons:\n",
    "# - Tune allows you to do hyperparameter tuning in a user-friendly way\n",
    "#   and at large scale!\n",
    "# - Tune automatically allocates needed resources for the different\n",
    "#   hyperparam trials and experiment runs on a cluster.\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Running stuff with tune, we can re-use the exact\n",
    "# same config that we used when working with RLlib directly!\n",
    "slateq_tune_config = slateq_config.copy()\n",
    "\n",
    "# Let's add our first hyperparameter search via our config.\n",
    "slateq_tune_config[\"lr\"] = tune.grid_search([0.00025, 0.001])\n",
    "\n",
    "# We will configure an \"output\" location here to make sure we record all environment interactions.\n",
    "# This for the second part of this tutorial, in which we will explore offline RL.\n",
    "slateq_tune_config[\"output\"] = \"logdir\"\n",
    "\n",
    "# Set max. output file size to 256Mb.\n",
    "slateq_tune_config[\"output_max_file_size\"] = 256 * 1024 * 1024  # 256 Mb\n",
    "\n",
    "# Now that we will run things \"automatically\" through tune, we have to\n",
    "# define one or more stopping criteria.\n",
    "# Tune will stop the run, once any single one of the criteria is matched (not all of them!).\n",
    "stop = {\n",
    "    # Note that the keys used here can be anything present in the above `rllib_trainer.train()` output dict.\n",
    "    \"training_iteration\": 50,\n",
    "    \"episode_reward_mean\": 163.0,\n",
    "}\n",
    "\n",
    "# \"SlateQ\" is a registered name that points to RLlib's SlateQTrainer.\n",
    "# See `ray/rllib/agents/registry.py`\n",
    "\n",
    "# Run a simple experiment until one of the stopping criteria is met.\n",
    "results = tune.run(\n",
    "    \"SlateQ\",\n",
    "    config=slateq_tune_config,\n",
    "    stop=stop,\n",
    "    verbose=2,\n",
    "    # Note that no trainers will be returned from this call here.\n",
    "    # Tune will create n Trainers internally, run them in parallel and destroy them at the end.\n",
    "    # However, you can ...\n",
    "    checkpoint_at_end=True,  # ... create a checkpoint when done.\n",
    "    checkpoint_freq=10,  # ... create a checkpoint every 10 training iterations.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d282a-4ad1-4d5f-9dec-00afb8154048",
   "metadata": {
    "tags": []
   },
   "source": [
    "------------------\n",
    "## 7 min break :)\n",
    "\n",
    "(while the above experiment is running (and hopefully learning))\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82057-6b4c-4075-bd32-93c3426a1700",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction to Offline RL\n",
    "\n",
    "<img src=\"images/offline_rl.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5114691-b74f-4b4e-8bdb-5df704bee067",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The previous tune.run (the one we did before the break) produced \"historic data\" output.\n",
    "# We will use this output in the following as input to a newly initialized, untrained offline RL algorithm.\n",
    "\n",
    "# Let's take a look at the generated file(s) first:\n",
    "output_dir = results.get_best_logdir(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "print(output_dir)\n",
    "\n",
    "# Here is what the best log directory contains:\n",
    "print(\"\\n\\nThe logdir contains the following files:\")\n",
    "all_output_files = os.listdir(os.path.dirname(output_dir + \"/\"))\n",
    "pprint(all_output_files)\n",
    "\n",
    "json_output_file = os.path.join(output_dir, [f for f in all_output_files if re.match(\"^.*worker.*\\.json$\", f)][0])\n",
    "print(\"\\n\\nThe JSON file with all sampled trajectories is:\")\n",
    "print(json_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4230f",
   "metadata": {},
   "source": [
    "### Using an (offline) input file with an offline RL algorithm.\n",
    "\n",
    "We will now pretend that we don't have a simulator for our problem (same recommender system problem as above) available, however, let's assume we possess a lot of pre-recorded, historic data from some legacy (non-RL) system.\n",
    "\n",
    "Assuming that this legacy system wrote some data into a JSON file (we'll simply use the same JSON file that our SlateQ algo produced above), how can we use this historic data to do RL either way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a0aaea-b811-41e1-9e6c-532d9ce1b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the output file first:\n",
    "dataframe = pandas.read_json(json_output_file, lines=True)  # don't forget lines=True -> Each line in the json is one \"rollout\" of 4 timesteps.\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d33782-b4f1-4160-aa7b-657879bc1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's configure a new RLlib Trainer, one that's capable of reading the JSON input described\n",
    "# above and able to learn from this input.\n",
    "\n",
    "# For simplicity, we'll start with a behavioral cloning (BC) trainer:\n",
    "from ray.rllib.agents.marwil.bc import BCTrainer\n",
    "\n",
    "offline_rl_config = {\n",
    "    # Specify your offline RL algo's historic (JSON) inputs:\n",
    "    \"input\": [json_output_file],\n",
    "    # Note: For non-offline RL algos, this is set to \"sampler\" by default.\n",
    "    #\"input\": \"sampler\",\n",
    "    \"observation_space\": interest_evolution_env.observation_space,\n",
    "    \"action_space\": interest_evolution_env.action_space,\n",
    "    \"_disable_preprocessor_api\": True,\n",
    "}\n",
    "\n",
    "bc_trainer = BCTrainer(config=offline_rl_config)\n",
    "bc_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train our new behavioral cloning Trainer for some iterations:\n",
    "for _ in range(5):\n",
    "    results = bc_trainer.train()\n",
    "    print(results[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh no! What happened?\n",
    "# We don't have an environment! No way to measure rewards per episode.\n",
    "\n",
    "# A quick fix would be:\n",
    "# We cheat! Let's use our environment from above to run some separate evaluation workers on while we train:\n",
    "\n",
    "offline_rl_config.update({\n",
    "    # Add an evaluation track\n",
    "    \"evaluation_interval\": 1,\n",
    "    \"evaluation_parallel_to_training\": True,\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"evaluation_duration\": 100,\n",
    "    \"evaluation_duration_unit\": \"episodes\",\n",
    "    \"evaluation_config\": {\n",
    "        \"env\": InterestEvolutionRecSimEnv,\n",
    "        \"env_config\": slateq_config[\"env_config\"],\n",
    "        \"input\": \"sampler\",\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ca875",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer = BCTrainer(config=offline_rl_config)\n",
    "print(bc_trainer.evaluation_workers)\n",
    "#bc_trainer.evaluate()\n",
    "\n",
    "# Let's train our new behavioral cloning Trainer for some iterations:\n",
    "for _ in range(5):\n",
    "    results = bc_trainer.train()\n",
    "    print(results[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd66c3-f07a-4795-84ea-6b232ba6a047",
   "metadata": {},
   "source": [
    "### Saving and restoring a trained Trainer.\n",
    "Currently, `rllib_trainer` is in an already trained state.\n",
    "It holds optimized weights in its Q-value/Policy's models that allow it to act\n",
    "already somewhat smart in our environment when given an observation.\n",
    "\n",
    "However, if we closed this notebook right now, all the effort would have been for nothing.\n",
    "Let's therefore save the state of our trainer to disk for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eae1e4-3cc4-4282-9a83-bc374bdad978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the `Trainer.save()` method to create a checkpoint.\n",
    "checkpoint_file = bc_trainer.save()\n",
    "print(f\"Trainer (at iteration {bc_trainer.iteration} was saved in '{checkpoint_file}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "os.listdir(os.path.dirname(checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e0ab-2c10-469a-97b1-4aadf1a1ec97",
   "metadata": {},
   "source": [
    "### Restoring and evaluating a Trainer\n",
    "In the following cell, we'll learn how to restore a saved Trainer from a checkpoint file.\n",
    "\n",
    "We'll also evaluate a completely new Trainer (should act more or less randomly) vs an already trained one (the one we just restored from the created checkpoint file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ceedb9-c225-46f2-ad1d-f902c81d3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_trainer = BCTrainer(config=offline_rl_config)\n",
    "# Evaluate the new trainer (this should yield random results).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "print(f\"Before restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "new_trainer.restore(checkpoint_file)\n",
    "print(f\"After restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "\n",
    "# Evaluate again (this should yield results we saw after having trained our saved agent).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating restored trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03c572-b98f-47e5-b0b4-404391be9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "serve.start()\n",
    "\n",
    "from starlette.requests import Request\n",
    "\n",
    "\n",
    "\n",
    "@serve.deployment(route_prefix=\"/interest-evolution\")\n",
    "class ServeModel:\n",
    "    def __init__(self, checkpoint_path) -> None:\n",
    "        self.trainer = BCTrainer(\n",
    "            config=offline_rl_config,\n",
    "        )\n",
    "        self.trainer.restore(checkpoint_path)\n",
    "\n",
    "    async def __call__(self, request: Request):\n",
    "        json_input = await request.json()\n",
    "        obs = json_input[\"observation\"]\n",
    "\n",
    "        action = self.trainer.compute_single_action(obs)\n",
    "        return {\"action\": int(action)}\n",
    "\n",
    "    \n",
    "ServeModel.deploy(checkpoint_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082340d-51f4-42a2-853a-508413a3d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    obs = interest_evolution_env.reset()\n",
    "\n",
    "    print(f\"-> Sending observation {obs}\")\n",
    "    resp = requests.get(\n",
    "        \"http://localhost:8000/interest-evolution\", json={\"observation\": tree.map_structure(lambda s: s.tolist() if isinstance(s, np.ndarray) else s, obs)}\n",
    "    )\n",
    "    print(f\"<- Received response {resp.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {},
   "source": [
    "## Time for Q&A\n",
    "\n",
    "...\n",
    "\n",
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful.\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials/tree/main/rl_conference_2022\">github repo of this tutorial</a>.\n",
    "- <a href=\"https://docs.ray.io/en/latest/rllib/index.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on Ray and its libraries.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d097a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
