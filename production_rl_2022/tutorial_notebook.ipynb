{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa06051",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Recommender Systems\n",
    "## From Contextual Bandits to Slate-Q\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"images/youtube.png\" style=\"width: 230px;\"/> </td>\n",
    "    <td> <img src=\"images/dota2.jpg\" style=\"width: 213px;\"/> </td>\n",
    "    <td> <img src=\"images/forklifts.jpg\" style=\"width: 169px;\"/> </td>\n",
    "    <td> <img src=\"images/spotify.jpg\" style=\"width: 254px;\"/> </td>\n",
    "    <td> <img src=\"images/robots.jpg\" style=\"width: 252px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Overview\n",
    "“Reinforcement Learning for Recommender Systems, From Contextual Bandits to Slate-Q” is a tutorial for industry researchers, domain-experts, and ML-engineers, showcasing ...\n",
    "\n",
    "1) .. how you can use RLlib to build a recommender system **simulator** for your industry applications and run Bandit algorithms and the Slate-Q algorithm against this simulator.\n",
    "\n",
    "2) .. how RLlib's offline algorithms pose solutions in case you **don't have a simulator** of your problem environment at hand.\n",
    "\n",
    "We will further explore how to deploy trained models to production using Ray Serve.\n",
    "\n",
    "During the live-coding phases, we will using a recommender system simulating environment by google's RecSim and configure and run 2 RLlib algorithms against it. We'll also demonstrate how you may use offline RL as a solution for recommender systems and how to deploy a learned policy into production.\n",
    "\n",
    "RLlib offers industry-grade scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL?) before proceeding to RLlib (recommender system) environments, neural network models, offline RL, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "### Intended Audience\n",
    "* Python programmers who are interested in using RL to solve their specific industry decision making problems and who want to get started with RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "To get this very notebook up and running on your local machine, you can follow these steps here:\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Linux):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install cmake \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "$ pip install grpcio # <- extra install only on apple M1 mac\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "$ pip install pywin32 # <- extra install only on Win10.\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials/production_rl_2022\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and RLlib?\n",
    "* How do recommender systems work? How do we build our own?\n",
    "* How do we train RLlib's different algorithms on a recommender system problem?\n",
    "* (Optional) What's offline RL and how can I use it with RLlib?\n",
    "\n",
    "\n",
    "\n",
    "### Tutorial Outline\n",
    "\n",
    "1. Reinforcement learning (RL) in a nutshell.\n",
    "1. How to formulate any problem as an RL-solvable one?\n",
    "1. Recommender systems - How they work.\n",
    "1. Why you should use RLlib.\n",
    "\n",
    "(10min break)\n",
    "\n",
    "1. Google RecSim - Build your own recom sys simulator.\n",
    "1. What are contextual bandits?\n",
    "1. How to use contextual Bandits with RLlib and start our first training run.\n",
    "1. What if the environment becomes more difficult? - Intro to Slate-Q.\n",
    "1. Starting a Slate-Q training run.\n",
    "\n",
    "(10min break)\n",
    "\n",
    "1. Intro to Offline RL.\n",
    "1. What if we don't have an environment? Pretending the output of our previous experiments is historic data with which we can train an offline RL agent.\n",
    "1. BC and MARWIL: Quick how-to and setup instructions.\n",
    "1. Off policy evaluation (OPE) as a means to estimate how well an offline-RL trained policy will perform in production.\n",
    "1. Ray Serve example: How can we deploy a trained policy into our production environment?\n",
    "\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n",
    "\n",
    "<img src=\"images/unity3d_blog_post.png\" width=400>\n",
    "\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7e21f-f3de-4bad-a3a7-4bbd0b015559",
   "metadata": {},
   "source": [
    "# Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930deb27-e739-4507-bc24-e39ded9caeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Let's get started with some basic imports.\n",
    "\n",
    "import ray  # .. of course\n",
    "from ray import serve\n",
    "from ray import tune\n",
    "\n",
    "from collections import OrderedDict\n",
    "import gym  # RL environments and action/observation spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "from pprint import pprint\n",
    "import re\n",
    "import recsim  # google's RecSim package.\n",
    "import requests\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "from scipy.stats import linregress, sem\n",
    "from starlette.requests import Request\n",
    "import tree  # dm_tree\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f010a6-7ee3-49b3-814a-2b9455c66c8f",
   "metadata": {},
   "source": [
    "## Introducing google RecSim\n",
    "\n",
    "<img src=\"images/recsim_documentation.png\" width=600 style=\"float:right;\">\n",
    "\n",
    "<a href=\"https://github.com/google-research/recsim\">Google's RecSim package</a> offers a flexible way for you to <a href=\"https://github.com/google-research/recsim/blob/master/recsim/colab/RecSim_Developing_an_Environment.ipynb\">define the different building blocks of a recommender system</a>:\n",
    "\n",
    "\n",
    "- User model (how do users change their preferences when having been faced with, selected, and consumed certain items?).\n",
    "- Document model: Features of documents and how do documents get pre-selected/sampled.\n",
    "- Reward functions.\n",
    "\n",
    "RLlib comes with 3 off-the-shelf RecSim environments that are ready for training (with RLlib):\n",
    "* Long Term Satisfaction (<- our first environment)\n",
    "* Interest Evolution (<- harder env, we'll work with later on)\n",
    "* Interest Exploration (<- not used today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3363126-0f38-4f92-a031-1ea791b9a747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space = Discrete(10)\n"
     ]
    }
   ],
   "source": [
    "# Import a built-in RecSim environment, ready to be trained by RLlib.\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n",
    "\n",
    "# Create a RecSim instance using the following config parameters (very similar to what we used above in our own recommender system env):\n",
    "lts_10_1_env = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 10,  # Discrete(10) -> int 0-9\n",
    "    \"slate_size\": 1,\n",
    "    # Set to False for re-using the same candidate doecuments each timestep.\n",
    "    \"resample_documents\": False,\n",
    "    # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "    # e.g. slate_size=2 and num_candidates=10 -> MultiDiscrete([10, 10]) -> Discrete(100)  # 10x10\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "})\n",
    "\n",
    "# What are our spaces?\n",
    "#print(f\"observation space = {lts_10_1_env.observation_space}\")\n",
    "print(f\"action space = {lts_10_1_env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa75102b-52da-474d-aabf-93964a2c8bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('user', array([], dtype=float32)),\n",
      "             ('doc',\n",
      "              {'0': array([0.5488135], dtype=float32),\n",
      "               '1': array([0.71518934], dtype=float32),\n",
      "               '2': array([0.60276335], dtype=float32),\n",
      "               '3': array([0.5448832], dtype=float32),\n",
      "               '4': array([0.4236548], dtype=float32),\n",
      "               '5': array([0.6458941], dtype=float32),\n",
      "               '6': array([0.4375872], dtype=float32),\n",
      "               '7': array([0.891773], dtype=float32),\n",
      "               '8': array([0.96366274], dtype=float32),\n",
      "               '9': array([0.3834415], dtype=float32)}),\n",
      "             ('response',\n",
      "              (OrderedDict([('click', 1),\n",
      "                            ('engagement',\n",
      "                             array(72.72687, dtype=float32))]),))])\n"
     ]
    }
   ],
   "source": [
    "# Start a new episode and look at initial observation.\n",
    "obs = lts_10_1_env.reset()\n",
    "pprint(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9857197-f79f-4f9d-8e3f-68eee20daf94",
   "metadata": {},
   "source": [
    "#### Let's play RL agent ourselves and recommend some items (pick some actions):\n",
    "\n",
    "**Task:** Execute the following cell a couple of times chosing different actions (from 0 - 9) to be sent into the environment's `step()` method. Each time, look at the returned next observation, reward, and `done` flag and write down what you find interesting about the dynamics and observations of this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f10b83e-993f-4c59-8e13-4e1074bfd7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('user', array([], dtype=float32)),\n",
      "             ('doc',\n",
      "              {'0': array([0.5488135], dtype=float32),\n",
      "               '1': array([0.71518934], dtype=float32),\n",
      "               '2': array([0.60276335], dtype=float32),\n",
      "               '3': array([0.5448832], dtype=float32),\n",
      "               '4': array([0.4236548], dtype=float32),\n",
      "               '5': array([0.6458941], dtype=float32),\n",
      "               '6': array([0.4375872], dtype=float32),\n",
      "               '7': array([0.891773], dtype=float32),\n",
      "               '8': array([0.96366274], dtype=float32),\n",
      "               '9': array([0.3834415], dtype=float32)}),\n",
      "             ('response',\n",
      "              ({'click': 1, 'engagement': array(41.123913, dtype=float32)},))])\n",
      "reward = 41.12; done = False\n"
     ]
    }
   ],
   "source": [
    "# Let's send our first action (1-slate back into the env) using the env's `step()` method.\n",
    "action = 0  # Discrete(10): 0-9 are all valid actions\n",
    "\n",
    "# This method returns 4 items:\n",
    "# - next observation (after having applied the action)\n",
    "# - reward (after having applied the action)\n",
    "# - `done` flag; if True, the episode is terminated and the environment needs to be `reset()` again.\n",
    "# - info dict (we'll ignore this)\n",
    "next_obs, reward, done, _ = lts_10_1_env.step(action)\n",
    "\n",
    "# Print out the next observation.\n",
    "# We expect the \"doc\" and \"user\" items to be the same as in the previous observation\n",
    "# b/c we set \"resample_documents\" to False.\n",
    "pprint(next_obs)\n",
    "# Print out rewards and the vlaue of the `done` flag.\n",
    "print(f\"reward = {reward:.2f}; done = {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c7982-b372-4fe9-806a-18a29101c094",
   "metadata": {},
   "source": [
    "<br /><br /><br /><br /><br />\n",
    "<br /><br /><br /><br /><br />\n",
    "<br /><br /><br /><br /><br />\n",
    "\n",
    "\n",
    "### What have we learnt from experimenting with the environment?\n",
    "\n",
    "* User's state (if any) is hidden to agent (not part of observation).\n",
    "* Episodes seem to last at least n timesteps -> user seems to have some time budget to spend.\n",
    "* User always seems to click, no matter what we recommend.\n",
    "* Reward seems to be always identical to the \"engagement\" value (of the clicked item). These values range somewhere between 0.0 and 20.0+.\n",
    "* Weak suspicion: If we always recommend the item with the highest feature value, rewards seem to taper off over time - in most of the episodes.\n",
    "* Weak suspicion: If we always recommend the item with the lowest feature value, rewards seem to increase over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05210c-69ea-4c09-acf8-831fffca5f8c",
   "metadata": {},
   "source": [
    "### What the environment actually does under the hood\n",
    "\n",
    "Let's take a quick look at a pre-configured RecSim environment: \"Long Term Satisfaction\".\n",
    "\n",
    "<img src=\"images/long_term_satisfaction_env.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958ff84-f7d0-45c9-aa43-b807243b8452",
   "metadata": {},
   "source": [
    "Now that we know, that there is a double objective built into the env (a. sweetness -> engagement; sweetness -> unhappyness; unhappyness -> low engagement), let's make this effect a tiny bit stronger by slightly modifying it. As said above, the effect is very weak and almost not measurable, which is a mistake on the env's part. We can use this following `gym.Wrapper` class in the cell below to \"fix\" that problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "375a469e-aa46-4038-9df7-02cabccdad50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "class LTSWithStrongerDissatisfactionEffect(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        env.environment._user_model._user_sampler._state_parameters.update({\n",
    "            \"sensitivity\": 0.059,\n",
    "            \"time_budget\": 120,\n",
    "            \"choc_stddev\": 0.1,\n",
    "            \"kale_stddev\": 0.1,\n",
    "            #\"innovation_stddev\": 0.01,\n",
    "            #\"choc_mean\": 1.25,\n",
    "            #\"kale_mean\": 1.0,\n",
    "            #\"memory_discount\": 0.9,\n",
    "        })\n",
    "        super().__init__(env)\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            self.observation_space.spaces[\"user\"] = gym.spaces.Box(0.0, 1.0, (1, ), dtype=np.float32)\n",
    "            for r in self.observation_space[\"response\"]:\n",
    "                if \"engagement\" in r.spaces:\n",
    "                    r.spaces[\"watch_time\"] = r.spaces[\"engagement\"]\n",
    "                    del r.spaces[\"engagement\"]\n",
    "                    break\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            observation[\"user\"] = np.array([self.env.environment._user_model._user_state.satisfaction])\n",
    "            for r in observation[\"response\"]:\n",
    "                if \"engagement\" in r:\n",
    "                    r[\"watch_time\"] = r[\"engagement\"]\n",
    "                    del r[\"engagement\"]\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Add the wrapping around \n",
    "tune.register_env(\"modified_lts\", lambda c: LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(c)))\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa9c3a56-1196-4d1d-95f5-dc54076ce512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005780688213627189\n"
     ]
    }
   ],
   "source": [
    "# This cell should help you with your own analysis of the two above \"suspicions\":\n",
    "# Always chosing the highest/lowest-valued action will lead to a decrease/increase in rewards over the course of an episode.\n",
    "modified_lts_10_1_env = LTSWithStrongerDissatisfactionEffect(lts_10_1_env)\n",
    "\n",
    "# Capture slopes of all trendlines over all episodes.\n",
    "slopes = []\n",
    "# Run 1000 episodes.\n",
    "for _ in range(1000):\n",
    "    obs = modified_lts_10_1_env.reset()  # Reset environment to get initial observation:\n",
    "\n",
    "    # Compute actions that pick doc with highest/lowest feature value.\n",
    "    action_high = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    action_low = np.argmin([value for _, value in obs[\"doc\"].items()])\n",
    "\n",
    "    # Play one episode.\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        #action = action_high\n",
    "        action = action_low\n",
    "        #action = np.random.choice([action_low, action_high])\n",
    "\n",
    "        obs, reward, done, _ = modified_lts_10_1_env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Create linear model of rewards over time.\n",
    "    reward_linreg = linregress(np.array((range(len(rewards)))), np.array(rewards))\n",
    "    slopes.append(reward_linreg.slope)\n",
    "\n",
    "print(np.mean(slopes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be848212-87b8-4eb1-9353-74e09ae72310",
   "metadata": {},
   "source": [
    "## Measuring random baseline of our environment\n",
    "\n",
    "In the cells above, we created a new environment instance (`lts_10_1_env`). As we have seen above, in order to start \"walking\" through a recommender system episode, we need to perform `reset()` and then several `step()` calls (with different actions) until the returned `done` flag is True.\n",
    "\n",
    "Let's find out how well a randomly acting agent performs in this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that measures and outputs the random baseline reward.\n",
    "# This is the expected accumulated reward per episode, if we act randomly (recommend random items) at each time step.\n",
    "def measure_random_performance_for_env(env, episodes=1000, verbose=False):\n",
    "\n",
    "    # Reset the env.\n",
    "    env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Enter while loop (to step through the episode).\n",
    "    while num_episodes < episodes:\n",
    "        # Produce a random action.\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Check, whether the episde is done, if yes, reset and increase episode counter.\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            elif num_episodes % 100 == 0:\n",
    "                print(f\" {num_episodes} \", end=\"\")\n",
    "            elif num_episodes % 10 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "            num_episodes += 1\n",
    "            env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # Print out and return mean episode reward (and standard error of the mean).\n",
    "    env_mean_random_reward = np.mean(episode_rewards)\n",
    "\n",
    "    print(f\"\\n\\nMean episode reward when acting randomly: {env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
    "\n",
    "    return env_mean_random_reward, sem(episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f144037c-bde0-45e0-8cfb-104455892cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 ......... 100 ......... 200 ......... 300 ......... 400 ......... 500 ......... 600 ......... 700 ......... 800 ......... 900 .........\n",
      "\n",
      "Mean episode reward when acting randomly: 1157.51+/-0.36\n"
     ]
    }
   ],
   "source": [
    "# Let's create a somewhat tougher version of this with 20 candidates (instead of 10) ad a slate-size of 1.\n",
    "lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config={\n",
    "    \"num_candidates\": 20,\n",
    "    \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "    \"resample_documents\": True,\n",
    "    # Convert to Discrete action space.\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "    # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "    \"wrap_for_bandits\": True,\n",
    "\n",
    "    # Seed environment for better comparability.\n",
    "    \"seed\": 42,\n",
    "}))\n",
    "\n",
    "lts_20_2_env_mean_random_reward, _ = measure_random_performance_for_env(lts_20_2_env, episodes=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
   "metadata": {},
   "source": [
    "# Plugging in RLlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd830b90-5762-4d22-8fa9-0abf0777a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray cluster already running.\n"
     ]
    }
   ],
   "source": [
    "# Start a new instance of Ray (when running this tutorial locally) or\n",
    "# connect to an already running one (when running this tutorial through Anyscale).\n",
    "\n",
    "if not ray.is_initialized():\n",
    "    ray.init()  # Hear the engine humming? ;)\n",
    "else:\n",
    "    print(\"Ray cluster already running.\")\n",
    "# In case you encounter the following error during our tutorial: `RuntimeError: Maybe you called ray.init twice by accident?`\n",
    "# Try: `ray.shutdown() + ray.init()` or `ray.init(ignore_reinit_error=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
   "metadata": {},
   "source": [
    "## Picking an RLlib algorithm (\"Trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_algorithms.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1b0b3-ec96-41c0-9d5b-93db1c5ce021",
   "metadata": {},
   "source": [
    "### Trying a \"Contextual n-armed Bandit\" on our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a26e094-9887-4fc6-88b6-d1448e931526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use one of the above algorithms, you may instantiate its associated Trainer class.\n",
    "# For example, to import a Bandit Trainer w/ Upper Confidence Bound (UCB) exploration, do:\n",
    "\n",
    "from ray.rllib.agents.bandit import BanditLinUCBTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0911f212-523e-4a75-846d-342dd2a681a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandit's default config is:\n",
      "{'_disable_action_flattening': False,\n",
      " '_disable_execution_plan_api': False,\n",
      " '_disable_preprocessor_api': False,\n",
      " '_fake_gpus': False,\n",
      " '_tf_policy_handles_more_than_one_loss': False,\n",
      " 'action_space': None,\n",
      " 'actions_in_input_normalized': False,\n",
      " 'always_attach_evaluation_results': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': False,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': -1,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'disable_env_checking': False,\n",
      " 'eager_max_retraces': 20,\n",
      " 'eager_tracing': False,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'env_task_fn': None,\n",
      " 'evaluation_config': {},\n",
      " 'evaluation_duration': 10,\n",
      " 'evaluation_duration_unit': 'episodes',\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': -1,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'evaluation_parallel_to_training': False,\n",
      " 'exploration_config': {'type': 'StochasticSampling'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'torch',\n",
      " 'gamma': 0.99,\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_config': {},\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'keep_per_episode_custom_metrics': False,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'lr': 0.0001,\n",
      " 'metrics_episode_collection_timeout_s': 180,\n",
      " 'metrics_num_episodes_for_smoothing': 100,\n",
      " 'metrics_smoothing_episodes': -1,\n",
      " 'min_iter_time_s': -1,\n",
      " 'min_sample_timesteps_per_reporting': None,\n",
      " 'min_time_s_per_reporting': None,\n",
      " 'min_train_timesteps_per_reporting': None,\n",
      " 'model': {'_disable_action_flattening': False,\n",
      "           '_disable_preprocessor_api': False,\n",
      "           '_time_major': False,\n",
      "           '_use_default_native_models': False,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': True,\n",
      "           'zero_mean': True},\n",
      " 'monitor': -1,\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_map_cache': None,\n",
      "                'policy_map_capacity': 100,\n",
      "                'policy_mapping_fn': None,\n",
      "                'replay_mode': 'independent'},\n",
      " 'no_done_at_end': False,\n",
      " 'normalize_actions': True,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_workers': 0,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'observation_space': None,\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_config': {},\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'record_env': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'rollout_fragment_length': 1,\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'seed': None,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'simple_optimizer': -1,\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 100,\n",
      " 'train_batch_size': 1}\n"
     ]
    }
   ],
   "source": [
    "# Configuration dicts for RLlib Trainers.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# E.g. Bandit algorithms:\n",
    "from ray.rllib.agents.bandit.bandit import DEFAULT_CONFIG as BANDIT_DEFAULT_CONFIG\n",
    "print(f\"Bandit's default config is:\")\n",
    "pprint(BANDIT_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c9bd9775-f2bb-41d9-8ff6-20be9abd68db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 10:50:38,864\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-03-28 10:50:38,865\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-03-28 10:50:38,876\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BanditLinUCBTrainer"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_config = {\n",
    "    \"env\": \"modified_lts\",\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 20,  # 20x19 = ~400 unique slates (arms)\n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "\n",
    "        # Bandit-specific flags:\n",
    "        \"convert_to_discrete_action_space\": True,\n",
    "        # Convert \"doc\" key into \"item\" key.\n",
    "        \"wrap_for_bandits\": True,\n",
    "        # Use consistent seeds for the environment ...\n",
    "        \"seed\": 0,\n",
    "    },\n",
    "    # ... and the Trainer itself.\n",
    "    \"seed\": 0,\n",
    "\n",
    "    # The following settings are affecting the reporting only:\n",
    "    # ---\n",
    "    # Generate a result dict every single time step.\n",
    "    \"timesteps_per_iteration\": 1,\n",
    "    # Report rewards as smoothed mean over this many episodes.\n",
    "    \"metrics_num_episodes_for_smoothing\": 200,\n",
    "}\n",
    "\n",
    "# Create the RLlib Trainer using above config.\n",
    "bandit_trainer = BanditLinUCBTrainer(config=bandit_config)\n",
    "bandit_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a22cc0-0efb-40be-85fe-720e62a7a419",
   "metadata": {},
   "source": [
    "#### Running a single training iteration, by calling the `.train()` method:\n",
    "\n",
    "One iteration for most algos involves:\n",
    "\n",
    "1. Sampling from the environment(s)\n",
    "1. Using the sampled data (observations, actions taken, rewards) to update the policy model (e.g. a neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ddd18251-2a1a-4822-8744-ca6df4a14787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_timesteps_total': 1,\n",
      " 'custom_metrics': {},\n",
      " 'date': '2022-03-28_10-50-39',\n",
      " 'done': False,\n",
      " 'episode_len_mean': nan,\n",
      " 'episode_media': {},\n",
      " 'episode_reward_max': nan,\n",
      " 'episode_reward_mean': nan,\n",
      " 'episode_reward_min': nan,\n",
      " 'episodes_this_iter': 0,\n",
      " 'episodes_total': 0,\n",
      " 'experiment_id': 'fe2974e7cfd34a3aaa93c8031eeb1fcc',\n",
      " 'hist_stats': {'episode_lengths': [], 'episode_reward': []},\n",
      " 'hostname': 'Svens-MacBook-Pro.local',\n",
      " 'info': {'learner': {'default_policy': {'learner_stats': {'update_latency': 0.0002689361572265625}}},\n",
      "          'num_agent_steps_sampled': 1,\n",
      "          'num_agent_steps_trained': 1,\n",
      "          'num_steps_sampled': 1,\n",
      "          'num_steps_trained': 1,\n",
      "          'num_steps_trained_this_iter': 1},\n",
      " 'iterations_since_restore': 1,\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_healthy_workers': 0,\n",
      " 'off_policy_estimator': {},\n",
      " 'perf': {'cpu_util_percent': 5.5, 'ram_util_percent': 66.1},\n",
      " 'pid': 57087,\n",
      " 'policy_reward_max': {},\n",
      " 'policy_reward_mean': {},\n",
      " 'policy_reward_min': {},\n",
      " 'sampler_perf': {},\n",
      " 'time_since_restore': 0.0067560672760009766,\n",
      " 'time_this_iter_s': 0.0067560672760009766,\n",
      " 'time_total_s': 0.0067560672760009766,\n",
      " 'timers': {'learn_throughput': 1880.011,\n",
      "            'learn_time_ms': 0.532,\n",
      "            'load_throughput': 2974.684,\n",
      "            'load_time_ms': 0.336,\n",
      "            'sample_throughput': 1.892,\n",
      "            'sample_time_ms': 528.519},\n",
      " 'timestamp': 1648457439,\n",
      " 'timesteps_since_restore': 1,\n",
      " 'timesteps_this_iter': 1,\n",
      " 'timesteps_total': 1,\n",
      " 'training_iteration': 1,\n",
      " 'trial_id': 'default',\n",
      " 'warmup_time': 0.5569050312042236}\n"
     ]
    }
   ],
   "source": [
    "# Perform single `.train()` call.\n",
    "result = bandit_trainer.train()\n",
    "# Erase config dict from result (for better overview).\n",
    "del result[\"config\"]\n",
    "# Print out training iteration results.\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6d6f089b-e0cc-47de-af9b-dc05a71e102f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 .... 500 .... 1000 .... 1500 .... 2000 .... 2500 .... 3000 .... 3500 .... 4000 .... 4500 .... 5000 .... 5500 .... 6000 .... 6500 .... 7000 .... 7500 .... 8000 .... 8500 .... 9000 .... 9500 .... 10000 .... 10500 .... 11000 .... 11500 ...."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/brrn254576lgnbqqtp5p1z280000gn/T/ipykernel_57087/1398439977.py:20: RuntimeWarning: Mean of empty slice\n",
      "  y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) for i in range(start_at, len(rewards))]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train for n more iterations (timesteps) and collect n-arm rewards.\n",
    "rewards = []\n",
    "for i in range(12000):\n",
    "    # Run a single timestep in the environment and update\n",
    "    # the model immediately on the received reward.\n",
    "    result = bandit_trainer.train()\n",
    "    # Extract reward from results.\n",
    "    #rewards.extend(result[\"hist_stats\"][\"episode_reward\"]\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    if i % 500 == 0:\n",
    "        print(f\" {i} \", end=\"\")\n",
    "    elif i % 100 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "start_at = 0\n",
    "smoothing_win = 200\n",
    "x = list(range(start_at, len(rewards)))\n",
    "y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) for i in range(start_at, len(rewards))]\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=lts_20_2_env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77c3cf-226c-4cab-82c6-7d8a54bfe9ea",
   "metadata": {},
   "source": [
    "### What does our trained Bandit actually recommend?\n",
    "\n",
    "The first method of the RLlib Trainer API we used above was `train()`.\n",
    "We'll now use another method of the Trainer, `compute_single_action(input_dict={})`.\n",
    "It takes a input_dict keyword arg, into which you may pass a single (unbatched!) observation to receive an action for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6eb62acb-0a16-4412-949a-4851201620bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action's feature value=0.9013410806655884; max-choc-feature=0.9013410806655884; \n",
      "action's feature value=0.9848997592926025; max-choc-feature=0.9848997592926025; \n",
      "action's feature value=0.9149371385574341; max-choc-feature=0.9149371385574341; \n",
      "action's feature value=0.9420996308326721; max-choc-feature=0.9420996308326721; \n",
      "action's feature value=0.9828138947486877; max-choc-feature=0.9828138947486877; \n",
      "action's feature value=0.9781996011734009; max-choc-feature=0.9781996011734009; \n",
      "action's feature value=0.9896430373191833; max-choc-feature=0.9896430373191833; \n",
      "action's feature value=0.9326859712600708; max-choc-feature=0.9326859712600708; \n",
      "action's feature value=0.9685329794883728; max-choc-feature=0.9685329794883728; \n",
      "action's feature value=0.988541841506958; max-choc-feature=0.988541841506958; \n",
      "action's feature value=0.9036354422569275; max-choc-feature=0.9036354422569275; \n",
      "action's feature value=0.9919232726097107; max-choc-feature=0.9919232726097107; \n",
      "action's feature value=0.9512972831726074; max-choc-feature=0.9512972831726074; \n",
      "action's feature value=0.8706805109977722; max-choc-feature=0.8706805109977722; \n",
      "action's feature value=0.7870740294456482; max-choc-feature=0.7870740294456482; \n",
      "action's feature value=0.994301974773407; max-choc-feature=0.994301974773407; \n",
      "action's feature value=0.9635453224182129; max-choc-feature=0.9635453224182129; \n",
      "action's feature value=0.998666524887085; max-choc-feature=0.998666524887085; \n",
      "action's feature value=0.9760560989379883; max-choc-feature=0.9760560989379883; \n",
      "action's feature value=0.8775285482406616; max-choc-feature=0.8775285482406616; \n",
      "action's feature value=0.9947766065597534; max-choc-feature=0.9947766065597534; \n",
      "action's feature value=0.9979191422462463; max-choc-feature=0.9979191422462463; \n",
      "action's feature value=0.9819768071174622; max-choc-feature=0.9819768071174622; \n",
      "action's feature value=0.9460340738296509; max-choc-feature=0.9460340738296509; \n",
      "action's feature value=0.9186283946037292; max-choc-feature=0.9186283946037292; \n",
      "action's feature value=0.9654234647750854; max-choc-feature=0.9654234647750854; \n",
      "action's feature value=0.9998379349708557; max-choc-feature=0.9998379349708557; \n",
      "action's feature value=0.9352909326553345; max-choc-feature=0.9352909326553345; \n",
      "action's feature value=0.994640588760376; max-choc-feature=0.994640588760376; \n",
      "action's feature value=0.9123474955558777; max-choc-feature=0.9123474955558777; \n",
      "action's feature value=0.9835779666900635; max-choc-feature=0.9835779666900635; \n",
      "action's feature value=0.9256758689880371; max-choc-feature=0.9256758689880371; \n",
      "action's feature value=0.9296865463256836; max-choc-feature=0.9296865463256836; \n",
      "action's feature value=0.9678759574890137; max-choc-feature=0.9678759574890137; \n",
      "action's feature value=0.9517591595649719; max-choc-feature=0.9517591595649719; \n",
      "action's feature value=0.9697434902191162; max-choc-feature=0.9697434902191162; \n",
      "action's feature value=0.989997386932373; max-choc-feature=0.989997386932373; \n",
      "action's feature value=0.9057956337928772; max-choc-feature=0.9057956337928772; \n",
      "action's feature value=0.9028633832931519; max-choc-feature=0.9028633832931519; \n",
      "action's feature value=0.9648652672767639; max-choc-feature=0.9648652672767639; \n",
      "action's feature value=0.9949660897254944; max-choc-feature=0.9949660897254944; \n",
      "action's feature value=0.8541142344474792; max-choc-feature=0.8541142344474792; \n",
      "action's feature value=0.9248694777488708; max-choc-feature=0.9248694777488708; \n",
      "action's feature value=0.9605425596237183; max-choc-feature=0.9605425596237183; \n",
      "action's feature value=0.9892858862876892; max-choc-feature=0.9892858862876892; \n",
      "action's feature value=0.9850065112113953; max-choc-feature=0.9850065112113953; \n",
      "action's feature value=0.9630672931671143; max-choc-feature=0.9630672931671143; \n",
      "action's feature value=0.9683616161346436; max-choc-feature=0.9683616161346436; \n",
      "action's feature value=0.9598739147186279; max-choc-feature=0.9598739147186279; \n",
      "action's feature value=0.883954644203186; max-choc-feature=0.883954644203186; \n",
      "action's feature value=0.9996995329856873; max-choc-feature=0.9996995329856873; \n",
      "action's feature value=0.9103609919548035; max-choc-feature=0.9103609919548035; \n",
      "action's feature value=0.9728519320487976; max-choc-feature=0.9728519320487976; \n",
      "action's feature value=0.9696793556213379; max-choc-feature=0.9696793556213379; \n",
      "action's feature value=0.9465672969818115; max-choc-feature=0.9465672969818115; \n",
      "action's feature value=0.9861553311347961; max-choc-feature=0.9861553311347961; \n",
      "action's feature value=0.9431528449058533; max-choc-feature=0.9431528449058533; \n",
      "action's feature value=0.9778868556022644; max-choc-feature=0.9778868556022644; \n",
      "action's feature value=0.8884769678115845; max-choc-feature=0.8884769678115845; \n",
      "action's feature value=0.9702693819999695; max-choc-feature=0.9702693819999695; \n",
      "action's feature value=0.9528592824935913; max-choc-feature=0.9528592824935913; \n",
      "action's feature value=0.9749237895011902; max-choc-feature=0.9749237895011902; \n",
      "action's feature value=0.9898673295974731; max-choc-feature=0.9898673295974731; \n",
      "action's feature value=0.9735749363899231; max-choc-feature=0.9735749363899231; \n",
      "action's feature value=0.9992828369140625; max-choc-feature=0.9992828369140625; \n",
      "action's feature value=0.9064218401908875; max-choc-feature=0.9064218401908875; \n",
      "action's feature value=0.939630925655365; max-choc-feature=0.939630925655365; \n",
      "action's feature value=0.9697129726409912; max-choc-feature=0.9697129726409912; \n",
      "action's feature value=0.9274624586105347; max-choc-feature=0.9274624586105347; \n",
      "action's feature value=0.9641268849372864; max-choc-feature=0.9641268849372864; \n",
      "action's feature value=0.9586440920829773; max-choc-feature=0.9586440920829773; \n",
      "action's feature value=0.7710433602333069; max-choc-feature=0.7710433602333069; \n",
      "action's feature value=0.9894372820854187; max-choc-feature=0.9894372820854187; \n",
      "action's feature value=0.9787902235984802; max-choc-feature=0.9787902235984802; \n",
      "action's feature value=0.9882153868675232; max-choc-feature=0.9882153868675232; \n",
      "action's feature value=0.9798690676689148; max-choc-feature=0.9798690676689148; \n",
      "action's feature value=0.9689604043960571; max-choc-feature=0.9689604043960571; \n",
      "action's feature value=0.9791313409805298; max-choc-feature=0.9791313409805298; \n",
      "action's feature value=0.9631713032722473; max-choc-feature=0.9631713032722473; \n",
      "action's feature value=0.9546357989311218; max-choc-feature=0.9546357989311218; \n",
      "action's feature value=0.8759862780570984; max-choc-feature=0.8759862780570984; \n",
      "action's feature value=0.7734934091567993; max-choc-feature=0.7734934091567993; \n",
      "action's feature value=0.9949907660484314; max-choc-feature=0.9949907660484314; \n",
      "action's feature value=0.9754519462585449; max-choc-feature=0.9754519462585449; \n",
      "action's feature value=0.9811046123504639; max-choc-feature=0.9811046123504639; \n",
      "action's feature value=0.934320867061615; max-choc-feature=0.934320867061615; \n",
      "action's feature value=0.9827705025672913; max-choc-feature=0.9827705025672913; \n",
      "action's feature value=0.9622542262077332; max-choc-feature=0.9622542262077332; \n",
      "action's feature value=0.9259800910949707; max-choc-feature=0.9259800910949707; \n",
      "action's feature value=0.9980040192604065; max-choc-feature=0.9980040192604065; \n",
      "action's feature value=0.950517475605011; max-choc-feature=0.950517475605011; \n",
      "action's feature value=0.9714840054512024; max-choc-feature=0.9714840054512024; \n",
      "action's feature value=0.9996949434280396; max-choc-feature=0.9996949434280396; \n",
      "action's feature value=0.9778310060501099; max-choc-feature=0.9778310060501099; \n",
      "action's feature value=0.9740731120109558; max-choc-feature=0.9740731120109558; \n",
      "action's feature value=0.910713255405426; max-choc-feature=0.910713255405426; \n",
      "action's feature value=0.9712516665458679; max-choc-feature=0.9712516665458679; \n",
      "action's feature value=0.993937075138092; max-choc-feature=0.993937075138092; \n",
      "action's feature value=0.9804303050041199; max-choc-feature=0.9804303050041199; \n",
      "action's feature value=0.9888405799865723; max-choc-feature=0.9888405799865723; \n",
      "action's feature value=0.9498463869094849; max-choc-feature=0.9498463869094849; \n",
      "action's feature value=0.9784277677536011; max-choc-feature=0.9784277677536011; \n",
      "action's feature value=0.9742754697799683; max-choc-feature=0.9742754697799683; \n",
      "action's feature value=0.8748195767402649; max-choc-feature=0.8748195767402649; \n",
      "action's feature value=0.905616044998169; max-choc-feature=0.905616044998169; \n",
      "action's feature value=0.9237788319587708; max-choc-feature=0.9237788319587708; \n",
      "action's feature value=0.9878206849098206; max-choc-feature=0.9878206849098206; \n",
      "action's feature value=0.9580094218254089; max-choc-feature=0.9580094218254089; \n",
      "action's feature value=0.9571616649627686; max-choc-feature=0.9571616649627686; \n",
      "action's feature value=0.9923229813575745; max-choc-feature=0.9923229813575745; \n",
      "action's feature value=0.9989432692527771; max-choc-feature=0.9989432692527771; \n",
      "action's feature value=0.9267414212226868; max-choc-feature=0.9267414212226868; \n",
      "action's feature value=0.9657601118087769; max-choc-feature=0.9657601118087769; \n",
      "action's feature value=0.9476619362831116; max-choc-feature=0.9476619362831116; \n",
      "action's feature value=0.9223950505256653; max-choc-feature=0.9223950505256653; \n",
      "action's feature value=0.9652732610702515; max-choc-feature=0.9652732610702515; \n",
      "action's feature value=0.9806386828422546; max-choc-feature=0.9806386828422546; \n",
      "action's feature value=0.9588943719863892; max-choc-feature=0.9588943719863892; \n",
      "action's feature value=0.9703181385993958; max-choc-feature=0.9703181385993958; \n",
      "action's feature value=0.9813711643218994; max-choc-feature=0.9813711643218994; \n"
     ]
    }
   ],
   "source": [
    "# Let's see what items our bandit recommends now that it has been trained and achieves good (>> random) rewards.\n",
    "obs = lts_20_2_env.reset()\n",
    "\n",
    "# Run a single episode.\n",
    "done = False\n",
    "while not done:\n",
    "    # Pass the single (unbatched) observation into the `compute_single_action` method of our Trainer.\n",
    "    # This is one way to perform inference on a learned policy.\n",
    "    action = bandit_trainer.compute_single_action(input_dict={\"obs\": obs})\n",
    "    feat_value_of_action = obs[\"item\"][action][0]\n",
    "    max_choc_feat = obs['item'][np.argmax(obs[\"item\"])][0]\n",
    "\n",
    "    #not_same = feat_value_of_action != max_choc_feat\n",
    "    #if not_same:\n",
    "    #    print(\"NOT SAME!\")\n",
    "\n",
    "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
    "    print(f\"action's feature value={feat_value_of_action}; max-choc-feature={max_choc_feat}; \")\n",
    "\n",
    "    # Apply the computed action in the environment and continue.\n",
    "    obs, r, done, _ = lts_20_2_env.step(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9355c1b-f0f7-4690-a7fe-332b01a651c4",
   "metadata": {},
   "source": [
    "### Ok, Bandits want Chocolate! :)\n",
    "#### Why is that?\n",
    "\n",
    "<img src=\"images/contextual_bandit.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84291b69-050f-489a-b822-239294bb3a2e",
   "metadata": {},
   "source": [
    "### Recap: Advantages and Disatvantages of Bandits:\n",
    "#### Advantages:\n",
    "* Very fast\n",
    "* Very sample-efficient\n",
    "* Easy to understand learning process\n",
    "\n",
    "#### Disadvantages\n",
    "* Need immediate reward (not capable of solving long-horizon credit assignment problem)\n",
    "* Models user -> If > 1 user, must train separate bandit per user\n",
    "* Not able to handle components of MultiDiscrete action space separately (works only on flattened Discrete action space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a830b-cc5f-454c-b986-75c59d40df89",
   "metadata": {},
   "source": [
    "### Switching to Slate-Q\n",
    "\n",
    "RLlib offers another algorithm - Slate-Q - designed for k-slate, long time horizon, and dynamic user recommendation problems. Let's take a quick look:\n",
    "\n",
    "<img src=\"images/slateq.png\" width=1000>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49c258f2-60db-4ed2-8c4f-8f83809d1d73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 10:54:22,206\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-03-28 10:54:22,207\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-03-28 10:54:36,030\tINFO trainable.py:145 -- Trainable.setup took 13.846 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-03-28 10:54:36,031\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SlateQTrainer"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.agents.slateq import SlateQTrainer\n",
    "\n",
    "slateq_config = {\n",
    "    \"env\": \"modified_lts\",\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 20,  # MultiDiscrete([20, 20]) -> no flattening necessary (see `convert_to_discrete_action_space=False` below)\n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "        \"wrap_for_bandits\": False,  # SlateQ != Bandit (will keep \"doc\" key, instead of \"items\")\n",
    "        \"convert_to_discrete_action_space\": False,  # SlateQ handles MultiDiscrete action spaces (slate recommendations).\n",
    "        \"seed\": 0,\n",
    "    },\n",
    "    \"seed\": 0,\n",
    "    # Setup exploratory behavior: Implemented as \"epsilon greedy\" strategy:\n",
    "    # Act randomly `e` percent of the time; `e` gets reduced from 1.0 to almost 0.0 over\n",
    "    # the course of `epsilon_timesteps`.\n",
    "    \"exploration_config\": {\n",
    "        \"warmup_timesteps\": 10000,\n",
    "        \"epsilon_timesteps\": 30000,\n",
    "    },\n",
    "    \"learning_starts\": 10000,\n",
    "    \"target_network_update_freq\": 3200,\n",
    "\n",
    "    # Report rewards as smoothed mean over this many episodes.\n",
    "    \"metrics_num_episodes_for_smoothing\": 200,\n",
    "}\n",
    "\n",
    "# Instantiate the Trainer object using the exact same config as in our Bandit experiment above.\n",
    "slateq_trainer = SlateQTrainer(config=slateq_config)\n",
    "slateq_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {},
   "source": [
    "Now that we have confirmed we have setup the Trainer correctly, let's call `train()` on it several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc47c75f-4f6f-4806-995e-80ec974cfd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=1; ts=10000: R(\"return\")=1156.8393079644813\n",
      "Iteration=2; ts=11000: R(\"return\")=1156.9101996607524\n",
      "Iteration=3; ts=12000: R(\"return\")=1157.1654568789556\n",
      "Iteration=4; ts=13000: R(\"return\")=1157.3299280819233\n",
      "Iteration=5; ts=14000: R(\"return\")=1157.8959780047394\n",
      "Iteration=6; ts=15000: R(\"return\")=1158.4471558545604\n",
      "Iteration=7; ts=16000: R(\"return\")=1158.9215738029075\n",
      "Iteration=8; ts=17000: R(\"return\")=1159.311567436698\n",
      "Iteration=9; ts=18000: R(\"return\")=1159.5565944734403\n",
      "Iteration=10; ts=19000: R(\"return\")=1159.6901006447542\n",
      "Iteration=11; ts=20000: R(\"return\")=1160.2592586255505\n",
      "Iteration=12; ts=21000: R(\"return\")=1160.6650484346037\n",
      "Iteration=13; ts=22000: R(\"return\")=1161.0134447211126\n",
      "Iteration=14; ts=23000: R(\"return\")=1161.4316240970927\n",
      "Iteration=15; ts=24000: R(\"return\")=1161.4963537853039\n",
      "Iteration=16; ts=25000: R(\"return\")=1161.7693483177497\n",
      "Iteration=17; ts=26000: R(\"return\")=1162.6794718901526\n",
      "Iteration=18; ts=27000: R(\"return\")=1163.3093933045652\n",
      "Iteration=19; ts=28000: R(\"return\")=1163.4832424507447\n",
      "Iteration=20; ts=29000: R(\"return\")=1163.976882615679\n",
      "Iteration=21; ts=30000: R(\"return\")=1164.5802580196448\n",
      "Iteration=22; ts=31000: R(\"return\")=1164.992688708397\n",
      "Iteration=23; ts=32000: R(\"return\")=1165.082466427657\n",
      "Iteration=24; ts=33000: R(\"return\")=1165.4084921017095\n",
      "Iteration=25; ts=34000: R(\"return\")=1165.9147440934303\n",
      "Iteration=26; ts=35000: R(\"return\")=1166.2684110741338\n",
      "Iteration=27; ts=36000: R(\"return\")=1166.4047235371256\n",
      "Iteration=28; ts=37000: R(\"return\")=1166.3329616822098\n",
      "Iteration=29; ts=38000: R(\"return\")=1166.4033860207492\n",
      "Iteration=30; ts=39000: R(\"return\")=1166.4529416626547\n"
     ]
    }
   ],
   "source": [
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "for _ in range(30):\n",
    "    results = slateq_trainer.train()\n",
    "    print(f\"Iteration={slateq_trainer.iteration}; ts={results['timesteps_total']}: R(\\\"return\\\")={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5752238e-5c71-402b-9bfa-82b03b30b5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature-value=0.9437480568885803 most-choc=0.978618323802948\n",
      "feature-value=0.9883738160133362 most-choc=0.9883738160133362\n",
      "feature-value=0.9767611026763916 most-choc=0.9767611026763916\n",
      "feature-value=0.5759465098381042 most-choc=0.9292961955070496\n",
      "feature-value=0.9527490139007568 most-choc=0.9621885418891907\n",
      "feature-value=0.6439902186393738 most-choc=0.9560836553573608\n",
      "feature-value=0.9988470077514648 most-choc=0.9988470077514648\n",
      "feature-value=0.9280812740325928 most-choc=0.9755215048789978\n",
      "feature-value=0.9443724155426025 most-choc=0.9443724155426025\n",
      "feature-value=0.990338921546936 most-choc=0.990338921546936\n",
      "feature-value=0.9527916312217712 most-choc=0.9527916312217712\n",
      "feature-value=0.961936354637146 most-choc=0.961936354637146\n",
      "feature-value=0.961570143699646 most-choc=0.9774951338768005\n",
      "feature-value=0.9818294048309326 most-choc=0.9818294048309326\n",
      "feature-value=0.4205394685268402 most-choc=0.9065554738044739\n",
      "feature-value=0.7774075865745544 most-choc=0.7885454893112183\n",
      "feature-value=0.9564056992530823 most-choc=0.9594333171844482\n",
      "feature-value=0.9591665863990784 most-choc=0.9591665863990784\n",
      "feature-value=0.9589827060699463 most-choc=0.9589827060699463\n",
      "feature-value=0.9453015327453613 most-choc=0.9453015327453613\n",
      "feature-value=0.9894098043441772 most-choc=0.9903450012207031\n",
      "feature-value=0.904948353767395 most-choc=0.9920112490653992\n",
      "feature-value=0.9944007992744446 most-choc=0.9944007992744446\n",
      "feature-value=0.96779465675354 most-choc=0.96779465675354\n",
      "feature-value=0.8623185753822327 most-choc=0.9241587519645691\n",
      "feature-value=0.9488610029220581 most-choc=0.9488610029220581\n",
      "feature-value=0.8869608044624329 most-choc=0.9627703428268433\n",
      "feature-value=0.968286395072937 most-choc=0.9805801510810852\n",
      "feature-value=0.997962236404419 most-choc=0.997962236404419\n",
      "feature-value=0.9992780089378357 most-choc=0.9992780089378357\n",
      "feature-value=0.9587409496307373 most-choc=0.9762256741523743\n",
      "feature-value=0.9834262132644653 most-choc=0.9834262132644653\n",
      "feature-value=0.8224067091941833 most-choc=0.857124924659729\n",
      "feature-value=0.95914226770401 most-choc=0.95914226770401\n",
      "feature-value=0.7814795970916748 most-choc=0.8562761545181274\n",
      "feature-value=0.8956912755966187 most-choc=0.987348735332489\n",
      "feature-value=0.8818964958190918 most-choc=0.9707314372062683\n",
      "feature-value=0.7918795943260193 most-choc=0.9918903112411499\n",
      "feature-value=0.9758838415145874 most-choc=0.9758838415145874\n",
      "feature-value=0.8743999004364014 most-choc=0.9543338418006897\n",
      "feature-value=0.47938454151153564 most-choc=0.9323939681053162\n",
      "feature-value=0.8822836875915527 most-choc=0.9195073843002319\n",
      "feature-value=0.9631972908973694 most-choc=0.9631972908973694\n",
      "feature-value=0.8667885661125183 most-choc=0.8667885661125183\n",
      "feature-value=0.9804856777191162 most-choc=0.9804856777191162\n",
      "feature-value=0.820022463798523 most-choc=0.9031496644020081\n",
      "feature-value=0.9738187193870544 most-choc=0.9738187193870544\n",
      "feature-value=0.9998085498809814 most-choc=0.9998085498809814\n",
      "feature-value=0.9384120106697083 most-choc=0.9384120106697083\n",
      "feature-value=0.9536756873130798 most-choc=0.9536756873130798\n",
      "feature-value=0.9649249315261841 most-choc=0.9958152770996094\n",
      "feature-value=0.7912274599075317 most-choc=0.9692058563232422\n",
      "feature-value=0.9750946760177612 most-choc=0.9750946760177612\n",
      "feature-value=0.9670549035072327 most-choc=0.9673377871513367\n",
      "feature-value=0.9851086735725403 most-choc=0.9851086735725403\n",
      "feature-value=0.9878891706466675 most-choc=0.9979940056800842\n",
      "feature-value=0.8268052935600281 most-choc=0.9555683732032776\n",
      "feature-value=0.923455536365509 most-choc=0.983853816986084\n",
      "feature-value=0.8668609261512756 most-choc=0.9279761910438538\n",
      "feature-value=0.8882650136947632 most-choc=0.9421847462654114\n",
      "feature-value=0.992667019367218 most-choc=0.992667019367218\n",
      "feature-value=0.9554659128189087 most-choc=0.9717630743980408\n",
      "feature-value=0.8615909814834595 most-choc=0.9792863130569458\n",
      "feature-value=0.9960712790489197 most-choc=0.9960712790489197\n",
      "feature-value=0.9942330718040466 most-choc=0.9942330718040466\n",
      "feature-value=0.9132835865020752 most-choc=0.9824448823928833\n",
      "feature-value=0.8103020787239075 most-choc=0.9492799043655396\n",
      "feature-value=0.9568705558776855 most-choc=0.9568705558776855\n",
      "feature-value=0.9853785634040833 most-choc=0.9853785634040833\n",
      "feature-value=0.9121509790420532 most-choc=0.9903685450553894\n",
      "feature-value=0.9351605176925659 most-choc=0.9980227947235107\n",
      "feature-value=0.9164190292358398 most-choc=0.9376630783081055\n",
      "feature-value=0.8429732918739319 most-choc=0.9834339618682861\n",
      "feature-value=0.9003674387931824 most-choc=0.9832748770713806\n",
      "feature-value=0.9792702198028564 most-choc=0.9804664850234985\n",
      "feature-value=0.9231590032577515 most-choc=0.9825738668441772\n",
      "feature-value=0.9983548521995544 most-choc=0.9983548521995544\n",
      "feature-value=0.74024498462677 most-choc=0.7697890400886536\n",
      "feature-value=0.9790109395980835 most-choc=0.9985265731811523\n",
      "feature-value=0.827313244342804 most-choc=0.9906516671180725\n",
      "feature-value=0.845364511013031 most-choc=0.9890884160995483\n",
      "feature-value=0.9596956372261047 most-choc=0.9605224132537842\n",
      "feature-value=0.8180772066116333 most-choc=0.9260265231132507\n",
      "feature-value=0.8526890277862549 most-choc=0.9829264879226685\n",
      "feature-value=0.9454309940338135 most-choc=0.9454309940338135\n",
      "feature-value=0.8257381916046143 most-choc=0.9270205497741699\n",
      "feature-value=0.9585323333740234 most-choc=0.9585323333740234\n",
      "feature-value=0.9738933444023132 most-choc=0.9738933444023132\n",
      "feature-value=0.904982328414917 most-choc=0.9793245196342468\n",
      "feature-value=0.9582793116569519 most-choc=0.9890880584716797\n",
      "feature-value=0.8618229627609253 most-choc=0.9670467972755432\n",
      "feature-value=0.9465571045875549 most-choc=0.9703752994537354\n",
      "feature-value=0.9550641179084778 most-choc=0.9550641179084778\n",
      "feature-value=0.9574885368347168 most-choc=0.9574885368347168\n",
      "feature-value=0.9229142665863037 most-choc=0.9229142665863037\n",
      "feature-value=0.8855226635932922 most-choc=0.9832027554512024\n",
      "feature-value=0.9427794218063354 most-choc=0.998198926448822\n",
      "feature-value=0.7808851599693298 most-choc=0.9642097353935242\n",
      "feature-value=0.9004101753234863 most-choc=0.9417421221733093\n",
      "feature-value=0.9040508270263672 most-choc=0.9040508270263672\n",
      "feature-value=0.9788569808006287 most-choc=0.9835554361343384\n",
      "feature-value=0.9334558844566345 most-choc=0.9747744202613831\n",
      "feature-value=0.8196898102760315 most-choc=0.8408302664756775\n",
      "feature-value=0.9482967257499695 most-choc=0.9682117700576782\n",
      "feature-value=0.9669559597969055 most-choc=0.974723219871521\n",
      "feature-value=0.9219157695770264 most-choc=0.9895250797271729\n",
      "feature-value=0.04585035517811775 most-choc=0.9818642139434814\n",
      "feature-value=0.9736415147781372 most-choc=0.987434983253479\n",
      "feature-value=0.8903785347938538 most-choc=0.9674006104469299\n",
      "feature-value=0.9522157907485962 most-choc=0.9522157907485962\n",
      "feature-value=0.04134637117385864 most-choc=0.9649279713630676\n",
      "feature-value=0.913392186164856 most-choc=0.9949018955230713\n",
      "feature-value=0.9238783121109009 most-choc=0.9630938768386841\n",
      "feature-value=0.9801590442657471 most-choc=0.9801590442657471\n",
      "feature-value=0.897433340549469 most-choc=0.9809793829917908\n",
      "feature-value=0.9905391931533813 most-choc=0.9905391931533813\n",
      "feature-value=0.9747872352600098 most-choc=0.9747872352600098\n",
      "feature-value=0.8878650069236755 most-choc=0.9961004257202148\n",
      "feature-value=0.815617561340332 most-choc=0.9473085999488831\n",
      "feature-value=0.9615751504898071 most-choc=0.9615751504898071\n"
     ]
    }
   ],
   "source": [
    "lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config=slateq_config[\"env_config\"]))\n",
    "\n",
    "obs = lts_20_2_env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = slateq_trainer.compute_single_action(input_dict={\"obs\": obs})\n",
    "    feat_value = obs[\"doc\"][str(action[0])][0]\n",
    "    max_feat_action = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    print(f\"feature-value={feat_value} most-choc={obs['doc'][str(max_feat_action)][0]}\")\n",
    "    obs, r, done, _ = lts_20_2_env.step(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK!\n",
    "\n",
    "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using the SlateQ Policy and its NN models inside our Trainer object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b2b62d74-6392-453a-b25f-f8cbc90009d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Policy right now is: SlateQTFPolicy\n",
      "Our Policy's observation space is: Dict(user:Box([0.], [1.], (1,), float32), doc:Dict(0:Box([0.], [1.], (1,), float32), 1:Box([0.], [1.], (1,), float32), 2:Box([0.], [1.], (1,), float32), 3:Box([0.], [1.], (1,), float32), 4:Box([0.], [1.], (1,), float32), 5:Box([0.], [1.], (1,), float32), 6:Box([0.], [1.], (1,), float32), 7:Box([0.], [1.], (1,), float32), 8:Box([0.], [1.], (1,), float32), 9:Box([0.], [1.], (1,), float32), 10:Box([0.], [1.], (1,), float32), 11:Box([0.], [1.], (1,), float32), 12:Box([0.], [1.], (1,), float32), 13:Box([0.], [1.], (1,), float32), 14:Box([0.], [1.], (1,), float32), 15:Box([0.], [1.], (1,), float32), 16:Box([0.], [1.], (1,), float32), 17:Box([0.], [1.], (1,), float32), 18:Box([0.], [1.], (1,), float32), 19:Box([0.], [1.], (1,), float32)), response:Tuple(Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), float32)), Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), float32))))\n",
      "\n",
      "Our Policy's action space is: MultiDiscrete([20 20])\n",
      "\n",
      "q_values_per_candidate=[[16.727158 16.888716 16.365984 16.338894 16.673454 16.134388 16.768316\n",
      "  16.32355  16.246939 16.777302 16.214003 16.44961  16.013039 16.451307\n",
      "  16.206713 16.199482 16.410765 16.695824 16.202103 16.332743]]\n"
     ]
    }
   ],
   "source": [
    "# To get the policy inside the Trainer, use `Trainer.get_policy([policy ID]=\"default_policy\")`:\n",
    "policy = slateq_trainer.get_policy()\n",
    "print(f\"Our Policy right now is: {policy}\")\n",
    "\n",
    "# To get to the model inside any policy, do:\n",
    "model = policy.model\n",
    "#print(f\"Our Policy's model is: {model}\")\n",
    "\n",
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\\n\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\\n\")\n",
    "\n",
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = lts_20_2_env.observation_space.sample()\n",
    "\n",
    "# tf-specific code: Use tf1.Session().\n",
    "sess = policy.get_session()\n",
    "\n",
    "# Get the action logits (as torch tensor).\n",
    "with sess.graph.as_default():\n",
    "    q_values_per_candidate = model.q_value_head([\n",
    "        np.expand_dims(obs[\"user\"], 0),\n",
    "        np.expand_dims(np.concatenate([value for value in obs[\"doc\"].values()]), 0),\n",
    "    ])\n",
    "print(f\"q_values_per_candidate={sess.run(q_values_per_candidate)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddf1390-bdea-412a-be31-ebbbc6f4ac72",
   "metadata": {},
   "source": [
    "#### !END: OPTIONAL HACK!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de603d14-f0cb-4363-a72b-8f147c094071",
   "metadata": {},
   "source": [
    "In order to release all resources from a Trainer, you can use a Trainer's `stop()` method.\n",
    "You should definitley run this cell as it frees resources that we'll need later in this tutorial, when we'll do parallel hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737dca4f-942f-4fda-abcc-0052263a103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to release resources that a Trainer uses, you can call its `stop()` method:\n",
    "slateq_trainer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce74b8-20ed-43c5-ad88-54a2dec32f71",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recap: Advantages and Disadvantages of SlateQ:\n",
    "#### Advantages:\n",
    "* Decomposes MultiDiscrete action space (better understanding of items inside a k-slate)\n",
    "* Handles long-horizon credit assignment better than bandits (Q-learning)\n",
    "* Handles > 1 user problems\n",
    "* Sample efficient (due to replay buffer + off-policy DQN-style learning)\n",
    "\n",
    "#### Disadvantages\n",
    "* Uses larger (deep) model(s): One Q-value NN head per candidate\n",
    "* Slower and heavier feel to it\n",
    "* Requires careful hyperparameter-tuning, e.g. exploration timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d282a-4ad1-4d5f-9dec-00afb8154048",
   "metadata": {
    "tags": []
   },
   "source": [
    "------------------\n",
    "## 15 min break :)\n",
    "\n",
    "... (while Slate-Q is running .. and hopefully learning).\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82057-6b4c-4075-bd32-93c3426a1700",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction to Offline RL\n",
    "\n",
    "<img src=\"images/offline_rl.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f5114691-b74f-4b4e-8bdb-5df704bee067",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The logdir contains the following files:\n",
      "['README.py',\n",
      " 'output-2022-03-14_13-16-05_worker-2_239.json',\n",
      " '.ipynb_checkpoints']\n",
      "\n",
      "\n",
      "The JSON file with all sampled trajectories is:\n",
      "offline_rl/output-2022-03-14_13-16-05_worker-2_239.json\n"
     ]
    }
   ],
   "source": [
    "# The previous tune.run (the one we did before the break) produced \"historic data\" output.\n",
    "# We will use this output in the following as input to a newly initialized, untrained offline RL algorithm.\n",
    "\n",
    "# Let's take a look at the generated file(s) first:\n",
    "output_dir = \"offline_rl/\"\n",
    "#print(output_dir)\n",
    "\n",
    "# Here is what the best log directory contains:\n",
    "print(\"\\n\\nThe logdir contains the following files:\")\n",
    "all_output_files = os.listdir(os.path.dirname(output_dir + \"/\"))\n",
    "pprint(all_output_files)\n",
    "\n",
    "json_output_file = os.path.join(output_dir, [f for f in all_output_files if re.match(\"^.*worker.*\\.json$\", f)][0])\n",
    "print(\"\\n\\nThe JSON file with all sampled trajectories is:\")\n",
    "print(json_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4230f",
   "metadata": {},
   "source": [
    "### Using an (offline) input file with an offline RL algorithm.\n",
    "\n",
    "We will now pretend that we don't have a simulator for our problem (same recommender system problem as above) available, however, let's assume we possess a lot of pre-recorded, historic data from some legacy (non-RL) system.\n",
    "\n",
    "Assuming that this legacy system wrote some data into a JSON file (we'll simply use the same JSON file that our SlateQ algo produced above), how can we use this historic data to do RL either way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "34a0aaea-b811-41e1-9e6c-532d9ce1b060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>obs</th>\n",
       "      <th>new_obs</th>\n",
       "      <th>actions</th>\n",
       "      <th>prev_actions</th>\n",
       "      <th>rewards</th>\n",
       "      <th>prev_rewards</th>\n",
       "      <th>dones</th>\n",
       "      <th>infos</th>\n",
       "      <th>eps_id</th>\n",
       "      <th>unroll_id</th>\n",
       "      <th>agent_index</th>\n",
       "      <th>t</th>\n",
       "      <th>vf_preds</th>\n",
       "      <th>action_dist_inputs</th>\n",
       "      <th>action_prob</th>\n",
       "      <th>action_logp</th>\n",
       "      <th>advantages</th>\n",
       "      <th>value_targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>[[0.947974801063537, 0.7422540783882141, -0.30...</td>\n",
       "      <td>[[0.947974801063537, 0.7422540783882141, -0.30...</td>\n",
       "      <td>[[18, 13], [16, 4], [19, 15], [19, 18], [19, 1...</td>\n",
       "      <td>[[4, 11], [18, 13], [16, 4], [19, 15], [19, 18...</td>\n",
       "      <td>[0.0, 4.0, 0.0, 4.0, 4.0, 0.0, 4.0, 0.0, 4.0, ...</td>\n",
       "      <td>[4.0, 0.0, 4.0, 0.0, 4.0, 4.0, 0.0, 4.0, 0.0, ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[{'env': '&lt;recsim.simulator.environment.Single...</td>\n",
       "      <td>[1118026545, 1118026545, 1118026545, 111802654...</td>\n",
       "      <td>[1613, 1613, 1613, 1613, 1613, 1613, 1613, 161...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>[29.020187377929688, 23.39920425415039, 13.822...</td>\n",
       "      <td>[[1.67626404762268, 0.6096312403678891, -1.030...</td>\n",
       "      <td>[0.0018913851818060001, 0.007846021093428001, ...</td>\n",
       "      <td>[-6.270445823669434, -4.847748756408691, -6.56...</td>\n",
       "      <td>[70.58807373046875, 77.21520233154297, 83.7674...</td>\n",
       "      <td>[99.60826110839844, 100.6144027709961, 97.5903...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                                                obs  \\\n",
       "0  SampleBatch  [[0.947974801063537, 0.7422540783882141, -0.30...   \n",
       "\n",
       "                                             new_obs  \\\n",
       "0  [[0.947974801063537, 0.7422540783882141, -0.30...   \n",
       "\n",
       "                                             actions  \\\n",
       "0  [[18, 13], [16, 4], [19, 15], [19, 18], [19, 1...   \n",
       "\n",
       "                                        prev_actions  \\\n",
       "0  [[4, 11], [18, 13], [16, 4], [19, 15], [19, 18...   \n",
       "\n",
       "                                             rewards  \\\n",
       "0  [0.0, 4.0, 0.0, 4.0, 4.0, 0.0, 4.0, 0.0, 4.0, ...   \n",
       "\n",
       "                                        prev_rewards  \\\n",
       "0  [4.0, 0.0, 4.0, 0.0, 4.0, 4.0, 0.0, 4.0, 0.0, ...   \n",
       "\n",
       "                                               dones  \\\n",
       "0  [False, False, False, False, False, False, Fal...   \n",
       "\n",
       "                                               infos  \\\n",
       "0  [{'env': '<recsim.simulator.environment.Single...   \n",
       "\n",
       "                                              eps_id  \\\n",
       "0  [1118026545, 1118026545, 1118026545, 111802654...   \n",
       "\n",
       "                                           unroll_id  \\\n",
       "0  [1613, 1613, 1613, 1613, 1613, 1613, 1613, 161...   \n",
       "\n",
       "                                         agent_index  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                   t  \\\n",
       "0  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "\n",
       "                                            vf_preds  \\\n",
       "0  [29.020187377929688, 23.39920425415039, 13.822...   \n",
       "\n",
       "                                  action_dist_inputs  \\\n",
       "0  [[1.67626404762268, 0.6096312403678891, -1.030...   \n",
       "\n",
       "                                         action_prob  \\\n",
       "0  [0.0018913851818060001, 0.007846021093428001, ...   \n",
       "\n",
       "                                         action_logp  \\\n",
       "0  [-6.270445823669434, -4.847748756408691, -6.56...   \n",
       "\n",
       "                                          advantages  \\\n",
       "0  [70.58807373046875, 77.21520233154297, 83.7674...   \n",
       "\n",
       "                                       value_targets  \n",
       "0  [99.60826110839844, 100.6144027709961, 97.5903...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the output file first:\n",
    "dataframe = pandas.read_json(json_output_file, lines=True)  # don't forget lines=True -> Each line in the json is one \"rollout\" of 4 timesteps.\n",
    "dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "11d33782-b4f1-4160-aa7b-657879bc1e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 11:11:09,457\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MARWILTrainer"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's configure a new RLlib Trainer, one that's capable of reading the JSON input described\n",
    "# above and able to learn from this input.\n",
    "\n",
    "# For simplicity, we'll start with a behavioral cloning (BC) trainer:\n",
    "from ray.rllib.agents.marwil import MARWILTrainer\n",
    "\n",
    "offline_rl_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 20,\n",
    "    \"slate_size\": 2,\n",
    "    \"wrap_for_bandits\": False,  # SlateQ != Bandit\n",
    "    \"convert_to_discrete_action_space\": False,\n",
    "}))\n",
    "\n",
    "\n",
    "\n",
    "offline_rl_config = {\n",
    "    # Specify your offline RL algo's historic (JSON) inputs:\n",
    "    \"input\": [json_output_file],\n",
    "    # Note: For non-offline RL algos, this is set to \"sampler\" by default.\n",
    "    #\"input\": \"sampler\",\n",
    "\n",
    "    # Since we don't have an environment and the obs/action-spaces are not defined in the JSON file,\n",
    "    # we need to provide these here manually.\n",
    "    \"env\": None,  # default\n",
    "    \"observation_space\": offline_rl_env.observation_space,\n",
    "    \"action_space\": offline_rl_env.action_space,\n",
    "\n",
    "    # Perform \"off-policy estimation\" (OPE) on train batches and report results.\n",
    "    \"input_evaluation\": [\"is\", \"wis\"],\n",
    "}\n",
    "\n",
    "# Create a behavior cloning (BC) Trainer.\n",
    "marwil_trainer = MARWILTrainer(config=offline_rl_config)\n",
    "marwil_trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d181dd19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 472) for Tensor default_policy/obs_1:0, which has shape (1, 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Let's train our new behavioral cloning Trainer for some iterations:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmarwil_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_agent_steps_trained\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m steps trained; reward = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/trainable.py:342\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    341\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 342\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:1093\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1091\u001b[0m             \u001b[38;5;66;03m# Allow logs messages to propagate.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m-> 1093\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1095\u001b[0m result \u001b[38;5;241m=\u001b[39m step_attempt_results\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkers\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers, WorkerSet):\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# Sync filters on workers.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:1074\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step_ctx\u001b[38;5;241m.\u001b[39mshould_stop(step_attempt_results):\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m         step_attempt_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_attempt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# @ray.remote RolloutWorker failure.\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1077\u001b[0m         \u001b[38;5;66;03m# Try to recover w/o the failed worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:1155\u001b[0m, in \u001b[0;36mTrainer.step_attempt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;66;03m# No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluate_this_iter:\n\u001b[0;32m-> 1155\u001b[0m     step_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_plan_or_training_iteration_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# We have to evaluate in this training iteration.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;66;03m# No parallelism.\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:2174\u001b[0m, in \u001b[0;36mTrainer._exec_plan_or_training_iteration_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2172\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_iteration()\n\u001b[1;32m   2173\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2174\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_exec_impl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:779\u001b[0m, in \u001b[0;36mLocalIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_once()\n\u001b[0;32m--> 779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilt_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:807\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[1;32m    809\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:869\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[1;32m    871\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:869\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[1;32m    871\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:807\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[1;32m    809\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:869\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[1;32m    871\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:1108\u001b[0m, in \u001b[0;36mLocalIterator.union.<locals>.build_union\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_pull):\n\u001b[0;32m-> 1108\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[1;32m   1110\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:779\u001b[0m, in \u001b[0;36mLocalIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_once()\n\u001b[0;32m--> 779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilt_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:807\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[1;32m    809\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:807\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[1;32m    809\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/util/iter.py:807\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[1;32m    809\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/execution/rollout_ops.py:137\u001b[0m, in \u001b[0;36mParallelRollouts.<locals>.sampler\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msampler\u001b[39m(_):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mworkers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:815\u001b[0m, in \u001b[0;36mRolloutWorker.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_start\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    809\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating sample batch of size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    811\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_fragment_length\n\u001b[1;32m    812\u001b[0m         )\n\u001b[1;32m    813\u001b[0m     )\n\u001b[0;32m--> 815\u001b[0m batches \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    816\u001b[0m steps_so_far \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    817\u001b[0m     batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcount\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount_steps_by \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39magent_steps()\n\u001b[1;32m    820\u001b[0m )\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# In truncate_episodes mode, never pull more than 1 batch per env.\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# This avoids over-running the target batch size.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/offline/shuffled_input.py:34\u001b[0m, in \u001b[0;36mShuffledInput.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@override\u001b[39m(InputReader)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleBatchType:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn:\n\u001b[1;32m     36\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilling shuffle buffer to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m batches\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/offline/json_reader.py:186\u001b[0m, in \u001b[0;36mJsonReader.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to read valid experience batch from file: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    182\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_file\n\u001b[1;32m    183\u001b[0m         )\n\u001b[1;32m    184\u001b[0m     )\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_postprocess_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/offline/json_reader.py:215\u001b[0m, in \u001b[0;36mJsonReader._postprocess_if_needed\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sub_batch \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39msplit_by_episode():\n\u001b[0;32m--> 215\u001b[0m         out\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_batch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SampleBatch\u001b[38;5;241m.\u001b[39mconcat_samples(out)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# TODO(ekl) this is trickier since the alignments between agent\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m#  trajectories in the episode are not available any more.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/policy/tf_policy_template.py:286\u001b[0m, in \u001b[0;36mbuild_tf_policy.<locals>.policy_cls.postprocess_trajectory\u001b[0;34m(self, sample_batch, other_agent_batches, episode)\u001b[0m\n\u001b[1;32m    284\u001b[0m sample_batch \u001b[38;5;241m=\u001b[39m Policy\u001b[38;5;241m.\u001b[39mpostprocess_trajectory(\u001b[38;5;28mself\u001b[39m, sample_batch)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m postprocess_fn:\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpostprocess_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_agent_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample_batch\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/agents/marwil/marwil_tf_policy.py:85\u001b[0m, in \u001b[0;36mpostprocess_advantages\u001b[0;34m(policy, sample_batch, other_agent_batches, episode)\u001b[0m\n\u001b[1;32m     81\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m SampleBatch\u001b[38;5;241m.\u001b[39mNEXT_OBS \u001b[38;5;129;01min\u001b[39;00m sample_batch \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     82\u001b[0m     input_dict \u001b[38;5;241m=\u001b[39m sample_batch\u001b[38;5;241m.\u001b[39mget_single_step_input_dict(\n\u001b[1;32m     83\u001b[0m         policy\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mview_requirements, index\u001b[38;5;241m=\u001b[39mindex\n\u001b[1;32m     84\u001b[0m     )\n\u001b[0;32m---> 85\u001b[0m     last_r \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Adds the \"advantages\" (which in the case of MARWIL are simply the\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# discounted cummulative rewards) to the SampleBatch.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compute_advantages(\n\u001b[1;32m     90\u001b[0m     sample_batch,\n\u001b[1;32m     91\u001b[0m     last_r,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m     use_critic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/rllib/utils/tf_utils.py:391\u001b[0m, in \u001b[0;36mmake_tf_callable.<locals>.make_wrapper.<locals>.call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m feed_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(args_placeholders, tree\u001b[38;5;241m.\u001b[39mflatten(args)))\n\u001b[1;32m    386\u001b[0m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m ph, v: feed_dict\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setitem__\u001b[39m(ph, v),\n\u001b[1;32m    388\u001b[0m     kwargs_placeholders,\n\u001b[1;32m    389\u001b[0m     kwargs,\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43msession_or_none\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbolic_out\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/tensorflow/python/client/session.py:970\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    967\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    973\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorial/lib/python3.9/site-packages/tensorflow/python/client/session.py:1167\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1163\u001b[0m   np_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(subfeed_val, dtype\u001b[38;5;241m=\u001b[39msubfeed_dtype)\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m is_tensor_handle_feed \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m subfeed_t\u001b[38;5;241m.\u001b[39mget_shape()\u001b[38;5;241m.\u001b[39mis_compatible_with(np_val\u001b[38;5;241m.\u001b[39mshape)):\n\u001b[0;32m-> 1167\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1168\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot feed value of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(np_val\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for Tensor \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1169\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfeed_t\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, which has shape \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1170\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(subfeed_t\u001b[38;5;241m.\u001b[39mget_shape())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mis_feedable(subfeed_t):\n\u001b[1;32m   1172\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfeed_t\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m may not be fed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (1, 472) for Tensor default_policy/obs_1:0, which has shape (1, 27)"
     ]
    }
   ],
   "source": [
    "# Let's train our new behavioral cloning Trainer for some iterations:\n",
    "for _ in range(5):\n",
    "    results = marwil_trainer.train()\n",
    "    print(f\"{results['info']['num_agent_steps_trained']} steps trained; reward = {results['episode_reward_mean']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh no! What happened?\n",
    "# We don't have an environment! No way to measure rewards per episode.\n",
    "\n",
    "# A quick fix would be:\n",
    "\n",
    "# A) We cheat! Let's use our environment from above to run some separate evaluation workers on while we train:\n",
    "\n",
    "offline_rl_config_w_evaluation = offline_rl_config.copy()\n",
    "offline_rl_config_w_evaluation.update({\n",
    "    # Add a (parallel) evaluation trackm, using 1 additional ray worker and running for\n",
    "    # 100 episodes (`evaluation_duration[_unit]?`) each training iteration (`evaluation_interval`).\n",
    "    \"evaluation_interval\": 1,\n",
    "    \"evaluation_parallel_to_training\": True,\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"evaluation_duration\": 100,\n",
    "    \"evaluation_duration_unit\": \"episodes\",\n",
    "    # Must change the input setting to point to an environment.\n",
    "    \"evaluation_config\": {\n",
    "        \"env\": InterestEvolutionRecSimEnv,\n",
    "        \"env_config\": slateq_config[\"env_config\"],\n",
    "        \"input\": \"sampler\",\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826e811e-c356-4c6e-a218-f93420e111bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer_w_evaluation = BCTrainer(config=offline_rl_config_w_evaluation)\n",
    "print(bc_trainer_w_evaluation.evaluation_workers)\n",
    "bc_trainer_w_evaluation.evaluate()\n",
    "# Let's train our new behavioral cloning Trainer for some iterations:\n",
    "for _ in range(5):\n",
    "    results_w_evaluation = bc_trainer_w_evaluation.train()\n",
    "    print(results_w_evaluation[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49219ba-45e3-43b3-8b5d-1b1284999e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) If you really, really don't have an environment, use \"off-policy estimation\" (OPE):\n",
    "\n",
    "# `results` still holds the last output of our non-evaluation BCTrainer.\n",
    "# Extract off-policy estimator (OPE) results for the different methods:\n",
    "# is=importance sampling\n",
    "# wis=weighted importance sampling\n",
    "pprint(results[\"off_policy_estimator\"][\"is\"])\n",
    "pprint(results[\"off_policy_estimator\"][\"wis\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd66c3-f07a-4795-84ea-6b232ba6a047",
   "metadata": {},
   "source": [
    "### Saving and restoring a trained Trainer.\n",
    "Currently, `bc_trainer` is in an already trained state.\n",
    "It holds optimized weights in its Q-value/Policy's models that allow it to act\n",
    "already somewhat smart in our environment when given an observation.\n",
    "\n",
    "However, if we closed this notebook right now, all the effort would have been for nothing.\n",
    "Let's therefore save the state of our trainer to disk for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eae1e4-3cc4-4282-9a83-bc374bdad978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the `Trainer.save()` method to create a checkpoint.\n",
    "checkpoint_file = bc_trainer.save()\n",
    "print(f\"Trainer (at iteration {bc_trainer.iteration} was saved in '{checkpoint_file}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "os.listdir(os.path.dirname(checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e0ab-2c10-469a-97b1-4aadf1a1ec97",
   "metadata": {},
   "source": [
    "### Restoring and evaluating a Trainer\n",
    "In the following cell, we'll learn how to restore a saved Trainer from a checkpoint file.\n",
    "\n",
    "We'll also evaluate a completely new Trainer (should act more or less randomly) vs an already trained one (the one we just restored from the created checkpoint file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ceedb9-c225-46f2-ad1d-f902c81d3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_trainer = BCTrainer(config=offline_rl_config)\n",
    "# Evaluate the new trainer (this should yield random results).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "print(f\"Before restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "new_trainer.restore(checkpoint_file)\n",
    "print(f\"After restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "\n",
    "# Evaluate again (this should yield results we saw after having trained our saved agent).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating restored trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03c572-b98f-47e5-b0b4-404391be9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.start()\n",
    "\n",
    "@serve.deployment(route_prefix=\"/interest-evolution\")\n",
    "class ServeModel:\n",
    "    def __init__(self, checkpoint_path) -> None:\n",
    "        self.trainer = BCTrainer(\n",
    "            config=offline_rl_config,\n",
    "        )\n",
    "        self.trainer.restore(checkpoint_path)\n",
    "\n",
    "    async def __call__(self, request: Request):\n",
    "        json_input = await request.json()\n",
    "        obs = json_input[\"observation\"]\n",
    "        # Translate obs back to np.arrays.\n",
    "        np_obs = OrderedDict(tree.map_structure(lambda s: np.array(s) if isinstance(s, list) else s, obs))\n",
    "        action = self.trainer.compute_single_action(np_obs)\n",
    "        return {\"action\": action}\n",
    "\n",
    "\n",
    "ServeModel.deploy(checkpoint_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082340d-51f4-42a2-853a-508413a3d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request 5 actions of an episode from served policy.\n",
    "\n",
    "obs = interest_evolution_env.reset()\n",
    "\n",
    "for _ in range(5):\n",
    "    # Convert numpy arrays to lists (needed for transfer).\n",
    "    obs = tree.map_structure(lambda s: s.tolist() if isinstance(s, np.ndarray) else s, obs)\n",
    "\n",
    "    print(f\"-> Sending observation {obs}\")\n",
    "    resp = requests.get(\n",
    "        \"http://localhost:8000/interest-evolution\", json={\"observation\": obs}\n",
    "    )\n",
    "    response_json = resp.json()\n",
    "    print(f\"<- Received response {response_json}\")\n",
    "    obs, _, _, _ = interest_evolution_env.step(np.array(response_json[\"action\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {},
   "source": [
    "## Time for Q&A\n",
    "\n",
    "...\n",
    "\n",
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful.\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials/tree/main/production_rl_2022\">github repo of this tutorial</a>.\n",
    "- <a href=\"https://docs.ray.io/en/latest/rllib/index.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on Ray and its libraries.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
