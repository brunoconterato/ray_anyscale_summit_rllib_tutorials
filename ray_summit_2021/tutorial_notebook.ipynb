{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "# Hands-on RL with Ray’s RLlib\n",
    "## A beginner’s tutorial for working with multi-agent environments, models, and algorithms\n",
    "\n",
    "<img src=\"images/pitfall.jpg\" width=250> <img src=\"images/tesla.jpg\" width=254> <img src=\"images/forklifts.jpg\" width=169> <img src=\"images/robots.jpg\" width=252> <img src=\"images/dota2.jpg\" width=213>\n",
    "\n",
    "### Overview\n",
    "“Hands-on RL with Ray’s RLlib” is a beginners tutorial for working with reinforcement learning (RL) environments, models, and algorithms using Ray’s RLlib library. RLlib offers high scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL) before proceeding to RLlib (multi- and single-agent) environments, neural network models, hyperparameter tuning, debugging, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo.\n",
    "\n",
    "### Intended Audience\n",
    "* Python programmers who want to get started with reinforcement learning and RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac and Linux):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install ray[rllib]\n",
    "$ pip install [tensorflow|torch]  # <- either one works!\n",
    "$ pip install jupyter-labs\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install ray[rllib]\n",
    "$ pip install [tensorflow|torch]  # <- either one works!\n",
    "$ pip install jupyter-labs\n",
    "$ conda install pywin32\n",
    "```\n",
    "\n",
    "Also, for Win10 Atari support, we have to install atari_py from a different source (gym does not support Atari envs on Windows).\n",
    "\n",
    "```\n",
    "$ pip install git+https://github.com/Kojoley/atari-py.git\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and why RLlib?\n",
    "* Core concepts of RLlib: Environments, Trainers, Policies, and Models.\n",
    "* How to configure, hyperparameter-tune, and parallelize RLlib.\n",
    "* RLlib debugging best practices.\n",
    "\n",
    "### Tutorial Outline\n",
    "1. RL and RLlib in a nutshell.\n",
    "1. Defining an RL-solvable problem: Our first environment.\n",
    "1. Exercise No.1 (env loop)\n",
    "1. Picking an algorithm and training our first RLlib Trainer.\n",
    "1. Configurations and hyperparameters - Easy tuning with Ray Tune.\n",
    "1. Fixing our experiment's config - Going multi-agent.\n",
    "1. The \"infinite laptop\": Quick intro into how to use RLlib with Anyscale's product.\n",
    "1. Exercise No.2 (run your own Ray RLlib+Tune experiment)\n",
    "1. Neural network models - Provide your custom models using tf.keras or torch.nn.\n",
    "1. Deeper dive into RLlib's parallelization architecture.\n",
    "1. Specifying different compute resources and parallelization options through our config.\n",
    "1. \"Hacking in\": Using callbacks to customize the RL loop and generate our own metrics.\n",
    "1. Exercise No.3 (write your own custom callback)\n",
    "1. \"Hacking in (part II)\" - Debugging with RLlib and PyCharm.\n",
    "1. Checking on the \"infinite laptop\" - Did RLlib learn to solve the problem?\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-yorkshire",
   "metadata": {},
   "source": [
    "<img src=\"images/rl-cycle.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62744730",
   "metadata": {},
   "source": [
    "### Coding/defining our \"problem\" via an RL environment.\n",
    "\n",
    "We will use the following (adversarial) multi-agent environment\n",
    "throughout this tutorial to demonstrate a large fraction of RLlib's\n",
    "APIs, features, and customization options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb35116-efda-4799-8bae-e96d7775a0d1",
   "metadata": {},
   "source": [
    "<img src=\"images/environment.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe753-d7e0-4de1-b937-160507f75ed8",
   "metadata": {},
   "source": [
    "### A word or two on Spaces:\n",
    "\n",
    "Spaces are used in ML to describe what possible/valid values inputs and outputs of a neural network can have.\n",
    "\n",
    "RL environments also use them to describe what their valid observations and actions are.\n",
    "\n",
    "Spaces are usually defined by their shape (e.g. 84x84x3 RGB images) and datatype (e.g. uint8 for RGB values between 0 and 255).\n",
    "However, spaces could also be composed of other spaces (see Tuple or Dict spaces) or could be simply discrete with n fixed possible values\n",
    "(represented by integers). For example, in our game, where each agent can only go up/down/left/right, the action space would be \"Discrete(4)\"\n",
    "(no datatype, no shape needs to be defined here)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e4135-98ed-4e65-9e26-66f340747529",
   "metadata": {},
   "source": [
    "<img src=\"images/spaces.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "871a3661-2d74-4a50-b4ef-a89c27d978f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "class MultiAgentArena(MultiAgentEnv):\n",
    "    def __init__(self, config=None):\n",
    "        # !LIVE CODING!\n",
    "        #from environment import _init\n",
    "        #_init(self, config)\n",
    "        config = config or {}\n",
    "        self.width = config.get(\"width\", 10)\n",
    "        self.height = config.get(\"height\", 10)\n",
    "\n",
    "        self.observation_space = MultiDiscrete([self.width * self.height,\n",
    "                                                self.width * self.height])\n",
    "        self.action_space = Discrete(4)\n",
    "        \n",
    "        self.timestep_limit = config.get(\"ts\", 100)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # !LIVE CODING!\n",
    "        #from environment import _reset\n",
    "        #return _reset(self)\n",
    "        self.agent1_pos = [0, 0]\n",
    "        self.agent2_pos = [self.height - 1, self.width - 1]\n",
    "        \n",
    "        self.timesteps = 0\n",
    "        \n",
    "        self.agent1_visited_states = set()\n",
    "        \n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: dict):\n",
    "        # !LIVE CODING!\n",
    "        #from environment import _step\n",
    "        #return _step(action)\n",
    "\n",
    "        # increase our time steps counter by 1.\n",
    "        self.timesteps += 1\n",
    "\n",
    "        # Determine, who is allowed to move first (50:50).\n",
    "        if random.random() > 0.5:\n",
    "            events = self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "            events |= self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        else:\n",
    "            events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "            events |= self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "\n",
    "        # Determine rewards based on the collected events:\n",
    "        rewards = {\n",
    "            \"agent1\": -1.0 if \"bumped\" in events else 1.0 if \"new\" in events else -0.5,\n",
    "            \"agent2\": 1.0 if \"bumped\" in events else -0.1,\n",
    "        }\n",
    "        # Get observations (based on new agent positions).\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Generate a `done` dict (per-agent and total).\n",
    "        # We are done only when we reach the time step limit.\n",
    "        is_done = self.timesteps >= self.timestep_limit\n",
    "        dones = {\n",
    "            \"agent1\": is_done,\n",
    "            \"agent2\": is_done,\n",
    "            # special `__all__` key indicates that the episode is done for all agents.\n",
    "            \"__all__\": is_done,\n",
    "        }\n",
    "\n",
    "        return obs, rewards, dones, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns obs dict (agent name to discrete-pos tuple) using each\n",
    "        agent's current x/y-positions.\n",
    "        \"\"\"\n",
    "        ag1_discrete_pos = self.agent1_pos[0] * self.width + \\\n",
    "            (self.agent1_pos[1] % self.width)\n",
    "        ag2_discrete_pos = self.agent2_pos[0] * self.width + \\\n",
    "            (self.agent2_pos[1] % self.width)\n",
    "        return {\n",
    "            \"agent1\": np.array([ag1_discrete_pos, ag2_discrete_pos]),\n",
    "            \"agent2\": np.array([ag2_discrete_pos, ag1_discrete_pos]),\n",
    "        }\n",
    "\n",
    "    def _move(self, coords, action, is_agent1):\n",
    "        \"\"\"\n",
    "        Moves an agent (agent1 iff is_agent1=True, else agent2) from `coords` (x/y) using the\n",
    "        given action (0=up, 1=right, etc..) and returns a resulting events dict:\n",
    "        Agent1: \"new\" when entering a new field. \"bumped\" when having been bumped into by agent2.\n",
    "        Agent2: \"bumped\" when bumping into agent1 (agent1 then gets -1.0).\n",
    "        \"\"\"\n",
    "        orig_coords = coords[:]\n",
    "        # Change the row: 0=up (-1), 2=down (+1)\n",
    "        coords[0] += -1 if action == 0 else 1 if action == 2 else 0\n",
    "        # Change the column: 1=right (+1), 3=left (-1)\n",
    "        coords[1] += 1 if action == 1 else -1 if action == 3 else 0\n",
    "\n",
    "        # Solve collisions.\n",
    "        # Make sure, we don't end up on the other agent's position.\n",
    "        # If yes, don't move (we are blocked).\n",
    "        if (is_agent1 and coords == self.agent2_pos) or (not is_agent1 and coords == self.agent1_pos):\n",
    "            coords[0], coords[1] = orig_coords\n",
    "            # Agent2 blocked agent1 (agent1 tried to run into agent2)\n",
    "            # OR Agent2 bumped into agent1 (agent2 tried to run into agent1)\n",
    "            return {\"bumped\"}\n",
    "\n",
    "        # No agent blocking -> check walls.\n",
    "        if coords[0] < 0:\n",
    "            coords[0] = 0\n",
    "        elif coords[0] >= self.height:\n",
    "            coords[0] = self.height - 1\n",
    "        if coords[1] < 0:\n",
    "            coords[1] = 0\n",
    "        elif coords[1] >= self.width:\n",
    "            coords[1] = self.width - 1\n",
    "\n",
    "        # If agent1 -> \"new\" if new tile covered.\n",
    "        if is_agent1 and not tuple(coords) in self.agent1_visited_states:\n",
    "            self.agent1_visited_states.add(tuple(coords))\n",
    "            return {\"new\"}\n",
    "        # No new tile for agent1.\n",
    "        return set()\n",
    "\n",
    "    # Optionally: Add `render` method returning some img.\n",
    "    def render(self, mode=None):\n",
    "        field_size = 40\n",
    "\n",
    "        if not hasattr(self, \"viewer\"):\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(400, 400)\n",
    "            self.fields = {}\n",
    "            # Add our grid, and the two agents to the viewer.\n",
    "            for i in range(self.width):\n",
    "                l = i * field_size\n",
    "                r = l + field_size\n",
    "                for j in range(self.height):\n",
    "                    b = 400 - j * field_size - field_size\n",
    "                    t = b + field_size\n",
    "                    field = rendering.PolyLine([(l, b), (l, t), (r, t), (r, b)], close=True)\n",
    "                    field.set_color(.0, .0, .0)\n",
    "                    field.set_linewidth(1.0)\n",
    "                    self.fields[(j, i)] = field\n",
    "                    self.viewer.add_geom(field)\n",
    "            \n",
    "            agent1 = rendering.make_circle(radius=field_size // 2 - 4)\n",
    "            agent1.set_color(.0, 0.8, 0.1)\n",
    "            self.agent1_trans = rendering.Transform()\n",
    "            agent1.add_attr(self.agent1_trans)\n",
    "            agent2 = rendering.make_circle(radius=field_size // 2 - 4)\n",
    "            agent2.set_color(.5, 0.1, 0.1)\n",
    "            self.agent2_trans = rendering.Transform()\n",
    "            agent2.add_attr(self.agent2_trans)\n",
    "            self.viewer.add_geom(agent1)\n",
    "            self.viewer.add_geom(agent2)\n",
    "\n",
    "        # Mark those fields green that have been covered by agent1,\n",
    "        # all others black.\n",
    "        for i in range(self.width):\n",
    "            for j in range(self.height):\n",
    "                self.fields[(j, i)].set_color(.0, .0, .0)\n",
    "                self.fields[(j, i)].set_linewidth(1.0)\n",
    "        for (j, i) in self.agent1_visited_states:\n",
    "            self.fields[(j, i)].set_color(.1, .5, .1)\n",
    "            self.fields[(j, i)].set_linewidth(5.0)\n",
    "        \n",
    "        # Edit the pole polygon vertex\n",
    "        self.agent1_trans.set_translation(self.agent1_pos[1] * field_size + field_size / 2, 400 - (self.agent1_pos[0] * field_size + field_size / 2))\n",
    "        self.agent2_trans.set_translation(self.agent2_pos[1] * field_size + field_size / 2, 400 - (self.agent2_pos[0] * field_size + field_size / 2))\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-sussex",
   "metadata": {},
   "source": [
    "## Exercise No 1\n",
    "\n",
    "<hr />\n",
    "\n",
    "Write an \"environment loop\" using our `MultiAgentArena` class.\n",
    "\n",
    "1. Create an env object.\n",
    "1. `reset` your environment to get the first (initial) observation.\n",
    "1. `step` through the environment using a provided\n",
    "   \"DummyTrainer.compute_action([obs])\" method to compute action dicts (see cell below, in which you can create a DummyTrainer object and query it for random actions).\n",
    "1. When an episode is done, remember to `reset()` your environment before the next call to `step()`.\n",
    "1. If you feel, this is way too easy for you ;) , try to extract each agent's reward, sum it up over one episode and - at the end of an episode (when done=True) - print out each agent's accumulated reward (also called \"return\").\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent1': 3, 'agent2': 2}\n",
      "{'agent1': 2, 'agent2': 3}\n",
      "{'agent1': 3, 'agent2': 1}\n"
     ]
    }
   ],
   "source": [
    "class DummyTrainer:\n",
    "    \"\"\"Dummy Trainer class used in Exercise #1.\n",
    "\n",
    "    Use its `compute_action` method to get a new action, given some environment\n",
    "    observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_action(self, obs=None):\n",
    "        # Returns a random action.\n",
    "        return {\n",
    "            \"agent1\": np.random.randint(4),\n",
    "            \"agent2\": np.random.randint(4)\n",
    "        }\n",
    "\n",
    "dummy_trainer = DummyTrainer()\n",
    "# Check, whether it's working.\n",
    "for _ in range(3):\n",
    "    print(dummy_trainer.compute_action({\"agent1\": np.array([0, 10]), \"agent2\": np.array([10, 0])}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "liable-district",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done. R1=34.5 R2=-4.499999999999997\n",
      "Episode done. R1=35.5 R2=-9.99999999999998\n",
      "Episode done. R1=33.5 R2=-5.599999999999999\n",
      "Episode done. R1=27.5 R2=-8.89999999999998\n",
      "Episode done. R1=21.5 R2=-2.3000000000000047\n",
      "Episode done. R1=10.0 R2=-9.99999999999998\n",
      "Episode done. R1=25.5 R2=-7.7999999999999865\n",
      "Episode done. R1=31.5 R2=-7.799999999999981\n",
      "Episode done. R1=29.5 R2=-9.99999999999998\n",
      "Episode done. R1=37.0 R2=-6.699999999999985\n"
     ]
    }
   ],
   "source": [
    "# Solution to Exercise #1\n",
    "# !LIVE CODING!\n",
    "# Solution:\n",
    "env = MultiAgentArena(config={\"width\": 10, \"height\": 10})\n",
    "obs = env.reset()\n",
    "# Play through a single episode.\n",
    "done = {\"__all__\": False}\n",
    "return_ag1 = return_ag2 = 0.0\n",
    "num_episodes = 0\n",
    "while num_episodes < 10:\n",
    "    action1 = rllib_trainer.compute_action(obs[\"agent1\"])\n",
    "    action2 = rllib_trainer.compute_action(obs[\"agent2\"])\n",
    "    #action = rllib_trainer.compute_action(None)\n",
    "\n",
    "    obs, rewards, done, _ = env.step({\"agent1\": action1, \"agent2\": action2})\n",
    "    return_ag1 += rewards[\"agent1\"]\n",
    "    return_ag2 += rewards[\"agent2\"]    \n",
    "    if done[\"__all__\"]:\n",
    "        print(f\"Episode done. R1={return_ag1} R2={return_ag2}\")\n",
    "        num_episodes += 1\n",
    "        return_ag1 = return_ag2 = 0.0\n",
    "        obs = env.reset()\n",
    "    # Optional:\n",
    "    env.render()\n",
    "\n",
    "    import time\n",
    "    time.sleep(0.05)\n",
    "\n",
    "env.viewer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49af8d95-6501-4a6d-b6fa-d823fce28921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-05 19:20:39,642\tINFO services.py:1262 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.100',\n",
       " 'raylet_ip_address': '192.168.0.100',\n",
       " 'redis_address': '192.168.0.100:53537',\n",
       " 'object_store_address': '/tmp/ray/session_2021-05-05_19-20-38_329023_80141/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-05-05_19-20-38_329023_80141/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-05-05_19-20-38_329023_80141',\n",
       " 'metrics_export_port': 64927,\n",
       " 'node_id': '1d45e6c38925e023e61aaeae0426d166a6188092f3f35dbeaa1c4572'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now for something completely different:\n",
    "# Plugging in RLlib!\n",
    "\n",
    "import ray\n",
    "\n",
    "# Start a new instance of Ray or connect to an already running one.\n",
    "ray.init()  # Hear the engine humming? ;)\n",
    "\n",
    "# In case you encounter the following error during our tutorial:\n",
    "# RuntimeError: Maybe you called ray.init twice by accident?\n",
    "# Try: ray.shutdown() or ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_algos.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bcc1116-a14c-4479-87c0-6ece58ab0464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=80221)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80221)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80221)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80221)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80221)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80221)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80214)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80214)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80214)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80214)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80214)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80214)\u001b[0m non-resource variables are not supported in the long term\n",
      "2021-05-05 19:28:55,572\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "# We use the PPO algorithm here b/c its very flexible wrt its supported\n",
    "# action spaces and model types and b/c it learns well almost any problem.\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "# Specify a very simple config, defining our environment and some environment\n",
    "# options (see environment.py).\n",
    "config = {\n",
    "    \"env\": MultiAgentArena,  # \"my_env\" <- if we previously have registered the env with `tune.register_env(\"[name]\", lambda config: [returns env object])`.\n",
    "    \"env_config\": {\n",
    "        \"config\": {\n",
    "            \"width\": 10,\n",
    "            \"height\": 10,\n",
    "        },\n",
    "    },\n",
    "    # \"framework\": \"torch\",  # If users have chosen to install torch instead of tf.\n",
    "    \"create_env_on_driver\": True,\n",
    "}\n",
    "# Instantiate the Trainer object using above config.\n",
    "rllib_trainer = PPOTrainer(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spectacular-guard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'episode_reward_max': 17.09999999999993, 'episode_reward_min': -37.50000000000006, 'episode_reward_mean': -9.930000000000001, 'episode_len_mean': 100.0, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-22.500000000000014, -13.499999999999973, -13.499999999999991, 5.100000000000016, -21.600000000000012, 9.000000000000002, 17.09999999999993, -14.99999999999998, -37.50000000000006, 2.095545958979983e-14, 1.80000000000001, -31.20000000000003, -7.499999999999982, -20.69999999999999, -21.00000000000005, 6.9000000000000234, 10.50000000000003, -22.5, -1.4999999999999782, -20.999999999999996], 'episode_lengths': [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.13907651205758356, 'mean_inference_ms': 0.5487502514422833, 'mean_action_processing_ms': 0.055511276443283276, 'mean_env_wait_ms': 0.029646076046146237, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_healthy_workers': 2, 'timesteps_total': 4000, 'agent_timesteps_total': 4000, 'timers': {'sample_time_ms': 796.594, 'sample_throughput': 5021.377, 'load_time_ms': 35.679, 'load_throughput': 112110.445, 'learn_time_ms': 2290.992, 'learn_throughput': 1745.969, 'update_time_ms': 1.638}, 'info': {'learner': {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': 26.267956, 'policy_loss': -0.05085376, 'vf_loss': 26.314976, 'vf_explained_var': 0.15982546, 'kl': 0.01915131, 'entropy': 1.3675513, 'entropy_coeff': 0.0, 'model': {}}}}, 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'episodes_total': 20, 'training_iteration': 1, 'experiment_id': 'eec406862a8f43048763d82ad0a79e60', 'date': '2021-05-05_19-29-31', 'timestamp': 1620235771, 'time_this_iter_s': 3.223253011703491, 'time_total_s': 3.223253011703491, 'pid': 80141, 'hostname': 'Svens-MacBook-Pro.local', 'node_ip': '192.168.0.100', 'config': {'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': True, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'train_batch_size': 4000, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'env_config': {'config': {'width': 10, 'height': 10}}, 'render_env': False, 'record_env': False, 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 5e-05, 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'time_since_restore': 3.223253011703491, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': 7.180392156862746, 'ram_util_percent': 61.26666666666667}}\n"
     ]
    }
   ],
   "source": [
    "# That's it, we are ready to train.\n",
    "# Calling `train` once runs a single \"training iteration\". One iteration\n",
    "# for most algos contains a) sampling from the environment(s) + b) using the\n",
    "# sampled data (observations, actions taken, rewards) to update the policy\n",
    "# model (neural network), such that it would pick better actions in the future,\n",
    "# leading to higher rewards.\n",
    "print(rllib_trainer.train())\n",
    "# !LIVE CODING! (call and print out `trainer.train()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3381a70-4b44-44e7-a9b3-852340c94b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=42: R=18.050999999999934\n",
      "Iteration=43: R=17.570999999999934\n",
      "Iteration=44: R=17.006999999999934\n",
      "Iteration=45: R=16.880999999999936\n",
      "Iteration=46: R=17.420999999999935\n",
      "Iteration=47: R=17.79299999999993\n",
      "Iteration=48: R=18.701999999999927\n",
      "Iteration=49: R=20.165999999999926\n",
      "Iteration=50: R=21.245999999999917\n",
      "Iteration=51: R=21.614999999999913\n"
     ]
    }
   ],
   "source": [
    "# Run `train()` n times. Try to repeatedly call this to see rewards increase.\n",
    "# Move on once you see episode rewards of 15.0 or more.\n",
    "for _ in range(10):\n",
    "    results = rllib_trainer.train()\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "736144ef-66fd-43bd-843d-9f0b29da721e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.agents.trainer_template.PPO at 0x7fd13898aa00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "# Use the above solution of Exercise #1 and replace our `dummy_trainer`\n",
    "# with the already trained `rllib_trainer`.\n",
    "# Note to self: Make sure you are computing actions for agent1 and agent2 separately!\n",
    "rllib_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa7e0d09-e4fa-4657-b602-aa9d6750a33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fd1389994c0>\n",
      "Observation-space: Box(-1.0, 1.0, (200,), float32)\n",
      "Action-space: Discrete(4)\n",
      "Action distribution class: <class 'ray.rllib.models.tf.tf_action_dist.Categorical'>\n",
      "Model: <ray.rllib.models.tf.fcnet.FullyConnectedNetwork object at 0x7fd1389994f0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "# Let's actually \"look inside\" our Trainer to see what's in there.\n",
    "\n",
    "# We can get the policy inside the Trainer like so:\n",
    "pol = rllib_trainer.get_policy()\n",
    "print(f\"Policy: {pol}\")\n",
    "# The Policy object has an observation space and an action space.\n",
    "print(f\"Observation-space: {pol.observation_space}\")\n",
    "print(f\"Action-space: {pol.action_space}\")\n",
    "# It also comes with an action distribution class ...\n",
    "print(f\"Action distribution class: {pol.dist_class}\")\n",
    "# ... and a (neural network) model.\n",
    "print(f\"Model: {pol.model}\")\n",
    "\n",
    "# Create a fake numpy B=1 (single) observation consisting of both agents positions (\"one-hot'd\" and \"concat'd\").\n",
    "from ray.rllib.utils.numpy import one_hot\n",
    "single_obs = np.concatenate([one_hot(0, depth=100), one_hot(99, depth=100)])\n",
    "single_obs = np.array([single_obs])\n",
    "#single_obs.shape\n",
    "\n",
    "# Generate the Model's output.\n",
    "out, state_out = pol.model({\"obs\": single_obs})\n",
    "\n",
    "# tf1.x (static graph) -> Need to run this through a tf session.\n",
    "numpy_out = pol.get_session().run(out)\n",
    "\n",
    "# RLlib then passes the model's output to the policy's \"action distribution\" to sample an action.\n",
    "action_dist = pol.dist_class(out)\n",
    "action = action_dist.sample()\n",
    "\n",
    "# Show us the actual action.\n",
    "pol.get_session().run(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "saved-equilibrium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer (at iteration 51 was saved in '/Users/sven/ray_results/PPO_MultiAgentArena_2021-05-05_19-28-48gg1cmrvr/checkpoint_000051/checkpoint-51'!\n",
      "The checkpoint directory contains the following files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['checkpoint-51.tune_metadata', 'checkpoint-51', '.is_checkpoint']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Currently, `rllib_trainer` is in an already trained state.\n",
    "# It holds optimized weights in its Policy's model that allow it to act\n",
    "# already somewhat smart in our environment when given an action.\n",
    "\n",
    "# If we closed this notebook, all the effort would have been for nothing.\n",
    "# Let's save the state of our trainer to disk for later!\n",
    "checkpoint_path = rllib_trainer.save()\n",
    "print(f\"Trainer (at iteration {rllib_trainer.iteration} was saved in '{checkpoint_path}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "import os\n",
    "os.listdir(os.path.dirname(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "global-canon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=80220)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80220)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80220)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80220)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80220)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80220)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80224)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80224)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80224)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80224)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80224)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80224)\u001b[0m non-resource variables are not supported in the long term\n",
      "2021-05-05 19:42:19,103\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PPO' object has no attribute 'evaluation_workers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2b5d6e6a7dc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnew_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPOTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Evaluate the new trainer (this should yield random results).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;31m# Sync weights to the evaluation WorkerSet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_weights_to_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_filters_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PPO' object has no attribute 'evaluation_workers'"
     ]
    }
   ],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_trainer = PPOTrainer(config=config)\n",
    "# Evaluate the new trainer (this should yield random results).\n",
    "results = new_trainer._evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "new_trainer.restore(checkpoint_path)\n",
    "\n",
    "# Evaluate again (this should yield results we saw after having trained our saved agent).\n",
    "results = new_trainer._evaluate()\n",
    "print(f\"Evaluating restored trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "continent-architecture",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO's default config is:\n",
      "{'_fake_gpus': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': True,\n",
      " 'clip_param': 0.3,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': 180,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'eager_tracing': False,\n",
      " 'entropy_coeff': 0.0,\n",
      " 'entropy_coeff_schedule': None,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'evaluation_config': {},\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': 10,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'evaluation_parallel_to_training': False,\n",
      " 'exploration_config': {'type': 'StochasticSampling'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'tf',\n",
      " 'gamma': 0.99,\n",
      " 'grad_clip': None,\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'kl_coeff': 0.2,\n",
      " 'kl_target': 0.01,\n",
      " 'lambda': 1.0,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'lr': 5e-05,\n",
      " 'lr_schedule': None,\n",
      " 'metrics_smoothing_episodes': 100,\n",
      " 'min_iter_time_s': 0,\n",
      " 'model': {'_time_major': False,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'num_framestacks': 'auto',\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': False,\n",
      "           'zero_mean': True},\n",
      " 'monitor': -1,\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_mapping_fn': None,\n",
      "                'replay_mode': 'independent'},\n",
      " 'no_done_at_end': False,\n",
      " 'normalize_actions': False,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_sgd_iter': 30,\n",
      " 'num_workers': 2,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'record_env': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'rollout_fragment_length': 200,\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'seed': None,\n",
      " 'sgd_minibatch_size': 128,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'shuffle_sequences': True,\n",
      " 'simple_optimizer': -1,\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 0,\n",
      " 'train_batch_size': 4000,\n",
      " 'use_critic': True,\n",
      " 'use_gae': True,\n",
      " 'vf_clip_param': 10.0,\n",
      " 'vf_loss_coeff': 1.0,\n",
      " 'vf_share_layers': -1}\n"
     ]
    }
   ],
   "source": [
    "# 5) Configuration dicts and Ray Tune.\n",
    "# Where are the default configuration dicts stored?\n",
    "import pprint\n",
    "\n",
    "# PPO algorithm:\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as PPO_DEFAULT_CONFIG\n",
    "print(f\"PPO's default config is:\")\n",
    "pprint.pprint(PPO_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint.pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint.pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "latest-feature",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_8cbc4_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=80219)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80219)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80219)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80219)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80219)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80219)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80219)\u001b[0m 2021-05-05 19:51:47,841\tINFO trainer.py:648 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=80219)\u001b[0m 2021-05-05 19:51:47,841\tINFO trainer.py:673 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=80219)\u001b[0m 2021-05-05 19:51:47,841\tINFO trainer.py:648 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=80219)\u001b[0m 2021-05-05 19:51:47,841\tINFO trainer.py:673 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=80216)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80216)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80216)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80216)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80216)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80216)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80215)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80215)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80215)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80215)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80215)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80215)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80219)\u001b[0m 2021-05-05 19:51:54,991\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=80219)\u001b[0m 2021-05-05 19:51:54,991\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_8cbc4_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_19-51-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 25.499999999999932\n",
      "  episode_reward_mean: -7.56\n",
      "  episode_reward_min: -39.00000000000006\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 4c18df4ded4147309ab290fd322e57b7\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.368713617324829\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0179552361369133\n",
      "          model: {}\n",
      "          policy_loss: -0.056025221943855286\n",
      "          total_loss: 32.07113265991211\n",
      "          vf_explained_var: 0.10238191485404968\n",
      "          vf_loss: 32.123565673828125\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.860000000000003\n",
      "    ram_util_percent: 60.24000000000001\n",
      "  pid: 80219\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0516940782834719\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.028335012041486352\n",
      "    mean_inference_ms: 0.5153391387436417\n",
      "    mean_raw_obs_processing_ms: 0.13185023785113814\n",
      "  time_since_restore: 3.1585471630096436\n",
      "  time_this_iter_s: 3.1585471630096436\n",
      "  time_total_s: 3.1585471630096436\n",
      "  timers:\n",
      "    learn_throughput: 1771.75\n",
      "    learn_time_ms: 2257.655\n",
      "    load_throughput: 111638.227\n",
      "    load_time_ms: 35.83\n",
      "    sample_throughput: 5288.677\n",
      "    sample_time_ms: 756.333\n",
      "    update_time_ms: 1.567\n",
      "  timestamp: 1620237118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 8cbc4_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_8cbc4_00000</td><td>RUNNING </td><td>192.168.0.100:80219</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.15855</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">   -7.56</td><td style=\"text-align: right;\">                25.5</td><td style=\"text-align: right;\">                 -39</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_8cbc4_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_19-52-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 25.499999999999932\n",
      "  episode_reward_mean: -4.2649999999999935\n",
      "  episode_reward_min: -39.00000000000006\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: 4c18df4ded4147309ab290fd322e57b7\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3033363819122314\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018584633246064186\n",
      "          model: {}\n",
      "          policy_loss: -0.055112652480602264\n",
      "          total_loss: 12.647034645080566\n",
      "          vf_explained_var: 0.3117234408855438\n",
      "          vf_loss: 12.696571350097656\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.625\n",
      "    ram_util_percent: 60.275000000000006\n",
      "  pid: 80219\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0515375787292497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.028274468669104668\n",
      "    mean_inference_ms: 0.5018174133088192\n",
      "    mean_raw_obs_processing_ms: 0.13161054697754088\n",
      "  time_since_restore: 8.8938307762146\n",
      "  time_this_iter_s: 2.8671376705169678\n",
      "  time_total_s: 8.8938307762146\n",
      "  timers:\n",
      "    learn_throughput: 1890.01\n",
      "    learn_time_ms: 2116.39\n",
      "    load_throughput: 297315.494\n",
      "    load_time_ms: 13.454\n",
      "    sample_throughput: 5513.798\n",
      "    sample_time_ms: 725.453\n",
      "    update_time_ms: 1.441\n",
      "  timestamp: 1620237123\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 8cbc4_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_8cbc4_00000</td><td>RUNNING </td><td>192.168.0.100:80219</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         8.89383</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -4.265</td><td style=\"text-align: right;\">                25.5</td><td style=\"text-align: right;\">                 -39</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_8cbc4_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_19-52-10\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 25.499999999999932\n",
      "  episode_reward_mean: -2.6009999999999946\n",
      "  episode_reward_min: -39.00000000000006\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: 4c18df4ded4147309ab290fd322e57b7\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2405627965927124\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01944688893854618\n",
      "          model: {}\n",
      "          policy_loss: -0.05826034024357796\n",
      "          total_loss: 21.072965621948242\n",
      "          vf_explained_var: 0.38389959931373596\n",
      "          vf_loss: 21.122478485107422\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.720000000000006\n",
      "    ram_util_percent: 60.3\n",
      "  pid: 80219\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05156572841754607\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.028285782003532173\n",
      "    mean_inference_ms: 0.4971510065165568\n",
      "    mean_raw_obs_processing_ms: 0.1316201060933291\n",
      "  time_since_restore: 15.00817084312439\n",
      "  time_this_iter_s: 3.2068159580230713\n",
      "  time_total_s: 15.00817084312439\n",
      "  timers:\n",
      "    learn_throughput: 1852.932\n",
      "    learn_time_ms: 2158.74\n",
      "    load_throughput: 434159.253\n",
      "    load_time_ms: 9.213\n",
      "    sample_throughput: 5528.7\n",
      "    sample_time_ms: 723.497\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1620237130\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 8cbc4_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_8cbc4_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         15.0082</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -2.601</td><td style=\"text-align: right;\">                25.5</td><td style=\"text-align: right;\">                 -39</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_8cbc4_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         15.0082</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -2.601</td><td style=\"text-align: right;\">                25.5</td><td style=\"text-align: right;\">                 -39</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-05 19:52:10,559\tINFO tune.py:549 -- Total run time: 28.48 seconds (27.98 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fd1411b2fd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plugging in Ray Tune.\n",
    "# Note that this is the recommended way to run any experiments with RLlib.\n",
    "# Reasons:\n",
    "# - Tune allows you to do hyperparameter tuning in a user-friendly way\n",
    "#   and at large scale!\n",
    "# - Tune automatically allocates needed resources for the different\n",
    "#   hyperparam trials and experiment runs.\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Now that we will run things \"automatically\" through tune, we have to\n",
    "# define one or more stopping criteria.\n",
    "stop = {\n",
    "    # explain that keys here can be anything present in the above print(trainer.train())\n",
    "    \"training_iteration\": 5,\n",
    "    \"episode_reward_mean\": 9999.9,\n",
    "}\n",
    "\n",
    "# \"PPO\" is a registered name that points to RLlib's PPOTrainer.\n",
    "# See `ray/rllib/agents/registry.py`\n",
    "# Run our simple experiment until one of the stop criteria is met.\n",
    "tune.run(\"PPO\", config=config, stop=stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "discrete-quilt",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_1664e_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_1664e_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.5   </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=80217)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80217)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80217)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80217)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80217)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80217)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80213)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80213)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80213)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80213)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80213)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80213)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80217)\u001b[0m 2021-05-05 19:55:38,860\tINFO trainer.py:648 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=80217)\u001b[0m 2021-05-05 19:55:38,861\tINFO trainer.py:673 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=80217)\u001b[0m 2021-05-05 19:55:38,860\tINFO trainer.py:648 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=80217)\u001b[0m 2021-05-05 19:55:38,861\tINFO trainer.py:673 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=80213)\u001b[0m 2021-05-05 19:55:38,860\tINFO trainer.py:648 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=80213)\u001b[0m 2021-05-05 19:55:38,860\tINFO trainer.py:673 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=80213)\u001b[0m 2021-05-05 19:55:38,860\tINFO trainer.py:648 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=80213)\u001b[0m 2021-05-05 19:55:38,860\tINFO trainer.py:673 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=80211)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80211)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80211)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80211)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80211)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80211)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80210)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80210)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80210)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80210)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80210)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80210)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80212)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80212)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80212)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80212)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80212)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80212)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80368)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80368)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80368)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80368)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80368)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80368)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80217)\u001b[0m 2021-05-05 19:55:48,519\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=80217)\u001b[0m 2021-05-05 19:55:48,519\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=80213)\u001b[0m 2021-05-05 19:55:48,523\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=80213)\u001b[0m 2021-05-05 19:55:48,523\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_1664e_00001:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_19-55-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.59999999999993\n",
      "  episode_reward_mean: -10.185000000000004\n",
      "  episode_reward_min: -31.50000000000003\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: b88594800f1f4574b1da6c71be580020\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.12736791372299194\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 12.461297035217285\n",
      "          model: {}\n",
      "          policy_loss: 0.38490647077560425\n",
      "          total_loss: 33.12248992919922\n",
      "          vf_explained_var: -0.002359993988648057\n",
      "          vf_loss: 30.24532699584961\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.5125\n",
      "    ram_util_percent: 62.2\n",
      "  pid: 80217\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06199847687255372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03308909279959543\n",
      "    mean_inference_ms: 0.673173905371667\n",
      "    mean_raw_obs_processing_ms: 0.15268899820424936\n",
      "  time_since_restore: 5.087424039840698\n",
      "  time_this_iter_s: 5.087424039840698\n",
      "  time_total_s: 5.087424039840698\n",
      "  timers:\n",
      "    learn_throughput: 1022.654\n",
      "    learn_time_ms: 3911.392\n",
      "    load_throughput: 99545.007\n",
      "    load_time_ms: 40.183\n",
      "    sample_throughput: 4196.722\n",
      "    sample_time_ms: 953.125\n",
      "    update_time_ms: 3.248\n",
      "  timestamp: 1620237353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 1664e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_1664e_00000</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_1664e_00001</td><td>RUNNING </td><td>192.168.0.100:80217</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.08742</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -10.185</td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               -31.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_1664e_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_19-55-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.40000000000002\n",
      "  episode_reward_mean: -9.794999999999996\n",
      "  episode_reward_min: -41.10000000000006\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 34d9273bb9e2421bb7bdb8ccaef970c2\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3498280048370361\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03733210638165474\n",
      "          model: {}\n",
      "          policy_loss: -0.0692160502076149\n",
      "          total_loss: 25.37936782836914\n",
      "          vf_explained_var: 0.1313427984714508\n",
      "          vf_loss: 25.44111442565918\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.5125\n",
      "    ram_util_percent: 62.2\n",
      "  pid: 80213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.061791021745283524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03328499617752854\n",
      "    mean_inference_ms: 0.6742255909221392\n",
      "    mean_raw_obs_processing_ms: 0.1530453160807089\n",
      "  time_since_restore: 5.103805303573608\n",
      "  time_this_iter_s: 5.103805303573608\n",
      "  time_total_s: 5.103805303573608\n",
      "  timers:\n",
      "    learn_throughput: 1013.476\n",
      "    learn_time_ms: 3946.813\n",
      "    load_throughput: 95086.834\n",
      "    load_time_ms: 42.067\n",
      "    sample_throughput: 4210.529\n",
      "    sample_time_ms: 949.999\n",
      "    update_time_ms: 3.285\n",
      "  timestamp: 1620237353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 1664e_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_1664e_00001:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_19-55-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.59999999999993\n",
      "  episode_reward_mean: -34.342500000000044\n",
      "  episode_reward_min: -58.50000000000009\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 40\n",
      "  experiment_id: b88594800f1f4574b1da6c71be580020\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.00030283030355349183\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.580394594697282e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.0011191654484719038\n",
      "          total_loss: 136.2122039794922\n",
      "          vf_explained_var: 0.0011615426046773791\n",
      "          vf_loss: 136.2133026123047\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.699999999999996\n",
      "    ram_util_percent: 62.68571428571429\n",
      "  pid: 80217\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06211265449703006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03300168371782693\n",
      "    mean_inference_ms: 0.6633193301055983\n",
      "    mean_raw_obs_processing_ms: 0.15244283037052675\n",
      "  time_since_restore: 10.01605486869812\n",
      "  time_this_iter_s: 4.928630828857422\n",
      "  time_total_s: 10.01605486869812\n",
      "  timers:\n",
      "    learn_throughput: 1027.287\n",
      "    learn_time_ms: 3893.75\n",
      "    load_throughput: 185576.356\n",
      "    load_time_ms: 21.554\n",
      "    sample_throughput: 4313.099\n",
      "    sample_time_ms: 927.407\n",
      "    update_time_ms: 2.978\n",
      "  timestamp: 1620237358\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 1664e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_1664e_00000</td><td>RUNNING </td><td>192.168.0.100:80213</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         10.012 </td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> -7.83  </td><td style=\"text-align: right;\">                 9.6</td><td style=\"text-align: right;\">               -41.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_1664e_00001</td><td>RUNNING </td><td>192.168.0.100:80217</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         10.0161</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-34.3425</td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               -58.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_1664e_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_19-56-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.59999999999993\n",
      "  episode_reward_mean: -42.39500000000006\n",
      "  episode_reward_min: -58.50000000000009\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: b88594800f1f4574b1da6c71be580020\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0006342960405163467\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.398860518354923e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.0012262860545888543\n",
      "          total_loss: 104.77656555175781\n",
      "          vf_explained_var: 0.010487637482583523\n",
      "          vf_loss: 104.77530670166016\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.300000000000004\n",
      "    ram_util_percent: 59.04285714285714\n",
      "  pid: 80217\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06199325758220704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03292039150516806\n",
      "    mean_inference_ms: 0.6572358449902457\n",
      "    mean_raw_obs_processing_ms: 0.1525515100713895\n",
      "  time_since_restore: 15.024275779724121\n",
      "  time_this_iter_s: 5.008220911026001\n",
      "  time_total_s: 15.024275779724121\n",
      "  timers:\n",
      "    learn_throughput: 1021.197\n",
      "    learn_time_ms: 3916.973\n",
      "    load_throughput: 259970.807\n",
      "    load_time_ms: 15.386\n",
      "    sample_throughput: 4357.222\n",
      "    sample_time_ms: 918.016\n",
      "    update_time_ms: 2.727\n",
      "  timestamp: 1620237363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 1664e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_1664e_00000</td><td>RUNNING </td><td>192.168.0.100:80213</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         10.012 </td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">  -7.83 </td><td style=\"text-align: right;\">                 9.6</td><td style=\"text-align: right;\">               -41.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_1664e_00001</td><td>RUNNING </td><td>192.168.0.100:80217</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         15.0243</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -42.395</td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               -58.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_1664e_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_19-56-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.999999999999968\n",
      "  episode_reward_mean: -5.309999999999993\n",
      "  episode_reward_min: -41.10000000000006\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: 34d9273bb9e2421bb7bdb8ccaef970c2\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2728362083435059\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028103772550821304\n",
      "          model: {}\n",
      "          policy_loss: -0.06841445714235306\n",
      "          total_loss: 15.778899192810059\n",
      "          vf_explained_var: 0.2877601385116577\n",
      "          vf_loss: 15.834668159484863\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.31428571428571\n",
      "    ram_util_percent: 59.04285714285714\n",
      "  pid: 80213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06228638786620471\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.033270831688699584\n",
      "    mean_inference_ms: 0.662858129541955\n",
      "    mean_raw_obs_processing_ms: 0.15298184771991769\n",
      "  time_since_restore: 15.032012224197388\n",
      "  time_this_iter_s: 5.020005941390991\n",
      "  time_total_s: 15.032012224197388\n",
      "  timers:\n",
      "    learn_throughput: 1017.345\n",
      "    learn_time_ms: 3931.802\n",
      "    load_throughput: 246089.24\n",
      "    load_time_ms: 16.254\n",
      "    sample_throughput: 4309.223\n",
      "    sample_time_ms: 928.242\n",
      "    update_time_ms: 2.442\n",
      "  timestamp: 1620237363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 1664e_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_1664e_00001:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_19-56-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.59999999999993\n",
      "  episode_reward_mean: -46.42125000000008\n",
      "  episode_reward_min: -58.50000000000009\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 80\n",
      "  experiment_id: b88594800f1f4574b1da6c71be580020\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 0.5\n",
      "          entropy: 8.076877122675796e-08\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.3498062401140487e-07\n",
      "          model: {}\n",
      "          policy_loss: 0.0002730065898504108\n",
      "          total_loss: 121.68385314941406\n",
      "          vf_explained_var: 0.0013108465354889631\n",
      "          vf_loss: 121.68358612060547\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.325\n",
      "    ram_util_percent: 57.787499999999994\n",
      "  pid: 80217\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06185578714019098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0328566588221122\n",
      "    mean_inference_ms: 0.6526136711732529\n",
      "    mean_raw_obs_processing_ms: 0.1527219295532744\n",
      "  time_since_restore: 20.37941884994507\n",
      "  time_this_iter_s: 5.355143070220947\n",
      "  time_total_s: 20.37941884994507\n",
      "  timers:\n",
      "    learn_throughput: 996.717\n",
      "    learn_time_ms: 4013.175\n",
      "    load_throughput: 324906.869\n",
      "    load_time_ms: 12.311\n",
      "    sample_throughput: 4386.721\n",
      "    sample_time_ms: 911.843\n",
      "    update_time_ms: 2.635\n",
      "  timestamp: 1620237369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 1664e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_1664e_00000</td><td>RUNNING </td><td>192.168.0.100:80213</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         15.032 </td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -5.31  </td><td style=\"text-align: right;\">                21  </td><td style=\"text-align: right;\">               -41.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_1664e_00001</td><td>RUNNING </td><td>192.168.0.100:80217</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         20.3794</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-46.4213</td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               -58.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_1664e_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_19-56-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.999999999999968\n",
      "  episode_reward_mean: -3.573749999999991\n",
      "  episode_reward_min: -41.10000000000006\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 80\n",
      "  experiment_id: 34d9273bb9e2421bb7bdb8ccaef970c2\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2485978603363037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02224148064851761\n",
      "          model: {}\n",
      "          policy_loss: -0.06387993693351746\n",
      "          total_loss: 14.692970275878906\n",
      "          vf_explained_var: 0.3112991154193878\n",
      "          vf_loss: 14.741838455200195\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.349999999999994\n",
      "    ram_util_percent: 57.787499999999994\n",
      "  pid: 80213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06234439206240453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.033292582278342496\n",
      "    mean_inference_ms: 0.6593872540171191\n",
      "    mean_raw_obs_processing_ms: 0.1532722411373197\n",
      "  time_since_restore: 20.370986223220825\n",
      "  time_this_iter_s: 5.3389739990234375\n",
      "  time_total_s: 20.370986223220825\n",
      "  timers:\n",
      "    learn_throughput: 995.038\n",
      "    learn_time_ms: 4019.949\n",
      "    load_throughput: 305671.085\n",
      "    load_time_ms: 13.086\n",
      "    sample_throughput: 4336.592\n",
      "    sample_time_ms: 922.383\n",
      "    update_time_ms: 2.552\n",
      "  timestamp: 1620237369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 1664e_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_1664e_00001:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_19-56-14\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.59999999999993\n",
      "  episode_reward_mean: -48.83700000000008\n",
      "  episode_reward_min: -58.50000000000009\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: b88594800f1f4574b1da6c71be580020\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 0.5\n",
      "          entropy: 7.858031381147157e-08\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.037813553103291e-11\n",
      "          model: {}\n",
      "          policy_loss: 0.001968191936612129\n",
      "          total_loss: 112.76441192626953\n",
      "          vf_explained_var: 0.0014003015821799636\n",
      "          vf_loss: 112.76244354248047\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.037499999999994\n",
      "    ram_util_percent: 58.75\n",
      "  pid: 80217\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06187926839916146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03287506211971056\n",
      "    mean_inference_ms: 0.6502992764032166\n",
      "    mean_raw_obs_processing_ms: 0.15311851140152138\n",
      "  time_since_restore: 25.76316475868225\n",
      "  time_this_iter_s: 5.383745908737183\n",
      "  time_total_s: 25.76316475868225\n",
      "  timers:\n",
      "    learn_throughput: 982.487\n",
      "    learn_time_ms: 4071.299\n",
      "    load_throughput: 380843.352\n",
      "    load_time_ms: 10.503\n",
      "    sample_throughput: 4366.608\n",
      "    sample_time_ms: 916.043\n",
      "    update_time_ms: 2.753\n",
      "  timestamp: 1620237374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 1664e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_1664e_00000</td><td>RUNNING </td><td>192.168.0.100:80213</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         20.371 </td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -3.57375</td><td style=\"text-align: right;\">                21  </td><td style=\"text-align: right;\">               -41.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_1664e_00001</td><td>RUNNING </td><td>192.168.0.100:80217</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         25.7632</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-48.837  </td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               -58.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_1664e_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_19-56-14\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.999999999999968\n",
      "  episode_reward_mean: -2.1719999999999913\n",
      "  episode_reward_min: -41.10000000000006\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: 34d9273bb9e2421bb7bdb8ccaef970c2\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2218016386032104\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016137147322297096\n",
      "          model: {}\n",
      "          policy_loss: -0.057371288537979126\n",
      "          total_loss: 13.115107536315918\n",
      "          vf_explained_var: 0.3720450699329376\n",
      "          vf_loss: 13.156139373779297\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.05\n",
      "    ram_util_percent: 58.75\n",
      "  pid: 80213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06246571659474723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03334590399703132\n",
      "    mean_inference_ms: 0.6576607416005418\n",
      "    mean_raw_obs_processing_ms: 0.1536038699788048\n",
      "  time_since_restore: 25.750737190246582\n",
      "  time_this_iter_s: 5.379750967025757\n",
      "  time_total_s: 25.750737190246582\n",
      "  timers:\n",
      "    learn_throughput: 981.437\n",
      "    learn_time_ms: 4075.657\n",
      "    load_throughput: 357328.494\n",
      "    load_time_ms: 11.194\n",
      "    sample_throughput: 4323.373\n",
      "    sample_time_ms: 925.204\n",
      "    update_time_ms: 2.503\n",
      "  timestamp: 1620237374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 1664e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_1664e_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         25.7507</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -2.172</td><td style=\"text-align: right;\">                21  </td><td style=\"text-align: right;\">               -41.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_1664e_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         25.7632</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> -48.837</td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               -58.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-05 19:56:15,324\tINFO tune.py:549 -- Total run time: 42.27 seconds (41.66 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis object at 0x7fd128ee4bb0>\n"
     ]
    }
   ],
   "source": [
    "# Updating an algo's default config dict and adding hyperparameter tuning\n",
    "# options to it.\n",
    "# Note: Hyperparameter tuning options (e.g. grid_search) will only work,\n",
    "# if we run these configs via `tune.run`.\n",
    "config.update(\n",
    "    {\n",
    "        # Try 2 different learning rates.\n",
    "        \"lr\": tune.grid_search([0.0001, 0.5]),\n",
    "        # NN model config to tweak the default model\n",
    "        # that'll be created by RLlib for the policy.\n",
    "        \"model\": {\n",
    "            # e.g. change the dense layer stack.\n",
    "            \"fcnet_hiddens\": [256, 256, 256],\n",
    "            # Alternatively, you can specify a custom model here\n",
    "            # (we'll cover that later).\n",
    "            # \"custom_model\": ...\n",
    "            # Pass kwargs to your custom model.\n",
    "            # \"custom_model_config\": {}\n",
    "        },\n",
    "    }\n",
    ")\n",
    "# Repeat our experiment using tune's grid-search feature.\n",
    "results = tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,\n",
    "    stop=stop,\n",
    "    checkpoint_at_end=True,  # create a checkpoint when done.\n",
    "    checkpoint_freq=1,  # create a checkpoint on every iteration.\n",
    ")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "experimental-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going multi-policy:\n",
    "\n",
    "# Our experiment is ill-configured b/c both\n",
    "# agents, which should behave differently due to their different\n",
    "# tasks and reward functions, learn the same policy (the \"default_policy\",\n",
    "# which RLlib always provides if you don't configure anything else; Remember\n",
    "# that RLlib does not know at Trainer setup time, how many and which agents\n",
    "# the environment will \"produce\").\n",
    "# Let's fix this and introduce the \"multiagent\" API.\n",
    "\n",
    "# 6.1.) Define an agent->policy mapping function.\n",
    "# Which agents (defined by the environment) use which policies\n",
    "# (defined by us)? Mapping is M (agents) -> N (policies), where M >= N.\n",
    "def policy_mapping_fn(agent: str):\n",
    "    assert agent in [\"agent1\", \"agent2\"], f\"ERROR: invalid agent {agent}!\"\n",
    "    return \"pol1\" if agent == \"agent1\" else \"pol2\"\n",
    "    \n",
    "# 6.2.) Define details for our two policies.\n",
    "#TODO: coding Sven: Make it possible to not need obs/action spaces\n",
    "#  if they are the default anyways.\n",
    "observation_space = rllib_trainer.workers.local_worker().env.observation_space\n",
    "action_space = rllib_trainer.workers.local_worker().env.action_space\n",
    "# Btw, the above is equivalent to saying:\n",
    "# >>> rllib_trainer.get_policy(\"default_policy\").obs/action_space\n",
    "policies = {\n",
    "    \"pol1\": (None, observation_space, action_space, {\"lr\": 0.0003}),\n",
    "    \"pol2\": (None, observation_space, action_space, {\"lr\": 0.0004}),\n",
    "}\n",
    "\n",
    "#policies_to_train = [\"pol1\", \"pol2\"]\n",
    "\n",
    "# 6.3) Adding the above to our config.\n",
    "config.update({\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        #\"policies_to_train\": policies_to_train,\n",
    "    },\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a74ec7-a6c1-431d-83aa-35df56d93185",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise No 2\n",
    "\n",
    "<hr />\n",
    "\n",
    "Try learning our environment using Ray tune.run and a simple hyperparameter grid_search over:\n",
    "- 2 different learning rates (pick your own values).\n",
    "- AND 2 different `train_batch_size` settings (use 2000 and 3000).\n",
    "\n",
    "Also, make RLlib use a [128,128] dense layer stack as the NN model.\n",
    "\n",
    "Also, use the config setting of `num_envs_per_worker=10` to increase the sampling throughput.\n",
    "\n",
    "In case your local machine has less than 12 CPUs, try setting `num_workers=1` to make all tune trials run at the same time.\n",
    "Background: PPO by default uses 2 workers, which makes 1 trial use 3 CPUs (2 workers + \"driver\" (\"local-worker\")),\n",
    "which makes the entire experiment use 12 CPUs. Tune will run trials in sequence in case it cannot allocate enough CPUs at once\n",
    "(which is also fine, but then takes longer).\n",
    "\n",
    "Try to reach a total reward (sum of agent1 and agent2) of 15.0.\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c0077e6-16b1-428b-80b7-2516e1302030",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=80480)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80480)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80480)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80480)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80480)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80480)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80485)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80485)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80485)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80485)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80485)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80485)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80482)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80482)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80482)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80482)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80482)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80482)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80480)\u001b[0m 2021-05-05 20:00:43,549\tINFO trainer.py:648 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=80480)\u001b[0m 2021-05-05 20:00:43,549\tINFO trainer.py:673 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=80480)\u001b[0m 2021-05-05 20:00:43,549\tINFO trainer.py:648 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=80480)\u001b[0m 2021-05-05 20:00:43,549\tINFO trainer.py:673 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=80485)\u001b[0m 2021-05-05 20:00:43,548\tINFO trainer.py:648 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=80485)\u001b[0m 2021-05-05 20:00:43,549\tINFO trainer.py:673 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=80485)\u001b[0m 2021-05-05 20:00:43,548\tINFO trainer.py:648 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=80485)\u001b[0m 2021-05-05 20:00:43,549\tINFO trainer.py:673 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m 2021-05-05 20:00:43,548\tINFO trainer.py:648 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m 2021-05-05 20:00:43,549\tINFO trainer.py:673 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m 2021-05-05 20:00:43,548\tINFO trainer.py:648 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m 2021-05-05 20:00:43,549\tINFO trainer.py:673 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=80482)\u001b[0m 2021-05-05 20:00:43,550\tINFO trainer.py:648 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=80482)\u001b[0m 2021-05-05 20:00:43,550\tINFO trainer.py:673 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=80482)\u001b[0m 2021-05-05 20:00:43,550\tINFO trainer.py:648 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=80482)\u001b[0m 2021-05-05 20:00:43,550\tINFO trainer.py:673 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=80487)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80487)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80487)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80487)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80487)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80487)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80486)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80486)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80486)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80486)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80486)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80486)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80476)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80476)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80476)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80476)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80476)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80476)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80478)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80478)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80478)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80478)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80478)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80478)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80488)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80488)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80488)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80488)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80488)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80488)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80502)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80502)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80502)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80502)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80502)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80502)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80503)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80503)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80503)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80503)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80503)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80503)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80504)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80504)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80504)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80504)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=80504)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=80504)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=80485)\u001b[0m 2021-05-05 20:00:54,103\tINFO trainable.py:101 -- Trainable.setup took 10.556 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=80485)\u001b[0m 2021-05-05 20:00:54,104\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=80485)\u001b[0m 2021-05-05 20:00:54,103\tINFO trainable.py:101 -- Trainable.setup took 10.556 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=80485)\u001b[0m 2021-05-05 20:00:54,104\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=80480)\u001b[0m 2021-05-05 20:00:54,126\tINFO trainable.py:101 -- Trainable.setup took 10.578 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=80480)\u001b[0m 2021-05-05 20:00:54,126\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=80480)\u001b[0m 2021-05-05 20:00:54,126\tINFO trainable.py:101 -- Trainable.setup took 10.578 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=80480)\u001b[0m 2021-05-05 20:00:54,126\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m 2021-05-05 20:00:54,115\tINFO trainable.py:101 -- Trainable.setup took 10.567 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m 2021-05-05 20:00:54,115\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m 2021-05-05 20:00:54,115\tINFO trainable.py:101 -- Trainable.setup took 10.567 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m 2021-05-05 20:00:54,115\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=80482)\u001b[0m 2021-05-05 20:00:54,113\tINFO trainable.py:101 -- Trainable.setup took 10.565 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=80482)\u001b[0m 2021-05-05 20:00:54,114\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=80482)\u001b[0m 2021-05-05 20:00:54,113\tINFO trainable.py:101 -- Trainable.setup took 10.565 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=80482)\u001b[0m 2021-05-05 20:00:54,114\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.500000000000025\n",
      "  episode_reward_mean: -9.637499999999998\n",
      "  episode_reward_min: -43.500000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.3486239910125732\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.038723256438970566\n",
      "          model: {}\n",
      "          policy_loss: -0.06343455612659454\n",
      "          total_loss: 35.706642150878906\n",
      "          vf_explained_var: 0.12014700472354889\n",
      "          vf_loss: 35.762332916259766\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.3483364582061768\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03942076116800308\n",
      "          model: {}\n",
      "          policy_loss: -0.07771336287260056\n",
      "          total_loss: 2.310598611831665\n",
      "          vf_explained_var: 0.3861842453479767\n",
      "          vf_loss: 2.3804280757904053\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.74545454545454\n",
      "    ram_util_percent: 62.01818181818181\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 17.5\n",
      "    pol2: -4.499999999999985\n",
      "  policy_reward_mean:\n",
      "    pol1: -0.875\n",
      "    pol2: -8.762499999999982\n",
      "  policy_reward_min:\n",
      "    pol1: -33.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38257167113954155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21641883090953923\n",
      "    mean_inference_ms: 2.1245983702626394\n",
      "    mean_raw_obs_processing_ms: 1.6010329497987357\n",
      "  time_since_restore: 7.407529354095459\n",
      "  time_this_iter_s: 7.407529354095459\n",
      "  time_total_s: 7.407529354095459\n",
      "  timers:\n",
      "    learn_throughput: 650.081\n",
      "    learn_time_ms: 6153.084\n",
      "    sample_throughput: 4124.191\n",
      "    sample_time_ms: 969.887\n",
      "    update_time_ms: 4.296\n",
      "  timestamp: 1620237661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: cb9b0_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.40753</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -9.6375</td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               -43.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.400000000000032\n",
      "  episode_reward_mean: -17.07750000000001\n",
      "  episode_reward_min: -37.50000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.3484574556350708\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03962294012308121\n",
      "          model: {}\n",
      "          policy_loss: -0.07537916302680969\n",
      "          total_loss: 38.138641357421875\n",
      "          vf_explained_var: 0.18260008096694946\n",
      "          vf_loss: 38.20609664916992\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.3495787382125854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.037878576666116714\n",
      "          model: {}\n",
      "          policy_loss: -0.06351681053638458\n",
      "          total_loss: 2.12650728225708\n",
      "          vf_explained_var: 0.38856399059295654\n",
      "          vf_loss: 2.182448387145996\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.654545454545456\n",
      "    ram_util_percent: 62.01818181818181\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 14.0\n",
      "    pol2: -3.3999999999999932\n",
      "  policy_reward_mean:\n",
      "    pol1: -7.875\n",
      "    pol2: -9.202499999999981\n",
      "  policy_reward_min:\n",
      "    pol1: -27.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3937548072776984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21678120342653187\n",
      "    mean_inference_ms: 2.1577747307013517\n",
      "    mean_raw_obs_processing_ms: 1.6237645599972546\n",
      "  time_since_restore: 7.469057083129883\n",
      "  time_this_iter_s: 7.469057083129883\n",
      "  time_total_s: 7.469057083129883\n",
      "  timers:\n",
      "    learn_throughput: 644.041\n",
      "    learn_time_ms: 6210.782\n",
      "    sample_throughput: 4049.406\n",
      "    sample_time_ms: 987.799\n",
      "    update_time_ms: 4.908\n",
      "  timestamp: 1620237661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: cb9b0_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 25.199999999999932\n",
      "  episode_reward_mean: -6.637499999999994\n",
      "  episode_reward_min: -31.50000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.3505887985229492\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03678395599126816\n",
      "          model: {}\n",
      "          policy_loss: -0.07785310596227646\n",
      "          total_loss: 42.48973846435547\n",
      "          vf_explained_var: 0.10298589617013931\n",
      "          vf_loss: 42.56023406982422\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.3479093313217163\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.039728038012981415\n",
      "          model: {}\n",
      "          policy_loss: -0.07102730870246887\n",
      "          total_loss: 2.2132108211517334\n",
      "          vf_explained_var: 0.4128945469856262\n",
      "          vf_loss: 2.276292562484741\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.027272727272724\n",
      "    ram_util_percent: 62.01818181818181\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 33.0\n",
      "    pol2: -2.299999999999988\n",
      "  policy_reward_mean:\n",
      "    pol1: 2.2625\n",
      "    pol2: -8.899999999999983\n",
      "  policy_reward_min:\n",
      "    pol1: -21.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40592245794647364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22400255820051354\n",
      "    mean_inference_ms: 2.1926840739463693\n",
      "    mean_raw_obs_processing_ms: 1.6771388884207503\n",
      "  time_since_restore: 7.51113486289978\n",
      "  time_this_iter_s: 7.51113486289978\n",
      "  time_total_s: 7.51113486289978\n",
      "  timers:\n",
      "    learn_throughput: 636.258\n",
      "    learn_time_ms: 6286.754\n",
      "    sample_throughput: 4029.018\n",
      "    sample_time_ms: 992.798\n",
      "    update_time_ms: 3.783\n",
      "  timestamp: 1620237661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.399999999999977\n",
      "  episode_reward_mean: -6.644999999999993\n",
      "  episode_reward_min: -27.90000000000003\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.343597412109375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.044279925525188446\n",
      "          model: {}\n",
      "          policy_loss: -0.07696963846683502\n",
      "          total_loss: 34.97779083251953\n",
      "          vf_explained_var: 0.18154528737068176\n",
      "          vf_loss: 35.04590606689453\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.3471778631210327\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04060014337301254\n",
      "          model: {}\n",
      "          policy_loss: -0.07120618224143982\n",
      "          total_loss: 2.0473170280456543\n",
      "          vf_explained_var: 0.399627149105072\n",
      "          vf_loss: 2.110403299331665\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.29090909090908\n",
      "    ram_util_percent: 62.01818181818181\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 29.0\n",
      "    pol2: -4.499999999999992\n",
      "  policy_reward_mean:\n",
      "    pol1: 2.2\n",
      "    pol2: -8.844999999999981\n",
      "  policy_reward_min:\n",
      "    pol1: -19.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3998155024514269\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22111840509063568\n",
      "    mean_inference_ms: 2.1750191550942795\n",
      "    mean_raw_obs_processing_ms: 1.6298893079235774\n",
      "  time_since_restore: 7.485661029815674\n",
      "  time_this_iter_s: 7.485661029815674\n",
      "  time_total_s: 7.485661029815674\n",
      "  timers:\n",
      "    learn_throughput: 641.286\n",
      "    learn_time_ms: 6237.467\n",
      "    sample_throughput: 4032.903\n",
      "    sample_time_ms: 991.841\n",
      "    update_time_ms: 4.375\n",
      "  timestamp: 1620237661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.499999999999993\n",
      "  episode_reward_mean: -5.204999999999993\n",
      "  episode_reward_min: -43.500000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.302917718887329\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03637516498565674\n",
      "          model: {}\n",
      "          policy_loss: -0.0677911713719368\n",
      "          total_loss: 36.15202331542969\n",
      "          vf_explained_var: 0.23810787498950958\n",
      "          vf_loss: 36.208900451660156\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.3102104663848877\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03928253427147865\n",
      "          model: {}\n",
      "          policy_loss: -0.07635778933763504\n",
      "          total_loss: 2.3661797046661377\n",
      "          vf_explained_var: 0.3406435251235962\n",
      "          vf_loss: 2.430752754211426\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.95454545454545\n",
      "    ram_util_percent: 60.66363636363636\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 26.5\n",
      "    pol2: 0.9999999999999992\n",
      "  policy_reward_mean:\n",
      "    pol1: 3.26875\n",
      "    pol2: -8.473749999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -33.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3955151309721644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22142598564542756\n",
      "    mean_inference_ms: 2.057099071392458\n",
      "    mean_raw_obs_processing_ms: 1.6571064188874352\n",
      "  time_since_restore: 14.735140562057495\n",
      "  time_this_iter_s: 7.327611207962036\n",
      "  time_total_s: 14.735140562057495\n",
      "  timers:\n",
      "    learn_throughput: 650.354\n",
      "    learn_time_ms: 6150.499\n",
      "    sample_throughput: 4282.31\n",
      "    sample_time_ms: 934.075\n",
      "    update_time_ms: 4.449\n",
      "  timestamp: 1620237668\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: cb9b0_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.48566</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -6.645 </td><td style=\"text-align: right;\">                23.4</td><td style=\"text-align: right;\">               -27.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.51113</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -6.6375</td><td style=\"text-align: right;\">                25.2</td><td style=\"text-align: right;\">               -31.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.46906</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-17.0775</td><td style=\"text-align: right;\">                 8.4</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        14.7351 </td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> -5.205 </td><td style=\"text-align: right;\">                16.5</td><td style=\"text-align: right;\">               -43.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 25.199999999999932\n",
      "  episode_reward_mean: -5.013749999999993\n",
      "  episode_reward_min: -31.50000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2987873554229736\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04003700241446495\n",
      "          model: {}\n",
      "          policy_loss: -0.08468735963106155\n",
      "          total_loss: 28.290122985839844\n",
      "          vf_explained_var: 0.18354330956935883\n",
      "          vf_loss: 28.36280059814453\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.311362624168396\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0375993475317955\n",
      "          model: {}\n",
      "          policy_loss: -0.07423878461122513\n",
      "          total_loss: 2.5039613246917725\n",
      "          vf_explained_var: 0.35455775260925293\n",
      "          vf_loss: 2.566920280456543\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.809090909090905\n",
      "    ram_util_percent: 60.67272727272727\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 33.0\n",
      "    pol2: -0.10000000000000153\n",
      "  policy_reward_mean:\n",
      "    pol1: 3.41875\n",
      "    pol2: -8.432499999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -21.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4012033046959479\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22211513768233243\n",
      "    mean_inference_ms: 2.075455050937526\n",
      "    mean_raw_obs_processing_ms: 1.6499713381780772\n",
      "  time_since_restore: 14.747114896774292\n",
      "  time_this_iter_s: 7.235980033874512\n",
      "  time_total_s: 14.747114896774292\n",
      "  timers:\n",
      "    learn_throughput: 642.826\n",
      "    learn_time_ms: 6222.525\n",
      "    sample_throughput: 4432.35\n",
      "    sample_time_ms: 902.456\n",
      "    update_time_ms: 5.002\n",
      "  timestamp: 1620237668\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.399999999999977\n",
      "  episode_reward_mean: -4.241249999999992\n",
      "  episode_reward_min: -30.000000000000036\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.3010932207107544\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03742225468158722\n",
      "          model: {}\n",
      "          policy_loss: -0.07142119109630585\n",
      "          total_loss: 27.62944221496582\n",
      "          vf_explained_var: 0.20291957259178162\n",
      "          vf_loss: 27.68963623046875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.3167760372161865\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.034583285450935364\n",
      "          model: {}\n",
      "          policy_loss: -0.07328575104475021\n",
      "          total_loss: 2.0322842597961426\n",
      "          vf_explained_var: 0.4138551950454712\n",
      "          vf_loss: 2.0951950550079346\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.88181818181818\n",
      "    ram_util_percent: 60.66363636363636\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 29.0\n",
      "    pol2: -4.499999999999982\n",
      "  policy_reward_mean:\n",
      "    pol1: 4.35625\n",
      "    pol2: -8.597499999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -20.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.409895955249498\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22764398040588799\n",
      "    mean_inference_ms: 2.0965537847895015\n",
      "    mean_raw_obs_processing_ms: 1.6715814195536076\n",
      "  time_since_restore: 14.77191424369812\n",
      "  time_this_iter_s: 7.286253213882446\n",
      "  time_total_s: 14.77191424369812\n",
      "  timers:\n",
      "    learn_throughput: 645.904\n",
      "    learn_time_ms: 6192.875\n",
      "    sample_throughput: 4226.77\n",
      "    sample_time_ms: 946.349\n",
      "    update_time_ms: 4.166\n",
      "  timestamp: 1620237669\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.599999999999987\n",
      "  episode_reward_mean: -9.20625\n",
      "  episode_reward_min: -37.50000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.300898551940918\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03821340948343277\n",
      "          model: {}\n",
      "          policy_loss: -0.07424856722354889\n",
      "          total_loss: 32.7962646484375\n",
      "          vf_explained_var: 0.31251299381256104\n",
      "          vf_loss: 32.85905456542969\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.3033596277236938\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03914916515350342\n",
      "          model: {}\n",
      "          policy_loss: -0.08175911754369736\n",
      "          total_loss: 2.2895236015319824\n",
      "          vf_explained_var: 0.4183140695095062\n",
      "          vf_loss: 2.3595380783081055\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.95454545454545\n",
      "    ram_util_percent: 60.66363636363636\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 30.5\n",
      "    pol2: -1.1999999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: -0.44375\n",
      "    pol2: -8.762499999999983\n",
      "  policy_reward_min:\n",
      "    pol1: -27.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4083432380210182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22475952927648368\n",
      "    mean_inference_ms: 2.1145892668887916\n",
      "    mean_raw_obs_processing_ms: 1.692192791647134\n",
      "  time_since_restore: 14.9205961227417\n",
      "  time_this_iter_s: 7.451539039611816\n",
      "  time_total_s: 14.9205961227417\n",
      "  timers:\n",
      "    learn_throughput: 640.336\n",
      "    learn_time_ms: 6246.721\n",
      "    sample_throughput: 4138.748\n",
      "    sample_time_ms: 966.476\n",
      "    update_time_ms: 3.973\n",
      "  timestamp: 1620237669\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: cb9b0_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.499999999999993\n",
      "  episode_reward_mean: -1.814999999999994\n",
      "  episode_reward_min: -33.30000000000003\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2813265323638916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029068656265735626\n",
      "          model: {}\n",
      "          policy_loss: -0.06279368698596954\n",
      "          total_loss: 28.714794158935547\n",
      "          vf_explained_var: 0.33632609248161316\n",
      "          vf_loss: 28.764507293701172\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2821522951126099\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.031006604433059692\n",
      "          model: {}\n",
      "          policy_loss: -0.06766685098409653\n",
      "          total_loss: 2.2444918155670166\n",
      "          vf_explained_var: 0.39739108085632324\n",
      "          vf_loss: 2.298205852508545\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.18888888888889\n",
      "    ram_util_percent: 60.51111111111111\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 26.5\n",
      "    pol2: 0.9999999999999992\n",
      "  policy_reward_mean:\n",
      "    pol1: 6.645\n",
      "    pol2: -8.459999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -25.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.417790260221568\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2314789131069147\n",
      "    mean_inference_ms: 2.0325816016841194\n",
      "    mean_raw_obs_processing_ms: 1.7321531494591238\n",
      "  time_since_restore: 21.596190452575684\n",
      "  time_this_iter_s: 6.8610498905181885\n",
      "  time_total_s: 21.596190452575684\n",
      "  timers:\n",
      "    learn_throughput: 669.931\n",
      "    learn_time_ms: 5970.764\n",
      "    sample_throughput: 4139.924\n",
      "    sample_time_ms: 966.201\n",
      "    update_time_ms: 4.132\n",
      "  timestamp: 1620237675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: cb9b0_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         14.7719</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">-4.24125</td><td style=\"text-align: right;\">                23.4</td><td style=\"text-align: right;\">               -30  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         14.7471</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">-5.01375</td><td style=\"text-align: right;\">                25.2</td><td style=\"text-align: right;\">               -31.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         14.9206</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">-9.20625</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         21.5962</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-1.815  </td><td style=\"text-align: right;\">                16.5</td><td style=\"text-align: right;\">               -33.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.899999999999974\n",
      "  episode_reward_mean: -2.378999999999989\n",
      "  episode_reward_min: -30.000000000000036\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2661799192428589\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02801455371081829\n",
      "          model: {}\n",
      "          policy_loss: -0.06668344885110855\n",
      "          total_loss: 22.77505874633789\n",
      "          vf_explained_var: 0.30767640471458435\n",
      "          vf_loss: 22.829133987426758\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2754497528076172\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.030049750581383705\n",
      "          model: {}\n",
      "          policy_loss: -0.07528340071439743\n",
      "          total_loss: 2.5082039833068848\n",
      "          vf_explained_var: 0.3275298476219177\n",
      "          vf_loss: 2.569965124130249\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.18888888888889\n",
      "    ram_util_percent: 60.51111111111111\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 19.5\n",
      "    pol2: -1.1999999999999986\n",
      "  policy_reward_mean:\n",
      "    pol1: 5.96\n",
      "    pol2: -8.338999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -20.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4225716143951405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2355375065240319\n",
      "    mean_inference_ms: 2.0402714478528936\n",
      "    mean_raw_obs_processing_ms: 1.7142357438904563\n",
      "  time_since_restore: 21.558053255081177\n",
      "  time_this_iter_s: 6.786139011383057\n",
      "  time_total_s: 21.558053255081177\n",
      "  timers:\n",
      "    learn_throughput: 667.166\n",
      "    learn_time_ms: 5995.505\n",
      "    sample_throughput: 4217.396\n",
      "    sample_time_ms: 948.453\n",
      "    update_time_ms: 4.12\n",
      "  timestamp: 1620237675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.599999999999987\n",
      "  episode_reward_mean: -4.649999999999997\n",
      "  episode_reward_min: -37.50000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2596465349197388\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03223307430744171\n",
      "          model: {}\n",
      "          policy_loss: -0.0766485258936882\n",
      "          total_loss: 29.237022399902344\n",
      "          vf_explained_var: 0.26916319131851196\n",
      "          vf_loss: 29.299165725708008\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2676405906677246\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.031102649867534637\n",
      "          model: {}\n",
      "          policy_loss: -0.06711222231388092\n",
      "          total_loss: 2.199174642562866\n",
      "          vf_explained_var: 0.3775292634963989\n",
      "          vf_loss: 2.252290725708008\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.18888888888889\n",
      "    ram_util_percent: 60.51111111111111\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 30.5\n",
      "    pol2: -1.1999999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: 3.92\n",
      "    pol2: -8.569999999999984\n",
      "  policy_reward_min:\n",
      "    pol1: -27.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4185351020771702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.232551706824307\n",
      "    mean_inference_ms: 2.0758300037340347\n",
      "    mean_raw_obs_processing_ms: 1.7434802495601065\n",
      "  time_since_restore: 21.635075092315674\n",
      "  time_this_iter_s: 6.714478969573975\n",
      "  time_total_s: 21.635075092315674\n",
      "  timers:\n",
      "    learn_throughput: 665.075\n",
      "    learn_time_ms: 6014.36\n",
      "    sample_throughput: 4174.44\n",
      "    sample_time_ms: 958.212\n",
      "    update_time_ms: 5.007\n",
      "  timestamp: 1620237675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: cb9b0_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.199999999999918\n",
      "  episode_reward_mean: -3.3539999999999934\n",
      "  episode_reward_min: -28.50000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2577645778656006\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.031449466943740845\n",
      "          model: {}\n",
      "          policy_loss: -0.07228178530931473\n",
      "          total_loss: 34.65220642089844\n",
      "          vf_explained_var: 0.19576992094516754\n",
      "          vf_loss: 34.710330963134766\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2795528173446655\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02959190309047699\n",
      "          model: {}\n",
      "          policy_loss: -0.070169597864151\n",
      "          total_loss: 2.070136070251465\n",
      "          vf_explained_var: 0.3834651708602905\n",
      "          vf_loss: 2.1269896030426025\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.160000000000004\n",
      "    ram_util_percent: 60.55\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 24.0\n",
      "    pol2: -0.10000000000000153\n",
      "  policy_reward_mean:\n",
      "    pol1: 5.15\n",
      "    pol2: -8.503999999999984\n",
      "  policy_reward_min:\n",
      "    pol1: -18.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41328545074130235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22986904583850054\n",
      "    mean_inference_ms: 2.0285250299020845\n",
      "    mean_raw_obs_processing_ms: 1.6891094305229446\n",
      "  time_since_restore: 21.63690209388733\n",
      "  time_this_iter_s: 6.889787197113037\n",
      "  time_total_s: 21.63690209388733\n",
      "  timers:\n",
      "    learn_throughput: 663.465\n",
      "    learn_time_ms: 6028.954\n",
      "    sample_throughput: 4243.679\n",
      "    sample_time_ms: 942.578\n",
      "    update_time_ms: 4.777\n",
      "  timestamp: 1620237675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.599999999999945\n",
      "  episode_reward_mean: 1.7730000000000075\n",
      "  episode_reward_min: -22.50000000000004\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2501425743103027\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023680590093135834\n",
      "          model: {}\n",
      "          policy_loss: -0.06642554700374603\n",
      "          total_loss: 26.222835540771484\n",
      "          vf_explained_var: 0.3815299868583679\n",
      "          vf_loss: 26.27327537536621\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.246468186378479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02482820302248001\n",
      "          model: {}\n",
      "          policy_loss: -0.05522734671831131\n",
      "          total_loss: 2.5803961753845215\n",
      "          vf_explained_var: 0.3476640582084656\n",
      "          vf_loss: 2.6188647747039795\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.32000000000001\n",
      "    ram_util_percent: 60.92999999999999\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 33.5\n",
      "    pol2: -2.2999999999999843\n",
      "  policy_reward_mean:\n",
      "    pol1: 10.09\n",
      "    pol2: -8.316999999999984\n",
      "  policy_reward_min:\n",
      "    pol1: -12.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4266465792862661\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2350845138814872\n",
      "    mean_inference_ms: 1.9756058905387837\n",
      "    mean_raw_obs_processing_ms: 1.7603600067679377\n",
      "  time_since_restore: 28.40170931816101\n",
      "  time_this_iter_s: 6.805518865585327\n",
      "  time_total_s: 28.40170931816101\n",
      "  timers:\n",
      "    learn_throughput: 674.298\n",
      "    learn_time_ms: 5932.093\n",
      "    sample_throughput: 4452.543\n",
      "    sample_time_ms: 898.363\n",
      "    update_time_ms: 4.301\n",
      "  timestamp: 1620237682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: cb9b0_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         21.5581</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -2.379</td><td style=\"text-align: right;\">                12.9</td><td style=\"text-align: right;\">               -30  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         21.6369</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -3.354</td><td style=\"text-align: right;\">                19.2</td><td style=\"text-align: right;\">               -28.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         21.6351</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -4.65 </td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         28.4017</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   1.773</td><td style=\"text-align: right;\">                24.6</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.799999999999955\n",
      "  episode_reward_mean: -0.34199999999999037\n",
      "  episode_reward_min: -31.500000000000043\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2238572835922241\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025144267827272415\n",
      "          model: {}\n",
      "          policy_loss: -0.0658692792057991\n",
      "          total_loss: 29.964096069335938\n",
      "          vf_explained_var: 0.28109848499298096\n",
      "          vf_loss: 30.01299285888672\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2537477016448975\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02349086105823517\n",
      "          model: {}\n",
      "          policy_loss: -0.06267167627811432\n",
      "          total_loss: 2.304245948791504\n",
      "          vf_explained_var: 0.383883535861969\n",
      "          vf_loss: 2.3510613441467285\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.35\n",
      "    ram_util_percent: 60.92999999999999\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 25.0\n",
      "    pol2: -1.2000000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: 8.085\n",
      "    pol2: -8.426999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -22.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4226138377621798\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23314653801622837\n",
      "    mean_inference_ms: 2.0131651513429896\n",
      "    mean_raw_obs_processing_ms: 1.7421831812015274\n",
      "  time_since_restore: 28.41396999359131\n",
      "  time_this_iter_s: 6.778894901275635\n",
      "  time_total_s: 28.41396999359131\n",
      "  timers:\n",
      "    learn_throughput: 671.665\n",
      "    learn_time_ms: 5955.351\n",
      "    sample_throughput: 4434.851\n",
      "    sample_time_ms: 901.947\n",
      "    update_time_ms: 5.119\n",
      "  timestamp: 1620237682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: cb9b0_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.10000000000003\n",
      "  episode_reward_mean: -0.6059999999999871\n",
      "  episode_reward_min: -19.499999999999986\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2373712062835693\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02407953143119812\n",
      "          model: {}\n",
      "          policy_loss: -0.05955330654978752\n",
      "          total_loss: 25.79761505126953\n",
      "          vf_explained_var: 0.2983354926109314\n",
      "          vf_loss: 25.840913772583008\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2473864555358887\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024579696357250214\n",
      "          model: {}\n",
      "          policy_loss: -0.06617771834135056\n",
      "          total_loss: 2.2999799251556396\n",
      "          vf_explained_var: 0.37450671195983887\n",
      "          vf_loss: 2.3495664596557617\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.02\n",
      "    ram_util_percent: 60.92999999999999\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 26.0\n",
      "    pol2: -1.199999999999996\n",
      "  policy_reward_mean:\n",
      "    pol1: 7.48\n",
      "    pol2: -8.085999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -9.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42633703107588594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2379765663420894\n",
      "    mean_inference_ms: 1.9669377332493787\n",
      "    mean_raw_obs_processing_ms: 1.7132997882722136\n",
      "  time_since_restore: 28.389304399490356\n",
      "  time_this_iter_s: 6.83125114440918\n",
      "  time_total_s: 28.389304399490356\n",
      "  timers:\n",
      "    learn_throughput: 670.704\n",
      "    learn_time_ms: 5963.882\n",
      "    sample_throughput: 4502.151\n",
      "    sample_time_ms: 888.464\n",
      "    update_time_ms: 4.233\n",
      "  timestamp: 1620237682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.199999999999918\n",
      "  episode_reward_mean: -1.280999999999992\n",
      "  episode_reward_min: -28.50000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2389496564865112\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02435423992574215\n",
      "          model: {}\n",
      "          policy_loss: -0.06688143312931061\n",
      "          total_loss: 23.015506744384766\n",
      "          vf_explained_var: 0.386806845664978\n",
      "          vf_loss: 23.065950393676758\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2508472204208374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024883251637220383\n",
      "          model: {}\n",
      "          policy_loss: -0.06544601172208786\n",
      "          total_loss: 2.0595712661743164\n",
      "          vf_explained_var: 0.39629223942756653\n",
      "          vf_loss: 2.1082210540771484\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.28888888888889\n",
      "    ram_util_percent: 60.94444444444444\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 25.0\n",
      "    pol2: -0.10000000000000153\n",
      "  policy_reward_mean:\n",
      "    pol1: 7.025\n",
      "    pol2: -8.305999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -18.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4194618292896745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23199028211461445\n",
      "    mean_inference_ms: 1.976533745855677\n",
      "    mean_raw_obs_processing_ms: 1.691255746989712\n",
      "  time_since_restore: 28.430360078811646\n",
      "  time_this_iter_s: 6.793457984924316\n",
      "  time_total_s: 28.430360078811646\n",
      "  timers:\n",
      "    learn_throughput: 669.338\n",
      "    learn_time_ms: 5976.058\n",
      "    sample_throughput: 4478.845\n",
      "    sample_time_ms: 893.087\n",
      "    update_time_ms: 4.758\n",
      "  timestamp: 1620237682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.599999999999945\n",
      "  episode_reward_mean: 2.5140000000000096\n",
      "  episode_reward_min: -17.99999999999999\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2298450469970703\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018746070563793182\n",
      "          model: {}\n",
      "          policy_loss: -0.056502826511859894\n",
      "          total_loss: 24.349628448486328\n",
      "          vf_explained_var: 0.41941604018211365\n",
      "          vf_loss: 24.387149810791016\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2230613231658936\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019891198724508286\n",
      "          model: {}\n",
      "          policy_loss: -0.060566600412130356\n",
      "          total_loss: 2.9972832202911377\n",
      "          vf_explained_var: 0.34534886479377747\n",
      "          vf_loss: 3.037710189819336\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.73\n",
      "    ram_util_percent: 61.6\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 33.5\n",
      "    pol2: 4.300000000000015\n",
      "  policy_reward_mean:\n",
      "    pol1: 10.435\n",
      "    pol2: -7.920999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -8.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4249491395048123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2339906022652213\n",
      "    mean_inference_ms: 1.9281183740408436\n",
      "    mean_raw_obs_processing_ms: 1.7436561670413229\n",
      "  time_since_restore: 35.582314252853394\n",
      "  time_this_iter_s: 7.180604934692383\n",
      "  time_total_s: 35.582314252853394\n",
      "  timers:\n",
      "    learn_throughput: 671.803\n",
      "    learn_time_ms: 5954.129\n",
      "    sample_throughput: 4481.177\n",
      "    sample_time_ms: 892.623\n",
      "    update_time_ms: 4.325\n",
      "  timestamp: 1620237689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: cb9b0_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         28.3893</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  -0.606</td><td style=\"text-align: right;\">                17.1</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         28.4304</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  -1.281</td><td style=\"text-align: right;\">                19.2</td><td style=\"text-align: right;\">               -28.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         28.414 </td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  -0.342</td><td style=\"text-align: right;\">                16.8</td><td style=\"text-align: right;\">               -31.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         35.5823</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   2.514</td><td style=\"text-align: right;\">                24.6</td><td style=\"text-align: right;\">               -18  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.299999999999933\n",
      "  episode_reward_mean: 1.4880000000000104\n",
      "  episode_reward_min: -18.90000000000002\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2245558500289917\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018586529418826103\n",
      "          model: {}\n",
      "          policy_loss: -0.06037398427724838\n",
      "          total_loss: 27.91238021850586\n",
      "          vf_explained_var: 0.34945371747016907\n",
      "          vf_loss: 27.953933715820312\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.234088659286499\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018674926832318306\n",
      "          model: {}\n",
      "          policy_loss: -0.0621548667550087\n",
      "          total_loss: 2.0643672943115234\n",
      "          vf_explained_var: 0.36801230907440186\n",
      "          vf_loss: 2.1076138019561768\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.13\n",
      "    ram_util_percent: 61.53000000000001\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 25.0\n",
      "    pol2: -3.399999999999988\n",
      "  policy_reward_mean:\n",
      "    pol1: 9.86\n",
      "    pol2: -8.371999999999984\n",
      "  policy_reward_min:\n",
      "    pol1: -10.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42222913193765726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23258716001457047\n",
      "    mean_inference_ms: 1.9606697059613074\n",
      "    mean_raw_obs_processing_ms: 1.7180645662290943\n",
      "  time_since_restore: 35.47168517112732\n",
      "  time_this_iter_s: 7.041325092315674\n",
      "  time_total_s: 35.47168517112732\n",
      "  timers:\n",
      "    learn_throughput: 673.733\n",
      "    learn_time_ms: 5937.07\n",
      "    sample_throughput: 4387.367\n",
      "    sample_time_ms: 911.709\n",
      "    update_time_ms: 4.841\n",
      "  timestamp: 1620237689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.799999999999955\n",
      "  episode_reward_mean: 0.4020000000000073\n",
      "  episode_reward_min: -31.500000000000043\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2083922624588013\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018895264714956284\n",
      "          model: {}\n",
      "          policy_loss: -0.0564107671380043\n",
      "          total_loss: 30.02867317199707\n",
      "          vf_explained_var: 0.39861956238746643\n",
      "          vf_loss: 30.06595230102539\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2333639860153198\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020168103277683258\n",
      "          model: {}\n",
      "          policy_loss: -0.06306587904691696\n",
      "          total_loss: 3.9079535007476807\n",
      "          vf_explained_var: 0.26074767112731934\n",
      "          vf_loss: 3.950598955154419\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.3\n",
      "    ram_util_percent: 61.618181818181824\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 25.0\n",
      "    pol2: 2.10000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: 8.51\n",
      "    pol2: -8.107999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -21.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41938720271002466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23088005201302067\n",
      "    mean_inference_ms: 1.9686239674541566\n",
      "    mean_raw_obs_processing_ms: 1.715739255939335\n",
      "  time_since_restore: 35.59018325805664\n",
      "  time_this_iter_s: 7.176213264465332\n",
      "  time_total_s: 35.59018325805664\n",
      "  timers:\n",
      "    learn_throughput: 670.124\n",
      "    learn_time_ms: 5969.041\n",
      "    sample_throughput: 4440.607\n",
      "    sample_time_ms: 900.778\n",
      "    update_time_ms: 5.535\n",
      "  timestamp: 1620237689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: cb9b0_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.99999999999995\n",
      "  episode_reward_mean: 0.21900000000000883\n",
      "  episode_reward_min: -39.00000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2069576978683472\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018453355878591537\n",
      "          model: {}\n",
      "          policy_loss: -0.06423570960760117\n",
      "          total_loss: 37.681182861328125\n",
      "          vf_explained_var: 0.24597874283790588\n",
      "          vf_loss: 37.72673416137695\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2241339683532715\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01905866712331772\n",
      "          model: {}\n",
      "          policy_loss: -0.05892682075500488\n",
      "          total_loss: 3.1101107597351074\n",
      "          vf_explained_var: 0.3391805589199066\n",
      "          vf_loss: 3.1497409343719482\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.381818181818176\n",
      "    ram_util_percent: 61.618181818181824\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 29.0\n",
      "    pol2: 2.0999999999999983\n",
      "  policy_reward_mean:\n",
      "    pol1: 7.81\n",
      "    pol2: -7.590999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -29.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42390912857244634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23576525274980448\n",
      "    mean_inference_ms: 1.9287731787753097\n",
      "    mean_raw_obs_processing_ms: 1.6954532872837624\n",
      "  time_since_restore: 35.531150341033936\n",
      "  time_this_iter_s: 7.141845941543579\n",
      "  time_total_s: 35.531150341033936\n",
      "  timers:\n",
      "    learn_throughput: 670.535\n",
      "    learn_time_ms: 5965.388\n",
      "    sample_throughput: 4470.957\n",
      "    sample_time_ms: 894.663\n",
      "    update_time_ms: 4.167\n",
      "  timestamp: 1620237689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.599999999999998\n",
      "  episode_reward_mean: 2.1660000000000106\n",
      "  episode_reward_min: -21.00000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1867165565490723\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01719837635755539\n",
      "          model: {}\n",
      "          policy_loss: -0.055951207876205444\n",
      "          total_loss: 26.987682342529297\n",
      "          vf_explained_var: 0.395652711391449\n",
      "          vf_loss: 27.02621841430664\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2204939126968384\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014219477772712708\n",
      "          model: {}\n",
      "          policy_loss: -0.053324513137340546\n",
      "          total_loss: 2.3722376823425293\n",
      "          vf_explained_var: 0.3162984848022461\n",
      "          vf_loss: 2.4039664268493652\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.888888888888886\n",
      "    ram_util_percent: 61.24444444444445\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 27.5\n",
      "    pol2: 2.10000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: 10.34\n",
      "    pol2: -8.173999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -11.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41823577046342275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23023137652161665\n",
      "    mean_inference_ms: 1.9414790637504915\n",
      "    mean_raw_obs_processing_ms: 1.705408503290762\n",
      "  time_since_restore: 42.39943528175354\n",
      "  time_this_iter_s: 6.809252023696899\n",
      "  time_total_s: 42.39943528175354\n",
      "  timers:\n",
      "    learn_throughput: 675.298\n",
      "    learn_time_ms: 5923.311\n",
      "    sample_throughput: 4459.64\n",
      "    sample_time_ms: 896.933\n",
      "    update_time_ms: 5.218\n",
      "  timestamp: 1620237696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: cb9b0_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         35.5312</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   0.219</td><td style=\"text-align: right;\">                21  </td><td style=\"text-align: right;\">               -39  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         35.4717</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   1.488</td><td style=\"text-align: right;\">                18.3</td><td style=\"text-align: right;\">               -18.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         42.3994</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   2.166</td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               -21  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         35.5823</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   2.514</td><td style=\"text-align: right;\">                24.6</td><td style=\"text-align: right;\">               -18  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.599999999999945\n",
      "  episode_reward_mean: 3.5850000000000124\n",
      "  episode_reward_min: -17.99999999999999\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1979690790176392\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017934303730726242\n",
      "          model: {}\n",
      "          policy_loss: -0.059876009821891785\n",
      "          total_loss: 27.669910430908203\n",
      "          vf_explained_var: 0.35560882091522217\n",
      "          vf_loss: 27.711627960205078\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.210357427597046\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01737274043262005\n",
      "          model: {}\n",
      "          policy_loss: -0.043363817036151886\n",
      "          total_loss: 3.1638951301574707\n",
      "          vf_explained_var: 0.4129200279712677\n",
      "          vf_loss: 3.189669132232666\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.15\n",
      "    ram_util_percent: 61.29\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 33.5\n",
      "    pol2: 8.699999999999998\n",
      "  policy_reward_mean:\n",
      "    pol1: 11.275\n",
      "    pol2: -7.689999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -12.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4234431803611906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23344169859750794\n",
      "    mean_inference_ms: 1.901230834027433\n",
      "    mean_raw_obs_processing_ms: 1.7334530295757429\n",
      "  time_since_restore: 42.53659105300903\n",
      "  time_this_iter_s: 6.95427680015564\n",
      "  time_total_s: 42.53659105300903\n",
      "  timers:\n",
      "    learn_throughput: 675.651\n",
      "    learn_time_ms: 5920.214\n",
      "    sample_throughput: 4414.128\n",
      "    sample_time_ms: 906.181\n",
      "    update_time_ms: 4.311\n",
      "  timestamp: 1620237696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.99999999999995\n",
      "  episode_reward_mean: 1.8240000000000078\n",
      "  episode_reward_min: -39.00000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.193285346031189\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018331393599510193\n",
      "          model: {}\n",
      "          policy_loss: -0.059056658297777176\n",
      "          total_loss: 25.017276763916016\n",
      "          vf_explained_var: 0.3837546110153198\n",
      "          vf_loss: 25.05777359008789\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1905885934829712\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019103078171610832\n",
      "          model: {}\n",
      "          policy_loss: -0.05286499112844467\n",
      "          total_loss: 3.112725019454956\n",
      "          vf_explained_var: 0.24581292271614075\n",
      "          vf_loss: 3.1462481021881104\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.75555555555556\n",
      "    ram_util_percent: 61.24444444444445\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 29.0\n",
      "    pol2: 3.1999999999999957\n",
      "  policy_reward_mean:\n",
      "    pol1: 9.25\n",
      "    pol2: -7.425999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -29.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42292183011633244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23444891173573576\n",
      "    mean_inference_ms: 1.9067284186789644\n",
      "    mean_raw_obs_processing_ms: 1.6910872562435342\n",
      "  time_since_restore: 42.35571837425232\n",
      "  time_this_iter_s: 6.824568033218384\n",
      "  time_total_s: 42.35571837425232\n",
      "  timers:\n",
      "    learn_throughput: 675.244\n",
      "    learn_time_ms: 5923.783\n",
      "    sample_throughput: 4480.447\n",
      "    sample_time_ms: 892.768\n",
      "    update_time_ms: 4.317\n",
      "  timestamp: 1620237696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.09999999999996\n",
      "  episode_reward_mean: 2.6940000000000106\n",
      "  episode_reward_min: -18.90000000000002\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1978905200958252\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018867164850234985\n",
      "          model: {}\n",
      "          policy_loss: -0.06360958516597748\n",
      "          total_loss: 28.457956314086914\n",
      "          vf_explained_var: 0.3088874816894531\n",
      "          vf_loss: 28.50246238708496\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2066690921783447\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017362885177135468\n",
      "          model: {}\n",
      "          policy_loss: -0.06122654676437378\n",
      "          total_loss: 2.657352924346924\n",
      "          vf_explained_var: 0.33297908306121826\n",
      "          vf_loss: 2.7009997367858887\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.39\n",
      "    ram_util_percent: 61.03000000000001\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 35.0\n",
      "    pol2: -0.09999999999999387\n",
      "  policy_reward_mean:\n",
      "    pol1: 10.835\n",
      "    pol2: -8.140999999999984\n",
      "  policy_reward_min:\n",
      "    pol1: -10.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42398223799985035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23241035199892102\n",
      "    mean_inference_ms: 1.942514208135152\n",
      "    mean_raw_obs_processing_ms: 1.7439497583051309\n",
      "  time_since_restore: 42.38809633255005\n",
      "  time_this_iter_s: 6.9164111614227295\n",
      "  time_total_s: 42.38809633255005\n",
      "  timers:\n",
      "    learn_throughput: 676.803\n",
      "    learn_time_ms: 5910.136\n",
      "    sample_throughput: 4387.007\n",
      "    sample_time_ms: 911.783\n",
      "    update_time_ms: 4.934\n",
      "  timestamp: 1620237696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.999999999999893\n",
      "  episode_reward_mean: 2.6670000000000105\n",
      "  episode_reward_min: -21.00000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1750458478927612\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018194664269685745\n",
      "          model: {}\n",
      "          policy_loss: -0.05623921751976013\n",
      "          total_loss: 26.10565948486328\n",
      "          vf_explained_var: 0.3244855999946594\n",
      "          vf_loss: 26.143476486206055\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.215268611907959\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013358233496546745\n",
      "          model: {}\n",
      "          policy_loss: -0.04914531856775284\n",
      "          total_loss: 2.1309516429901123\n",
      "          vf_explained_var: 0.3492116630077362\n",
      "          vf_loss: 2.159809112548828\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.67\n",
      "    ram_util_percent: 58.879999999999995\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 34.0\n",
      "    pol2: 2.10000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: 11.215\n",
      "    pol2: -8.547999999999984\n",
      "  policy_reward_min:\n",
      "    pol1: -11.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41666357310423174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22881023150355467\n",
      "    mean_inference_ms: 1.9156620147818797\n",
      "    mean_raw_obs_processing_ms: 1.700215619739079\n",
      "  time_since_restore: 49.169851303100586\n",
      "  time_this_iter_s: 6.770416021347046\n",
      "  time_total_s: 49.169851303100586\n",
      "  timers:\n",
      "    learn_throughput: 678.12\n",
      "    learn_time_ms: 5898.661\n",
      "    sample_throughput: 4573.581\n",
      "    sample_time_ms: 874.588\n",
      "    update_time_ms: 5.365\n",
      "  timestamp: 1620237703\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: cb9b0_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         42.3557</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   1.824</td><td style=\"text-align: right;\">                21  </td><td style=\"text-align: right;\">               -39  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         42.3881</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   2.694</td><td style=\"text-align: right;\">                26.1</td><td style=\"text-align: right;\">               -18.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         49.1699</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   2.667</td><td style=\"text-align: right;\">                24  </td><td style=\"text-align: right;\">               -21  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         42.5366</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   3.585</td><td style=\"text-align: right;\">                24.6</td><td style=\"text-align: right;\">               -18  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.99999999999991\n",
      "  episode_reward_mean: 3.903000000000008\n",
      "  episode_reward_min: -16.499999999999986\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.196864128112793\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01645570620894432\n",
      "          model: {}\n",
      "          policy_loss: -0.057790931314229965\n",
      "          total_loss: 37.81082534790039\n",
      "          vf_explained_var: 0.38023316860198975\n",
      "          vf_loss: 37.851959228515625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1947579383850098\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01759328693151474\n",
      "          model: {}\n",
      "          policy_loss: -0.05581020936369896\n",
      "          total_loss: 2.194079875946045\n",
      "          vf_explained_var: 0.3962835371494293\n",
      "          vf_loss: 2.23207688331604\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.69000000000001\n",
      "    ram_util_percent: 58.879999999999995\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 43.0\n",
      "    pol2: 8.699999999999998\n",
      "  policy_reward_mean:\n",
      "    pol1: 11.67\n",
      "    pol2: -7.766999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -12.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.423114822301839\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23308755179682403\n",
      "    mean_inference_ms: 1.8838739334573467\n",
      "    mean_raw_obs_processing_ms: 1.7337869101387549\n",
      "  time_since_restore: 49.38208985328674\n",
      "  time_this_iter_s: 6.84549880027771\n",
      "  time_total_s: 49.38208985328674\n",
      "  timers:\n",
      "    learn_throughput: 676.85\n",
      "    learn_time_ms: 5909.731\n",
      "    sample_throughput: 4521.807\n",
      "    sample_time_ms: 884.602\n",
      "    update_time_ms: 4.464\n",
      "  timestamp: 1620237703\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.599999999999945\n",
      "  episode_reward_mean: 2.30700000000001\n",
      "  episode_reward_min: -19.499999999999986\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1711788177490234\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017012815922498703\n",
      "          model: {}\n",
      "          policy_loss: -0.04798262193799019\n",
      "          total_loss: 29.58285903930664\n",
      "          vf_explained_var: 0.29913753271102905\n",
      "          vf_loss: 29.613615036010742\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1662025451660156\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018889091908931732\n",
      "          model: {}\n",
      "          policy_loss: -0.061893220990896225\n",
      "          total_loss: 2.4341959953308105\n",
      "          vf_explained_var: 0.2949129641056061\n",
      "          vf_loss: 2.476963996887207\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.45000000000002\n",
      "    ram_util_percent: 58.90000000000001\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 28.0\n",
      "    pol2: 3.1999999999999957\n",
      "  policy_reward_mean:\n",
      "    pol1: 9.535\n",
      "    pol2: -7.2279999999999855\n",
      "  policy_reward_min:\n",
      "    pol1: -15.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42267566832441367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23431177256709723\n",
      "    mean_inference_ms: 1.8920008353234343\n",
      "    mean_raw_obs_processing_ms: 1.6973596102137625\n",
      "  time_since_restore: 49.22886848449707\n",
      "  time_this_iter_s: 6.873150110244751\n",
      "  time_total_s: 49.22886848449707\n",
      "  timers:\n",
      "    learn_throughput: 676.696\n",
      "    learn_time_ms: 5911.077\n",
      "    sample_throughput: 4552.789\n",
      "    sample_time_ms: 878.582\n",
      "    update_time_ms: 4.154\n",
      "  timestamp: 1620237703\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.09999999999996\n",
      "  episode_reward_mean: 1.9650000000000105\n",
      "  episode_reward_min: -25.500000000000018\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1829898357391357\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018611758947372437\n",
      "          model: {}\n",
      "          policy_loss: -0.05515590310096741\n",
      "          total_loss: 36.00413513183594\n",
      "          vf_explained_var: 0.21588249504566193\n",
      "          vf_loss: 36.040443420410156\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1963953971862793\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018131248652935028\n",
      "          model: {}\n",
      "          policy_loss: -0.05303288251161575\n",
      "          total_loss: 2.4645023345947266\n",
      "          vf_explained_var: 0.315582811832428\n",
      "          vf_loss: 2.4991774559020996\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.67999999999999\n",
      "    ram_util_percent: 58.879999999999995\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 35.0\n",
      "    pol2: -0.09999999999999387\n",
      "  policy_reward_mean:\n",
      "    pol1: 9.875\n",
      "    pol2: -7.909999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -21.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4260672680941586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2326537931647341\n",
      "    mean_inference_ms: 1.92961133806193\n",
      "    mean_raw_obs_processing_ms: 1.7607222547347232\n",
      "  time_since_restore: 49.22392916679382\n",
      "  time_this_iter_s: 6.835832834243774\n",
      "  time_total_s: 49.22392916679382\n",
      "  timers:\n",
      "    learn_throughput: 679.315\n",
      "    learn_time_ms: 5888.282\n",
      "    sample_throughput: 4439.409\n",
      "    sample_time_ms: 901.021\n",
      "    update_time_ms: 4.969\n",
      "  timestamp: 1620237703\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.399999999999913\n",
      "  episode_reward_mean: 3.0300000000000056\n",
      "  episode_reward_min: -21.00000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 320\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.151078701019287\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01848771795630455\n",
      "          model: {}\n",
      "          policy_loss: -0.06089356914162636\n",
      "          total_loss: 33.11467361450195\n",
      "          vf_explained_var: 0.35291939973831177\n",
      "          vf_loss: 33.15684509277344\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1858201026916504\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01376313902437687\n",
      "          model: {}\n",
      "          policy_loss: -0.05574704706668854\n",
      "          total_loss: 2.791090488433838\n",
      "          vf_explained_var: 0.36611849069595337\n",
      "          vf_loss: 2.825934648513794\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.029999999999994\n",
      "    ram_util_percent: 58.89\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 35.0\n",
      "    pol2: 1.0000000000000093\n",
      "  policy_reward_mean:\n",
      "    pol1: 11.38\n",
      "    pol2: -8.349999999999984\n",
      "  policy_reward_min:\n",
      "    pol1: -11.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4187983000888641\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22910803745206537\n",
      "    mean_inference_ms: 1.9056284530801566\n",
      "    mean_raw_obs_processing_ms: 1.7088009744375419\n",
      "  time_since_restore: 56.20597052574158\n",
      "  time_this_iter_s: 7.036119222640991\n",
      "  time_total_s: 56.20597052574158\n",
      "  timers:\n",
      "    learn_throughput: 680.264\n",
      "    learn_time_ms: 5880.069\n",
      "    sample_throughput: 4475.281\n",
      "    sample_time_ms: 893.799\n",
      "    update_time_ms: 5.304\n",
      "  timestamp: 1620237710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: cb9b0_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         49.2289</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   2.307</td><td style=\"text-align: right;\">                24.6</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         49.2239</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   1.965</td><td style=\"text-align: right;\">                26.1</td><td style=\"text-align: right;\">               -25.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         56.206 </td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">   3.03 </td><td style=\"text-align: right;\">                29.4</td><td style=\"text-align: right;\">               -21  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         49.3821</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   3.903</td><td style=\"text-align: right;\">                33  </td><td style=\"text-align: right;\">               -16.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.599999999999945\n",
      "  episode_reward_mean: 2.9520000000000106\n",
      "  episode_reward_min: -19.499999999999986\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 320\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1686789989471436\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01762952283024788\n",
      "          model: {}\n",
      "          policy_loss: -0.06268365681171417\n",
      "          total_loss: 22.67396354675293\n",
      "          vf_explained_var: 0.44134843349456787\n",
      "          vf_loss: 22.71879768371582\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1586965322494507\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018509989604353905\n",
      "          model: {}\n",
      "          policy_loss: -0.05347898602485657\n",
      "          total_loss: 3.01401948928833\n",
      "          vf_explained_var: 0.2798824906349182\n",
      "          vf_loss: 3.048757314682007\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.580000000000005\n",
      "    ram_util_percent: 58.89\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 30.0\n",
      "    pol2: 0.999999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: 10.345\n",
      "    pol2: -7.392999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -15.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4212856624040988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2339056628718755\n",
      "    mean_inference_ms: 1.879212082052431\n",
      "    mean_raw_obs_processing_ms: 1.697924389422821\n",
      "  time_since_restore: 56.1212215423584\n",
      "  time_this_iter_s: 6.892353057861328\n",
      "  time_total_s: 56.1212215423584\n",
      "  timers:\n",
      "    learn_throughput: 679.042\n",
      "    learn_time_ms: 5890.648\n",
      "    sample_throughput: 4539.376\n",
      "    sample_time_ms: 881.178\n",
      "    update_time_ms: 4.133\n",
      "  timestamp: 1620237710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.99999999999991\n",
      "  episode_reward_mean: 4.8540000000000045\n",
      "  episode_reward_min: -16.499999999999986\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 320\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.177809238433838\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017080511897802353\n",
      "          model: {}\n",
      "          policy_loss: -0.0531916469335556\n",
      "          total_loss: 25.280563354492188\n",
      "          vf_explained_var: 0.3505967855453491\n",
      "          vf_loss: 25.31645965576172\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1936216354370117\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017304398119449615\n",
      "          model: {}\n",
      "          policy_loss: -0.05375992879271507\n",
      "          total_loss: 2.8011152744293213\n",
      "          vf_explained_var: 0.4091070294380188\n",
      "          vf_loss: 2.8373546600341797\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.52\n",
      "    ram_util_percent: 58.89\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 43.0\n",
      "    pol2: 8.699999999999998\n",
      "  policy_reward_mean:\n",
      "    pol1: 12.225\n",
      "    pol2: -7.370999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -12.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4230106591302769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2325260995836389\n",
      "    mean_inference_ms: 1.8720717136694913\n",
      "    mean_raw_obs_processing_ms: 1.7318751789611384\n",
      "  time_since_restore: 56.34079313278198\n",
      "  time_this_iter_s: 6.958703279495239\n",
      "  time_total_s: 56.34079313278198\n",
      "  timers:\n",
      "    learn_throughput: 678.608\n",
      "    learn_time_ms: 5894.416\n",
      "    sample_throughput: 4484.014\n",
      "    sample_time_ms: 892.058\n",
      "    update_time_ms: 4.264\n",
      "  timestamp: 1620237710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.39999999999992\n",
      "  episode_reward_mean: 2.8380000000000076\n",
      "  episode_reward_min: -25.500000000000018\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 320\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.171645998954773\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017435666173696518\n",
      "          model: {}\n",
      "          policy_loss: -0.06015368551015854\n",
      "          total_loss: 40.713775634765625\n",
      "          vf_explained_var: 0.28942060470581055\n",
      "          vf_loss: 40.75627517700195\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.184486746788025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018798084929585457\n",
      "          model: {}\n",
      "          policy_loss: -0.05617734417319298\n",
      "          total_loss: 2.7708730697631836\n",
      "          vf_explained_var: 0.31177932024002075\n",
      "          vf_loss: 2.8080174922943115\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.529999999999994\n",
      "    ram_util_percent: 58.89\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 29.5\n",
      "    pol2: -0.09999999999999387\n",
      "  policy_reward_mean:\n",
      "    pol1: 10.605\n",
      "    pol2: -7.766999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -21.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4269579961523563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2327254068224181\n",
      "    mean_inference_ms: 1.9178076368878243\n",
      "    mean_raw_obs_processing_ms: 1.7712232636492475\n",
      "  time_since_restore: 56.12682223320007\n",
      "  time_this_iter_s: 6.90289306640625\n",
      "  time_total_s: 56.12682223320007\n",
      "  timers:\n",
      "    learn_throughput: 681.64\n",
      "    learn_time_ms: 5868.196\n",
      "    sample_throughput: 4399.911\n",
      "    sample_time_ms: 909.109\n",
      "    update_time_ms: 4.732\n",
      "  timestamp: 1620237710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.399999999999913\n",
      "  episode_reward_mean: 2.9520000000000066\n",
      "  episode_reward_min: -16.79999999999999\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 360\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.135878324508667\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01724349707365036\n",
      "          model: {}\n",
      "          policy_loss: -0.06260545551776886\n",
      "          total_loss: 27.929706573486328\n",
      "          vf_explained_var: 0.3191704750061035\n",
      "          vf_loss: 27.974853515625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1881229877471924\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01216989103704691\n",
      "          model: {}\n",
      "          policy_loss: -0.04855720326304436\n",
      "          total_loss: 3.849277973175049\n",
      "          vf_explained_var: 0.3101250231266022\n",
      "          vf_loss: 3.87935209274292\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.78\n",
      "    ram_util_percent: 59.17\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 35.0\n",
      "    pol2: 6.5000000000000036\n",
      "  policy_reward_mean:\n",
      "    pol1: 11.06\n",
      "    pol2: -8.107999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -9.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42170465952953196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22995345005287962\n",
      "    mean_inference_ms: 1.9000635469366165\n",
      "    mean_raw_obs_processing_ms: 1.7204891532090507\n",
      "  time_since_restore: 62.88257360458374\n",
      "  time_this_iter_s: 6.676603078842163\n",
      "  time_total_s: 62.88257360458374\n",
      "  timers:\n",
      "    learn_throughput: 683.724\n",
      "    learn_time_ms: 5850.311\n",
      "    sample_throughput: 4510.454\n",
      "    sample_time_ms: 886.829\n",
      "    update_time_ms: 5.234\n",
      "  timestamp: 1620237717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: cb9b0_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         56.1212</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">   2.952</td><td style=\"text-align: right;\">                24.6</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         56.1268</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">   2.838</td><td style=\"text-align: right;\">                20.4</td><td style=\"text-align: right;\">               -25.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         62.8826</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   2.952</td><td style=\"text-align: right;\">                29.4</td><td style=\"text-align: right;\">               -16.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         56.3408</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">   4.854</td><td style=\"text-align: right;\">                33  </td><td style=\"text-align: right;\">               -16.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.99999999999991\n",
      "  episode_reward_mean: 5.612999999999999\n",
      "  episode_reward_min: -21.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 360\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1554176807403564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018537860363721848\n",
      "          model: {}\n",
      "          policy_loss: -0.06271504610776901\n",
      "          total_loss: 37.18227767944336\n",
      "          vf_explained_var: 0.30555063486099243\n",
      "          vf_loss: 37.22622299194336\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1625077724456787\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01690586283802986\n",
      "          model: {}\n",
      "          policy_loss: -0.05095505714416504\n",
      "          total_loss: 5.905592918395996\n",
      "          vf_explained_var: 0.36506223678588867\n",
      "          vf_loss: 5.939431190490723\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.78999999999999\n",
      "    ram_util_percent: 59.17\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 43.0\n",
      "    pol2: 15.29999999999999\n",
      "  policy_reward_mean:\n",
      "    pol1: 12.555\n",
      "    pol2: -6.941999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -11.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4218223394144718\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2313406496253569\n",
      "    mean_inference_ms: 1.8593791737536114\n",
      "    mean_raw_obs_processing_ms: 1.7267826046869663\n",
      "  time_since_restore: 62.99828600883484\n",
      "  time_this_iter_s: 6.6574928760528564\n",
      "  time_total_s: 62.99828600883484\n",
      "  timers:\n",
      "    learn_throughput: 681.642\n",
      "    learn_time_ms: 5868.179\n",
      "    sample_throughput: 4551.116\n",
      "    sample_time_ms: 878.905\n",
      "    update_time_ms: 4.251\n",
      "  timestamp: 1620237717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.599999999999945\n",
      "  episode_reward_mean: 3.0780000000000083\n",
      "  episode_reward_min: -19.49999999999999\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 360\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1495641469955444\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016383958980441093\n",
      "          model: {}\n",
      "          policy_loss: -0.0523441843688488\n",
      "          total_loss: 43.097015380859375\n",
      "          vf_explained_var: 0.24924823641777039\n",
      "          vf_loss: 43.132774353027344\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1285154819488525\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020084619522094727\n",
      "          model: {}\n",
      "          policy_loss: -0.06293228268623352\n",
      "          total_loss: 4.27175235748291\n",
      "          vf_explained_var: 0.2649080455303192\n",
      "          vf_loss: 4.3143486976623535\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.91\n",
      "    ram_util_percent: 59.17\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 30.0\n",
      "    pol2: 6.500000000000011\n",
      "  policy_reward_mean:\n",
      "    pol1: 9.635\n",
      "    pol2: -6.556999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -26.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4212992710570711\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2339277038301865\n",
      "    mean_inference_ms: 1.872009098186653\n",
      "    mean_raw_obs_processing_ms: 1.699771973933427\n",
      "  time_since_restore: 62.8391375541687\n",
      "  time_this_iter_s: 6.717916011810303\n",
      "  time_total_s: 62.8391375541687\n",
      "  timers:\n",
      "    learn_throughput: 682.27\n",
      "    learn_time_ms: 5862.781\n",
      "    sample_throughput: 4559.026\n",
      "    sample_time_ms: 877.38\n",
      "    update_time_ms: 4.102\n",
      "  timestamp: 1620237717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-01-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 31.199999999999896\n",
      "  episode_reward_mean: 3.849000000000006\n",
      "  episode_reward_min: -18.899999999999988\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 360\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1448668241500854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018137682229280472\n",
      "          model: {}\n",
      "          policy_loss: -0.05665414035320282\n",
      "          total_loss: 29.135459899902344\n",
      "          vf_explained_var: 0.3062135577201843\n",
      "          vf_loss: 29.173748016357422\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.159262776374817\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019182544201612473\n",
      "          model: {}\n",
      "          policy_loss: -0.04863505810499191\n",
      "          total_loss: 2.428757667541504\n",
      "          vf_explained_var: 0.31319403648376465\n",
      "          vf_loss: 2.457970142364502\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.78999999999999\n",
      "    ram_util_percent: 59.17\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 39.0\n",
      "    pol2: -0.10000000000000464\n",
      "  policy_reward_mean:\n",
      "    pol1: 11.385\n",
      "    pol2: -7.535999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -10.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4248376244868605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.231552320991226\n",
      "    mean_inference_ms: 1.899944478460223\n",
      "    mean_raw_obs_processing_ms: 1.7714546803035196\n",
      "  time_since_restore: 62.78006935119629\n",
      "  time_this_iter_s: 6.653247117996216\n",
      "  time_total_s: 62.78006935119629\n",
      "  timers:\n",
      "    learn_throughput: 684.11\n",
      "    learn_time_ms: 5847.011\n",
      "    sample_throughput: 4485.468\n",
      "    sample_time_ms: 891.769\n",
      "    update_time_ms: 4.687\n",
      "  timestamp: 1620237717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.399999999999906\n",
      "  episode_reward_mean: 7.232999999999996\n",
      "  episode_reward_min: -21.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 400\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1321977376937866\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017432020977139473\n",
      "          model: {}\n",
      "          policy_loss: -0.05296771600842476\n",
      "          total_loss: 26.056730270385742\n",
      "          vf_explained_var: 0.45250648260116577\n",
      "          vf_loss: 26.09204864501953\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1551477909088135\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016796447336673737\n",
      "          model: {}\n",
      "          policy_loss: -0.05164627730846405\n",
      "          total_loss: 3.0772039890289307\n",
      "          vf_explained_var: 0.3453267216682434\n",
      "          vf_loss: 3.111844062805176\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.8\n",
      "    ram_util_percent: 59.709999999999994\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 34.0\n",
      "    pol2: 15.29999999999999\n",
      "  policy_reward_mean:\n",
      "    pol1: 14.45\n",
      "    pol2: -7.2169999999999845\n",
      "  policy_reward_min:\n",
      "    pol1: -11.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41943271161785034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22997101911195933\n",
      "    mean_inference_ms: 1.847825355900447\n",
      "    mean_raw_obs_processing_ms: 1.7152127514296387\n",
      "  time_since_restore: 69.85230088233948\n",
      "  time_this_iter_s: 6.854014873504639\n",
      "  time_total_s: 69.85230088233948\n",
      "  timers:\n",
      "    learn_throughput: 682.742\n",
      "    learn_time_ms: 5858.728\n",
      "    sample_throughput: 4597.133\n",
      "    sample_time_ms: 870.108\n",
      "    update_time_ms: 4.364\n",
      "  timestamp: 1620237724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: cb9b0_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         62.8391</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   3.078</td><td style=\"text-align: right;\">                24.6</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         62.7801</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   3.849</td><td style=\"text-align: right;\">                31.2</td><td style=\"text-align: right;\">               -18.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         62.8826</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   2.952</td><td style=\"text-align: right;\">                29.4</td><td style=\"text-align: right;\">               -16.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         69.8523</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">   7.233</td><td style=\"text-align: right;\">                26.4</td><td style=\"text-align: right;\">               -21  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.599999999999973\n",
      "  episode_reward_mean: 4.1400000000000095\n",
      "  episode_reward_min: -15.899999999999993\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 400\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1392654180526733\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01658860594034195\n",
      "          model: {}\n",
      "          policy_loss: -0.04870859533548355\n",
      "          total_loss: 25.632568359375\n",
      "          vf_explained_var: 0.38717877864837646\n",
      "          vf_loss: 25.664478302001953\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1899843215942383\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012238621711730957\n",
      "          model: {}\n",
      "          policy_loss: -0.05422907695174217\n",
      "          total_loss: 2.3734898567199707\n",
      "          vf_explained_var: 0.34881991147994995\n",
      "          vf_loss: 2.4091315269470215\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.79\n",
      "    ram_util_percent: 59.71999999999999\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 33.5\n",
      "    pol2: 6.5000000000000036\n",
      "  policy_reward_mean:\n",
      "    pol1: 11.665\n",
      "    pol2: -7.524999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -7.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4239941601445014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23098114096930972\n",
      "    mean_inference_ms: 1.8992834741570033\n",
      "    mean_raw_obs_processing_ms: 1.7321299694985124\n",
      "  time_since_restore: 69.84957790374756\n",
      "  time_this_iter_s: 6.967004299163818\n",
      "  time_total_s: 69.84957790374756\n",
      "  timers:\n",
      "    learn_throughput: 684.284\n",
      "    learn_time_ms: 5845.529\n",
      "    sample_throughput: 4486.575\n",
      "    sample_time_ms: 891.549\n",
      "    update_time_ms: 5.03\n",
      "  timestamp: 1620237724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: cb9b0_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.29999999999994\n",
      "  episode_reward_mean: 3.4560000000000097\n",
      "  episode_reward_min: -19.49999999999999\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 400\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.13921320438385\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018796168267726898\n",
      "          model: {}\n",
      "          policy_loss: -0.056295547634363174\n",
      "          total_loss: 24.514850616455078\n",
      "          vf_explained_var: 0.33769655227661133\n",
      "          vf_loss: 24.55211639404297\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1075228452682495\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01270555704832077\n",
      "          model: {}\n",
      "          policy_loss: -0.05122939497232437\n",
      "          total_loss: 3.601313591003418\n",
      "          vf_explained_var: 0.2514982223510742\n",
      "          vf_loss: 3.633246421813965\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.67999999999999\n",
      "    ram_util_percent: 59.69\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 29.5\n",
      "    pol2: 6.500000000000011\n",
      "  policy_reward_mean:\n",
      "    pol1: 10.42\n",
      "    pol2: -6.963999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -26.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42031031218198367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23324584588873068\n",
      "    mean_inference_ms: 1.8652690292436387\n",
      "    mean_raw_obs_processing_ms: 1.6967867138038357\n",
      "  time_since_restore: 69.68608331680298\n",
      "  time_this_iter_s: 6.846945762634277\n",
      "  time_total_s: 69.68608331680298\n",
      "  timers:\n",
      "    learn_throughput: 682.934\n",
      "    learn_time_ms: 5857.082\n",
      "    sample_throughput: 4604.041\n",
      "    sample_time_ms: 868.802\n",
      "    update_time_ms: 4.166\n",
      "  timestamp: 1620237724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 31.199999999999896\n",
      "  episode_reward_mean: 5.309999999999998\n",
      "  episode_reward_min: -18.899999999999988\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 400\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1232328414916992\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018894635140895844\n",
      "          model: {}\n",
      "          policy_loss: -0.06307847797870636\n",
      "          total_loss: 34.062095642089844\n",
      "          vf_explained_var: 0.35455289483070374\n",
      "          vf_loss: 34.106040954589844\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1452314853668213\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017536159604787827\n",
      "          model: {}\n",
      "          policy_loss: -0.05508923530578613\n",
      "          total_loss: 2.4347786903381348\n",
      "          vf_explained_var: 0.31006357073783875\n",
      "          vf_loss: 2.4721126556396484\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.8\n",
      "    ram_util_percent: 59.709999999999994\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 39.0\n",
      "    pol2: -0.09999999999999076\n",
      "  policy_reward_mean:\n",
      "    pol1: 12.78\n",
      "    pol2: -7.4699999999999855\n",
      "  policy_reward_min:\n",
      "    pol1: -10.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4207579404646741\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2296383805547704\n",
      "    mean_inference_ms: 1.8789042487876848\n",
      "    mean_raw_obs_processing_ms: 1.7554015024962455\n",
      "  time_since_restore: 69.58409214019775\n",
      "  time_this_iter_s: 6.804022789001465\n",
      "  time_total_s: 69.58409214019775\n",
      "  timers:\n",
      "    learn_throughput: 684.749\n",
      "    learn_time_ms: 5841.553\n",
      "    sample_throughput: 4545.464\n",
      "    sample_time_ms: 879.998\n",
      "    update_time_ms: 4.568\n",
      "  timestamp: 1620237724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999987\n",
      "  episode_reward_mean: 3.6420000000000106\n",
      "  episode_reward_min: -19.499999999999986\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 440\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1116383075714111\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015521354041993618\n",
      "          model: {}\n",
      "          policy_loss: -0.05230912193655968\n",
      "          total_loss: 29.350875854492188\n",
      "          vf_explained_var: 0.378787100315094\n",
      "          vf_loss: 29.387466430664062\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1761901378631592\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011387184262275696\n",
      "          model: {}\n",
      "          policy_loss: -0.0446225106716156\n",
      "          total_loss: 3.9116714000701904\n",
      "          vf_explained_var: 0.27378109097480774\n",
      "          vf_loss: 3.938999652862549\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.725\n",
      "    ram_util_percent: 61.841666666666676\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 32.0\n",
      "    pol2: 6.500000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: 11.145\n",
      "    pol2: -7.502999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -9.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4264351670751688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2320809592958129\n",
      "    mean_inference_ms: 1.9050472495524804\n",
      "    mean_raw_obs_processing_ms: 1.7504677646359168\n",
      "  time_since_restore: 78.2186439037323\n",
      "  time_this_iter_s: 8.369065999984741\n",
      "  time_total_s: 78.2186439037323\n",
      "  timers:\n",
      "    learn_throughput: 674.887\n",
      "    learn_time_ms: 5926.915\n",
      "    sample_throughput: 4466.752\n",
      "    sample_time_ms: 895.505\n",
      "    update_time_ms: 5.103\n",
      "  timestamp: 1620237733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: cb9b0_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         69.6861</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">   3.456</td><td style=\"text-align: right;\">                21.3</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         69.5841</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">   5.31 </td><td style=\"text-align: right;\">                31.2</td><td style=\"text-align: right;\">               -18.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         78.2186</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   3.642</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         69.8523</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">   7.233</td><td style=\"text-align: right;\">                26.4</td><td style=\"text-align: right;\">               -21  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.999999999999908\n",
      "  episode_reward_mean: 8.669999999999993\n",
      "  episode_reward_min: -20.999999999999993\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 440\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1069669723510742\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016530446708202362\n",
      "          model: {}\n",
      "          policy_loss: -0.05600178614258766\n",
      "          total_loss: 35.22227478027344\n",
      "          vf_explained_var: 0.30403196811676025\n",
      "          vf_loss: 35.26153564453125\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1369376182556152\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01593851111829281\n",
      "          model: {}\n",
      "          policy_loss: -0.04689352959394455\n",
      "          total_loss: 3.086893081665039\n",
      "          vf_explained_var: 0.27790212631225586\n",
      "          vf_loss: 3.1176486015319824\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.60833333333333\n",
      "    ram_util_percent: 61.841666666666676\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 37.0\n",
      "    pol2: 15.29999999999999\n",
      "  policy_reward_mean:\n",
      "    pol1: 16.195\n",
      "    pol2: -7.524999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -11.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4201408261474016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23040342219153792\n",
      "    mean_inference_ms: 1.8496152474446785\n",
      "    mean_raw_obs_processing_ms: 1.7210959258061098\n",
      "  time_since_restore: 78.29930782318115\n",
      "  time_this_iter_s: 8.447006940841675\n",
      "  time_total_s: 78.29930782318115\n",
      "  timers:\n",
      "    learn_throughput: 672.017\n",
      "    learn_time_ms: 5952.23\n",
      "    sample_throughput: 4549.221\n",
      "    sample_time_ms: 879.271\n",
      "    update_time_ms: 4.477\n",
      "  timestamp: 1620237733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 27.299999999999905\n",
      "  episode_reward_mean: 6.995999999999993\n",
      "  episode_reward_min: -18.899999999999988\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 440\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0973238945007324\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017179135233163834\n",
      "          model: {}\n",
      "          policy_loss: -0.05835132300853729\n",
      "          total_loss: 32.185279846191406\n",
      "          vf_explained_var: 0.36814504861831665\n",
      "          vf_loss: 32.226234436035156\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1275187730789185\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016842734068632126\n",
      "          model: {}\n",
      "          policy_loss: -0.056535616517066956\n",
      "          total_loss: 3.156289577484131\n",
      "          vf_explained_var: 0.28127121925354004\n",
      "          vf_loss: 3.195772171020508\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.93333333333332\n",
      "    ram_util_percent: 61.841666666666676\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 35.0\n",
      "    pol2: 3.2000000000000095\n",
      "  policy_reward_mean:\n",
      "    pol1: 14.345\n",
      "    pol2: -7.348999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -10.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4223333391411782\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.230589068240597\n",
      "    mean_inference_ms: 1.8853117705214533\n",
      "    mean_raw_obs_processing_ms: 1.7617858783140918\n",
      "  time_since_restore: 77.982093334198\n",
      "  time_this_iter_s: 8.398001194000244\n",
      "  time_total_s: 77.982093334198\n",
      "  timers:\n",
      "    learn_throughput: 676.466\n",
      "    learn_time_ms: 5913.085\n",
      "    sample_throughput: 4460.344\n",
      "    sample_time_ms: 896.792\n",
      "    update_time_ms: 4.616\n",
      "  timestamp: 1620237733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.79999999999999\n",
      "  episode_reward_mean: 3.4230000000000076\n",
      "  episode_reward_min: -20.39999999999999\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 440\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1100760698318481\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01701219379901886\n",
      "          model: {}\n",
      "          policy_loss: -0.05096125975251198\n",
      "          total_loss: 28.769763946533203\n",
      "          vf_explained_var: 0.3973524570465088\n",
      "          vf_loss: 28.80350112915039\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0906238555908203\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012411966919898987\n",
      "          model: {}\n",
      "          policy_loss: -0.04750128090381622\n",
      "          total_loss: 2.420497179031372\n",
      "          vf_explained_var: 0.3157970905303955\n",
      "          vf_loss: 2.4491477012634277\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.21666666666667\n",
      "    ram_util_percent: 61.83333333333334\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 29.5\n",
      "    pol2: 6.500000000000011\n",
      "  policy_reward_mean:\n",
      "    pol1: 10.475\n",
      "    pol2: -7.051999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -26.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4218807332780078\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23392240225876662\n",
      "    mean_inference_ms: 1.8727458975367346\n",
      "    mean_raw_obs_processing_ms: 1.7109275679768994\n",
      "  time_since_restore: 78.08190417289734\n",
      "  time_this_iter_s: 8.39582085609436\n",
      "  time_total_s: 78.08190417289734\n",
      "  timers:\n",
      "    learn_throughput: 673.882\n",
      "    learn_time_ms: 5935.757\n",
      "    sample_throughput: 4545.803\n",
      "    sample_time_ms: 879.933\n",
      "    update_time_ms: 4.22\n",
      "  timestamp: 1620237733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999987\n",
      "  episode_reward_mean: 4.9770000000000065\n",
      "  episode_reward_min: -19.499999999999986\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 480\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0936439037322998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016343336552381516\n",
      "          model: {}\n",
      "          policy_loss: -0.04251978546380997\n",
      "          total_loss: 27.03094482421875\n",
      "          vf_explained_var: 0.37268444895744324\n",
      "          vf_loss: 27.056917190551758\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1557538509368896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013157574459910393\n",
      "          model: {}\n",
      "          policy_loss: -0.05138856917619705\n",
      "          total_loss: 2.300049304962158\n",
      "          vf_explained_var: 0.3675481677055359\n",
      "          vf_loss: 2.3314549922943115\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.89999999999999\n",
      "    ram_util_percent: 61.136363636363626\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 32.5\n",
      "    pol2: 6.500000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: 12.645\n",
      "    pol2: -7.667999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -9.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.43062557077903507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23435901421981506\n",
      "    mean_inference_ms: 1.9209604047977535\n",
      "    mean_raw_obs_processing_ms: 1.7761574470180523\n",
      "  time_since_restore: 85.86983489990234\n",
      "  time_this_iter_s: 7.651190996170044\n",
      "  time_total_s: 85.86983489990234\n",
      "  timers:\n",
      "    learn_throughput: 673.914\n",
      "    learn_time_ms: 5935.478\n",
      "    sample_throughput: 4447.068\n",
      "    sample_time_ms: 899.469\n",
      "    update_time_ms: 5.707\n",
      "  timestamp: 1620237740\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: cb9b0_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         78.0819</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   3.423</td><td style=\"text-align: right;\">                22.8</td><td style=\"text-align: right;\">               -20.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         77.9821</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   6.996</td><td style=\"text-align: right;\">                27.3</td><td style=\"text-align: right;\">               -18.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         85.8698</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">   4.977</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         78.2993</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   8.67 </td><td style=\"text-align: right;\">                27  </td><td style=\"text-align: right;\">               -21  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.49999999999992\n",
      "  episode_reward_mean: 9.494999999999989\n",
      "  episode_reward_min: -8.999999999999975\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 480\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0975826978683472\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018019065260887146\n",
      "          model: {}\n",
      "          policy_loss: -0.05512842535972595\n",
      "          total_loss: 40.95720672607422\n",
      "          vf_explained_var: 0.33136841654777527\n",
      "          vf_loss: 40.99409484863281\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1251275539398193\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016476036980748177\n",
      "          model: {}\n",
      "          policy_loss: -0.06494230031967163\n",
      "          total_loss: 2.5480175018310547\n",
      "          vf_explained_var: 0.38965803384780884\n",
      "          vf_loss: 2.5962777137756348\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.88181818181819\n",
      "    ram_util_percent: 61.136363636363626\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 44.5\n",
      "    pol2: 3.199999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: 17.185\n",
      "    pol2: -7.689999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -4.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42400611881677674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23227589113733693\n",
      "    mean_inference_ms: 1.8643106806099883\n",
      "    mean_raw_obs_processing_ms: 1.7392333697245113\n",
      "  time_since_restore: 85.96143889427185\n",
      "  time_this_iter_s: 7.662131071090698\n",
      "  time_total_s: 85.96143889427185\n",
      "  timers:\n",
      "    learn_throughput: 668.674\n",
      "    learn_time_ms: 5981.985\n",
      "    sample_throughput: 4521.236\n",
      "    sample_time_ms: 884.714\n",
      "    update_time_ms: 4.513\n",
      "  timestamp: 1620237740\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.099999999999916\n",
      "  episode_reward_mean: 8.717999999999991\n",
      "  episode_reward_min: -14.699999999999992\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 480\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0703026056289673\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016983482986688614\n",
      "          model: {}\n",
      "          policy_loss: -0.05726175755262375\n",
      "          total_loss: 39.85174560546875\n",
      "          vf_explained_var: 0.3309887647628784\n",
      "          vf_loss: 39.891807556152344\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1017524003982544\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01725500449538231\n",
      "          model: {}\n",
      "          policy_loss: -0.05528506264090538\n",
      "          total_loss: 2.788914203643799\n",
      "          vf_explained_var: 0.25499171018600464\n",
      "          vf_loss: 2.8267288208007812\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.88181818181819\n",
      "    ram_util_percent: 61.136363636363626\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 41.0\n",
      "    pol2: 3.2000000000000095\n",
      "  policy_reward_mean:\n",
      "    pol1: 15.825\n",
      "    pol2: -7.106999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -8.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4309420901181567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2350636130543048\n",
      "    mean_inference_ms: 1.9181168359409115\n",
      "    mean_raw_obs_processing_ms: 1.797841978792316\n",
      "  time_since_restore: 85.76758933067322\n",
      "  time_this_iter_s: 7.78549599647522\n",
      "  time_total_s: 85.76758933067322\n",
      "  timers:\n",
      "    learn_throughput: 673.886\n",
      "    learn_time_ms: 5935.718\n",
      "    sample_throughput: 4302.936\n",
      "    sample_time_ms: 929.598\n",
      "    update_time_ms: 4.454\n",
      "  timestamp: 1620237741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.79999999999999\n",
      "  episode_reward_mean: 4.665000000000004\n",
      "  episode_reward_min: -20.39999999999999\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 480\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1168181896209717\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01735174097120762\n",
      "          model: {}\n",
      "          policy_loss: -0.054575350135564804\n",
      "          total_loss: 29.933589935302734\n",
      "          vf_explained_var: 0.3436850309371948\n",
      "          vf_loss: 29.970596313476562\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0755548477172852\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011775186285376549\n",
      "          model: {}\n",
      "          policy_loss: -0.05091240257024765\n",
      "          total_loss: 3.3231797218322754\n",
      "          vf_explained_var: 0.31147319078445435\n",
      "          vf_loss: 3.356208324432373\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.07272727272728\n",
      "    ram_util_percent: 61.22727272727273\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 29.5\n",
      "    pol2: 5.4000000000000075\n",
      "  policy_reward_mean:\n",
      "    pol1: 11.805\n",
      "    pol2: -7.1399999999999855\n",
      "  policy_reward_min:\n",
      "    pol1: -13.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4277699967855037\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23647339297522277\n",
      "    mean_inference_ms: 1.8984192615406084\n",
      "    mean_raw_obs_processing_ms: 1.7445643245754707\n",
      "  time_since_restore: 85.83228611946106\n",
      "  time_this_iter_s: 7.750381946563721\n",
      "  time_total_s: 85.83228611946106\n",
      "  timers:\n",
      "    learn_throughput: 670.91\n",
      "    learn_time_ms: 5962.054\n",
      "    sample_throughput: 4451.524\n",
      "    sample_time_ms: 898.569\n",
      "    update_time_ms: 4.433\n",
      "  timestamp: 1620237741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.49999999999992\n",
      "  episode_reward_mean: 10.232999999999986\n",
      "  episode_reward_min: -8.999999999999975\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 520\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0690321922302246\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016758400946855545\n",
      "          model: {}\n",
      "          policy_loss: -0.049424175173044205\n",
      "          total_loss: 28.71925163269043\n",
      "          vf_explained_var: 0.3975854218006134\n",
      "          vf_loss: 28.751705169677734\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1252193450927734\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017055673524737358\n",
      "          model: {}\n",
      "          policy_loss: -0.05017583817243576\n",
      "          total_loss: 3.075622081756592\n",
      "          vf_explained_var: 0.25384092330932617\n",
      "          vf_loss: 3.1085290908813477\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.43999999999998\n",
      "    ram_util_percent: 61.25\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 44.5\n",
      "    pol2: -0.09999999999998549\n",
      "  policy_reward_mean:\n",
      "    pol1: 17.285\n",
      "    pol2: -7.051999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -4.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4275708405516648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2341446856359608\n",
      "    mean_inference_ms: 1.8769239110597444\n",
      "    mean_raw_obs_processing_ms: 1.7545666933741972\n",
      "  time_since_restore: 93.28485870361328\n",
      "  time_this_iter_s: 7.323419809341431\n",
      "  time_total_s: 93.28485870361328\n",
      "  timers:\n",
      "    learn_throughput: 662.774\n",
      "    learn_time_ms: 6035.24\n",
      "    sample_throughput: 4574.949\n",
      "    sample_time_ms: 874.327\n",
      "    update_time_ms: 4.766\n",
      "  timestamp: 1620237748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: cb9b0_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         85.8323</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">   4.665</td><td style=\"text-align: right;\">                22.8</td><td style=\"text-align: right;\">               -20.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         85.7676</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">   8.718</td><td style=\"text-align: right;\">                32.1</td><td style=\"text-align: right;\">               -14.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         85.8698</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">   4.977</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         93.2849</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  10.233</td><td style=\"text-align: right;\">                34.5</td><td style=\"text-align: right;\">                -9  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.599999999999923\n",
      "  episode_reward_mean: 6.033000000000002\n",
      "  episode_reward_min: -11.999999999999979\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 520\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0590343475341797\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015824107453227043\n",
      "          model: {}\n",
      "          policy_loss: -0.05223042517900467\n",
      "          total_loss: 32.82256317138672\n",
      "          vf_explained_var: 0.35390713810920715\n",
      "          vf_loss: 32.858768463134766\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.132981538772583\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010881001129746437\n",
      "          model: {}\n",
      "          policy_loss: -0.04186583310365677\n",
      "          total_loss: 2.6024060249328613\n",
      "          vf_explained_var: 0.2970053553581238\n",
      "          vf_loss: 2.627746105194092\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.46\n",
      "    ram_util_percent: 61.25\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 33.5\n",
      "    pol2: 0.9999999999999941\n",
      "  policy_reward_mean:\n",
      "    pol1: 13.745\n",
      "    pol2: -7.711999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -2.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.43499817920291456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2369591588339156\n",
      "    mean_inference_ms: 1.9368613813533317\n",
      "    mean_raw_obs_processing_ms: 1.7992432405744336\n",
      "  time_since_restore: 93.28448581695557\n",
      "  time_this_iter_s: 7.414650917053223\n",
      "  time_total_s: 93.28448581695557\n",
      "  timers:\n",
      "    learn_throughput: 667.29\n",
      "    learn_time_ms: 5994.398\n",
      "    sample_throughput: 4412.908\n",
      "    sample_time_ms: 906.432\n",
      "    update_time_ms: 5.829\n",
      "  timestamp: 1620237748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: cb9b0_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.599999999999905\n",
      "  episode_reward_mean: 5.381999999999997\n",
      "  episode_reward_min: -20.39999999999999\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 520\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0818085670471191\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017880883067846298\n",
      "          model: {}\n",
      "          policy_loss: -0.06309329718351364\n",
      "          total_loss: 33.14733123779297\n",
      "          vf_explained_var: 0.3147008419036865\n",
      "          vf_loss: 33.19232177734375\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0546677112579346\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011126933619379997\n",
      "          model: {}\n",
      "          policy_loss: -0.044255323708057404\n",
      "          total_loss: 2.3410348892211914\n",
      "          vf_explained_var: 0.36041340231895447\n",
      "          vf_loss: 2.368391275405884\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.40909090909092\n",
      "    ram_util_percent: 61.27272727272728\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 34.0\n",
      "    pol2: 5.4000000000000075\n",
      "  policy_reward_mean:\n",
      "    pol1: 12.61\n",
      "    pol2: -7.227999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -13.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4326552551667123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23858428886183802\n",
      "    mean_inference_ms: 1.917281950017495\n",
      "    mean_raw_obs_processing_ms: 1.7703971028038257\n",
      "  time_since_restore: 93.0854012966156\n",
      "  time_this_iter_s: 7.253115177154541\n",
      "  time_total_s: 93.0854012966156\n",
      "  timers:\n",
      "    learn_throughput: 664.696\n",
      "    learn_time_ms: 6017.787\n",
      "    sample_throughput: 4499.165\n",
      "    sample_time_ms: 889.054\n",
      "    update_time_ms: 4.501\n",
      "  timestamp: 1620237748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.39999999999993\n",
      "  episode_reward_mean: 9.23399999999999\n",
      "  episode_reward_min: -14.999999999999977\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 520\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0515227317810059\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016839005053043365\n",
      "          model: {}\n",
      "          policy_loss: -0.05464763939380646\n",
      "          total_loss: 38.29395294189453\n",
      "          vf_explained_var: 0.33461713790893555\n",
      "          vf_loss: 38.33155059814453\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0985209941864014\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0168839730322361\n",
      "          model: {}\n",
      "          policy_loss: -0.05190229415893555\n",
      "          total_loss: 2.3138866424560547\n",
      "          vf_explained_var: 0.3508748412132263\n",
      "          vf_loss: 2.34869384765625\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.24545454545454\n",
      "    ram_util_percent: 61.27272727272728\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 41.0\n",
      "    pol2: -1.1999999999999973\n",
      "  policy_reward_mean:\n",
      "    pol1: 16.33\n",
      "    pol2: -7.095999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -8.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.439041126880808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23917996365884334\n",
      "    mean_inference_ms: 1.9448246359361008\n",
      "    mean_raw_obs_processing_ms: 1.829747984139384\n",
      "  time_since_restore: 93.11100125312805\n",
      "  time_this_iter_s: 7.343411922454834\n",
      "  time_total_s: 93.11100125312805\n",
      "  timers:\n",
      "    learn_throughput: 667.911\n",
      "    learn_time_ms: 5988.825\n",
      "    sample_throughput: 4340.735\n",
      "    sample_time_ms: 921.503\n",
      "    update_time_ms: 4.472\n",
      "  timestamp: 1620237748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.49999999999991\n",
      "  episode_reward_mean: 11.207999999999977\n",
      "  episode_reward_min: -8.999999999999975\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 560\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0321991443634033\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017519280314445496\n",
      "          model: {}\n",
      "          policy_loss: -0.05170167237520218\n",
      "          total_loss: 33.108489990234375\n",
      "          vf_explained_var: 0.43578609824180603\n",
      "          vf_loss: 33.142452239990234\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0921452045440674\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015513299033045769\n",
      "          model: {}\n",
      "          policy_loss: -0.04442385584115982\n",
      "          total_loss: 3.930198907852173\n",
      "          vf_explained_var: 0.2883872985839844\n",
      "          vf_loss: 3.9589157104492188\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.86999999999999\n",
      "    ram_util_percent: 61.81\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 47.5\n",
      "    pol2: 6.499999999999998\n",
      "  policy_reward_mean:\n",
      "    pol1: 18.205\n",
      "    pol2: -6.996999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: 1.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42832704875684924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23443336613972818\n",
      "    mean_inference_ms: 1.8786610066135776\n",
      "    mean_raw_obs_processing_ms: 1.7559069062428438\n",
      "  time_since_restore: 100.23855638504028\n",
      "  time_this_iter_s: 6.953697681427002\n",
      "  time_total_s: 100.23855638504028\n",
      "  timers:\n",
      "    learn_throughput: 662.672\n",
      "    learn_time_ms: 6036.165\n",
      "    sample_throughput: 4506.618\n",
      "    sample_time_ms: 887.584\n",
      "    update_time_ms: 4.698\n",
      "  timestamp: 1620237755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: cb9b0_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         93.0854</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   5.382</td><td style=\"text-align: right;\">                30.6</td><td style=\"text-align: right;\">               -20.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         93.111 </td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   9.234</td><td style=\"text-align: right;\">                35.4</td><td style=\"text-align: right;\">               -15  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         93.2845</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   6.033</td><td style=\"text-align: right;\">                24.6</td><td style=\"text-align: right;\">               -12  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">        100.239 </td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  11.208</td><td style=\"text-align: right;\">                37.5</td><td style=\"text-align: right;\">                -9  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 31.499999999999922\n",
      "  episode_reward_mean: 7.997999999999994\n",
      "  episode_reward_min: -10.49999999999998\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 560\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0541672706604004\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016092058271169662\n",
      "          model: {}\n",
      "          policy_loss: -0.04986872151494026\n",
      "          total_loss: 34.061737060546875\n",
      "          vf_explained_var: 0.4095231890678406\n",
      "          vf_loss: 34.095314025878906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.127025842666626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012746978551149368\n",
      "          model: {}\n",
      "          policy_loss: -0.04975530505180359\n",
      "          total_loss: 3.2851009368896484\n",
      "          vf_explained_var: 0.28194165229797363\n",
      "          vf_loss: 3.3154969215393066\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.85\n",
      "    ram_util_percent: 61.81\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 37.5\n",
      "    pol2: 4.299999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: 15.16\n",
      "    pol2: -7.161999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -1.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4366892204390156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23822394593711352\n",
      "    mean_inference_ms: 1.9406249650607732\n",
      "    mean_raw_obs_processing_ms: 1.8064349885991744\n",
      "  time_since_restore: 100.22292995452881\n",
      "  time_this_iter_s: 6.938444137573242\n",
      "  time_total_s: 100.22292995452881\n",
      "  timers:\n",
      "    learn_throughput: 666.612\n",
      "    learn_time_ms: 6000.49\n",
      "    sample_throughput: 4368.637\n",
      "    sample_time_ms: 915.617\n",
      "    update_time_ms: 5.739\n",
      "  timestamp: 1620237755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: cb9b0_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.599999999999905\n",
      "  episode_reward_mean: 6.983999999999992\n",
      "  episode_reward_min: -16.49999999999998\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 560\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.065073847770691\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016577541828155518\n",
      "          model: {}\n",
      "          policy_loss: -0.04825787991285324\n",
      "          total_loss: 30.831226348876953\n",
      "          vf_explained_var: 0.4260927438735962\n",
      "          vf_loss: 30.86269760131836\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0327794551849365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0113886259496212\n",
      "          model: {}\n",
      "          policy_loss: -0.041294608265161514\n",
      "          total_loss: 2.6554341316223145\n",
      "          vf_explained_var: 0.35044026374816895\n",
      "          vf_loss: 2.6794321537017822\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.58000000000001\n",
      "    ram_util_percent: 61.870000000000005\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 38.5\n",
      "    pol2: 5.4000000000000075\n",
      "  policy_reward_mean:\n",
      "    pol1: 13.915\n",
      "    pol2: -6.930999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -17.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4329767452019482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2384361998951735\n",
      "    mean_inference_ms: 1.919615851583432\n",
      "    mean_raw_obs_processing_ms: 1.7731674520274\n",
      "  time_since_restore: 100.04648733139038\n",
      "  time_this_iter_s: 6.96108603477478\n",
      "  time_total_s: 100.04648733139038\n",
      "  timers:\n",
      "    learn_throughput: 664.433\n",
      "    learn_time_ms: 6020.167\n",
      "    sample_throughput: 4433.665\n",
      "    sample_time_ms: 902.188\n",
      "    update_time_ms: 4.499\n",
      "  timestamp: 1620237755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.39999999999993\n",
      "  episode_reward_mean: 10.979999999999983\n",
      "  episode_reward_min: -15.599999999999993\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 560\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0312230587005615\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01722315512597561\n",
      "          model: {}\n",
      "          policy_loss: -0.05039341375231743\n",
      "          total_loss: 36.44157028198242\n",
      "          vf_explained_var: 0.4163665473461151\n",
      "          vf_loss: 36.474525451660156\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.073534369468689\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016987822949886322\n",
      "          model: {}\n",
      "          policy_loss: -0.05441867560148239\n",
      "          total_loss: 3.7176027297973633\n",
      "          vf_explained_var: 0.2508799433708191\n",
      "          vf_loss: 3.7548210620880127\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.61\n",
      "    ram_util_percent: 61.870000000000005\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 41.0\n",
      "    pol2: 7.60000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: 18.12\n",
      "    pol2: -7.139999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -15.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.44143016034403587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2403227119664799\n",
      "    mean_inference_ms: 1.9507547525152658\n",
      "    mean_raw_obs_processing_ms: 1.8354608111385096\n",
      "  time_since_restore: 100.07066106796265\n",
      "  time_this_iter_s: 6.959659814834595\n",
      "  time_total_s: 100.07066106796265\n",
      "  timers:\n",
      "    learn_throughput: 667.164\n",
      "    learn_time_ms: 5995.529\n",
      "    sample_throughput: 4287.267\n",
      "    sample_time_ms: 932.995\n",
      "    update_time_ms: 4.377\n",
      "  timestamp: 1620237755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.49999999999994\n",
      "  episode_reward_mean: 7.604999999999994\n",
      "  episode_reward_min: -16.49999999999998\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 600\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0522441864013672\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017154432833194733\n",
      "          model: {}\n",
      "          policy_loss: -0.0475449338555336\n",
      "          total_loss: 24.560970306396484\n",
      "          vf_explained_var: 0.43864837288856506\n",
      "          vf_loss: 24.591148376464844\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0185041427612305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011963285505771637\n",
      "          model: {}\n",
      "          policy_loss: -0.04564335197210312\n",
      "          total_loss: 3.830575942993164\n",
      "          vf_explained_var: 0.2913558781147003\n",
      "          vf_loss: 3.8580498695373535\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.36\n",
      "    ram_util_percent: 59.139999999999986\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 38.5\n",
      "    pol2: 5.400000000000004\n",
      "  policy_reward_mean:\n",
      "    pol1: 13.975\n",
      "    pol2: -6.369999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -17.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4313767443875895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23752555653118557\n",
      "    mean_inference_ms: 1.9150290301072068\n",
      "    mean_raw_obs_processing_ms: 1.7661500794195837\n",
      "  time_since_restore: 107.18548130989075\n",
      "  time_this_iter_s: 7.138993978500366\n",
      "  time_total_s: 107.18548130989075\n",
      "  timers:\n",
      "    learn_throughput: 663.715\n",
      "    learn_time_ms: 6026.681\n",
      "    sample_throughput: 4468.135\n",
      "    sample_time_ms: 895.228\n",
      "    update_time_ms: 4.482\n",
      "  timestamp: 1620237762\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: cb9b0_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         107.185</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">   7.605</td><td style=\"text-align: right;\">                28.5</td><td style=\"text-align: right;\">               -16.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         100.071</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  10.98 </td><td style=\"text-align: right;\">                35.4</td><td style=\"text-align: right;\">               -15.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         100.223</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">   7.998</td><td style=\"text-align: right;\">                31.5</td><td style=\"text-align: right;\">               -10.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         100.239</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  11.208</td><td style=\"text-align: right;\">                37.5</td><td style=\"text-align: right;\">                -9  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.49999999999991\n",
      "  episode_reward_mean: 13.067999999999966\n",
      "  episode_reward_min: -10.199999999999983\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 600\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0266005992889404\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01582265831530094\n",
      "          model: {}\n",
      "          policy_loss: -0.051514022052288055\n",
      "          total_loss: 39.734100341796875\n",
      "          vf_explained_var: 0.38242244720458984\n",
      "          vf_loss: 39.76959228515625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.08197021484375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016428181901574135\n",
      "          model: {}\n",
      "          policy_loss: -0.04795346036553383\n",
      "          total_loss: 2.7521512508392334\n",
      "          vf_explained_var: 0.21536996960639954\n",
      "          vf_loss: 2.78347110748291\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.0\n",
      "    ram_util_percent: 59.41818181818181\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 47.5\n",
      "    pol2: 6.499999999999998\n",
      "  policy_reward_mean:\n",
      "    pol1: 19.845\n",
      "    pol2: -6.7769999999999895\n",
      "  policy_reward_min:\n",
      "    pol1: -3.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42983328620926486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23487500146051613\n",
      "    mean_inference_ms: 1.8819645479326699\n",
      "    mean_raw_obs_processing_ms: 1.7571959040881862\n",
      "  time_since_restore: 107.56550025939941\n",
      "  time_this_iter_s: 7.326943874359131\n",
      "  time_total_s: 107.56550025939941\n",
      "  timers:\n",
      "    learn_throughput: 661.834\n",
      "    learn_time_ms: 6043.812\n",
      "    sample_throughput: 4458.973\n",
      "    sample_time_ms: 897.068\n",
      "    update_time_ms: 4.73\n",
      "  timestamp: 1620237762\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 31.499999999999922\n",
      "  episode_reward_mean: 9.545999999999985\n",
      "  episode_reward_min: -8.999999999999973\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 600\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0281202793121338\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015413693152368069\n",
      "          model: {}\n",
      "          policy_loss: -0.05334797501564026\n",
      "          total_loss: 35.05754852294922\n",
      "          vf_explained_var: 0.34652644395828247\n",
      "          vf_loss: 35.09529113769531\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.106231451034546\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011903677135705948\n",
      "          model: {}\n",
      "          policy_loss: -0.050452783703804016\n",
      "          total_loss: 2.426176071166992\n",
      "          vf_explained_var: 0.36374032497406006\n",
      "          vf_loss: 2.458549976348877\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.0090909090909\n",
      "    ram_util_percent: 59.41818181818181\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 37.5\n",
      "    pol2: 4.299999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: 16.565\n",
      "    pol2: -7.018999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -2.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.43659309461790746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2384247585680158\n",
      "    mean_inference_ms: 1.9393241415013467\n",
      "    mean_raw_obs_processing_ms: 1.8054587847926984\n",
      "  time_since_restore: 107.48518204689026\n",
      "  time_this_iter_s: 7.26225209236145\n",
      "  time_total_s: 107.48518204689026\n",
      "  timers:\n",
      "    learn_throughput: 665.721\n",
      "    learn_time_ms: 6008.523\n",
      "    sample_throughput: 4353.172\n",
      "    sample_time_ms: 918.87\n",
      "    update_time_ms: 5.391\n",
      "  timestamp: 1620237762\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: cb9b0_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.09999999999994\n",
      "  episode_reward_mean: 12.728999999999974\n",
      "  episode_reward_min: -15.599999999999993\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 600\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0090221166610718\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016817886382341385\n",
      "          model: {}\n",
      "          policy_loss: -0.04908616840839386\n",
      "          total_loss: 33.91453170776367\n",
      "          vf_explained_var: 0.44263529777526855\n",
      "          vf_loss: 33.946590423583984\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0635907649993896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01581237092614174\n",
      "          model: {}\n",
      "          policy_loss: -0.05345052480697632\n",
      "          total_loss: 2.765157699584961\n",
      "          vf_explained_var: 0.3283708095550537\n",
      "          vf_loss: 2.802598237991333\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.36\n",
      "    ram_util_percent: 59.139999999999986\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 41.0\n",
      "    pol2: 7.60000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: 19.88\n",
      "    pol2: -7.150999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -15.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4404482101164058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23972802462691306\n",
      "    mean_inference_ms: 1.9475616796147992\n",
      "    mean_raw_obs_processing_ms: 1.8274604890021768\n",
      "  time_since_restore: 107.28884291648865\n",
      "  time_this_iter_s: 7.218181848526001\n",
      "  time_total_s: 107.28884291648865\n",
      "  timers:\n",
      "    learn_throughput: 662.928\n",
      "    learn_time_ms: 6033.84\n",
      "    sample_throughput: 4343.385\n",
      "    sample_time_ms: 920.941\n",
      "    update_time_ms: 4.149\n",
      "  timestamp: 1620237762\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.699999999999903\n",
      "  episode_reward_mean: 8.801999999999994\n",
      "  episode_reward_min: -9.599999999999982\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 640\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0326439142227173\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016699716448783875\n",
      "          model: {}\n",
      "          policy_loss: -0.053054727613925934\n",
      "          total_loss: 31.71788215637207\n",
      "          vf_explained_var: 0.41693776845932007\n",
      "          vf_loss: 31.7540283203125\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.000304937362671\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010444528423249722\n",
      "          model: {}\n",
      "          policy_loss: -0.044388893991708755\n",
      "          total_loss: 3.4925529956817627\n",
      "          vf_explained_var: 0.3325260281562805\n",
      "          vf_loss: 3.5210790634155273\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.11\n",
      "    ram_util_percent: 58.919999999999995\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 37.5\n",
      "    pol2: 9.800000000000013\n",
      "  policy_reward_mean:\n",
      "    pol1: 15.315\n",
      "    pol2: -6.5129999999999875\n",
      "  policy_reward_min:\n",
      "    pol1: -8.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.43021610577930874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23694445922877644\n",
      "    mean_inference_ms: 1.9103610174555996\n",
      "    mean_raw_obs_processing_ms: 1.7608276865691062\n",
      "  time_since_restore: 114.13209939002991\n",
      "  time_this_iter_s: 6.94661808013916\n",
      "  time_total_s: 114.13209939002991\n",
      "  timers:\n",
      "    learn_throughput: 662.201\n",
      "    learn_time_ms: 6040.461\n",
      "    sample_throughput: 4483.151\n",
      "    sample_time_ms: 892.23\n",
      "    update_time_ms: 4.527\n",
      "  timestamp: 1620237769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: cb9b0_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         114.132</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">   8.802</td><td style=\"text-align: right;\">                29.7</td><td style=\"text-align: right;\">                -9.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         107.289</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  12.729</td><td style=\"text-align: right;\">                32.1</td><td style=\"text-align: right;\">               -15.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         107.485</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">   9.546</td><td style=\"text-align: right;\">                31.5</td><td style=\"text-align: right;\">                -9  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         107.566</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  13.068</td><td style=\"text-align: right;\">                37.5</td><td style=\"text-align: right;\">               -10.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.09999999999994\n",
      "  episode_reward_mean: 13.802999999999967\n",
      "  episode_reward_min: -15.599999999999993\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 640\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9753525853157043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016500258818268776\n",
      "          model: {}\n",
      "          policy_loss: -0.049142371863126755\n",
      "          total_loss: 36.821136474609375\n",
      "          vf_explained_var: 0.4418574571609497\n",
      "          vf_loss: 36.85356903076172\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.050537347793579\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015276303514838219\n",
      "          model: {}\n",
      "          policy_loss: -0.04634333774447441\n",
      "          total_loss: 3.781263828277588\n",
      "          vf_explained_var: 0.21194672584533691\n",
      "          vf_loss: 3.8121399879455566\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.129999999999995\n",
      "    ram_util_percent: 58.919999999999995\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 41.0\n",
      "    pol2: 7.60000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: 20.525\n",
      "    pol2: -6.721999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -15.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4379040494930625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23840246934660209\n",
      "    mean_inference_ms: 1.9385082892333276\n",
      "    mean_raw_obs_processing_ms: 1.8126603918194615\n",
      "  time_since_restore: 114.07904481887817\n",
      "  time_this_iter_s: 6.790201902389526\n",
      "  time_total_s: 114.07904481887817\n",
      "  timers:\n",
      "    learn_throughput: 662.623\n",
      "    learn_time_ms: 6036.613\n",
      "    sample_throughput: 4413.347\n",
      "    sample_time_ms: 906.342\n",
      "    update_time_ms: 3.994\n",
      "  timestamp: 1620237769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.0999999999999\n",
      "  episode_reward_mean: 10.073999999999982\n",
      "  episode_reward_min: -8.399999999999977\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 640\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0119709968566895\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016002409160137177\n",
      "          model: {}\n",
      "          policy_loss: -0.04756856709718704\n",
      "          total_loss: 31.118722915649414\n",
      "          vf_explained_var: 0.4220544099807739\n",
      "          vf_loss: 31.150089263916016\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0893347263336182\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011912691406905651\n",
      "          model: {}\n",
      "          policy_loss: -0.04811014235019684\n",
      "          total_loss: 2.6385996341705322\n",
      "          vf_explained_var: 0.36007606983184814\n",
      "          vf_loss: 2.6686172485351562\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.120000000000005\n",
      "    ram_util_percent: 58.919999999999995\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 38.5\n",
      "    pol2: 4.299999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: 17.06\n",
      "    pol2: -6.9859999999999856\n",
      "  policy_reward_min:\n",
      "    pol1: -2.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.43516786337388796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23771681354241048\n",
      "    mean_inference_ms: 1.9332266602150059\n",
      "    mean_raw_obs_processing_ms: 1.7999025436325327\n",
      "  time_since_restore: 114.4144401550293\n",
      "  time_this_iter_s: 6.929258108139038\n",
      "  time_total_s: 114.4144401550293\n",
      "  timers:\n",
      "    learn_throughput: 663.69\n",
      "    learn_time_ms: 6026.908\n",
      "    sample_throughput: 4371.05\n",
      "    sample_time_ms: 915.112\n",
      "    update_time_ms: 5.361\n",
      "  timestamp: 1620237769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: cb9b0_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 40.199999999999896\n",
      "  episode_reward_mean: 13.859999999999957\n",
      "  episode_reward_min: -10.199999999999983\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 640\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0020294189453125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014315037056803703\n",
      "          model: {}\n",
      "          policy_loss: -0.0357622355222702\n",
      "          total_loss: 40.41931915283203\n",
      "          vf_explained_var: 0.37415051460266113\n",
      "          vf_loss: 40.440589904785156\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0684953927993774\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01454838551580906\n",
      "          model: {}\n",
      "          policy_loss: -0.04621171951293945\n",
      "          total_loss: 3.113527297973633\n",
      "          vf_explained_var: 0.26539212465286255\n",
      "          vf_loss: 3.1450085639953613\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.08\n",
      "    ram_util_percent: 58.919999999999995\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 48.0\n",
      "    pol2: 6.499999999999998\n",
      "  policy_reward_mean:\n",
      "    pol1: 20.395\n",
      "    pol2: -6.534999999999989\n",
      "  policy_reward_min:\n",
      "    pol1: -5.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4309486025855995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23519758901973148\n",
      "    mean_inference_ms: 1.8838455960600822\n",
      "    mean_raw_obs_processing_ms: 1.760052095039756\n",
      "  time_since_restore: 114.51933717727661\n",
      "  time_this_iter_s: 6.953836917877197\n",
      "  time_total_s: 114.51933717727661\n",
      "  timers:\n",
      "    learn_throughput: 660.638\n",
      "    learn_time_ms: 6054.75\n",
      "    sample_throughput: 4510.614\n",
      "    sample_time_ms: 886.797\n",
      "    update_time_ms: 4.716\n",
      "  timestamp: 1620237769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.699999999999903\n",
      "  episode_reward_mean: 8.924999999999997\n",
      "  episode_reward_min: -10.799999999999978\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 680\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0229192972183228\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01520090363919735\n",
      "          model: {}\n",
      "          policy_loss: -0.05488033592700958\n",
      "          total_loss: 30.025129318237305\n",
      "          vf_explained_var: 0.37882477045059204\n",
      "          vf_loss: 30.064619064331055\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9889450073242188\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01089293509721756\n",
      "          model: {}\n",
      "          policy_loss: -0.040026865899562836\n",
      "          total_loss: 3.6165595054626465\n",
      "          vf_explained_var: 0.30256927013397217\n",
      "          vf_loss: 3.640042781829834\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.78999999999999\n",
      "    ram_util_percent: 60.53000000000001\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 37.5\n",
      "    pol2: 9.800000000000013\n",
      "  policy_reward_mean:\n",
      "    pol1: 15.68\n",
      "    pol2: -6.754999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -8.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42845886863948635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23620477937091042\n",
      "    mean_inference_ms: 1.9018764695908106\n",
      "    mean_raw_obs_processing_ms: 1.7537750054702013\n",
      "  time_since_restore: 121.33065938949585\n",
      "  time_this_iter_s: 7.198559999465942\n",
      "  time_total_s: 121.33065938949585\n",
      "  timers:\n",
      "    learn_throughput: 659.257\n",
      "    learn_time_ms: 6067.433\n",
      "    sample_throughput: 4492.266\n",
      "    sample_time_ms: 890.419\n",
      "    update_time_ms: 4.712\n",
      "  timestamp: 1620237776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: cb9b0_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         121.331</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">   8.925</td><td style=\"text-align: right;\">                29.7</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         114.079</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  13.803</td><td style=\"text-align: right;\">                32.1</td><td style=\"text-align: right;\">               -15.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         114.414</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  10.074</td><td style=\"text-align: right;\">                35.1</td><td style=\"text-align: right;\">                -8.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         114.519</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  13.86 </td><td style=\"text-align: right;\">                40.2</td><td style=\"text-align: right;\">               -10.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 31.799999999999912\n",
      "  episode_reward_mean: 15.446999999999962\n",
      "  episode_reward_min: -5.999999999999998\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 680\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9616758823394775\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015210719779133797\n",
      "          model: {}\n",
      "          policy_loss: -0.04672204703092575\n",
      "          total_loss: 39.0045051574707\n",
      "          vf_explained_var: 0.4549851715564728\n",
      "          vf_loss: 39.03582763671875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0392004251480103\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01626610942184925\n",
      "          model: {}\n",
      "          policy_loss: -0.04059857502579689\n",
      "          total_loss: 3.1936964988708496\n",
      "          vf_explained_var: 0.30403876304626465\n",
      "          vf_loss: 3.217825412750244\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.81\n",
      "    ram_util_percent: 60.53000000000001\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 41.5\n",
      "    pol2: 2.1000000000000103\n",
      "  policy_reward_mean:\n",
      "    pol1: 21.96\n",
      "    pol2: -6.512999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -1.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4348382562210968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2368826444812452\n",
      "    mean_inference_ms: 1.9256162168892939\n",
      "    mean_raw_obs_processing_ms: 1.7967150479731224\n",
      "  time_since_restore: 121.26175594329834\n",
      "  time_this_iter_s: 7.182711124420166\n",
      "  time_total_s: 121.26175594329834\n",
      "  timers:\n",
      "    learn_throughput: 658.921\n",
      "    learn_time_ms: 6070.531\n",
      "    sample_throughput: 4437.618\n",
      "    sample_time_ms: 901.384\n",
      "    update_time_ms: 3.961\n",
      "  timestamp: 1620237776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.6999999999999\n",
      "  episode_reward_mean: 12.206999999999972\n",
      "  episode_reward_min: -6.5999999999999925\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 680\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9839694499969482\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016409888863563538\n",
      "          model: {}\n",
      "          policy_loss: -0.05548371747136116\n",
      "          total_loss: 49.81418991088867\n",
      "          vf_explained_var: 0.2508084177970886\n",
      "          vf_loss: 49.853057861328125\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0690650939941406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011829668655991554\n",
      "          model: {}\n",
      "          policy_loss: -0.04783383756875992\n",
      "          total_loss: 3.2284443378448486\n",
      "          vf_explained_var: 0.23751230537891388\n",
      "          vf_loss: 3.2583117485046387\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.79999999999998\n",
      "    ram_util_percent: 60.53000000000001\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 41.0\n",
      "    pol2: 2.099999999999995\n",
      "  policy_reward_mean:\n",
      "    pol1: 18.885\n",
      "    pol2: -6.677999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -1.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4347011643462352\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23774741641379016\n",
      "    mean_inference_ms: 1.92940598512413\n",
      "    mean_raw_obs_processing_ms: 1.799520361906992\n",
      "  time_since_restore: 121.67771625518799\n",
      "  time_this_iter_s: 7.263276100158691\n",
      "  time_total_s: 121.67771625518799\n",
      "  timers:\n",
      "    learn_throughput: 659.688\n",
      "    learn_time_ms: 6063.476\n",
      "    sample_throughput: 4279.67\n",
      "    sample_time_ms: 934.652\n",
      "    update_time_ms: 5.067\n",
      "  timestamp: 1620237776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: cb9b0_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-02-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 40.199999999999896\n",
      "  episode_reward_mean: 15.098999999999947\n",
      "  episode_reward_min: -13.199999999999976\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 680\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9801012873649597\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016721922904253006\n",
      "          model: {}\n",
      "          policy_loss: -0.047953058034181595\n",
      "          total_loss: 36.456607818603516\n",
      "          vf_explained_var: 0.470806360244751\n",
      "          vf_loss: 36.48762512207031\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0492603778839111\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015546233393251896\n",
      "          model: {}\n",
      "          policy_loss: -0.05118773505091667\n",
      "          total_loss: 2.9765355587005615\n",
      "          vf_explained_var: 0.2424178123474121\n",
      "          vf_loss: 3.0119826793670654\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.3\n",
      "    ram_util_percent: 60.53000000000001\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 48.0\n",
      "    pol2: 3.200000000000008\n",
      "  policy_reward_mean:\n",
      "    pol1: 21.59\n",
      "    pol2: -6.490999999999989\n",
      "  policy_reward_min:\n",
      "    pol1: -6.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.43241736783034895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23556140691574373\n",
      "    mean_inference_ms: 1.8854459436222835\n",
      "    mean_raw_obs_processing_ms: 1.7668146487091525\n",
      "  time_since_restore: 121.75761342048645\n",
      "  time_this_iter_s: 7.238276243209839\n",
      "  time_total_s: 121.75761342048645\n",
      "  timers:\n",
      "    learn_throughput: 657.866\n",
      "    learn_time_ms: 6080.267\n",
      "    sample_throughput: 4421.023\n",
      "    sample_time_ms: 904.768\n",
      "    update_time_ms: 4.474\n",
      "  timestamp: 1620237777\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.699999999999903\n",
      "  episode_reward_mean: 9.989999999999995\n",
      "  episode_reward_min: -10.799999999999978\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 720\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9977753162384033\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016160007566213608\n",
      "          model: {}\n",
      "          policy_loss: -0.057065293192863464\n",
      "          total_loss: 32.23808288574219\n",
      "          vf_explained_var: 0.4464064836502075\n",
      "          vf_loss: 32.278785705566406\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.972366988658905\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011389559134840965\n",
      "          model: {}\n",
      "          policy_loss: -0.04479311779141426\n",
      "          total_loss: 3.8204660415649414\n",
      "          vf_explained_var: 0.2829270362854004\n",
      "          vf_loss: 3.84796142578125\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.529999999999994\n",
      "    ram_util_percent: 61.92\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 37.5\n",
      "    pol2: 9.800000000000013\n",
      "  policy_reward_mean:\n",
      "    pol1: 16.8\n",
      "    pol2: -6.809999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -8.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42659475613823944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23531148338024885\n",
      "    mean_inference_ms: 1.8926942070636164\n",
      "    mean_raw_obs_processing_ms: 1.7434789888967044\n",
      "  time_since_restore: 128.2451832294464\n",
      "  time_this_iter_s: 6.9145238399505615\n",
      "  time_total_s: 128.2451832294464\n",
      "  timers:\n",
      "    learn_throughput: 658.382\n",
      "    learn_time_ms: 6075.504\n",
      "    sample_throughput: 4540.585\n",
      "    sample_time_ms: 880.944\n",
      "    update_time_ms: 4.8\n",
      "  timestamp: 1620237783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: cb9b0_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         128.245</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">   9.99 </td><td style=\"text-align: right;\">                29.7</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         121.262</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  15.447</td><td style=\"text-align: right;\">                31.8</td><td style=\"text-align: right;\">                -6  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         121.678</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  12.207</td><td style=\"text-align: right;\">                38.7</td><td style=\"text-align: right;\">                -6.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         121.758</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  15.099</td><td style=\"text-align: right;\">                40.2</td><td style=\"text-align: right;\">               -13.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 33.5999999999999\n",
      "  episode_reward_mean: 16.54499999999996\n",
      "  episode_reward_min: -6.599999999999982\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 720\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9522113800048828\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01667182706296444\n",
      "          model: {}\n",
      "          policy_loss: -0.05492858961224556\n",
      "          total_loss: 33.16084289550781\n",
      "          vf_explained_var: 0.5019678473472595\n",
      "          vf_loss: 33.198890686035156\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0264301300048828\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016469374299049377\n",
      "          model: {}\n",
      "          policy_loss: -0.05334606021642685\n",
      "          total_loss: 2.7331113815307617\n",
      "          vf_explained_var: 0.2653369903564453\n",
      "          vf_loss: 2.769782304763794\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.510000000000005\n",
      "    ram_util_percent: 61.92\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 41.5\n",
      "    pol2: 2.1000000000000103\n",
      "  policy_reward_mean:\n",
      "    pol1: 22.97\n",
      "    pol2: -6.424999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -1.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4320608922861965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23539621486158907\n",
      "    mean_inference_ms: 1.9130273710657377\n",
      "    mean_raw_obs_processing_ms: 1.7826038596083265\n",
      "  time_since_restore: 128.1174087524414\n",
      "  time_this_iter_s: 6.855652809143066\n",
      "  time_total_s: 128.1174087524414\n",
      "  timers:\n",
      "    learn_throughput: 657.938\n",
      "    learn_time_ms: 6079.596\n",
      "    sample_throughput: 4518.382\n",
      "    sample_time_ms: 885.273\n",
      "    update_time_ms: 4.051\n",
      "  timestamp: 1620237783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.6999999999999\n",
      "  episode_reward_mean: 13.703999999999969\n",
      "  episode_reward_min: -8.999999999999979\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 720\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9696096777915955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015052955597639084\n",
      "          model: {}\n",
      "          policy_loss: -0.03991057723760605\n",
      "          total_loss: 37.959896087646484\n",
      "          vf_explained_var: 0.3641103506088257\n",
      "          vf_loss: 37.98456573486328\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0533461570739746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011364945210516453\n",
      "          model: {}\n",
      "          policy_loss: -0.04782463237643242\n",
      "          total_loss: 2.273469924926758\n",
      "          vf_explained_var: 0.36226147413253784\n",
      "          vf_loss: 2.3040342330932617\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.519999999999996\n",
      "    ram_util_percent: 61.92\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 41.0\n",
      "    pol2: 2.099999999999995\n",
      "  policy_reward_mean:\n",
      "    pol1: 20.635\n",
      "    pol2: -6.9309999999999885\n",
      "  policy_reward_min:\n",
      "    pol1: -1.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4337922598990433\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.237589947303131\n",
      "    mean_inference_ms: 1.9253471221740746\n",
      "    mean_raw_obs_processing_ms: 1.795938164482563\n",
      "  time_since_restore: 128.5832324028015\n",
      "  time_this_iter_s: 6.905516147613525\n",
      "  time_total_s: 128.5832324028015\n",
      "  timers:\n",
      "    learn_throughput: 658.485\n",
      "    learn_time_ms: 6074.546\n",
      "    sample_throughput: 4377.935\n",
      "    sample_time_ms: 913.673\n",
      "    update_time_ms: 4.875\n",
      "  timestamp: 1620237783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: cb9b0_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.99999999999993\n",
      "  episode_reward_mean: 14.708999999999948\n",
      "  episode_reward_min: -13.199999999999976\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 720\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9759976863861084\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015248746611177921\n",
      "          model: {}\n",
      "          policy_loss: -0.0425354540348053\n",
      "          total_loss: 41.58281326293945\n",
      "          vf_explained_var: 0.3547093868255615\n",
      "          vf_loss: 41.60990905761719\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0414693355560303\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015249720774590969\n",
      "          model: {}\n",
      "          policy_loss: -0.0505562350153923\n",
      "          total_loss: 2.8584508895874023\n",
      "          vf_explained_var: 0.2579086422920227\n",
      "          vf_loss: 2.893566608428955\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.510000000000005\n",
      "    ram_util_percent: 61.910000000000004\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 37.5\n",
      "    pol2: -0.0999999999999841\n",
      "  policy_reward_mean:\n",
      "    pol1: 21.09\n",
      "    pol2: -6.38099999999999\n",
      "  policy_reward_min:\n",
      "    pol1: -6.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4325176724496421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23545489749770382\n",
      "    mean_inference_ms: 1.8838883133329438\n",
      "    mean_raw_obs_processing_ms: 1.7680057656592174\n",
      "  time_since_restore: 128.6406066417694\n",
      "  time_this_iter_s: 6.882993221282959\n",
      "  time_total_s: 128.6406066417694\n",
      "  timers:\n",
      "    learn_throughput: 657.339\n",
      "    learn_time_ms: 6085.137\n",
      "    sample_throughput: 4472.559\n",
      "    sample_time_ms: 894.343\n",
      "    update_time_ms: 4.481\n",
      "  timestamp: 1620237783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.49999999999992\n",
      "  episode_reward_mean: 14.870999999999963\n",
      "  episode_reward_min: -8.999999999999979\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 760\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9431305527687073\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015690304338932037\n",
      "          model: {}\n",
      "          policy_loss: -0.045902594923973083\n",
      "          total_loss: 37.732765197753906\n",
      "          vf_explained_var: 0.38498246669769287\n",
      "          vf_loss: 37.762786865234375\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0463955402374268\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011593885719776154\n",
      "          model: {}\n",
      "          policy_loss: -0.04845979064702988\n",
      "          total_loss: 3.200882911682129\n",
      "          vf_explained_var: 0.28480011224746704\n",
      "          vf_loss: 3.23173451423645\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.71000000000001\n",
      "    ram_util_percent: 59.94999999999999\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 39.5\n",
      "    pol2: 5.400000000000004\n",
      "  policy_reward_mean:\n",
      "    pol1: 21.78\n",
      "    pol2: -6.908999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -1.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.43170359040322076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.236616874321788\n",
      "    mean_inference_ms: 1.9169070576132112\n",
      "    mean_raw_obs_processing_ms: 1.7853456804788561\n",
      "  time_since_restore: 135.34153032302856\n",
      "  time_this_iter_s: 6.758297920227051\n",
      "  time_total_s: 135.34153032302856\n",
      "  timers:\n",
      "    learn_throughput: 656.765\n",
      "    learn_time_ms: 6090.456\n",
      "    sample_throughput: 4417.872\n",
      "    sample_time_ms: 905.413\n",
      "    update_time_ms: 4.704\n",
      "  timestamp: 1620237790\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: cb9b0_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         128.245</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">   9.99 </td><td style=\"text-align: right;\">                29.7</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         128.117</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  16.545</td><td style=\"text-align: right;\">                33.6</td><td style=\"text-align: right;\">                -6.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         135.342</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  14.871</td><td style=\"text-align: right;\">                34.5</td><td style=\"text-align: right;\">                -9  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         128.641</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  14.709</td><td style=\"text-align: right;\">                30  </td><td style=\"text-align: right;\">               -13.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.699999999999925\n",
      "  episode_reward_mean: 16.283999999999946\n",
      "  episode_reward_min: -13.199999999999976\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 760\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9326965808868408\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014160866849124432\n",
      "          model: {}\n",
      "          policy_loss: -0.04371863603591919\n",
      "          total_loss: 44.559326171875\n",
      "          vf_explained_var: 0.3980241119861603\n",
      "          vf_loss: 44.588706970214844\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0281574726104736\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015415789559483528\n",
      "          model: {}\n",
      "          policy_loss: -0.0438518151640892\n",
      "          total_loss: 4.413232803344727\n",
      "          vf_explained_var: 0.1896938532590866\n",
      "          vf_loss: 4.441476345062256\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.72\n",
      "    ram_util_percent: 59.94999999999999\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 46.5\n",
      "    pol2: 5.400000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: 21.95\n",
      "    pol2: -5.66599999999999\n",
      "  policy_reward_min:\n",
      "    pol1: -6.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4310698027128062\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23466395892873013\n",
      "    mean_inference_ms: 1.8774184403941705\n",
      "    mean_raw_obs_processing_ms: 1.7614893966170364\n",
      "  time_since_restore: 135.38211250305176\n",
      "  time_this_iter_s: 6.741505861282349\n",
      "  time_total_s: 135.38211250305176\n",
      "  timers:\n",
      "    learn_throughput: 656.745\n",
      "    learn_time_ms: 6090.645\n",
      "    sample_throughput: 4480.697\n",
      "    sample_time_ms: 892.718\n",
      "    update_time_ms: 4.647\n",
      "  timestamp: 1620237790\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.799999999999905\n",
      "  episode_reward_mean: 10.685999999999993\n",
      "  episode_reward_min: -10.799999999999978\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 760\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9759976267814636\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01684654876589775\n",
      "          model: {}\n",
      "          policy_loss: -0.05891885608434677\n",
      "          total_loss: 35.20738220214844\n",
      "          vf_explained_var: 0.3904208540916443\n",
      "          vf_loss: 35.24924087524414\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9667128324508667\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011698679998517036\n",
      "          model: {}\n",
      "          policy_loss: -0.043208129703998566\n",
      "          total_loss: 3.2425084114074707\n",
      "          vf_explained_var: 0.2337241768836975\n",
      "          vf_loss: 3.267949104309082\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.7\n",
      "    ram_util_percent: 59.94999999999999\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 39.0\n",
      "    pol2: 2.100000000000005\n",
      "  policy_reward_mean:\n",
      "    pol1: 17.32\n",
      "    pol2: -6.633999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -8.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42619841991188545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23525425792585608\n",
      "    mean_inference_ms: 1.8888679409247677\n",
      "    mean_raw_obs_processing_ms: 1.7399124191215654\n",
      "  time_since_restore: 135.25155115127563\n",
      "  time_this_iter_s: 7.006367921829224\n",
      "  time_total_s: 135.25155115127563\n",
      "  timers:\n",
      "    learn_throughput: 656.336\n",
      "    learn_time_ms: 6094.442\n",
      "    sample_throughput: 4484.943\n",
      "    sample_time_ms: 891.873\n",
      "    update_time_ms: 4.777\n",
      "  timestamp: 1620237790\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.09999999999991\n",
      "  episode_reward_mean: 17.165999999999954\n",
      "  episode_reward_min: -6.599999999999982\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 760\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9414563179016113\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014741897583007812\n",
      "          model: {}\n",
      "          policy_loss: -0.04596871882677078\n",
      "          total_loss: 43.72021484375\n",
      "          vf_explained_var: 0.4087233543395996\n",
      "          vf_loss: 43.75125503540039\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0157523155212402\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01666201278567314\n",
      "          model: {}\n",
      "          policy_loss: -0.05422214791178703\n",
      "          total_loss: 3.3662221431732178\n",
      "          vf_explained_var: 0.1966918408870697\n",
      "          vf_loss: 3.403573989868164\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.72\n",
      "    ram_util_percent: 59.94999999999999\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 42.5\n",
      "    pol2: 2.100000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: 23.47\n",
      "    pol2: -6.30399999999999\n",
      "  policy_reward_min:\n",
      "    pol1: -2.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4306025590451846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23471970313243687\n",
      "    mean_inference_ms: 1.9054292557972616\n",
      "    mean_raw_obs_processing_ms: 1.7754378277625988\n",
      "  time_since_restore: 135.0422487258911\n",
      "  time_this_iter_s: 6.924839973449707\n",
      "  time_total_s: 135.0422487258911\n",
      "  timers:\n",
      "    learn_throughput: 656.419\n",
      "    learn_time_ms: 6093.67\n",
      "    sample_throughput: 4448.395\n",
      "    sample_time_ms: 899.201\n",
      "    update_time_ms: 3.937\n",
      "  timestamp: 1620237790\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.99999999999996\n",
      "  episode_reward_mean: 15.134999999999966\n",
      "  episode_reward_min: -8.999999999999979\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 800\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9155139923095703\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01450948603451252\n",
      "          model: {}\n",
      "          policy_loss: -0.04212581738829613\n",
      "          total_loss: 36.75975036621094\n",
      "          vf_explained_var: 0.39803963899612427\n",
      "          vf_loss: 36.78718566894531\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.031430959701538\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011460917070508003\n",
      "          model: {}\n",
      "          policy_loss: -0.03446808457374573\n",
      "          total_loss: 3.2552859783172607\n",
      "          vf_explained_var: 0.3054288625717163\n",
      "          vf_loss: 3.2723476886749268\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.18000000000001\n",
      "    ram_util_percent: 59.660000000000004\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 41.0\n",
      "    pol2: 5.400000000000004\n",
      "  policy_reward_mean:\n",
      "    pol1: 22.055\n",
      "    pol2: -6.919999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: 1.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42863954819981614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2349468751843967\n",
      "    mean_inference_ms: 1.9057757218085325\n",
      "    mean_raw_obs_processing_ms: 1.7705269299977875\n",
      "  time_since_restore: 142.35048532485962\n",
      "  time_this_iter_s: 7.008955001831055\n",
      "  time_total_s: 142.35048532485962\n",
      "  timers:\n",
      "    learn_throughput: 655.034\n",
      "    learn_time_ms: 6106.552\n",
      "    sample_throughput: 4499.045\n",
      "    sample_time_ms: 889.078\n",
      "    update_time_ms: 5.174\n",
      "  timestamp: 1620237797\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: cb9b0_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         135.252</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  10.686</td><td style=\"text-align: right;\">                37.8</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         135.042</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  17.166</td><td style=\"text-align: right;\">                38.1</td><td style=\"text-align: right;\">                -6.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         142.35 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  15.135</td><td style=\"text-align: right;\">                33  </td><td style=\"text-align: right;\">                -9  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         135.382</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  16.284</td><td style=\"text-align: right;\">                38.7</td><td style=\"text-align: right;\">               -13.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.699999999999925\n",
      "  episode_reward_mean: 16.343999999999948\n",
      "  episode_reward_min: -14.999999999999982\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 800\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.945174515247345\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015016630291938782\n",
      "          model: {}\n",
      "          policy_loss: -0.05658692121505737\n",
      "          total_loss: 43.63652420043945\n",
      "          vf_explained_var: 0.38146549463272095\n",
      "          vf_loss: 43.67790603637695\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.993251621723175\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015000980347394943\n",
      "          model: {}\n",
      "          policy_loss: -0.04822441190481186\n",
      "          total_loss: 7.4388813972473145\n",
      "          vf_explained_var: 0.22902798652648926\n",
      "          vf_loss: 7.471916675567627\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.19000000000001\n",
      "    ram_util_percent: 59.64\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 46.5\n",
      "    pol2: 19.699999999999953\n",
      "  policy_reward_mean:\n",
      "    pol1: 21.68\n",
      "    pol2: -5.335999999999989\n",
      "  policy_reward_min:\n",
      "    pol1: -15.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4287060916872952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23339403527820665\n",
      "    mean_inference_ms: 1.868664676696169\n",
      "    mean_raw_obs_processing_ms: 1.7507240660456416\n",
      "  time_since_restore: 142.3974871635437\n",
      "  time_this_iter_s: 7.015374660491943\n",
      "  time_total_s: 142.3974871635437\n",
      "  timers:\n",
      "    learn_throughput: 654.53\n",
      "    learn_time_ms: 6111.258\n",
      "    sample_throughput: 4478.11\n",
      "    sample_time_ms: 893.234\n",
      "    update_time_ms: 4.437\n",
      "  timestamp: 1620237797\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.799999999999905\n",
      "  episode_reward_mean: 11.360999999999985\n",
      "  episode_reward_min: -5.999999999999984\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 800\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9565659761428833\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016496874392032623\n",
      "          model: {}\n",
      "          policy_loss: -0.05070364475250244\n",
      "          total_loss: 34.558616638183594\n",
      "          vf_explained_var: 0.4242979884147644\n",
      "          vf_loss: 34.592613220214844\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9608132839202881\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011789917945861816\n",
      "          model: {}\n",
      "          policy_loss: -0.06088224798440933\n",
      "          total_loss: 3.1779768466949463\n",
      "          vf_explained_var: 0.2912764549255371\n",
      "          vf_loss: 3.2209529876708984\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.47\n",
      "    ram_util_percent: 59.61\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 39.5\n",
      "    pol2: 2.1000000000000036\n",
      "  policy_reward_mean:\n",
      "    pol1: 18.05\n",
      "    pol2: -6.6889999999999885\n",
      "  policy_reward_min:\n",
      "    pol1: -5.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.426189301908412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23536332394439313\n",
      "    mean_inference_ms: 1.8871190221693683\n",
      "    mean_raw_obs_processing_ms: 1.7386517989689714\n",
      "  time_since_restore: 142.271790266037\n",
      "  time_this_iter_s: 7.0202391147613525\n",
      "  time_total_s: 142.271790266037\n",
      "  timers:\n",
      "    learn_throughput: 654.527\n",
      "    learn_time_ms: 6111.287\n",
      "    sample_throughput: 4469.743\n",
      "    sample_time_ms: 894.906\n",
      "    update_time_ms: 4.664\n",
      "  timestamp: 1620237797\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.599999999999895\n",
      "  episode_reward_mean: 18.362999999999946\n",
      "  episode_reward_min: -4.799999999999979\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 800\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8952597975730896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014924846589565277\n",
      "          model: {}\n",
      "          policy_loss: -0.04810794070363045\n",
      "          total_loss: 42.884498596191406\n",
      "          vf_explained_var: 0.44631433486938477\n",
      "          vf_loss: 42.91749572753906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9970316886901855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015170438215136528\n",
      "          model: {}\n",
      "          policy_loss: -0.04911041259765625\n",
      "          total_loss: 2.601705551147461\n",
      "          vf_explained_var: 0.2602307200431824\n",
      "          vf_loss: 2.635456085205078\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.21000000000001\n",
      "    ram_util_percent: 59.61\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 48.5\n",
      "    pol2: 2.099999999999998\n",
      "  policy_reward_mean:\n",
      "    pol1: 24.48\n",
      "    pol2: -6.11699999999999\n",
      "  policy_reward_min:\n",
      "    pol1: -2.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42978875410796413\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23425155321620886\n",
      "    mean_inference_ms: 1.9009123868707887\n",
      "    mean_raw_obs_processing_ms: 1.7747126248906415\n",
      "  time_since_restore: 141.97533082962036\n",
      "  time_this_iter_s: 6.933082103729248\n",
      "  time_total_s: 141.97533082962036\n",
      "  timers:\n",
      "    learn_throughput: 656.288\n",
      "    learn_time_ms: 6094.884\n",
      "    sample_throughput: 4392.558\n",
      "    sample_time_ms: 910.631\n",
      "    update_time_ms: 3.939\n",
      "  timestamp: 1620237797\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 33.599999999999895\n",
      "  episode_reward_mean: 16.376999999999956\n",
      "  episode_reward_min: -7.499999999999979\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 840\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9113146066665649\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014464903622865677\n",
      "          model: {}\n",
      "          policy_loss: -0.05169518291950226\n",
      "          total_loss: 41.56230163574219\n",
      "          vf_explained_var: 0.39895346760749817\n",
      "          vf_loss: 41.59934997558594\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0249980688095093\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010831557214260101\n",
      "          model: {}\n",
      "          policy_loss: -0.04098283499479294\n",
      "          total_loss: 3.4406676292419434\n",
      "          vf_explained_var: 0.25651270151138306\n",
      "          vf_loss: 3.4651999473571777\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.839999999999996\n",
      "    ram_util_percent: 60.8\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 42.5\n",
      "    pol2: 5.400000000000004\n",
      "  policy_reward_mean:\n",
      "    pol1: 22.67\n",
      "    pol2: -6.2929999999999895\n",
      "  policy_reward_min:\n",
      "    pol1: -1.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4264935874294241\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.233766769680949\n",
      "    mean_inference_ms: 1.8987288325762648\n",
      "    mean_raw_obs_processing_ms: 1.7601818961640943\n",
      "  time_since_restore: 149.68605136871338\n",
      "  time_this_iter_s: 7.33556604385376\n",
      "  time_total_s: 149.68605136871338\n",
      "  timers:\n",
      "    learn_throughput: 663.846\n",
      "    learn_time_ms: 6025.49\n",
      "    sample_throughput: 4582.75\n",
      "    sample_time_ms: 872.838\n",
      "    update_time_ms: 4.988\n",
      "  timestamp: 1620237805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: cb9b0_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         142.272</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  11.361</td><td style=\"text-align: right;\">                37.8</td><td style=\"text-align: right;\">                -6  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>RUNNING </td><td>192.168.0.100:80480</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         141.975</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  18.363</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                -4.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         149.686</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  16.377</td><td style=\"text-align: right;\">                33.6</td><td style=\"text-align: right;\">                -7.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         142.397</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  16.344</td><td style=\"text-align: right;\">                38.7</td><td style=\"text-align: right;\">               -15  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.39999999999993\n",
      "  episode_reward_mean: 16.88999999999994\n",
      "  episode_reward_min: -14.999999999999982\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 840\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9267206192016602\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013508189469575882\n",
      "          model: {}\n",
      "          policy_loss: -0.036311075091362\n",
      "          total_loss: 39.247928619384766\n",
      "          vf_explained_var: 0.43722862005233765\n",
      "          vf_loss: 39.27056121826172\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9833910465240479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013889736495912075\n",
      "          model: {}\n",
      "          policy_loss: -0.042798008769750595\n",
      "          total_loss: 4.336048126220703\n",
      "          vf_explained_var: 0.21006709337234497\n",
      "          vf_loss: 4.364782333374023\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.82727272727273\n",
      "    ram_util_percent: 60.85454545454545\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 38.0\n",
      "    pol2: 19.699999999999953\n",
      "  policy_reward_mean:\n",
      "    pol1: 21.335\n",
      "    pol2: -4.444999999999992\n",
      "  policy_reward_min:\n",
      "    pol1: -15.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4272468892995254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23250104685658105\n",
      "    mean_inference_ms: 1.8639725431215566\n",
      "    mean_raw_obs_processing_ms: 1.7440761617371145\n",
      "  time_since_restore: 149.70497608184814\n",
      "  time_this_iter_s: 7.307488918304443\n",
      "  time_total_s: 149.70497608184814\n",
      "  timers:\n",
      "    learn_throughput: 664.426\n",
      "    learn_time_ms: 6020.231\n",
      "    sample_throughput: 4567.846\n",
      "    sample_time_ms: 875.686\n",
      "    update_time_ms: 4.322\n",
      "  timestamp: 1620237805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.59999999999998\n",
      "  episode_reward_mean: 11.063999999999975\n",
      "  episode_reward_min: -7.499999999999979\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 840\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9574567079544067\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016231270506978035\n",
      "          model: {}\n",
      "          policy_loss: -0.052144281566143036\n",
      "          total_loss: 30.555002212524414\n",
      "          vf_explained_var: 0.48432356119155884\n",
      "          vf_loss: 30.590713500976562\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9359143376350403\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010620662942528725\n",
      "          model: {}\n",
      "          policy_loss: -0.041686829179525375\n",
      "          total_loss: 3.2206521034240723\n",
      "          vf_explained_var: 0.3353082537651062\n",
      "          vf_loss: 3.246208906173706\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.75454545454544\n",
      "    ram_util_percent: 60.85454545454545\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 39.5\n",
      "    pol2: 3.2000000000000037\n",
      "  policy_reward_mean:\n",
      "    pol1: 18.05\n",
      "    pol2: -6.985999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -5.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42648938818873006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23547809224589986\n",
      "    mean_inference_ms: 1.8869701187226429\n",
      "    mean_raw_obs_processing_ms: 1.7395867343066285\n",
      "  time_since_restore: 149.68157315254211\n",
      "  time_this_iter_s: 7.409782886505127\n",
      "  time_total_s: 149.68157315254211\n",
      "  timers:\n",
      "    learn_throughput: 662.773\n",
      "    learn_time_ms: 6035.252\n",
      "    sample_throughput: 4569.825\n",
      "    sample_time_ms: 875.307\n",
      "    update_time_ms: 4.553\n",
      "  timestamp: 1620237805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00001:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-25\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.599999999999895\n",
      "  episode_reward_mean: 20.85599999999994\n",
      "  episode_reward_min: 0.6000000000000163\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 840\n",
      "  experiment_id: 3953b7f839aa4f4fb4db97e5dbf30e49\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8893854022026062\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014634115621447563\n",
      "          model: {}\n",
      "          policy_loss: -0.04958489537239075\n",
      "          total_loss: 38.66370391845703\n",
      "          vf_explained_var: 0.48382753133773804\n",
      "          vf_loss: 38.6984748840332\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.971358060836792\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015794169157743454\n",
      "          model: {}\n",
      "          policy_loss: -0.04853866249322891\n",
      "          total_loss: 3.1108131408691406\n",
      "          vf_explained_var: 0.2794318199157715\n",
      "          vf_loss: 3.1433603763580322\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.75454545454544\n",
      "    ram_util_percent: 60.85454545454545\n",
      "  pid: 80480\n",
      "  policy_reward_max:\n",
      "    pol1: 48.5\n",
      "    pol2: 2.099999999999998\n",
      "  policy_reward_mean:\n",
      "    pol1: 26.335\n",
      "    pol2: -5.478999999999992\n",
      "  policy_reward_min:\n",
      "    pol1: 7.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42997016033121827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23412987488943343\n",
      "    mean_inference_ms: 1.9006707892415498\n",
      "    mean_raw_obs_processing_ms: 1.7776043051394828\n",
      "  time_since_restore: 149.37494587898254\n",
      "  time_this_iter_s: 7.399615049362183\n",
      "  time_total_s: 149.37494587898254\n",
      "  timers:\n",
      "    learn_throughput: 664.804\n",
      "    learn_time_ms: 6016.811\n",
      "    sample_throughput: 4500.933\n",
      "    sample_time_ms: 888.705\n",
      "    update_time_ms: 3.902\n",
      "  timestamp: 1620237805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: cb9b0_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.4999999999999\n",
      "  episode_reward_mean: 17.07599999999995\n",
      "  episode_reward_min: 9.520162436160717e-15\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 880\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9060282707214355\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01365598663687706\n",
      "          model: {}\n",
      "          policy_loss: -0.034379541873931885\n",
      "          total_loss: 33.79689025878906\n",
      "          vf_explained_var: 0.4619900584220886\n",
      "          vf_loss: 33.81744384765625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.006157636642456\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010395691730082035\n",
      "          model: {}\n",
      "          policy_loss: -0.03921952471137047\n",
      "          total_loss: 3.5592880249023438\n",
      "          vf_explained_var: 0.29250991344451904\n",
      "          vf_loss: 3.582718849182129\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.56999999999999\n",
      "    ram_util_percent: 59.470000000000006\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 42.5\n",
      "    pol2: 5.400000000000018\n",
      "  policy_reward_mean:\n",
      "    pol1: 23.38\n",
      "    pol2: -6.303999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -1.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4248393307911709\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23306101978526314\n",
      "    mean_inference_ms: 1.893422594544384\n",
      "    mean_raw_obs_processing_ms: 1.7528024161629776\n",
      "  time_since_restore: 155.9858682155609\n",
      "  time_this_iter_s: 6.299816846847534\n",
      "  time_total_s: 155.9858682155609\n",
      "  timers:\n",
      "    learn_throughput: 675.984\n",
      "    learn_time_ms: 5917.298\n",
      "    sample_throughput: 4684.96\n",
      "    sample_time_ms: 853.796\n",
      "    update_time_ms: 4.471\n",
      "  timestamp: 1620237811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: cb9b0_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (3 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING   </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         149.682</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  11.064</td><td style=\"text-align: right;\">                30.6</td><td style=\"text-align: right;\">        -7.5        </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING   </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         155.986</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  17.076</td><td style=\"text-align: right;\">                34.5</td><td style=\"text-align: right;\">         9.52016e-15</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING   </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         149.705</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  16.89 </td><td style=\"text-align: right;\">                32.4</td><td style=\"text-align: right;\">       -15          </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         149.375</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  20.856</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">         0.6        </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.39999999999993\n",
      "  episode_reward_mean: 16.772999999999943\n",
      "  episode_reward_min: -19.499999999999993\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 880\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9183897972106934\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014347020536661148\n",
      "          model: {}\n",
      "          policy_loss: -0.041683379560709\n",
      "          total_loss: 43.516143798828125\n",
      "          vf_explained_var: 0.3513191044330597\n",
      "          vf_loss: 43.543304443359375\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9640944004058838\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01345110684633255\n",
      "          model: {}\n",
      "          policy_loss: -0.04330069199204445\n",
      "          total_loss: 6.122193336486816\n",
      "          vf_explained_var: 0.21014398336410522\n",
      "          vf_loss: 6.151875019073486\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.98888888888889\n",
      "    ram_util_percent: 59.25555555555556\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 36.5\n",
      "    pol2: 9.800000000000006\n",
      "  policy_reward_mean:\n",
      "    pol1: 21.405\n",
      "    pol2: -4.631999999999992\n",
      "  policy_reward_min:\n",
      "    pol1: -9.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4266758283776603\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23212951749634148\n",
      "    mean_inference_ms: 1.8629565882497292\n",
      "    mean_raw_obs_processing_ms: 1.7430715821433587\n",
      "  time_since_restore: 155.94566893577576\n",
      "  time_this_iter_s: 6.240692853927612\n",
      "  time_total_s: 155.94566893577576\n",
      "  timers:\n",
      "    learn_throughput: 678.972\n",
      "    learn_time_ms: 5891.26\n",
      "    sample_throughput: 4607.949\n",
      "    sample_time_ms: 868.065\n",
      "    update_time_ms: 4.214\n",
      "  timestamp: 1620237811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 31.199999999999903\n",
      "  episode_reward_mean: 12.179999999999966\n",
      "  episode_reward_min: -7.499999999999979\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 880\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9320420622825623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016099823638796806\n",
      "          model: {}\n",
      "          policy_loss: -0.05073530972003937\n",
      "          total_loss: 30.222549438476562\n",
      "          vf_explained_var: 0.5163427591323853\n",
      "          vf_loss: 30.256980895996094\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9430259466171265\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010810986161231995\n",
      "          model: {}\n",
      "          policy_loss: -0.04813406616449356\n",
      "          total_loss: 2.714923620223999\n",
      "          vf_explained_var: 0.34053438901901245\n",
      "          vf_loss: 2.746638298034668\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.0\n",
      "    ram_util_percent: 59.26666666666668\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 39.5\n",
      "    pol2: 3.2000000000000037\n",
      "  policy_reward_mean:\n",
      "    pol1: 19.045\n",
      "    pol2: -6.864999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: 2.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.426696126703229\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23550844792273673\n",
      "    mean_inference_ms: 1.8878652816613342\n",
      "    mean_raw_obs_processing_ms: 1.7413366943710882\n",
      "  time_since_restore: 155.86916613578796\n",
      "  time_this_iter_s: 6.18759298324585\n",
      "  time_total_s: 155.86916613578796\n",
      "  timers:\n",
      "    learn_throughput: 677.791\n",
      "    learn_time_ms: 5901.524\n",
      "    sample_throughput: 4653.066\n",
      "    sample_time_ms: 859.648\n",
      "    update_time_ms: 4.239\n",
      "  timestamp: 1620237811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.4999999999999\n",
      "  episode_reward_mean: 17.93999999999995\n",
      "  episode_reward_min: -4.499999999999977\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 920\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8781508207321167\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01334398239850998\n",
      "          model: {}\n",
      "          policy_loss: -0.04129646718502045\n",
      "          total_loss: 43.166343688964844\n",
      "          vf_explained_var: 0.3794849216938019\n",
      "          vf_loss: 43.194129943847656\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9934129118919373\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010063737630844116\n",
      "          model: {}\n",
      "          policy_loss: -0.04064472019672394\n",
      "          total_loss: 4.17885160446167\n",
      "          vf_explained_var: 0.25606411695480347\n",
      "          vf_loss: 4.204212188720703\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.462500000000006\n",
      "    ram_util_percent: 58.849999999999994\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 42.5\n",
      "    pol2: 5.400000000000018\n",
      "  policy_reward_mean:\n",
      "    pol1: 23.98\n",
      "    pol2: -6.039999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -1.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42259967353430283\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23225591733547574\n",
      "    mean_inference_ms: 1.885653319944379\n",
      "    mean_raw_obs_processing_ms: 1.74430025400989\n",
      "  time_since_restore: 161.84594821929932\n",
      "  time_this_iter_s: 5.860080003738403\n",
      "  time_total_s: 161.84594821929932\n",
      "  timers:\n",
      "    learn_throughput: 690.623\n",
      "    learn_time_ms: 5791.868\n",
      "    sample_throughput: 4833.293\n",
      "    sample_time_ms: 827.593\n",
      "    update_time_ms: 3.958\n",
      "  timestamp: 1620237817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: cb9b0_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (3 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING   </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         155.869</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  12.18 </td><td style=\"text-align: right;\">                31.2</td><td style=\"text-align: right;\">                -7.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING   </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         161.846</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  17.94 </td><td style=\"text-align: right;\">                34.5</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING   </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         155.946</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  16.773</td><td style=\"text-align: right;\">                32.4</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         149.375</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  20.856</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 31.199999999999903\n",
      "  episode_reward_mean: 14.168999999999961\n",
      "  episode_reward_min: -9.299999999999978\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 920\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9342248439788818\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01682404801249504\n",
      "          model: {}\n",
      "          policy_loss: -0.05160912498831749\n",
      "          total_loss: 39.154258728027344\n",
      "          vf_explained_var: 0.4929683208465576\n",
      "          vf_loss: 39.18883514404297\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9374641180038452\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009485850110650063\n",
      "          model: {}\n",
      "          policy_loss: -0.03709360584616661\n",
      "          total_loss: 9.876279830932617\n",
      "          vf_explained_var: 0.39652493596076965\n",
      "          vf_loss: 9.898966789245605\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.425\n",
      "    ram_util_percent: 58.849999999999994\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 38.5\n",
      "    pol2: 33.99999999999996\n",
      "  policy_reward_mean:\n",
      "    pol1: 19.835\n",
      "    pol2: -5.665999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -11.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4260248638590676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23521840206641215\n",
      "    mean_inference_ms: 1.886205736138534\n",
      "    mean_raw_obs_processing_ms: 1.740184511820454\n",
      "  time_since_restore: 161.73127913475037\n",
      "  time_this_iter_s: 5.862112998962402\n",
      "  time_total_s: 161.73127913475037\n",
      "  timers:\n",
      "    learn_throughput: 692.476\n",
      "    learn_time_ms: 5776.371\n",
      "    sample_throughput: 4696.085\n",
      "    sample_time_ms: 851.773\n",
      "    update_time_ms: 4.086\n",
      "  timestamp: 1620237817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.899999999999906\n",
      "  episode_reward_mean: 17.000999999999944\n",
      "  episode_reward_min: -19.499999999999993\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 920\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9156502485275269\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013249792158603668\n",
      "          model: {}\n",
      "          policy_loss: -0.03674045205116272\n",
      "          total_loss: 37.047569274902344\n",
      "          vf_explained_var: 0.41626089811325073\n",
      "          vf_loss: 37.07089614868164\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9428904056549072\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01449282094836235\n",
      "          model: {}\n",
      "          policy_loss: -0.04163099080324173\n",
      "          total_loss: 5.309782981872559\n",
      "          vf_explained_var: 0.21564432978630066\n",
      "          vf_loss: 5.336740016937256\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.462500000000006\n",
      "    ram_util_percent: 58.849999999999994\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 39.5\n",
      "    pol2: 9.800000000000006\n",
      "  policy_reward_mean:\n",
      "    pol1: 21.49\n",
      "    pol2: -4.488999999999993\n",
      "  policy_reward_min:\n",
      "    pol1: -9.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42521842813798194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2316034888733384\n",
      "    mean_inference_ms: 1.859551020658546\n",
      "    mean_raw_obs_processing_ms: 1.7396232175510205\n",
      "  time_since_restore: 161.81592774391174\n",
      "  time_this_iter_s: 5.870258808135986\n",
      "  time_total_s: 161.81592774391174\n",
      "  timers:\n",
      "    learn_throughput: 693.564\n",
      "    learn_time_ms: 5767.309\n",
      "    sample_throughput: 4693.503\n",
      "    sample_time_ms: 852.242\n",
      "    update_time_ms: 3.96\n",
      "  timestamp: 1620237817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.69999999999991\n",
      "  episode_reward_mean: 19.532999999999944\n",
      "  episode_reward_min: -4.499999999999977\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 960\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8667417764663696\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014633800834417343\n",
      "          model: {}\n",
      "          policy_loss: -0.04037009924650192\n",
      "          total_loss: 40.7574462890625\n",
      "          vf_explained_var: 0.4470077157020569\n",
      "          vf_loss: 40.78300094604492\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9845162630081177\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010779567062854767\n",
      "          model: {}\n",
      "          policy_loss: -0.04080700874328613\n",
      "          total_loss: 3.2286715507507324\n",
      "          vf_explained_var: 0.319045752286911\n",
      "          vf_loss: 3.2531073093414307\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.9375\n",
      "    ram_util_percent: 59.0\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 46.5\n",
      "    pol2: 5.400000000000018\n",
      "  policy_reward_mean:\n",
      "    pol1: 25.76\n",
      "    pol2: -6.226999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: 2.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4192499337653299\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23086196163474845\n",
      "    mean_inference_ms: 1.8724617546682787\n",
      "    mean_raw_obs_processing_ms: 1.7313755319969517\n",
      "  time_since_restore: 167.69588541984558\n",
      "  time_this_iter_s: 5.849937200546265\n",
      "  time_total_s: 167.69588541984558\n",
      "  timers:\n",
      "    learn_throughput: 701.907\n",
      "    learn_time_ms: 5698.76\n",
      "    sample_throughput: 4924.338\n",
      "    sample_time_ms: 812.292\n",
      "    update_time_ms: 3.91\n",
      "  timestamp: 1620237823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: cb9b0_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (3 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING   </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         161.731</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  14.169</td><td style=\"text-align: right;\">                31.2</td><td style=\"text-align: right;\">                -9.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING   </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         167.696</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">  19.533</td><td style=\"text-align: right;\">                38.7</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING   </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         161.816</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  17.001</td><td style=\"text-align: right;\">                30.9</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         149.375</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  20.856</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.99999999999994\n",
      "  episode_reward_mean: 14.351999999999968\n",
      "  episode_reward_min: -9.299999999999978\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 960\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9320690631866455\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015933865681290627\n",
      "          model: {}\n",
      "          policy_loss: -0.05446206033229828\n",
      "          total_loss: 39.71845245361328\n",
      "          vf_explained_var: 0.39550215005874634\n",
      "          vf_loss: 39.75678253173828\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9317327737808228\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011245407164096832\n",
      "          model: {}\n",
      "          policy_loss: -0.03802504390478134\n",
      "          total_loss: 2.896461009979248\n",
      "          vf_explained_var: 0.3439876437187195\n",
      "          vf_loss: 2.9174070358276367\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.75\n",
      "    ram_util_percent: 59.0125\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 43.0\n",
      "    pol2: 33.99999999999996\n",
      "  policy_reward_mean:\n",
      "    pol1: 20.535\n",
      "    pol2: -6.182999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -11.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4237420843741153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23418959386733135\n",
      "    mean_inference_ms: 1.87794011619865\n",
      "    mean_raw_obs_processing_ms: 1.7315268161714135\n",
      "  time_since_restore: 167.54990100860596\n",
      "  time_this_iter_s: 5.818621873855591\n",
      "  time_total_s: 167.54990100860596\n",
      "  timers:\n",
      "    learn_throughput: 704.589\n",
      "    learn_time_ms: 5677.065\n",
      "    sample_throughput: 4773.484\n",
      "    sample_time_ms: 837.962\n",
      "    update_time_ms: 3.95\n",
      "  timestamp: 1620237823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: cb9b0_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.6999999999999\n",
      "  episode_reward_mean: 17.92199999999994\n",
      "  episode_reward_min: -19.499999999999993\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 960\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9026077389717102\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013328159227967262\n",
      "          model: {}\n",
      "          policy_loss: -0.0423901192843914\n",
      "          total_loss: 39.8502197265625\n",
      "          vf_explained_var: 0.43492740392684937\n",
      "          vf_loss: 39.879112243652344\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9386177062988281\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014022335410118103\n",
      "          model: {}\n",
      "          policy_loss: -0.04124141484498978\n",
      "          total_loss: 3.236349105834961\n",
      "          vf_explained_var: 0.22236835956573486\n",
      "          vf_loss: 3.263392925262451\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.233333333333334\n",
      "    ram_util_percent: 59.01111111111111\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 40.5\n",
      "    pol2: 9.800000000000006\n",
      "  policy_reward_mean:\n",
      "    pol1: 22.785\n",
      "    pol2: -4.86299999999999\n",
      "  policy_reward_min:\n",
      "    pol1: -9.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42267290388588724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23057002043214161\n",
      "    mean_inference_ms: 1.8511429752121311\n",
      "    mean_raw_obs_processing_ms: 1.7317686631724525\n",
      "  time_since_restore: 167.6696925163269\n",
      "  time_this_iter_s: 5.853764772415161\n",
      "  time_total_s: 167.6696925163269\n",
      "  timers:\n",
      "    learn_throughput: 704.348\n",
      "    learn_time_ms: 5679.013\n",
      "    sample_throughput: 4746.865\n",
      "    sample_time_ms: 842.661\n",
      "    update_time_ms: 3.807\n",
      "  timestamp: 1620237823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.79999999999991\n",
      "  episode_reward_mean: 14.312999999999958\n",
      "  episode_reward_min: -3.8999999999999844\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8857129812240601\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015446474775671959\n",
      "          model: {}\n",
      "          policy_loss: -0.05040998384356499\n",
      "          total_loss: 34.97151184082031\n",
      "          vf_explained_var: 0.44016268849372864\n",
      "          vf_loss: 35.00627899169922\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.907407820224762\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01120966486632824\n",
      "          model: {}\n",
      "          policy_loss: -0.04969033598899841\n",
      "          total_loss: 2.763144016265869\n",
      "          vf_explained_var: 0.34949034452438354\n",
      "          vf_loss: 2.795809507369995\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.5625\n",
      "    ram_util_percent: 59.262499999999996\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 44.5\n",
      "    pol2: 33.99999999999996\n",
      "  policy_reward_mean:\n",
      "    pol1: 20.815\n",
      "    pol2: -6.5019999999999865\n",
      "  policy_reward_min:\n",
      "    pol1: -11.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4202948680085953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23255178635408888\n",
      "    mean_inference_ms: 1.864440110083037\n",
      "    mean_raw_obs_processing_ms: 1.7174224734429817\n",
      "  time_since_restore: 172.89035415649414\n",
      "  time_this_iter_s: 5.340453147888184\n",
      "  time_total_s: 172.89035415649414\n",
      "  timers:\n",
      "    learn_throughput: 725.074\n",
      "    learn_time_ms: 5516.681\n",
      "    sample_throughput: 4870.889\n",
      "    sample_time_ms: 821.205\n",
      "    update_time_ms: 3.952\n",
      "  timestamp: 1620237828\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: cb9b0_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (3 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING   </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         172.89 </td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  14.313</td><td style=\"text-align: right;\">                34.8</td><td style=\"text-align: right;\">                -3.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>RUNNING   </td><td>192.168.0.100:80485</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         167.696</td><td style=\"text-align: right;\"> 96000</td><td style=\"text-align: right;\">  19.533</td><td style=\"text-align: right;\">                38.7</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING   </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         167.67 </td><td style=\"text-align: right;\"> 96000</td><td style=\"text-align: right;\">  17.922</td><td style=\"text-align: right;\">                32.7</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         149.375</td><td style=\"text-align: right;\"> 84000</td><td style=\"text-align: right;\">  20.856</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00002:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-48\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.69999999999991\n",
      "  episode_reward_mean: 20.396999999999938\n",
      "  episode_reward_min: -4.499999999999977\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1000\n",
      "  experiment_id: d7bdf14eedaa445a9267af681b70299f\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8556892275810242\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014373543672263622\n",
      "          model: {}\n",
      "          policy_loss: -0.04934139922261238\n",
      "          total_loss: 42.87417221069336\n",
      "          vf_explained_var: 0.4056709408760071\n",
      "          vf_loss: 42.90896224975586\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9665523767471313\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010533567517995834\n",
      "          model: {}\n",
      "          policy_loss: -0.048322685062885284\n",
      "          total_loss: 3.3069653511047363\n",
      "          vf_explained_var: 0.25525960326194763\n",
      "          vf_loss: 3.3392906188964844\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.175\n",
      "    ram_util_percent: 59.262499999999996\n",
      "  pid: 80485\n",
      "  policy_reward_max:\n",
      "    pol1: 46.5\n",
      "    pol2: 5.400000000000016\n",
      "  policy_reward_mean:\n",
      "    pol1: 26.855\n",
      "    pol2: -6.4579999999999895\n",
      "  policy_reward_min:\n",
      "    pol1: 2.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4162337063808937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2294540110110643\n",
      "    mean_inference_ms: 1.8597081767566706\n",
      "    mean_raw_obs_processing_ms: 1.7196068131240134\n",
      "  time_since_restore: 173.13775753974915\n",
      "  time_this_iter_s: 5.4418721199035645\n",
      "  time_total_s: 173.13775753974915\n",
      "  timers:\n",
      "    learn_throughput: 722.682\n",
      "    learn_time_ms: 5534.938\n",
      "    sample_throughput: 5019.898\n",
      "    sample_time_ms: 796.829\n",
      "    update_time_ms: 3.862\n",
      "  timestamp: 1620237828\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: cb9b0_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.19999999999991\n",
      "  episode_reward_mean: 18.800999999999934\n",
      "  episode_reward_min: -3.2999999999999807\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8716104030609131\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013419192284345627\n",
      "          model: {}\n",
      "          policy_loss: -0.046382054686546326\n",
      "          total_loss: 36.50579833984375\n",
      "          vf_explained_var: 0.41815435886383057\n",
      "          vf_loss: 36.53858947753906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8897809982299805\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014677444472908974\n",
      "          model: {}\n",
      "          policy_loss: -0.04208814352750778\n",
      "          total_loss: 7.127830982208252\n",
      "          vf_explained_var: 0.16404138505458832\n",
      "          vf_loss: 7.15505838394165\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.25714285714286\n",
      "    ram_util_percent: 59.285714285714285\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 40.5\n",
      "    pol2: 14.200000000000008\n",
      "  policy_reward_mean:\n",
      "    pol1: 23.29\n",
      "    pol2: -4.488999999999991\n",
      "  policy_reward_min:\n",
      "    pol1: 4.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4194383830805027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22914474892722694\n",
      "    mean_inference_ms: 1.839208373131835\n",
      "    mean_raw_obs_processing_ms: 1.7200444662019916\n",
      "  time_since_restore: 173.01090955734253\n",
      "  time_this_iter_s: 5.341217041015625\n",
      "  time_total_s: 173.01090955734253\n",
      "  timers:\n",
      "    learn_throughput: 725.401\n",
      "    learn_time_ms: 5514.192\n",
      "    sample_throughput: 4897.459\n",
      "    sample_time_ms: 816.75\n",
      "    update_time_ms: 3.626\n",
      "  timestamp: 1620237828\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 40.499999999999915\n",
      "  episode_reward_mean: 16.64999999999995\n",
      "  episode_reward_min: -11.699999999999987\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1080\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8701527118682861\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014820693992078304\n",
      "          model: {}\n",
      "          policy_loss: -0.043136268854141235\n",
      "          total_loss: 44.36013412475586\n",
      "          vf_explained_var: 0.4494297504425049\n",
      "          vf_loss: 44.38826370239258\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9015394449234009\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010131261311471462\n",
      "          model: {}\n",
      "          policy_loss: -0.048981983214616776\n",
      "          total_loss: 2.848602294921875\n",
      "          vf_explained_var: 0.3781311511993408\n",
      "          vf_loss: 2.882197380065918\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.49999999999999\n",
      "    ram_util_percent: 57.666666666666664\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 50.5\n",
      "    pol2: 4.299999999999999\n",
      "  policy_reward_mean:\n",
      "    pol1: 22.91\n",
      "    pol2: -6.259999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -5.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4127569186236428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22880084620308666\n",
      "    mean_inference_ms: 1.8330368294340735\n",
      "    mean_raw_obs_processing_ms: 1.6882943720062173\n",
      "  time_since_restore: 181.94822025299072\n",
      "  time_this_iter_s: 4.5257179737091064\n",
      "  time_total_s: 181.94822025299072\n",
      "  timers:\n",
      "    learn_throughput: 789.965\n",
      "    learn_time_ms: 5063.514\n",
      "    sample_throughput: 5078.706\n",
      "    sample_time_ms: 787.602\n",
      "    update_time_ms: 3.349\n",
      "  timestamp: 1620237837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: cb9b0_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING   </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         181.948</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  16.65 </td><td style=\"text-align: right;\">                40.5</td><td style=\"text-align: right;\">               -11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING   </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         177.532</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">  18.972</td><td style=\"text-align: right;\">                37.8</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         149.375</td><td style=\"text-align: right;\"> 84000</td><td style=\"text-align: right;\">  20.856</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         173.138</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  20.397</td><td style=\"text-align: right;\">                38.7</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-03-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 41.099999999999916\n",
      "  episode_reward_mean: 19.226999999999936\n",
      "  episode_reward_min: -5.699999999999984\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1080\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8516989946365356\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013366274535655975\n",
      "          model: {}\n",
      "          policy_loss: -0.03211168944835663\n",
      "          total_loss: 45.97438049316406\n",
      "          vf_explained_var: 0.40258970856666565\n",
      "          vf_loss: 45.992958068847656\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.875889241695404\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015602415427565575\n",
      "          model: {}\n",
      "          policy_loss: -0.04990646615624428\n",
      "          total_loss: 4.202436447143555\n",
      "          vf_explained_var: 0.22588451206684113\n",
      "          vf_loss: 4.236545562744141\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.4\n",
      "    ram_util_percent: 57.73333333333333\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 45.5\n",
      "    pol2: 49.39999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: 23.21\n",
      "    pol2: -3.982999999999992\n",
      "  policy_reward_min:\n",
      "    pol1: -26.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4130875594045993\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2261555044047055\n",
      "    mean_inference_ms: 1.817775855619728\n",
      "    mean_raw_obs_processing_ms: 1.6965808383285337\n",
      "  time_since_restore: 182.15787720680237\n",
      "  time_this_iter_s: 4.625988721847534\n",
      "  time_total_s: 182.15787720680237\n",
      "  timers:\n",
      "    learn_throughput: 791.214\n",
      "    learn_time_ms: 5055.52\n",
      "    sample_throughput: 5120.373\n",
      "    sample_time_ms: 781.193\n",
      "    update_time_ms: 3.323\n",
      "  timestamp: 1620237838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-04-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.199999999999896\n",
      "  episode_reward_mean: 16.403999999999954\n",
      "  episode_reward_min: -11.699999999999987\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1160\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8275084495544434\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014963807538151741\n",
      "          model: {}\n",
      "          policy_loss: -0.04145985096693039\n",
      "          total_loss: 45.24787521362305\n",
      "          vf_explained_var: 0.3947610855102539\n",
      "          vf_loss: 45.2741813659668\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8807225227355957\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009834867902100086\n",
      "          model: {}\n",
      "          policy_loss: -0.04141034930944443\n",
      "          total_loss: 4.276864051818848\n",
      "          vf_explained_var: 0.3325343132019043\n",
      "          vf_loss: 4.303338050842285\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.2\n",
      "    ram_util_percent: 58.912499999999994\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 45.0\n",
      "    pol2: 6.50000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: 22.455\n",
      "    pol2: -6.050999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -5.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40494588045225755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22493548466730787\n",
      "    mean_inference_ms: 1.807325743343413\n",
      "    mean_raw_obs_processing_ms: 1.658430176832094\n",
      "  time_since_restore: 191.94215321540833\n",
      "  time_this_iter_s: 5.41618800163269\n",
      "  time_total_s: 191.94215321540833\n",
      "  timers:\n",
      "    learn_throughput: 848.491\n",
      "    learn_time_ms: 4714.251\n",
      "    sample_throughput: 5329.662\n",
      "    sample_time_ms: 750.517\n",
      "    update_time_ms: 3.236\n",
      "  timestamp: 1620237848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: cb9b0_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING   </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         191.942</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  16.404</td><td style=\"text-align: right;\">                37.2</td><td style=\"text-align: right;\">               -11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>RUNNING   </td><td>192.168.0.100:80482</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         186.684</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">  19.755</td><td style=\"text-align: right;\">                41.1</td><td style=\"text-align: right;\">                -5.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         149.375</td><td style=\"text-align: right;\"> 84000</td><td style=\"text-align: right;\">  20.856</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         173.138</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  20.397</td><td style=\"text-align: right;\">                38.7</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00003:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-04-08\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.59999999999991\n",
      "  episode_reward_mean: 20.477999999999934\n",
      "  episode_reward_min: -5.699999999999984\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1160\n",
      "  experiment_id: 1af777aec03e4ac1950c31804df8af64\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8317999839782715\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013249723240733147\n",
      "          model: {}\n",
      "          policy_loss: -0.037187956273555756\n",
      "          total_loss: 40.497474670410156\n",
      "          vf_explained_var: 0.3932393789291382\n",
      "          vf_loss: 40.52124786376953\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8234711289405823\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01284693367779255\n",
      "          model: {}\n",
      "          policy_loss: -0.04130464792251587\n",
      "          total_loss: 8.708199501037598\n",
      "          vf_explained_var: 0.22080054879188538\n",
      "          vf_loss: 8.73649787902832\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.375\n",
      "    ram_util_percent: 58.89999999999999\n",
      "  pid: 80482\n",
      "  policy_reward_max:\n",
      "    pol1: 43.0\n",
      "    pol2: 20.799999999999983\n",
      "  policy_reward_mean:\n",
      "    pol1: 23.295\n",
      "    pol2: -2.816999999999993\n",
      "  policy_reward_min:\n",
      "    pol1: 0.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4057181377176596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22244973212141758\n",
      "    mean_inference_ms: 1.7924033130631911\n",
      "    mean_raw_obs_processing_ms: 1.6683980838104122\n",
      "  time_since_restore: 192.10586023330688\n",
      "  time_this_iter_s: 5.421370029449463\n",
      "  time_total_s: 192.10586023330688\n",
      "  timers:\n",
      "    learn_throughput: 848.254\n",
      "    learn_time_ms: 4715.571\n",
      "    sample_throughput: 5270.281\n",
      "    sample_time_ms: 758.973\n",
      "    update_time_ms: 3.315\n",
      "  timestamp: 1620237848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: cb9b0_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-04-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.399999999999906\n",
      "  episode_reward_mean: 17.603999999999953\n",
      "  episode_reward_min: -7.499999999999979\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1240\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8071858286857605\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014799578115344048\n",
      "          model: {}\n",
      "          policy_loss: -0.04358407482504845\n",
      "          total_loss: 53.48197937011719\n",
      "          vf_explained_var: 0.40865200757980347\n",
      "          vf_loss: 53.51057434082031\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.862085223197937\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009601067751646042\n",
      "          model: {}\n",
      "          policy_loss: -0.04446053132414818\n",
      "          total_loss: 2.8492119312286377\n",
      "          vf_explained_var: 0.29511845111846924\n",
      "          vf_loss: 2.8790907859802246\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.099999999999998\n",
      "    ram_util_percent: 58.31666666666667\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 46.0\n",
      "    pol2: 12.000000000000012\n",
      "  policy_reward_mean:\n",
      "    pol1: 23.545\n",
      "    pol2: -5.940999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -1.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39798081619732545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22137040304783676\n",
      "    mean_inference_ms: 1.7859693050307146\n",
      "    mean_raw_obs_processing_ms: 1.6302758826147266\n",
      "  time_since_restore: 199.59233713150024\n",
      "  time_this_iter_s: 3.5488007068634033\n",
      "  time_total_s: 199.59233713150024\n",
      "  timers:\n",
      "    learn_throughput: 973.422\n",
      "    learn_time_ms: 4109.213\n",
      "    sample_throughput: 5754.473\n",
      "    sample_time_ms: 695.111\n",
      "    update_time_ms: 2.879\n",
      "  timestamp: 1620237855\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: cb9b0_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>RUNNING   </td><td>192.168.0.100:80483</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         199.592</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  17.604</td><td style=\"text-align: right;\">                38.4</td><td style=\"text-align: right;\">                -7.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         149.375</td><td style=\"text-align: right;\"> 84000</td><td style=\"text-align: right;\">  20.856</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         173.138</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  20.397</td><td style=\"text-align: right;\">                38.7</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         192.106</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  20.478</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                -5.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_cb9b0_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-05_20-04-22\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 43.79999999999991\n",
      "  episode_reward_mean: 21.317999999999934\n",
      "  episode_reward_min: -7.79999999999999\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1320\n",
      "  experiment_id: 476320c3f885462b9e54ae95d0c60e70\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7630941271781921\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014042488299310207\n",
      "          model: {}\n",
      "          policy_loss: -0.04581364244222641\n",
      "          total_loss: 56.02145004272461\n",
      "          vf_explained_var: 0.38919752836227417\n",
      "          vf_loss: 56.05304718017578\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8525566458702087\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010124243795871735\n",
      "          model: {}\n",
      "          policy_loss: -0.041061822324991226\n",
      "          total_loss: 2.543821334838867\n",
      "          vf_explained_var: 0.35186871886253357\n",
      "          vf_loss: 2.569506883621216\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.839999999999996\n",
      "    ram_util_percent: 58.3\n",
      "  pid: 80483\n",
      "  policy_reward_max:\n",
      "    pol1: 50.5\n",
      "    pol2: 8.700000000000015\n",
      "  policy_reward_mean:\n",
      "    pol1: 27.105\n",
      "    pol2: -5.78699999999999\n",
      "  policy_reward_min:\n",
      "    pol1: 0.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38946862743723815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2168448243148719\n",
      "    mean_inference_ms: 1.7447895051002786\n",
      "    mean_raw_obs_processing_ms: 1.5958451958560187\n",
      "  time_since_restore: 206.23058319091797\n",
      "  time_this_iter_s: 3.3306779861450195\n",
      "  time_total_s: 206.23058319091797\n",
      "  timers:\n",
      "    learn_throughput: 1095.851\n",
      "    learn_time_ms: 3650.132\n",
      "    sample_throughput: 6414.788\n",
      "    sample_time_ms: 623.559\n",
      "    update_time_ms: 2.606\n",
      "  timestamp: 1620237862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: cb9b0_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         206.231</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  21.318</td><td style=\"text-align: right;\">                43.8</td><td style=\"text-align: right;\">                -7.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         149.375</td><td style=\"text-align: right;\"> 84000</td><td style=\"text-align: right;\">  20.856</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         173.138</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  20.397</td><td style=\"text-align: right;\">                38.7</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         192.106</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  20.478</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                -5.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.73 GiB heap, 0.0/2.36 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         206.231</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  21.318</td><td style=\"text-align: right;\">                43.8</td><td style=\"text-align: right;\">                -7.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         149.375</td><td style=\"text-align: right;\"> 84000</td><td style=\"text-align: right;\">  20.856</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         173.138</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  20.397</td><td style=\"text-align: right;\">                38.7</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_cb9b0_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         192.106</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  20.478</td><td style=\"text-align: right;\">                39.6</td><td style=\"text-align: right;\">                -5.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m 2021-05-05 20:04:22,826\tERROR worker.py:395 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 566, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1077, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 42, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/json/__init__.py\", line 231, in dumps\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m     return _default_encoder.encode(obj)\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/json/encoder.py\", line 199, in encode\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m     chunks = self.iterencode(o, _one_shot=True)\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/json/encoder.py\", line 257, in iterencode\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m     return _iterencode(o, 0)\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 392, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m 2021-05-05 20:04:22,826\tERROR worker.py:395 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 566, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1077, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 42, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/json/__init__.py\", line 231, in dumps\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m     return _default_encoder.encode(obj)\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/json/encoder.py\", line 199, in encode\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m     chunks = self.iterencode(o, _one_shot=True)\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/json/encoder.py\", line 257, in iterencode\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m     return _iterencode(o, 0)\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 392, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=80483)\u001b[0m SystemExit: 1\n",
      "2021-05-05 20:04:22,938\tINFO tune.py:549 -- Total run time: 225.86 seconds (225.50 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fd129c4a880>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution to Exercise #2\n",
    "# !LIVE CODING!\n",
    "# Solution to Exercise #2:\n",
    "\n",
    "# Update our config and set it up for 2x tune grid-searches (leading to 4 parallel trials in total).\n",
    "config.update({\n",
    "    \"lr\": tune.grid_search([0.0001, 0.0005]),\n",
    "    \"train_batch_size\": tune.grid_search([2000, 3000]),\n",
    "    \"num_envs_per_worker\": 10,\n",
    "    # Change our model to be simpler.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [128, 128],\n",
    "    },\n",
    "})\n",
    "\n",
    "# Run the experiment.\n",
    "tune.run(\"PPO\", config=config, stop={\"episode_reward_mean\": 15.0, \"training_iteration\": 100})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anyscale's Infinite laptop:\n",
    "\n",
    "# NOTE: The following cell will only work if you are already on-boarded to our Anyscale Inc. \"Infinite Laptop\".\n",
    "# To get more information, see https://www.anyscale.com/product\n",
    "\n",
    "# Let's quickly divert from our MultiAgentArena and move to something much heavier in terms of environment/simulator complexity.\n",
    "# We will now demonstrate, how you can use Anyscale's infinite laptop to launch an RLlib experiment on a cloud 4 GPU + 32 CPU machine\n",
    "# all from within this Jupyter cell here.\n",
    "# Start an experiment in the cloud using Anyscale's product, RLlib, and a more complex multi-agent env.\n",
    "\n",
    "# NOTE \n",
    "import anyscale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5516d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Neural Network Models.\n",
    "# \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                input_space,\n",
    "                action_space,\n",
    "                num_outputs,\n",
    "                name=\"\",\n",
    "                *,\n",
    "                layers = (256, 256)):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense_layers = []\n",
    "        for i, layer_size in enumerate(layers):\n",
    "            self.dense_layers.append(tf.keras.layers.Dense(\n",
    "                layer_size, activation=tf.nn.relu, name=f\"dense_{i}\"))\n",
    "\n",
    "        self.logits = tf.keras.layers.Dense(\n",
    "            num_outputs,\n",
    "            activation=tf.keras.activations.linear,\n",
    "            name=\"logits\")\n",
    "        self.values = tf.keras.layers.Dense(\n",
    "            1, activation=None, name=\"values\")\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # Standardized input args:\n",
    "        # - input_dict (RLlib `SampleBatch` object, which is basically a dict with numpy arrays\n",
    "        # in it)\n",
    "        out = inputs[\"obs\"]\n",
    "        for l in self.dense_layers:\n",
    "            out = l(out)\n",
    "        logits = self.logits(out)\n",
    "        values = self.values(out)\n",
    "\n",
    "        # Standardized output:\n",
    "        # - \"normal\" model output tensor (e.g. action logits).\n",
    "        # - list of internal state outputs (only needed for RNN-/memory enhanced models).\n",
    "        # - \"extra outs\", such as model's side branches, e.g. value function outputs.\n",
    "        return logits, [], {\"vf_preds\": tf.reshape(values, [-1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "controversial-repair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'my_model/logits/BiasAdd:0' shape=(1, 2) dtype=float64>,\n",
       " [],\n",
       " {'vf_preds': <tf.Tensor 'my_model/Reshape:0' shape=(1,) dtype=float64>})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do a quick test on the custom model class.\n",
    "from gym.spaces import Box\n",
    "test_model = MyModel(\n",
    "    input_space=Box(-1.0, 1.0, (2, )),\n",
    "    action_space=None,\n",
    "    num_outputs=2,\n",
    ")\n",
    "test_model({\"obs\": np.array([[0.5, 0.5]])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our custom model and re-run the experiment.\n",
    "\n",
    "config.update({\n",
    "    \"model\": {\n",
    "        \"custom_model\": MyModel,\n",
    "        \"custom_model_config\": {\n",
    "            \"layers\": [128, 128],\n",
    "        },\n",
    "    },\n",
    "    # Revert these to single trials (and use those hyperparams that performed well in our Exercise #2).\n",
    "    \"lr\": 0.0005,\n",
    "    \"train_batch_size\": 2000,\n",
    "})\n",
    "\n",
    "tune.run(\"PPO\", config=config, stop=stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Hacking in\": How do we customize our RL loop?\n",
    "# RLlib offers a callbacks API that allows you to add custom behavior at\n",
    "# all major events during the environment sampling and learning process.\n",
    "\n",
    "# Our problem: So far, we can only see the total reward (sum for both agents).\n",
    "# This does not give us enough insights into the question of which agent\n",
    "# learns what (maybe agent2 doesn't learn anything and the reward we are observing\n",
    "# is mostly due to agent1's progress in covering the map!).\n",
    "# The following custom callbacks class allows us to add each agents single reward to\n",
    "# the returned metrics, which will then be displayed in tensorboard.\n",
    "\n",
    "# We will override RLlib's DefaultCallbacks class and implement the\n",
    "# `on_episode_step` and `on_episode_end` methods therein.\n",
    "\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(self, *, worker, base_env,\n",
    "                         policies, episode,\n",
    "                         env_index, **kwargs):\n",
    "        episode.user_data[\"agent1_rewards\"] = []\n",
    "        episode.user_data[\"agent2_rewards\"] = []\n",
    "\n",
    "    def on_episode_step(self, *, worker, base_env,\n",
    "                        episode, env_index, **kwargs):\n",
    "        # Make sure this episode is ongoing.\n",
    "        #assert episode.length > 0, \\\n",
    "        #    \"ERROR: `on_episode_step()` callback should not be called right \" \\\n",
    "        #    \"after env reset!\"\n",
    "        ag1_r = episode.prev_reward_for(\"agent1\")\n",
    "        ag2_r = episode.prev_reward_for(\"agent2\")\n",
    "        #print(\"ag1_r={} ag2_r={}\".format(ag1_r, ag2_r))\n",
    "        episode.user_data[\"agent1_rewards\"].append(ag1_r)\n",
    "        episode.user_data[\"agent2_rewards\"].append(ag2_r)\n",
    "\n",
    "    def on_episode_end(self, *, worker, base_env,\n",
    "                       policies, episode,\n",
    "                       env_index, **kwargs):\n",
    "        episode.custom_metrics[\"ag1_R\"] = sum(episode.user_data[\"agent1_rewards\"])\n",
    "        episode.custom_metrics[\"ag2_R\"] = sum(episode.user_data[\"agent2_rewards\"])\n",
    "        episode.hist_data[\"agent1_rewards\"] = episode.user_data[\"agent1_rewards\"]\n",
    "        episode.hist_data[\"agent2_rewards\"] = episode.user_data[\"agent2_rewards\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2fe8eb-c52f-4a26-9067-96ad9fe160a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our config to point to our new custom callbacks class:\n",
    "config.update({\n",
    "    \"env\": MultiAgentArena,  # force \"reload\"\n",
    "    \"callbacks\": MyCallbacks,  # by default, this would point to `rllib.agents.callbacks.DefaultCallbacks`, which does nothing.\n",
    "    #TODO: remove this once native keras models are supported!\n",
    "    \"model\": {\n",
    "        \"custom_model\": None,\n",
    "    },\n",
    "})\n",
    "\n",
    "results = tune.run(\"PPO\", config=config, stop={\"training_iteration\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa6a24",
   "metadata": {},
   "source": [
    "### Let's check tensorboard for the new custom metrics!\n",
    "\n",
    "1. Head over to ~/ray_results/PPO/PPO_MultiAgentArena_[some key]_00000_0_[date]_[time]/\n",
    "1. In that directory, you should see a `event.out....` file.\n",
    "1. Run `tensorboard --logdir .` and head to https://localhost:6006\n",
    "\n",
    "<img src=\"images/tensorboard.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd73713f-581a-493d-9198-3c9a98109176",
   "metadata": {},
   "source": [
    "## Exercise No 3\n",
    "\n",
    "<hr />\n",
    "\n",
    "Assume we would like to know exactly how much (new) ground agent1 \n",
    "covers on average in an episode.\n",
    "Write your own custom callback class (sub-class\n",
    "ray.rllib.agents.callback::DefaultCallbacks) and override one or more methods\n",
    "therein to collect the following data:\n",
    "- The number of (unique) fields agent1 has covered in an episode. Try to get \n",
    "- The number of times agent2 has blocked agent1.\n",
    "\n",
    "Run a simple experiment using tune.run (and your custom callbacks class)\n",
    "and confirm the new metric shows up in tensorboard.\n",
    "\n",
    "Hints:\n",
    "\n",
    "To get the last reward for an agent, use `episode.prev_reward_for([agent-name])`.\n",
    "\n",
    "To get the current observation for an agent, use `episode.last_raw_obs_for([agent-name])`.\n",
    "\n",
    "**Good luck! :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d904ad01-7d6c-43c4-aa89-2bf1ff58e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercise #3\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "class MyCallback(DefaultCallbacks):\n",
    "    def on_episode_start(self, *, worker, base_env,\n",
    "                         policies, episode,\n",
    "                         env_index, **kwargs):\n",
    "        # Set per-episode object to capture, which states (observations)\n",
    "        # have been visited by agent1.\n",
    "        episode.user_data[\"ground_covered\"] = set()\n",
    "        # Set per-episode agent2-blocks counter (how many times has agent2 blocked agent1?).\n",
    "        episode.user_data[\"num_blocks\"] = 0\n",
    "\n",
    "    def on_episode_step(self, *, worker, base_env,\n",
    "                        episode, env_index, **kwargs):\n",
    "        # Add agent1's observation to our set of unique observations.\n",
    "        ag1_obs = episode.last_raw_obs_for(\"agent1\")\n",
    "        episode.user_data[\"ground_covered\"].add(ag1_obs)\n",
    "        # If agent2's reward > 0.0, it means she has blocked agent1.\n",
    "        ag2_r = episode.prev_reward_for(\"agent2\")\n",
    "        if ag2_r > 0.0:\n",
    "            episode.user_data[\"num_blocks\"] += 1\n",
    "\n",
    "    def on_episode_end(self, *, worker, base_env,\n",
    "                       policies, episode,\n",
    "                       env_index, **kwargs):\n",
    "        # Reset everything.\n",
    "        episode.user_data[\"ground_covered\"] = set()\n",
    "        episode.user_data[\"num_blocks\"] = 0\n",
    "\n",
    "\n",
    "\n",
    "ray.init()\n",
    "\n",
    "stop = {\"training_iteration\": 10}\n",
    "# Specify env and custom callbacks in our config (leave everything else\n",
    "# as-is (defaults)).\n",
    "config = {\n",
    "    \"env\": MultiAgentArena,\n",
    "    \"callbacks\": MyCallback,\n",
    "}\n",
    "\n",
    "# Run for a few iterations.\n",
    "tune.run(\"PPO\", stop=stop, config=config)\n",
    "\n",
    "# Check tensorboard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85ad95",
   "metadata": {},
   "source": [
    "### A closer look at RLlib's APIs and structure\n",
    "\n",
    "We already took a quick look inside an RLlib Trainer object and extracted its Policy(ies) and the Policy's model (neural network). Here is a much more detailed overview of what's inside a Trainer object.\n",
    "\n",
    "At the core is the so-called `WorkerSet` sitting under `Trainer.workers`. A WorkerSet is a group of `RolloutWorker` (`rllib.evaluation.rollout_worker.py`) objects that always consists of a \"local worker\" (`Trainer.workers.local_worker()`) and n \"remote workers\" (`Trainer.workers.remote_workers()`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f37549",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_structure.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d72883",
   "metadata": {},
   "source": [
    "### Scaling RLlib\n",
    "\n",
    "Scaling RLlib works by parallelizing the \"jobs\" that the remote `RolloutWorkers` do. In a vanilla RL algorithm, like PPO, DQN, and many others, the `@ray.remote` labeled RolloutWorkers in the figure above are responsible for interacting with one or more environments and thereby collecting experiences. Observations are produced by the environment, actions are then computed by the Policy(ies) copy located on the remote worker and sent to the environment in order to produce yet another observation. This cycle is repeated endlessly and only sometimes interrupted to send experience batches (\"train batches\") of a certain size to the \"local worker\". There these batches are used to call `Policy.learn_on_batch()`, which performs a loss calculation, followed by a model weights update, and a subsequent weights broadcast back to all the remote workers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29328f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
