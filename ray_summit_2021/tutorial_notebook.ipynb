{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "# Hands-on RL with Ray’s RLlib\n",
    "## A beginner’s tutorial for working with multi-agent environments, models, and algorithms\n",
    "\n",
    "<img src=\"images/pitfall.jpg\" width=250> <img src=\"images/tesla.jpg\" width=254> <img src=\"images/forklifts.jpg\" width=169> <img src=\"images/robots.jpg\" width=252> <img src=\"images/dota2.jpg\" width=213>\n",
    "\n",
    "### Overview\n",
    "“Hands-on RL with Ray’s RLlib” is a beginners tutorial for working with reinforcement learning (RL) environments, models, and algorithms using Ray’s RLlib library. RLlib offers high scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL) before proceeding to RLlib (multi- and single-agent) environments, neural network models, hyperparameter tuning, debugging, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo.\n",
    "\n",
    "### Intended Audience\n",
    "* Python programmers who want to get started with reinforcement learning and RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Linux):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install ray[rllib]\n",
    "$ pip install tensorflow  # <- either one works!\n",
    "$ pip install torch  # <- either one works!\n",
    "$ pip install jupyterlab\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install cmake \"ray[rllib]\"\n",
    "$ pip install tensorflow  # <- either one works!\n",
    "$ pip install torch  # <- either one works!\n",
    "$ pip install jupyterlab\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install ray[rllib]\n",
    "$ pip install [tensorflow|torch]  # <- either one works!\n",
    "$ pip install jupyterlab\n",
    "$ conda install pywin32\n",
    "```\n",
    "\n",
    "Also, for Win10 Atari support, we have to install atari_py from a different source (gym does not support Atari envs on Windows).\n",
    "\n",
    "```\n",
    "$ pip install git+https://github.com/Kojoley/atari-py.git\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and why RLlib?\n",
    "* Core concepts of RLlib: Environments, Trainers, Policies, and Models.\n",
    "* How to configure, hyperparameter-tune, and parallelize RLlib.\n",
    "* RLlib debugging best practices.\n",
    "\n",
    "### Tutorial Outline\n",
    "1. RL and RLlib in a nutshell.\n",
    "1. Defining an RL-solvable problem: Our first environment.\n",
    "1. **Exercise No.1**: Environment loop.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. Picking an algorithm and training our first RLlib Trainer.\n",
    "1. Configurations and hyperparameters - Easy tuning with Ray Tune.\n",
    "1. Fixing our experiment's config - Going multi-agent.\n",
    "1. The \"infinite laptop\": Quick intro into how to use RLlib with Anyscale's product.\n",
    "1. **Exercise No.2**: Run your own Ray RLlib+Tune experiment)\n",
    "1. Neural network models - Provide your custom models using tf.keras or torch.nn.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. Deeper dive into RLlib's parallelization architecture.\n",
    "1. Specifying different compute resources and parallelization options through our config.\n",
    "1. \"Hacking in\": Using callbacks to customize the RL loop and generate our own metrics.\n",
    "1. **Exercise No.3**: Write your own custom callback.\n",
    "1. \"Hacking in (part II)\" - Debugging with RLlib and PyCharm.\n",
    "1. Checking on the \"infinite laptop\" - Did RLlib learn to solve the problem?\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-yorkshire",
   "metadata": {},
   "source": [
    "<img src=\"images/rl-cycle.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62744730",
   "metadata": {},
   "source": [
    "### Coding/defining our \"problem\" via an RL environment.\n",
    "\n",
    "We will use the following (adversarial) multi-agent environment\n",
    "throughout this tutorial to demonstrate a large fraction of RLlib's\n",
    "APIs, features, and customization options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb35116-efda-4799-8bae-e96d7775a0d1",
   "metadata": {},
   "source": [
    "<img src=\"images/environment.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe753-d7e0-4de1-b937-160507f75ed8",
   "metadata": {},
   "source": [
    "### A word or two on Spaces:\n",
    "\n",
    "Spaces are used in ML to describe what possible/valid values inputs and outputs of a neural network can have.\n",
    "\n",
    "RL environments also use them to describe what their valid observations and actions are.\n",
    "\n",
    "Spaces are usually defined by their shape (e.g. 84x84x3 RGB images) and datatype (e.g. uint8 for RGB values between 0 and 255).\n",
    "However, spaces could also be composed of other spaces (see Tuple or Dict spaces) or could be simply discrete with n fixed possible values\n",
    "(represented by integers). For example, in our game, where each agent can only go up/down/left/right, the action space would be \"Discrete(4)\"\n",
    "(no datatype, no shape needs to be defined here)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e4135-98ed-4e65-9e26-66f340747529",
   "metadata": {},
   "source": [
    "<img src=\"images/spaces.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6925507-0210-49d2-9e68-5d1e1157ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________\n",
      "|.         |\n",
      "|1         |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|         2|\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "\n",
      "R1= 1.0\n",
      "R2=-0.1\n",
      "\n",
      "Agent1's x/y position=[1, 0]\n",
      "Agent2's x/y position=[8, 9]\n",
      "Env timesteps=1\n"
     ]
    }
   ],
   "source": [
    "# Let's code (parts of) our multi-agent environment.\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "class MultiAgentArena(MultiAgentEnv):\n",
    "    def __init__(self, config=None):\n",
    "        config = config or {}\n",
    "        # Dimensions of the grid.\n",
    "        self.width = config.get(\"width\", 10)\n",
    "        self.height = config.get(\"height\", 10)\n",
    "\n",
    "        # End an episode after this many timesteps.\n",
    "        self.timestep_limit = config.get(\"ts\", 100)\n",
    "\n",
    "        self.observation_space = MultiDiscrete([self.width * self.height,\n",
    "                                                self.width * self.height])\n",
    "        # 0=up, 1=right, 2=down, 3=left.\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        # Reset env.\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Returns initial observation of next(!) episode.\"\"\"\n",
    "        # Row-major coords.\n",
    "        self.agent1_pos = [0, 0]  # upper left corner\n",
    "        self.agent2_pos = [self.height - 1, self.width - 1]  # lower bottom corner\n",
    "\n",
    "        # Accumulated rewards in this episode.\n",
    "        self.agent1_R = 0.0\n",
    "        self.agent2_R = 0.0\n",
    "\n",
    "        # Reset agent1's visited fields.\n",
    "        self.agent1_visited_fields = set([tuple(self.agent1_pos)])\n",
    "\n",
    "        # How many timesteps have we done in this episode.\n",
    "        self.timesteps = 0\n",
    "\n",
    "        # Return the initial observation in the new episode.\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: dict):\n",
    "        \"\"\"\n",
    "        Returns (next observation, rewards, dones, infos) after having taken the given actions.\n",
    "        \n",
    "        e.g.\n",
    "        `action={\"agent1\": action_for_agent1, \"agent2\": action_for_agent2}`\n",
    "        \"\"\"\n",
    "        \n",
    "        # increase our time steps counter by 1.\n",
    "        self.timesteps += 1\n",
    "        # An episode is \"done\" when we reach the time step limit.\n",
    "        is_done = self.timesteps >= self.timestep_limit\n",
    "\n",
    "        # Agent2 always moves first.\n",
    "        # events = [collision|agent1_new_field]\n",
    "        events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        events |= self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "\n",
    "        # Useful for rendering.\n",
    "        self.collision = \"collision\" in events\n",
    "            \n",
    "        # Get observations (based on new agent positions).\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Determine rewards based on the collected events:\n",
    "        r1 = -1.0 if \"collision\" in events else 1.0 if \"agent1_new_field\" in events else -0.5\n",
    "        r2 = 1.0 if \"collision\" in events else -0.1\n",
    "\n",
    "        self.agent1_R += r1\n",
    "        self.agent2_R += r2\n",
    "        \n",
    "        rewards = {\n",
    "            \"agent1\": r1,\n",
    "            \"agent2\": r2,\n",
    "        }\n",
    "\n",
    "        # Generate a `done` dict (per-agent and total).\n",
    "        dones = {\n",
    "            \"agent1\": is_done,\n",
    "            \"agent2\": is_done,\n",
    "            # special `__all__` key indicates that the episode is done for all agents.\n",
    "            \"__all__\": is_done,\n",
    "        }\n",
    "\n",
    "        return obs, rewards, dones, {}  # <- info dict (not needed here).\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns obs dict (agent name to discrete-pos tuple) using each\n",
    "        agent's current x/y-positions.\n",
    "        \"\"\"\n",
    "        ag1_discrete_pos = self.agent1_pos[0] * self.width + \\\n",
    "            (self.agent1_pos[1] % self.width)\n",
    "        ag2_discrete_pos = self.agent2_pos[0] * self.width + \\\n",
    "            (self.agent2_pos[1] % self.width)\n",
    "        return {\n",
    "            \"agent1\": np.array([ag1_discrete_pos, ag2_discrete_pos]),\n",
    "            \"agent2\": np.array([ag2_discrete_pos, ag1_discrete_pos]),\n",
    "        }\n",
    "\n",
    "    def _move(self, coords, action, is_agent1):\n",
    "        \"\"\"\n",
    "        Moves an agent (agent1 iff is_agent1=True, else agent2) from `coords` (x/y) using the\n",
    "        given action (0=up, 1=right, etc..) and returns a resulting events dict:\n",
    "        Agent1: \"new\" when entering a new field. \"bumped\" when having been bumped into by agent2.\n",
    "        Agent2: \"bumped\" when bumping into agent1 (agent1 then gets -1.0).\n",
    "        \"\"\"\n",
    "        orig_coords = coords[:]\n",
    "        # Change the row: 0=up (-1), 2=down (+1)\n",
    "        coords[0] += -1 if action == 0 else 1 if action == 2 else 0\n",
    "        # Change the column: 1=right (+1), 3=left (-1)\n",
    "        coords[1] += 1 if action == 1 else -1 if action == 3 else 0\n",
    "\n",
    "        # Solve collisions.\n",
    "        # Make sure, we don't end up on the other agent's position.\n",
    "        # If yes, don't move (we are blocked).\n",
    "        if (is_agent1 and coords == self.agent2_pos) or (not is_agent1 and coords == self.agent1_pos):\n",
    "            coords[0], coords[1] = orig_coords\n",
    "            # Agent2 blocked agent1 (agent1 tried to run into agent2)\n",
    "            # OR Agent2 bumped into agent1 (agent2 tried to run into agent1)\n",
    "            return {\"collision\"}\n",
    "\n",
    "        # No agent blocking -> check walls.\n",
    "        if coords[0] < 0:\n",
    "            coords[0] = 0\n",
    "        elif coords[0] >= self.height:\n",
    "            coords[0] = self.height - 1\n",
    "        if coords[1] < 0:\n",
    "            coords[1] = 0\n",
    "        elif coords[1] >= self.width:\n",
    "            coords[1] = self.width - 1\n",
    "\n",
    "        # If agent1 -> \"new\" if new tile covered.\n",
    "        if is_agent1 and not tuple(coords) in self.agent1_visited_fields:\n",
    "            self.agent1_visited_fields.add(tuple(coords))\n",
    "            return {\"agent1_new_field\"}\n",
    "        # No new tile for agent1.\n",
    "        return set()\n",
    "\n",
    "    def render(self, mode=None):\n",
    "        print(\"_\" * (self.width + 2))\n",
    "        for r in range(self.height):\n",
    "            print(\"|\", end=\"\")\n",
    "            for c in range(self.width):\n",
    "                field = r * self.width + c % self.width\n",
    "                if self.agent1_pos == [r, c]:\n",
    "                    print(\"1\", end=\"\")\n",
    "                elif self.agent2_pos == [r, c]:\n",
    "                    print(\"2\", end=\"\")\n",
    "                elif (r, c) in self.agent1_visited_fields:\n",
    "                    print(\".\", end=\"\")\n",
    "                else:\n",
    "                    print(\" \", end=\"\")\n",
    "            print(\"|\")\n",
    "        print(\"‾\" * (self.width + 2))\n",
    "        print(f\"{'!!Collision!!' if self.collision else ''}\")\n",
    "        print(\"R1={: .1f}\".format(self.agent1_R))\n",
    "        print(\"R2={: .1f}\".format(self.agent2_R))\n",
    "        print()\n",
    "\n",
    "\n",
    "env = MultiAgentArena()\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# Agent1 will move down, Agent2 moves up.\n",
    "obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(\"Agent1's x/y position={}\".format(env.agent1_pos))\n",
    "print(\"Agent2's x/y position={}\".format(env.agent2_pos))\n",
    "print(\"Env timesteps={}\".format(env.timesteps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-sussex",
   "metadata": {},
   "source": [
    "## Exercise No 1\n",
    "\n",
    "<hr />\n",
    "\n",
    "<img src=\"images/exercise1.png\" width=400>\n",
    "\n",
    "In the cell above, we performed a `reset()` and a single `step()` call. To walk through an entire episode, one would normally call `step()` repeatedly (with different actions) until the returned `done` dict has the \"agent1\" or \"agent2\" (or \"__all__\") key set to True. Your task is to write an \"environment loop\" that runs for exactly one episode using our `MultiAgentArena` class.\n",
    "\n",
    "Follow these instructions here to get this done.\n",
    "\n",
    "1. Create an env object.\n",
    "1. `reset` your environment to get the first (initial) observation.\n",
    "1. Compute the actions for \"agent1\" and \"agent2\" calling `DummyTrainer.compute_action([obs])` twice and putting the results into an action dict to be passed into `step()`, just like it's done in the above cell (where we do a single `step()`).\n",
    "1. Repeat this, `step`ing through an entire episode.\n",
    "1. When an episode is done, `step()` will return a done dict with key `__all__` set to True.\n",
    "1. If you feel, this is way too easy for you ;) , try to extract each agent's reward, sum it up over the episode and - at the end of the episode - print out each agent's accumulated reward (also called the \"return\" of an episode).\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_agent1=3\n",
      "action_agent2=1\n",
      "\n",
      "action_agent1=0\n",
      "action_agent2=1\n",
      "\n",
      "action_agent1=0\n",
      "action_agent2=2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DummyTrainer:\n",
    "    \"\"\"Dummy Trainer class used in Exercise #1.\n",
    "\n",
    "    Use its `compute_action` method to get a new action for one of the agents,\n",
    "    given the agent's observation (a single discrete value encoding the field\n",
    "    the agent is currently in).\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_action(self, single_agent_obs=None):\n",
    "        # Returns a random action for a single agent.\n",
    "        return np.random.randint(4)  # Discrete(4) -> return rand int between 0 and 3 (incl. 3).\n",
    "\n",
    "dummy_trainer = DummyTrainer()\n",
    "# Check, whether it's working.\n",
    "for _ in range(3):\n",
    "    # Get action for agent1 (providing agent1's and agent2's positions).\n",
    "    print(\"action_agent1={}\".format(dummy_trainer.compute_action(np.array([0, 99]))))\n",
    "\n",
    "    # Get action for agent2 (providing agent2's and agent1's positions).\n",
    "    print(\"action_agent2={}\".format(dummy_trainer.compute_action(np.array([99, 0]))))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa8a1d-7d82-4b79-b2d4-3ce7ffa6fae1",
   "metadata": {},
   "source": [
    "Write your solution code into this cell here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b12373b-4b71-4ee9-a7a1-13077a59840b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2d4608bd894cb5ad4a233671f1a0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "\n",
    "# Leave the following as-is. It'll help us with rendering the env in this very cell's output.\n",
    "import time\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "with out:\n",
    "\n",
    "    # Solution to Exercise #1:\n",
    "    # Start coding here inside this `with`-block:\n",
    "    # 1) Reset the env.\n",
    "    obs = env.reset()  # start new episode\n",
    "\n",
    "    # 2) Enter an infinite while loop (to step through the episode).\n",
    "    while env.timesteps < 100:\n",
    "        # 3) Calculate both agents' actions individually, using dummy_trainer.compute_action([individual agent's obs])\n",
    "        a1 = dummy_trainer.compute_action(obs[\"agent1\"])\n",
    "        a2 = dummy_trainer.compute_action(obs[\"agent2\"])\n",
    "\n",
    "        # 4) Compile the actions dict from both individual actions.\n",
    "        actions = {\n",
    "            \"agent1\": a1, \"agent2\": a2,\n",
    "        }\n",
    "\n",
    "        # 5) Send the actions dict to the env's `step()` method to receive: obs, rewards, dones, info dicts\n",
    "        obs, rewards, dones, _ = env.step(actions)\n",
    "\n",
    "        # 6) We'll do this together: Render the env.\n",
    "        # Don't write any code here (skip directly to 7).\n",
    "        out.clear_output(wait=True)\n",
    "        time.sleep(0.05)\n",
    "        env.render()\n",
    "\n",
    "        # 7) Check, whether the episde is done, if yes, break out of the while loop.\n",
    "        if dones[\"agent1\"] is True:\n",
    "            break\n",
    "\n",
    "# 8) Run it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4196a5-7e7a-442a-8100-96bc7393c59d",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 15 min break :)\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
   "metadata": {},
   "source": [
    "### And now for something completely different:\n",
    "#### Plugging in RLlib!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd830b90-5762-4d22-8fa9-0abf0777a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 10:42:20,283\tINFO services.py:1272 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.179',\n",
       " 'raylet_ip_address': '192.168.0.179',\n",
       " 'redis_address': '192.168.0.179:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-06-24_10-42-18_600631_2196/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-06-24_10-42-18_600631_2196/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-06-24_10-42-18_600631_2196',\n",
       " 'metrics_export_port': 63988,\n",
       " 'node_id': 'fb379f8d24eba01da8c8027447994ca7c9754701cb243183c55e28d7'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import ray\n",
    "\n",
    "# Start a new instance of Ray (when running this tutorial locally) or\n",
    "# connect to an already running one (when running this tutorial through Anyscale).\n",
    "\n",
    "ray.init()  # Hear the engine humming? ;)\n",
    "\n",
    "# In case you encounter the following error during our tutorial: `RuntimeError: Maybe you called ray.init twice by accident?`\n",
    "# Try: `ray.shutdown() + ray.init()` or `ray.init(ignore_reinit_error=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
   "metadata": {},
   "source": [
    "### Picking an RLlib algorithm - We'll use PPO throughout this tutorial (one-size-fits-all-kind-of-algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_algos.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bcc1116-a14c-4479-87c0-6ece58ab0464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 10:42:23,897\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-06-24 10:42:23,898\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-06-24 10:42:33,238\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPO"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "# We use the PPO algorithm here b/c its very flexible wrt its supported\n",
    "# action spaces and model types and b/c it learns well almost any problem.\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "# Specify a very simple config, defining our environment and some environment\n",
    "# options (see environment.py).\n",
    "config = {\n",
    "    \"env\": MultiAgentArena,  # \"my_env\" <- if we previously have registered the env with `tune.register_env(\"[name]\", lambda config: [returns env object])`.\n",
    "    \"env_config\": {\n",
    "        \"config\": {\n",
    "            \"width\": 10,\n",
    "            \"height\": 10,\n",
    "            \"ts\": 100,\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # !PyTorch users!\n",
    "    #\"framework\": \"torch\",  # If users have chosen to install torch instead of tf.\n",
    "\n",
    "    \"create_env_on_driver\": True,\n",
    "}\n",
    "# Instantiate the Trainer object using above config.\n",
    "rllib_trainer = PPOTrainer(config=config)\n",
    "rllib_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ae150-c0a3-477f-8d78-d0a34f147958",
   "metadata": {},
   "source": [
    "### Ready to train with RLlib's PPO algorithm\n",
    "\n",
    "That's it, we are ready to train.\n",
    "Calling `Trainer.train()` will execute a single \"training iteration\".\n",
    "\n",
    "One iteration for most algos involves:\n",
    "\n",
    "1) sampling from the environment(s)\n",
    "2) using the sampled data (observations, actions taken, rewards) to update the policy model (neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f6c94d4-6871-4d20-81af-3d4081f05f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_timesteps_total': 4000,\n",
      " 'custom_metrics': {},\n",
      " 'date': '2021-06-24_10-42-36',\n",
      " 'done': False,\n",
      " 'episode_len_mean': 100.0,\n",
      " 'episode_media': {},\n",
      " 'episode_reward_max': 14.999999999999998,\n",
      " 'episode_reward_mean': -9.104999999999999,\n",
      " 'episode_reward_min': -34.50000000000005,\n",
      " 'episodes_this_iter': 20,\n",
      " 'episodes_total': 20,\n",
      " 'experiment_id': '74285469b30e44cdbdfb8f3ba86dfb6c',\n",
      " 'hist_stats': {'episode_lengths': [100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100],\n",
      "                'episode_reward': [-30.000000000000036,\n",
      "                                   -10.499999999999996,\n",
      "                                   -14.999999999999986,\n",
      "                                   -11.399999999999993,\n",
      "                                   -5.999999999999982,\n",
      "                                   4.500000000000023,\n",
      "                                   -5.999999999999986,\n",
      "                                   -13.499999999999991,\n",
      "                                   1.4999999999999978,\n",
      "                                   -16.499999999999993,\n",
      "                                   -6.899999999999999,\n",
      "                                   -15.00000000000002,\n",
      "                                   -15.000000000000009,\n",
      "                                   -20.39999999999999,\n",
      "                                   4.500000000000025,\n",
      "                                   14.999999999999998,\n",
      "                                   -34.50000000000005,\n",
      "                                   5.700000000000026,\n",
      "                                   -9.899999999999991,\n",
      "                                   -2.699999999999981]},\n",
      " 'hostname': 'Svens-MacBook-Pro.local',\n",
      " 'info': {'learner': {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224,\n",
      "                                                           'cur_lr': 4.999999873689376e-05,\n",
      "                                                           'entropy': 1.3686056,\n",
      "                                                           'entropy_coeff': 0.0,\n",
      "                                                           'kl': 0.018066175,\n",
      "                                                           'model': {},\n",
      "                                                           'policy_loss': -0.043953087,\n",
      "                                                           'total_loss': 20.284836,\n",
      "                                                           'vf_explained_var': 0.10961369,\n",
      "                                                           'vf_loss': 20.325178}}},\n",
      "          'num_agent_steps_sampled': 4000,\n",
      "          'num_agent_steps_trained': 4000,\n",
      "          'num_steps_sampled': 4000,\n",
      "          'num_steps_trained': 4000},\n",
      " 'iterations_since_restore': 1,\n",
      " 'node_ip': '192.168.0.179',\n",
      " 'num_healthy_workers': 2,\n",
      " 'off_policy_estimator': {},\n",
      " 'perf': {'cpu_util_percent': 28.82, 'ram_util_percent': 65.97999999999999},\n",
      " 'pid': 2196,\n",
      " 'policy_reward_max': {},\n",
      " 'policy_reward_mean': {},\n",
      " 'policy_reward_min': {},\n",
      " 'sampler_perf': {'mean_action_processing_ms': 0.05331537225744226,\n",
      "                  'mean_env_render_ms': 0.0,\n",
      "                  'mean_env_wait_ms': 0.028410515227875154,\n",
      "                  'mean_inference_ms': 0.5918417300854053,\n",
      "                  'mean_raw_obs_processing_ms': 0.13975651709588022},\n",
      " 'time_since_restore': 3.4800119400024414,\n",
      " 'time_this_iter_s': 3.4800119400024414,\n",
      " 'time_total_s': 3.4800119400024414,\n",
      " 'timers': {'learn_throughput': 1539.993,\n",
      "            'learn_time_ms': 2597.414,\n",
      "            'load_throughput': 109748.975,\n",
      "            'load_time_ms': 36.447,\n",
      "            'sample_throughput': 4764.843,\n",
      "            'sample_time_ms': 839.482,\n",
      "            'update_time_ms': 1.433},\n",
      " 'timestamp': 1624524156,\n",
      " 'timesteps_since_restore': 0,\n",
      " 'timesteps_total': 4000,\n",
      " 'training_iteration': 1}\n"
     ]
    }
   ],
   "source": [
    "results = rllib_trainer.train()\n",
    "\n",
    "# Delete the config from the results for clarity.\n",
    "# Only the stats will remain, then.\n",
    "del results[\"config\"]\n",
    "# Pretty print the stats.\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96f682-fc7d-46a0-b136-f5d62cd7ad67",
   "metadata": {},
   "source": [
    "### Going from single policy (RLlib's default) to multi-policy:\n",
    "\n",
    "So far, our experiment has been ill-configured, because both\n",
    "agents, which should behave differently due to their different\n",
    "tasks and reward functions, learn the same policy: the \"default_policy\",\n",
    "which RLlib always provides if you don't configure anything else.\n",
    "Remember that RLlib does not know at Trainer setup time, how many and which agents\n",
    "the environment will \"produce\". Agent control (adding agents, removing them, terminating\n",
    "episodes for agents) is entirely in the Env's hands.\n",
    "Let's fix our single policy problem and introduce the \"multiagent\" API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13900163-f520-40f1-87be-d759760bd3a5",
   "metadata": {},
   "source": [
    "<img src=\"images/from_single_agent_to_multi_agent.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a813b988-e40f-4890-8c9c-f5f7d0f49cc9",
   "metadata": {},
   "source": [
    "In order to turn on RLlib's multi-agent functionality, we need two things:\n",
    "\n",
    "1. A policy mapping function, mapping agent IDs (e.g. a string like \"agent1\", produced by the environment in the returned observation/rewards/dones-dicts) to a policy ID (another string, e.g. \"policy1\", which is under our control).\n",
    "1. A policies definition dict, mapping policy IDs (e.g. \"policy1\") to 4-tuples consisting of 1) policy class (None for using the default class), 2) observation space, 3) action space, and 4) config overrides (empty dict for no overrides and using the Trainer's main config dict).\n",
    "\n",
    "Let's take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dff7017-f1b9-41e8-94fd-266bbe56cf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'create_env_on_driver': True,\n",
      " 'env': <class '__main__.MultiAgentArena'>,\n",
      " 'env_config': {'config': {'height': 10, 'ts': 100, 'width': 10}},\n",
      " 'multiagent': {'policies': {'policy1': (None,\n",
      "                                         MultiDiscrete([100 100]),\n",
      "                                         Discrete(4),\n",
      "                                         {}),\n",
      "                             'policy2': (None,\n",
      "                                         MultiDiscrete([100 100]),\n",
      "                                         Discrete(4),\n",
      "                                         {'lr': 0.0002})},\n",
      "                'policy_mapping_fn': <function policy_mapping_fn at 0x7fbc3a8b2af0>}}\n",
      "\n",
      "agent1 is now mapped to policy1\n",
      "agent2 is now mapped to policy2\n"
     ]
    }
   ],
   "source": [
    "# Define the policies definition dict:\n",
    "# Each policy in there is defined by its ID (key) mapping to a 4-tuple (value):\n",
    "# - Policy class (None for using the \"default\" class, e.g. PPOTFPolicy for PPO+tf or PPOTorchPolicy for PPO+torch).\n",
    "# - obs-space (we get this directly from our already created env object).\n",
    "# - act-space (we get this directly from our already created env object).\n",
    "# - config-overrides dict (leave empty for using the Trainer's config as-is)\n",
    "policies = {\n",
    "    \"policy1\": (None, env.observation_space, env.action_space, {}),\n",
    "    \"policy2\": (None, env.observation_space, env.action_space, {\"lr\": 0.0002}),\n",
    "}\n",
    "# Note that now we won't have a \"default_policy\" anymore, just \"policy1\" and \"policy2\".\n",
    "\n",
    "# Define an agent->policy mapping function.\n",
    "# Which agents (defined by the environment) use which policies (defined by us)?\n",
    "# The mapping here is M (agents) -> N (policies), where M >= N.\n",
    "def policy_mapping_fn(agent_id: str):\n",
    "    # Make sure agent ID is valid.\n",
    "    assert agent_id in [\"agent1\", \"agent2\"], f\"ERROR: invalid agent ID {agent_id}!\"\n",
    "    # Map agent1 to policy1, and agent2 to policy2.\n",
    "    return \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
    "\n",
    "# We could - if we wanted - specify, which policies should be learnt (by default, RLlib learns all).\n",
    "# Non-learnt policies will be frozen and not updated:\n",
    "# policies_to_train = [\"policy1\", \"policy2\"]\n",
    "\n",
    "# Adding the above to our config.\n",
    "config.update({\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        # We'll leave this empty: Means, we train both policy1 and policy2.\n",
    "        # \"policies_to_train\": policies_to_train,\n",
    "    },\n",
    "})\n",
    "\n",
    "pprint.pprint(config)\n",
    "print()\n",
    "print(f\"agent1 is now mapped to {policy_mapping_fn('agent1')}\")\n",
    "print(f\"agent2 is now mapped to {policy_mapping_fn('agent2')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "646f8800-941b-43cb-a924-622af6788aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 10:42:49,739\tINFO trainable.py:101 -- Trainable.setup took 12.972 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-06-24 10:42:49,740\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPO"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate our Trainer (we cannot just change the config on-the-fly).\n",
    "rllib_trainer.stop()\n",
    "\n",
    "# Using our updated (now multiagent!) config dict.\n",
    "rllib_trainer = PPOTrainer(config=config)\n",
    "rllib_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {},
   "source": [
    "Now that we are setup correctly with two policies as per our \"multiagent\" config, let's call `train()` on the new Trainer several times (what about 10 times?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17ae724d-71cc-422b-96cb-3dc9faa2d111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=1: R(\"return\")=-8.587500000000002\n",
      "Iteration=2: R(\"return\")=-5.167499999999999\n",
      "Iteration=3: R(\"return\")=-4.460999999999993\n",
      "Iteration=4: R(\"return\")=-2.3789999999999885\n",
      "Iteration=5: R(\"return\")=-1.7069999999999876\n",
      "Iteration=6: R(\"return\")=-0.26699999999998636\n",
      "Iteration=7: R(\"return\")=1.4850000000000128\n",
      "Iteration=8: R(\"return\")=2.2110000000000114\n",
      "Iteration=9: R(\"return\")=3.3690000000000078\n",
      "Iteration=10: R(\"return\")=3.6810000000000063\n"
     ]
    }
   ],
   "source": [
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "# Move on once you see (agent1 + agent2) episode rewards of 10.0 or more.\n",
    "for _ in range(10):\n",
    "    results = rllib_trainer.train()\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R(\\\"return\\\")={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "365ef0d7-9977-4d9d-9fa5-ffaa7c111b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=11: R1=11.675 R2=-6.655999999999988\n",
      "Iteration=12: R1=13.175 R2=-6.325999999999986\n",
      "Iteration=13: R1=12.185 R2=-5.929999999999989\n",
      "Iteration=14: R1=12.515 R2=-5.995999999999988\n",
      "Iteration=15: R1=12.13 R2=-5.610999999999991\n",
      "Iteration=16: R1=13.305 R2=-5.3249999999999895\n",
      "Iteration=17: R1=14.36 R2=-5.2699999999999925\n",
      "Iteration=18: R1=15.555 R2=-5.78699999999999\n",
      "Iteration=19: R1=16.79 R2=-5.698999999999991\n",
      "Iteration=20: R1=17.795 R2=-5.66599999999999\n"
     ]
    }
   ],
   "source": [
    "# Do another loop, but this time, we will print out each policies' individual rewards.\n",
    "for _ in range(10):\n",
    "    results = rllib_trainer.train()\n",
    "    r1 = results['policy_reward_mean']['policy1']\n",
    "    r2 = results['policy_reward_mean']['policy2']\n",
    "    r = r1 + r2\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R(\\\"return\\\")={r} R1={r1} R2={r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80ad33-a55b-4e18-857b-b884eedda0a4",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK! (<-- we will not do these during the tutorial, but feel free to try these cells by yourself)\n",
    "\n",
    "Use the above solution of Exercise #1 and replace our `dummy_trainer` in that solution\n",
    "with the now trained `rllib_trainer`. You should see a better performance of the two agents.\n",
    "\n",
    "However, keep in mind that we are mostly training agent1 as we only trian a single policy and agent1\n",
    "is the \"easier\" one to collect high rewards with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK!\n",
    "\n",
    "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using Policies and their model(s) inside our Trainer object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aff679e8-74b4-4603-9d5c-4cc0c6ebe45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our (only!) Policy right now is: <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fbc409bd160>\n",
      "Our Policy's observation space is: Box(-1.0, 1.0, (200,), float32)\n",
      "Our Policy's action space is: Discrete(4)\n",
      "sampled action=3\n"
     ]
    }
   ],
   "source": [
    "# Let's actually \"look inside\" our Trainer to see what's in there.\n",
    "from ray.rllib.utils.numpy import softmax\n",
    "\n",
    "# To get to one of the policies inside the Trainer, use `Trainer.get_policy([policy ID])`:\n",
    "policy = rllib_trainer.get_policy(\"policy1\")\n",
    "print(f\"Our (only!) Policy right now is: {policy}\")\n",
    "\n",
    "# To get to the model inside any policy, do:\n",
    "model = policy.model\n",
    "#print(f\"Our Policy's model is: {model}\")\n",
    "\n",
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\")\n",
    "\n",
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = np.array([policy.observation_space.sample()])\n",
    "# Alternatively for PyTorch:\n",
    "#import torch\n",
    "#obs = torch.from_numpy(obs)\n",
    "\n",
    "# Get the action logits (as tf tensor).\n",
    "# If you are using torch, you would get a torch tensor here.\n",
    "logits, _ = model({\"obs\": obs})\n",
    "logits\n",
    "\n",
    "# Numpyize the tensor by running `logits` through the Policy's own tf.Session.\n",
    "logits_np = policy.get_session().run(logits)\n",
    "# For torch, you can simply do: `logits_np = logits.detach().cpu().numpy()`.\n",
    "\n",
    "# Convert logits into action probabilities and remove the B=1.\n",
    "action_probs = np.squeeze(softmax(logits_np))\n",
    "\n",
    "# Sample an action, using the probabilities.\n",
    "action = np.random.choice([0, 1, 2, 3], p=action_probs)\n",
    "\n",
    "# Print out the action.\n",
    "print(f\"sampled action={action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd66c3-f07a-4795-84ea-6b232ba6a047",
   "metadata": {},
   "source": [
    "### Saving and restoring a trained Trainer.\n",
    "Currently, `rllib_trainer` is in an already trained state.\n",
    "It holds optimized weights in its Policy's model that allow it to act\n",
    "already somewhat smart in our environment when given an observation.\n",
    "\n",
    "However, if we closed this notebook right now, all the effort would have been for nothing.\n",
    "Let's therefore save the state of our trainer to disk for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57eae1e4-3cc4-4282-9a83-bc374bdad978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer (at iteration 20 was saved in '/Users/sven/ray_results/PPO_MultiAgentArena_2021-06-24_10-42-36tjwkrob9/checkpoint_000020/checkpoint-20'!\n",
      "The checkpoint directory contains the following files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['checkpoint-20', 'checkpoint-20.tune_metadata', '.is_checkpoint']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the `Trainer.save()` method to create a checkpoint.\n",
    "checkpoint_file = rllib_trainer.save()\n",
    "print(f\"Trainer (at iteration {rllib_trainer.iteration} was saved in '{checkpoint_file}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "import os\n",
    "os.listdir(os.path.dirname(checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e0ab-2c10-469a-97b1-4aadf1a1ec97",
   "metadata": {},
   "source": [
    "### Restoring and evaluating a Trainer\n",
    "In the following cell, we'll learn how to restore a saved Trainer from a checkpoint file.\n",
    "\n",
    "We'll also evaluate a completely new Trainer (should act more or less randomly) vs an already trained one (the one we just restored from the created checkpoint file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74ceedb9-c225-46f2-ad1d-f902c81d3256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 10:45:16,711\tINFO trainable.py:101 -- Trainable.setup took 13.248 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-06-24 10:45:16,713\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating new trainer: R=-7.379999999999993\n",
      "Before restoring: Trainer is at iteration=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 10:45:19,816\tINFO trainable.py:377 -- Restored on 192.168.0.179 from checkpoint: /Users/sven/ray_results/PPO_MultiAgentArena_2021-06-24_10-42-36tjwkrob9/checkpoint_000020/checkpoint-20\n",
      "2021-06-24 10:45:19,817\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': None, '_time_total': 132.94485211372375, '_episodes_total': 800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After restoring: Trainer is at iteration=20\n",
      "Evaluating restored trainer: R=15.284999999999972\n"
     ]
    }
   ],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_trainer = PPOTrainer(config=config)\n",
    "# Evaluate the new trainer (this should yield random results).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "print(f\"Before restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "new_trainer.restore(checkpoint_file)\n",
    "print(f\"After restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "\n",
    "# Evaluate again (this should yield results we saw after having trained our saved agent).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating restored trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de603d14-f0cb-4363-a72b-8f147c094071",
   "metadata": {},
   "source": [
    "In order to release all resources from a Trainer, you can use a Trainer's `stop()` method.\n",
    "You should definitley run this cell as it frees resources that we'll need later in this tutorial, when we'll do parallel hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "737dca4f-942f-4fda-abcc-0052263a103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rllib_trainer.stop()\n",
    "new_trainer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c1e4c-cb02-4719-ac5a-0106172a6c6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Moving stuff to the professional level: RLlib in connection w/ Ray Tune\n",
    "\n",
    "Running any experiments through Ray Tune is the recommended way of doing things with RLlib. If you look at our\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/rllib/examples\">examples scripts folder</a>, you will see that almost all of the scripts use Ray Tune to run the particular RLlib workload demonstrated in each script.\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdacebb-d27f-4174-9002-35c5657f146c",
   "metadata": {
    "tags": []
   },
   "source": [
    "When setting up hyperparameter sweeps for Tune, we'll do this in our already familiar config dict.\n",
    "\n",
    "So let's take a quick look at our PPO algo's default config to understand, which hyperparameters we may want to play around with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1b32582-52bd-4585-9009-2f877a0723a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO's default config is:\n",
      "{'_fake_gpus': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': True,\n",
      " 'clip_param': 0.3,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': 180,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'eager_tracing': False,\n",
      " 'entropy_coeff': 0.0,\n",
      " 'entropy_coeff_schedule': None,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'env_task_fn': None,\n",
      " 'evaluation_config': {},\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': 10,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'evaluation_parallel_to_training': False,\n",
      " 'exploration_config': {'type': 'StochasticSampling'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'tf',\n",
      " 'gamma': 0.99,\n",
      " 'grad_clip': None,\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'kl_coeff': 0.2,\n",
      " 'kl_target': 0.01,\n",
      " 'lambda': 1.0,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'lr': 5e-05,\n",
      " 'lr_schedule': None,\n",
      " 'metrics_smoothing_episodes': 100,\n",
      " 'min_iter_time_s': 0,\n",
      " 'model': {'_time_major': False,\n",
      "           '_use_default_native_models': False,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'num_framestacks': 'auto',\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': False,\n",
      "           'zero_mean': True},\n",
      " 'monitor': -1,\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_mapping_fn': None,\n",
      "                'replay_mode': 'independent'},\n",
      " 'no_done_at_end': False,\n",
      " 'normalize_actions': False,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_sgd_iter': 30,\n",
      " 'num_workers': 2,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'record_env': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'rollout_fragment_length': 200,\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'seed': None,\n",
      " 'sgd_minibatch_size': 128,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'shuffle_sequences': True,\n",
      " 'simple_optimizer': -1,\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 0,\n",
      " 'train_batch_size': 4000,\n",
      " 'use_critic': True,\n",
      " 'use_gae': True,\n",
      " 'vf_clip_param': 10.0,\n",
      " 'vf_loss_coeff': 1.0,\n",
      " 'vf_share_layers': -1}\n"
     ]
    }
   ],
   "source": [
    "# Configuration dicts and Ray Tune.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# PPO algorithm:\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as PPO_DEFAULT_CONFIG\n",
    "print(f\"PPO's default config is:\")\n",
    "pprint.pprint(PPO_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint.pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint.pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded886cc-436e-46cd-8fea-d68af8b41236",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's do a very simple grid-search over two learning rates with tune.run().\n",
    "\n",
    "In particular, we will try the learning rates 0.00005 and 0.5 using `tune.grid_search([...])`\n",
    "inside our config dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5063991e-173b-49be-a4e7-467e2e18321a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2675)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=2675)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=2675)\u001b[0m 2021-06-24 10:45:29,349\tWARNING ppo.py:135 -- `train_batch_size` (3000) cannot be achieved with your other settings (num_workers=2 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1500.\n",
      "\u001b[2m\u001b[36m(pid=2677)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=2677)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=2670)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=2670)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=2670)\u001b[0m 2021-06-24 10:45:29,349\tWARNING ppo.py:135 -- `train_batch_size` (3000) cannot be achieved with your other settings (num_workers=2 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1500.\n",
      "\u001b[2m\u001b[36m(pid=2674)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=2674)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=2675)\u001b[0m 2021-06-24 10:45:44,763\tINFO trainable.py:101 -- Trainable.setup took 15.416 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=2675)\u001b[0m 2021-06-24 10:45:44,764\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=2677)\u001b[0m 2021-06-24 10:45:44,776\tINFO trainable.py:101 -- Trainable.setup took 15.429 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=2677)\u001b[0m 2021-06-24 10:45:44,776\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=2670)\u001b[0m 2021-06-24 10:45:44,779\tINFO trainable.py:101 -- Trainable.setup took 15.432 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=2670)\u001b[0m 2021-06-24 10:45:44,780\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=2674)\u001b[0m 2021-06-24 10:45:44,765\tINFO trainable.py:101 -- Trainable.setup took 15.418 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=2674)\u001b[0m 2021-06-24 10:45:44,766\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_83919_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-45-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.000000000000014\n",
      "  episode_reward_mean: -8.909999999999993\n",
      "  episode_reward_min: -34.500000000000036\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 30\n",
      "  experiment_id: 43c5f1bf54a446d6ac106c951f38d022\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3664278984069824\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02037949115037918\n",
      "          model: {}\n",
      "          policy_loss: -0.05178745463490486\n",
      "          total_loss: 53.435001373291016\n",
      "          vf_explained_var: 0.13389408588409424\n",
      "          vf_loss: 53.48271560668945\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3454874753952026\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04217240586876869\n",
      "          model: {}\n",
      "          policy_loss: -0.08426131308078766\n",
      "          total_loss: 1.8325474262237549\n",
      "          vf_explained_var: 0.41607698798179626\n",
      "          vf_loss: 1.9083740711212158\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.22\n",
      "    ram_util_percent: 68.74\n",
      "  pid: 2675\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -7.799999999999981\n",
      "  policy_reward_mean:\n",
      "    policy1: 0.8333333333333334\n",
      "    policy2: -9.743333333333316\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11689038692832711\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05074622708904511\n",
      "    mean_inference_ms: 1.8129266952372325\n",
      "    mean_raw_obs_processing_ms: 0.2514038937318969\n",
      "  time_since_restore: 10.05943489074707\n",
      "  time_this_iter_s: 10.05943489074707\n",
      "  time_total_s: 10.05943489074707\n",
      "  timers:\n",
      "    learn_throughput: 469.142\n",
      "    learn_time_ms: 6394.648\n",
      "    load_throughput: 23594.744\n",
      "    load_time_ms: 127.147\n",
      "    sample_throughput: 882.612\n",
      "    sample_time_ms: 3399.003\n",
      "    update_time_ms: 5.25\n",
      "  timestamp: 1624524354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: '83919_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>RUNNING </td><td>192.168.0.179:2675</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.0594</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">   -8.91</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_83919_00001:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-45-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.000000000000016\n",
      "  episode_reward_mean: -5.909999999999995\n",
      "  episode_reward_min: -34.50000000000003\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 30\n",
      "  experiment_id: 38bdf32c865b44ec9dfdfa051cfd3bca\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.078101746737957\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 18.170473098754883\n",
      "          model: {}\n",
      "          policy_loss: 0.47826075553894043\n",
      "          total_loss: 56.070186614990234\n",
      "          vf_explained_var: 0.009776771068572998\n",
      "          vf_loss: 51.957828521728516\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3435875177383423\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.044070493429899216\n",
      "          model: {}\n",
      "          policy_loss: -0.09309946000576019\n",
      "          total_loss: 1.7173503637313843\n",
      "          vf_explained_var: 0.4769733250141144\n",
      "          vf_loss: 1.8016357421875\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.26666666666666\n",
      "    ram_util_percent: 68.74666666666666\n",
      "  pid: 2670\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -4.499999999999982\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.1\n",
      "    policy2: -9.00999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11567764485541542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05062598534062415\n",
      "    mean_inference_ms: 1.8227679343480891\n",
      "    mean_raw_obs_processing_ms: 0.24832319530306624\n",
      "  time_since_restore: 10.054849863052368\n",
      "  time_this_iter_s: 10.054849863052368\n",
      "  time_total_s: 10.054849863052368\n",
      "  timers:\n",
      "    learn_throughput: 468.629\n",
      "    learn_time_ms: 6401.65\n",
      "    load_throughput: 24602.476\n",
      "    load_time_ms: 121.939\n",
      "    sample_throughput: 882.943\n",
      "    sample_time_ms: 3397.727\n",
      "    update_time_ms: 6.46\n",
      "  timestamp: 1624524354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: '83919_00001'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00002:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-45-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.700000000000021\n",
      "  episode_reward_mean: -11.864999999999995\n",
      "  episode_reward_min: -33.000000000000036\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: c49aef7b1b5542e194a1608b246ea2a9\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3629132509231567\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02385992743074894\n",
      "          model: {}\n",
      "          policy_loss: -0.04925725609064102\n",
      "          total_loss: 30.79827308654785\n",
      "          vf_explained_var: 0.08894110471010208\n",
      "          vf_loss: 30.842761993408203\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3493726253509521\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03826555982232094\n",
      "          model: {}\n",
      "          policy_loss: -0.07087726891040802\n",
      "          total_loss: 2.229379177093506\n",
      "          vf_explained_var: 0.3409762978553772\n",
      "          vf_loss: 2.2926034927368164\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.05263157894737\n",
      "    ram_util_percent: 68.77894736842106\n",
      "  pid: 2674\n",
      "  policy_reward_max:\n",
      "    policy1: 13.5\n",
      "    policy2: -4.499999999999985\n",
      "  policy_reward_mean:\n",
      "    policy1: -2.6625\n",
      "    policy2: -9.202499999999981\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1123193381489187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04853444478322363\n",
      "    mean_inference_ms: 1.760626184767571\n",
      "    mean_raw_obs_processing_ms: 0.23792303543815246\n",
      "  time_since_restore: 13.129771947860718\n",
      "  time_this_iter_s: 13.129771947860718\n",
      "  time_total_s: 13.129771947860718\n",
      "  timers:\n",
      "    learn_throughput: 475.88\n",
      "    learn_time_ms: 8405.473\n",
      "    load_throughput: 27262.782\n",
      "    load_time_ms: 146.72\n",
      "    sample_throughput: 899.984\n",
      "    sample_time_ms: 4444.524\n",
      "    update_time_ms: 5.544\n",
      "  timestamp: 1624524357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '83919_00002'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00003:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-45-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.000000000000021\n",
      "  episode_reward_mean: -9.817499999999999\n",
      "  episode_reward_min: -34.50000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 777bea0709a34c368f94f4b08cbe8c9b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.10365462303161621\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 15.364961624145508\n",
      "          model: {}\n",
      "          policy_loss: 0.45147833228111267\n",
      "          total_loss: 57.60213851928711\n",
      "          vf_explained_var: -0.010712865740060806\n",
      "          vf_loss: 54.07766342163086\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3498151302337646\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03807845339179039\n",
      "          model: {}\n",
      "          policy_loss: -0.06896308064460754\n",
      "          total_loss: 1.8189435005187988\n",
      "          vf_explained_var: 0.4419904053211212\n",
      "          vf_loss: 1.8802908658981323\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.64210526315791\n",
      "    ram_util_percent: 68.77894736842106\n",
      "  pid: 2677\n",
      "  policy_reward_max:\n",
      "    policy1: 22.0\n",
      "    policy2: -3.3999999999999906\n",
      "  policy_reward_mean:\n",
      "    policy1: -0.5875\n",
      "    policy2: -9.22999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11233381483925395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04932631378707619\n",
      "    mean_inference_ms: 1.789251248399238\n",
      "    mean_raw_obs_processing_ms: 0.2394804413589104\n",
      "  time_since_restore: 13.130576133728027\n",
      "  time_this_iter_s: 13.130576133728027\n",
      "  time_total_s: 13.130576133728027\n",
      "  timers:\n",
      "    learn_throughput: 479.719\n",
      "    learn_time_ms: 8338.208\n",
      "    load_throughput: 25797.092\n",
      "    load_time_ms: 155.056\n",
      "    sample_throughput: 886.902\n",
      "    sample_time_ms: 4510.082\n",
      "    update_time_ms: 4.399\n",
      "  timestamp: 1624524357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '83919_00003'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.000000000000016\n",
      "  episode_reward_mean: -25.160000000000025\n",
      "  episode_reward_min: -48.00000000000008\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 60\n",
      "  experiment_id: 38bdf32c865b44ec9dfdfa051cfd3bca\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.04965648800134659\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.443717122077942\n",
      "          model: {}\n",
      "          policy_loss: 0.056447744369506836\n",
      "          total_loss: 169.8813018798828\n",
      "          vf_explained_var: -0.16828449070453644\n",
      "          vf_loss: 169.3917694091797\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3145511150360107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027634384110569954\n",
      "          model: {}\n",
      "          policy_loss: -0.05799044668674469\n",
      "          total_loss: 1.9219611883163452\n",
      "          vf_explained_var: 0.47436726093292236\n",
      "          vf_loss: 1.97166109085083\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.10714285714288\n",
      "    ram_util_percent: 69.05714285714288\n",
      "  pid: 2670\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -4.499999999999982\n",
      "  policy_reward_mean:\n",
      "    policy1: -16.058333333333334\n",
      "    policy2: -9.101666666666649\n",
      "  policy_reward_min:\n",
      "    policy1: -38.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12051659211884705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0528642218110237\n",
      "    mean_inference_ms: 1.8961515698803892\n",
      "    mean_raw_obs_processing_ms: 0.25930044203067726\n",
      "  time_since_restore: 19.843579053878784\n",
      "  time_this_iter_s: 9.788729190826416\n",
      "  time_total_s: 19.843579053878784\n",
      "  timers:\n",
      "    learn_throughput: 490.272\n",
      "    learn_time_ms: 6119.057\n",
      "    load_throughput: 47593.319\n",
      "    load_time_ms: 63.034\n",
      "    sample_throughput: 818.742\n",
      "    sample_time_ms: 3664.158\n",
      "    update_time_ms: 7.123\n",
      "  timestamp: 1624524364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: '83919_00001'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>RUNNING </td><td>192.168.0.179:2675</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.0594</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\"> -8.91  </td><td style=\"text-align: right;\">                15  </td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>RUNNING </td><td>192.168.0.179:2670</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         19.8436</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">-25.16  </td><td style=\"text-align: right;\">                15  </td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>RUNNING </td><td>192.168.0.179:2674</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.1298</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-11.865 </td><td style=\"text-align: right;\">                 5.7</td><td style=\"text-align: right;\">               -33  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>RUNNING </td><td>192.168.0.179:2677</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.1306</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -9.8175</td><td style=\"text-align: right;\">                12  </td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_83919_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.000000000000014\n",
      "  episode_reward_mean: -5.68499999999999\n",
      "  episode_reward_min: -34.500000000000036\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 60\n",
      "  experiment_id: 43c5f1bf54a446d6ac106c951f38d022\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3404359817504883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019646519795060158\n",
      "          model: {}\n",
      "          policy_loss: -0.05948462709784508\n",
      "          total_loss: 29.73061752319336\n",
      "          vf_explained_var: 0.20065376162528992\n",
      "          vf_loss: 29.78420639038086\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3029606342315674\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03657953068614006\n",
      "          model: {}\n",
      "          policy_loss: -0.08567504584789276\n",
      "          total_loss: 2.230103015899658\n",
      "          vf_explained_var: 0.38956695795059204\n",
      "          vf_loss: 2.3048043251037598\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.12142857142857\n",
      "    ram_util_percent: 69.05714285714288\n",
      "  pid: 2675\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -2.3000000000000016\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.6\n",
      "    policy2: -9.28499999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12243444409471098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05305036450166201\n",
      "    mean_inference_ms: 1.8900885114154373\n",
      "    mean_raw_obs_processing_ms: 0.26302880406828194\n",
      "  time_since_restore: 20.00106167793274\n",
      "  time_this_iter_s: 9.941626787185669\n",
      "  time_total_s: 20.00106167793274\n",
      "  timers:\n",
      "    learn_throughput: 486.496\n",
      "    learn_time_ms: 6166.543\n",
      "    load_throughput: 45949.869\n",
      "    load_time_ms: 65.289\n",
      "    sample_throughput: 812.428\n",
      "    sample_time_ms: 3692.633\n",
      "    update_time_ms: 4.975\n",
      "  timestamp: 1624524364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: '83919_00000'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00002:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.999999999999943\n",
      "  episode_reward_mean: -5.5387499999999905\n",
      "  episode_reward_min: -33.000000000000036\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: c49aef7b1b5542e194a1608b246ea2a9\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3350809812545776\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018097825348377228\n",
      "          model: {}\n",
      "          policy_loss: -0.047358158975839615\n",
      "          total_loss: 30.488140106201172\n",
      "          vf_explained_var: 0.20678654313087463\n",
      "          vf_loss: 30.53006935119629\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3102316856384277\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03630093112587929\n",
      "          model: {}\n",
      "          policy_loss: -0.07370398193597794\n",
      "          total_loss: 3.1660776138305664\n",
      "          vf_explained_var: 0.3064298629760742\n",
      "          vf_loss: 3.228891372680664\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.01578947368421\n",
      "    ram_util_percent: 69.15789473684212\n",
      "  pid: 2674\n",
      "  policy_reward_max:\n",
      "    policy1: 31.0\n",
      "    policy2: -1.1999999999999866\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.03125\n",
      "    policy2: -8.569999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11715029688576059\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0504215349344249\n",
      "    mean_inference_ms: 1.8516685061844107\n",
      "    mean_raw_obs_processing_ms: 0.2470169052622661\n",
      "  time_since_restore: 26.6622576713562\n",
      "  time_this_iter_s: 13.532485723495483\n",
      "  time_total_s: 26.6622576713562\n",
      "  timers:\n",
      "    learn_throughput: 480.501\n",
      "    learn_time_ms: 8324.648\n",
      "    load_throughput: 52482.02\n",
      "    load_time_ms: 76.217\n",
      "    sample_throughput: 823.96\n",
      "    sample_time_ms: 4854.602\n",
      "    update_time_ms: 6.82\n",
      "  timestamp: 1624524371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: '83919_00002'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>RUNNING </td><td>192.168.0.179:2675</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         20.0011</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\"> -5.685  </td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>RUNNING </td><td>192.168.0.179:2670</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         19.8436</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">-25.16   </td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>RUNNING </td><td>192.168.0.179:2674</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         26.6623</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> -5.53875</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">               -33  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>RUNNING </td><td>192.168.0.179:2677</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.1306</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -9.8175 </td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_83919_00003:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.000000000000021\n",
      "  episode_reward_mean: -27.71625000000003\n",
      "  episode_reward_min: -46.500000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 777bea0709a34c368f94f4b08cbe8c9b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.011390337720513344\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.7135323286056519\n",
      "          model: {}\n",
      "          policy_loss: 0.028235359117388725\n",
      "          total_loss: 87.91030883789062\n",
      "          vf_explained_var: 0.04217272251844406\n",
      "          vf_loss: 87.36800384521484\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3063417673110962\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028647078201174736\n",
      "          model: {}\n",
      "          policy_loss: -0.048121437430381775\n",
      "          total_loss: 2.3983969688415527\n",
      "          vf_explained_var: 0.438218891620636\n",
      "          vf_loss: 2.4379241466522217\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.05\n",
      "    ram_util_percent: 69.16000000000001\n",
      "  pid: 2677\n",
      "  policy_reward_max:\n",
      "    policy1: 22.0\n",
      "    policy2: -1.2\n",
      "  policy_reward_mean:\n",
      "    policy1: -18.9125\n",
      "    policy2: -8.803749999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -40.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11705511535269011\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.051375631702234616\n",
      "    mean_inference_ms: 1.8818053188675505\n",
      "    mean_raw_obs_processing_ms: 0.25049687105924917\n",
      "  time_since_restore: 26.892842054367065\n",
      "  time_this_iter_s: 13.762265920639038\n",
      "  time_total_s: 26.892842054367065\n",
      "  timers:\n",
      "    learn_throughput: 480.15\n",
      "    learn_time_ms: 8330.735\n",
      "    load_throughput: 49495.131\n",
      "    load_time_ms: 80.816\n",
      "    sample_throughput: 805.941\n",
      "    sample_time_ms: 4963.141\n",
      "    update_time_ms: 5.102\n",
      "  timestamp: 1624524371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: '83919_00003'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999945\n",
      "  episode_reward_mean: -3.733333333333324\n",
      "  episode_reward_min: -34.500000000000036\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 90\n",
      "  experiment_id: 43c5f1bf54a446d6ac106c951f38d022\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3026436567306519\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02130306325852871\n",
      "          model: {}\n",
      "          policy_loss: -0.06467930227518082\n",
      "          total_loss: 39.7680778503418\n",
      "          vf_explained_var: 0.3046334385871887\n",
      "          vf_loss: 39.826358795166016\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2618191242218018\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03267037868499756\n",
      "          model: {}\n",
      "          policy_loss: -0.0773257315158844\n",
      "          total_loss: 2.8779094219207764\n",
      "          vf_explained_var: 0.39631444215774536\n",
      "          vf_loss: 2.940533399581909\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.64375\n",
      "    ram_util_percent: 69.2\n",
      "  pid: 2675\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: 2.100000000000002\n",
      "  policy_reward_mean:\n",
      "    policy1: 4.983333333333333\n",
      "    policy2: -8.71666666666665\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12719069821097068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.054957949531456586\n",
      "    mean_inference_ms: 1.962962074043572\n",
      "    mean_raw_obs_processing_ms: 0.27342046464026004\n",
      "  time_since_restore: 31.078620672225952\n",
      "  time_this_iter_s: 11.077558994293213\n",
      "  time_total_s: 31.078620672225952\n",
      "  timers:\n",
      "    learn_throughput: 475.052\n",
      "    learn_time_ms: 6315.098\n",
      "    load_throughput: 66745.767\n",
      "    load_time_ms: 44.947\n",
      "    sample_throughput: 760.883\n",
      "    sample_time_ms: 3942.79\n",
      "    update_time_ms: 5.264\n",
      "  timestamp: 1624524375\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 3\n",
      "  trial_id: '83919_00000'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00001:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.000000000000016\n",
      "  episode_reward_mean: -31.26000000000004\n",
      "  episode_reward_min: -48.00000000000008\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 90\n",
      "  experiment_id: 38bdf32c865b44ec9dfdfa051cfd3bca\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0488428920507431\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.7631802558898926\n",
      "          model: {}\n",
      "          policy_loss: 0.07700446993112564\n",
      "          total_loss: 56.744136810302734\n",
      "          vf_explained_var: 0.06207181140780449\n",
      "          vf_loss: 55.4237060546875\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.273618221282959\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027872901409864426\n",
      "          model: {}\n",
      "          policy_loss: -0.06016816198825836\n",
      "          total_loss: 2.4626219272613525\n",
      "          vf_explained_var: 0.5540760159492493\n",
      "          vf_loss: 2.5102474689483643\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.0375\n",
      "    ram_util_percent: 69.2\n",
      "  pid: 2670\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: 4.299999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: -22.738888888888887\n",
      "    policy2: -8.521111111111093\n",
      "  policy_reward_min:\n",
      "    policy1: -43.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12567760495290625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0549953162051934\n",
      "    mean_inference_ms: 1.9768780640657528\n",
      "    mean_raw_obs_processing_ms: 0.27164816813410947\n",
      "  time_since_restore: 31.025849103927612\n",
      "  time_this_iter_s: 11.182270050048828\n",
      "  time_total_s: 31.025849103927612\n",
      "  timers:\n",
      "    learn_throughput: 479.399\n",
      "    learn_time_ms: 6257.838\n",
      "    load_throughput: 68822.514\n",
      "    load_time_ms: 43.59\n",
      "    sample_throughput: 752.804\n",
      "    sample_time_ms: 3985.099\n",
      "    update_time_ms: 6.56\n",
      "  timestamp: 1624524376\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 3\n",
      "  trial_id: '83919_00001'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00002:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.699999999999918\n",
      "  episode_reward_mean: -2.08799999999999\n",
      "  episode_reward_min: -33.000000000000036\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: c49aef7b1b5542e194a1608b246ea2a9\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3165600299835205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017024165019392967\n",
      "          model: {}\n",
      "          policy_loss: -0.04819672927260399\n",
      "          total_loss: 27.994075775146484\n",
      "          vf_explained_var: 0.43264496326446533\n",
      "          vf_loss: 28.037168502807617\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2749582529067993\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026819780468940735\n",
      "          model: {}\n",
      "          policy_loss: -0.061643145978450775\n",
      "          total_loss: 2.8891725540161133\n",
      "          vf_explained_var: 0.27856311202049255\n",
      "          vf_loss: 2.938746452331543\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.02916666666667\n",
      "    ram_util_percent: 69.47083333333333\n",
      "  pid: 2674\n",
      "  policy_reward_max:\n",
      "    policy1: 34.5\n",
      "    policy2: -0.0999999999999881\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.69\n",
      "    policy2: -7.777999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12735953995346633\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05454683190509087\n",
      "    mean_inference_ms: 2.0021295381334543\n",
      "    mean_raw_obs_processing_ms: 0.2671495726129366\n",
      "  time_since_restore: 43.40005970001221\n",
      "  time_this_iter_s: 16.737802028656006\n",
      "  time_total_s: 43.40005970001221\n",
      "  timers:\n",
      "    learn_throughput: 448.993\n",
      "    learn_time_ms: 8908.826\n",
      "    load_throughput: 76056.679\n",
      "    load_time_ms: 52.592\n",
      "    sample_throughput: 734.11\n",
      "    sample_time_ms: 5448.772\n",
      "    update_time_ms: 7.195\n",
      "  timestamp: 1624524388\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: '83919_00002'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>RUNNING </td><td>192.168.0.179:2675</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         31.0786</td><td style=\"text-align: right;\"> 9000</td><td style=\"text-align: right;\"> -3.73333</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>RUNNING </td><td>192.168.0.179:2670</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         31.0258</td><td style=\"text-align: right;\"> 9000</td><td style=\"text-align: right;\">-31.26   </td><td style=\"text-align: right;\">                15  </td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>RUNNING </td><td>192.168.0.179:2674</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         43.4001</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -2.088  </td><td style=\"text-align: right;\">                26.7</td><td style=\"text-align: right;\">               -33  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>RUNNING </td><td>192.168.0.179:2677</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         26.8928</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">-27.7163 </td><td style=\"text-align: right;\">                12  </td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_83919_00003:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.699999999999969\n",
      "  episode_reward_mean: -37.944000000000045\n",
      "  episode_reward_min: -46.500000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 777bea0709a34c368f94f4b08cbe8c9b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.331962483699158e-09\n",
      "          model: {}\n",
      "          policy_loss: -0.0007407463272102177\n",
      "          total_loss: 84.66165924072266\n",
      "          vf_explained_var: 0.10813942551612854\n",
      "          vf_loss: 84.66240692138672\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2471791505813599\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02472626231610775\n",
      "          model: {}\n",
      "          policy_loss: -0.0469743013381958\n",
      "          total_loss: 6.413702487945557\n",
      "          vf_explained_var: 0.30558180809020996\n",
      "          vf_loss: 6.449549674987793\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.6\n",
      "    ram_util_percent: 69.47826086956523\n",
      "  pid: 2677\n",
      "  policy_reward_max:\n",
      "    policy1: 19.5\n",
      "    policy2: 8.699999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: -31.255\n",
      "    policy2: -6.688999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -45.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12622916369021578\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0548617613642206\n",
      "    mean_inference_ms: 2.018634722977499\n",
      "    mean_raw_obs_processing_ms: 0.2694390828811978\n",
      "  time_since_restore: 43.391782999038696\n",
      "  time_this_iter_s: 16.49894094467163\n",
      "  time_total_s: 43.391782999038696\n",
      "  timers:\n",
      "    learn_throughput: 449.228\n",
      "    learn_time_ms: 8904.17\n",
      "    load_throughput: 70716.738\n",
      "    load_time_ms: 56.564\n",
      "    sample_throughput: 733.875\n",
      "    sample_time_ms: 5450.517\n",
      "    update_time_ms: 5.206\n",
      "  timestamp: 1624524388\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: '83919_00003'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999945\n",
      "  episode_reward_mean: -0.7859999999999892\n",
      "  episode_reward_min: -24.00000000000005\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 120\n",
      "  experiment_id: 43c5f1bf54a446d6ac106c951f38d022\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.280354380607605\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019933432340621948\n",
      "          model: {}\n",
      "          policy_loss: -0.06500809639692307\n",
      "          total_loss: 26.117982864379883\n",
      "          vf_explained_var: 0.4396723806858063\n",
      "          vf_loss: 26.174026489257812\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2293742895126343\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02492038533091545\n",
      "          model: {}\n",
      "          policy_loss: -0.06643515825271606\n",
      "          total_loss: 3.5759403705596924\n",
      "          vf_explained_var: 0.3846901059150696\n",
      "          vf_loss: 3.625553846359253\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.44117647058823\n",
      "    ram_util_percent: 69.58823529411765\n",
      "  pid: 2675\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: 9.800000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.135\n",
      "    policy2: -7.920999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -17.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1329172032885749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.057550090204053175\n",
      "    mean_inference_ms: 2.0580447193036906\n",
      "    mean_raw_obs_processing_ms: 0.28657437187765616\n",
      "  time_since_restore: 43.349509716033936\n",
      "  time_this_iter_s: 12.270889043807983\n",
      "  time_total_s: 43.349509716033936\n",
      "  timers:\n",
      "    learn_throughput: 448.515\n",
      "    learn_time_ms: 6688.734\n",
      "    load_throughput: 86618.012\n",
      "    load_time_ms: 34.635\n",
      "    sample_throughput: 737.537\n",
      "    sample_time_ms: 4067.593\n",
      "    update_time_ms: 5.465\n",
      "  timestamp: 1624524388\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 4\n",
      "  trial_id: '83919_00000'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.600000000000026\n",
      "  episode_reward_mean: -40.51200000000005\n",
      "  episode_reward_min: -48.00000000000008\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 120\n",
      "  experiment_id: 38bdf32c865b44ec9dfdfa051cfd3bca\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0018394350772723556\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029386624693870544\n",
      "          model: {}\n",
      "          policy_loss: -0.00014596334949601442\n",
      "          total_loss: 126.78753662109375\n",
      "          vf_explained_var: 0.1086287572979927\n",
      "          vf_loss: 126.76785278320312\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2640140056610107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015236832201480865\n",
      "          model: {}\n",
      "          policy_loss: -0.04080624878406525\n",
      "          total_loss: 17.182188034057617\n",
      "          vf_explained_var: 0.3381587862968445\n",
      "          vf_loss: 17.212709426879883\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.41666666666669\n",
      "    ram_util_percent: 69.58888888888889\n",
      "  pid: 2670\n",
      "  policy_reward_max:\n",
      "    policy1: 15.5\n",
      "    policy2: 47.19999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: -32.69\n",
      "    policy2: -7.821999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -62.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13219237169798656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05778962328583804\n",
      "    mean_inference_ms: 2.0796389559956636\n",
      "    mean_raw_obs_processing_ms: 0.2860392304493828\n",
      "  time_since_restore: 43.41788196563721\n",
      "  time_this_iter_s: 12.392032861709595\n",
      "  time_total_s: 43.41788196563721\n",
      "  timers:\n",
      "    learn_throughput: 451.267\n",
      "    learn_time_ms: 6647.946\n",
      "    load_throughput: 89237.663\n",
      "    load_time_ms: 33.618\n",
      "    sample_throughput: 726.997\n",
      "    sample_time_ms: 4126.564\n",
      "    update_time_ms: 6.747\n",
      "  timestamp: 1624524388\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 4\n",
      "  trial_id: '83919_00001'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-40\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999945\n",
      "  episode_reward_mean: 1.0890000000000097\n",
      "  episode_reward_min: -24.00000000000005\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 150\n",
      "  experiment_id: 43c5f1bf54a446d6ac106c951f38d022\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.251846432685852\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016989026218652725\n",
      "          model: {}\n",
      "          policy_loss: -0.059918273240327835\n",
      "          total_loss: 25.520841598510742\n",
      "          vf_explained_var: 0.3757430911064148\n",
      "          vf_loss: 25.57311248779297\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2176501750946045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0200651865452528\n",
      "          model: {}\n",
      "          policy_loss: -0.06906390190124512\n",
      "          total_loss: 2.931469202041626\n",
      "          vf_explained_var: 0.3495412766933441\n",
      "          vf_loss: 2.980217218399048\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.21176470588236\n",
      "    ram_util_percent: 69.78823529411764\n",
      "  pid: 2675\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: 9.800000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.46\n",
      "    policy2: -7.370999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -17.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13770736103379852\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.059650622142528906\n",
      "    mean_inference_ms: 2.1370365550005266\n",
      "    mean_raw_obs_processing_ms: 0.2975132662660366\n",
      "  time_since_restore: 54.92128086090088\n",
      "  time_this_iter_s: 11.571771144866943\n",
      "  time_total_s: 54.92128086090088\n",
      "  timers:\n",
      "    learn_throughput: 437.844\n",
      "    learn_time_ms: 6851.757\n",
      "    load_throughput: 105406.416\n",
      "    load_time_ms: 28.461\n",
      "    sample_throughput: 738.239\n",
      "    sample_time_ms: 4063.727\n",
      "    update_time_ms: 5.697\n",
      "  timestamp: 1624524400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 5\n",
      "  trial_id: '83919_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>RUNNING </td><td>192.168.0.179:2675</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         54.9213</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">   1.089</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -24  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>RUNNING </td><td>192.168.0.179:2670</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         43.4179</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -40.512</td><td style=\"text-align: right;\">                 6.6</td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>RUNNING </td><td>192.168.0.179:2674</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         43.4001</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -2.088</td><td style=\"text-align: right;\">                26.7</td><td style=\"text-align: right;\">               -33  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>RUNNING </td><td>192.168.0.179:2677</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         43.3918</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -37.944</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_83919_00001:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-40\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -15.299999999999994\n",
      "  episode_reward_mean: -43.872000000000064\n",
      "  episode_reward_min: -48.00000000000008\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 150\n",
      "  experiment_id: 38bdf32c865b44ec9dfdfa051cfd3bca\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.004095461219549179\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.16092397272586823\n",
      "          model: {}\n",
      "          policy_loss: 0.051739536225795746\n",
      "          total_loss: 95.20844268798828\n",
      "          vf_explained_var: 0.10867179930210114\n",
      "          vf_loss: 94.99378204345703\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2013071775436401\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02281377837061882\n",
      "          model: {}\n",
      "          policy_loss: -0.046474866569042206\n",
      "          total_loss: 9.4592866897583\n",
      "          vf_explained_var: 0.3502912223339081\n",
      "          vf_loss: 9.490362167358398\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.3\n",
      "    ram_util_percent: 69.79375\n",
      "  pid: 2670\n",
      "  policy_reward_max:\n",
      "    policy1: -29.0\n",
      "    policy2: 47.19999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: -37.48\n",
      "    policy2: -6.391999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -62.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13779135145873325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06025668216811887\n",
      "    mean_inference_ms: 2.166517606009812\n",
      "    mean_raw_obs_processing_ms: 0.29902887312290694\n",
      "  time_since_restore: 55.04816913604736\n",
      "  time_this_iter_s: 11.630287170410156\n",
      "  time_total_s: 55.04816913604736\n",
      "  timers:\n",
      "    learn_throughput: 439.589\n",
      "    learn_time_ms: 6824.557\n",
      "    load_throughput: 107811.824\n",
      "    load_time_ms: 27.826\n",
      "    sample_throughput: 728.669\n",
      "    sample_time_ms: 4117.095\n",
      "    update_time_ms: 6.546\n",
      "  timestamp: 1624524400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 5\n",
      "  trial_id: '83919_00001'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00002:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 27.59999999999991\n",
      "  episode_reward_mean: 2.1210000000000075\n",
      "  episode_reward_min: -19.500000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: c49aef7b1b5542e194a1608b246ea2a9\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2858389616012573\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019233517348766327\n",
      "          model: {}\n",
      "          policy_loss: -0.05634910985827446\n",
      "          total_loss: 26.89621925354004\n",
      "          vf_explained_var: 0.3860708475112915\n",
      "          vf_loss: 26.946792602539062\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.256722092628479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0231766514480114\n",
      "          model: {}\n",
      "          policy_loss: -0.06373517960309982\n",
      "          total_loss: 2.837135076522827\n",
      "          vf_explained_var: 0.2912205457687378\n",
      "          vf_loss: 2.885225534439087\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.99545454545456\n",
      "    ram_util_percent: 68.90909090909089\n",
      "  pid: 2674\n",
      "  policy_reward_max:\n",
      "    policy1: 36.5\n",
      "    policy2: 0.9999999999999974\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.635\n",
      "    policy2: -7.513999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -15.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13762167797970593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.058437466686413364\n",
      "    mean_inference_ms: 2.144904586647178\n",
      "    mean_raw_obs_processing_ms: 0.2882661375597641\n",
      "  time_since_restore: 58.85286235809326\n",
      "  time_this_iter_s: 15.452802658081055\n",
      "  time_total_s: 58.85286235809326\n",
      "  timers:\n",
      "    learn_throughput: 439.829\n",
      "    learn_time_ms: 9094.451\n",
      "    load_throughput: 95701.235\n",
      "    load_time_ms: 41.797\n",
      "    sample_throughput: 723.112\n",
      "    sample_time_ms: 5531.65\n",
      "    update_time_ms: 6.431\n",
      "  timestamp: 1624524403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: '83919_00002'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00003:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -25.500000000000014\n",
      "  episode_reward_mean: -41.40600000000004\n",
      "  episode_reward_min: -46.500000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 777bea0709a34c368f94f4b08cbe8c9b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          model: {}\n",
      "          policy_loss: 0.0024662308860570192\n",
      "          total_loss: 108.67005157470703\n",
      "          vf_explained_var: 0.08102112263441086\n",
      "          vf_loss: 108.66757202148438\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.163246512413025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02405012585222721\n",
      "          model: {}\n",
      "          policy_loss: -0.050368454307317734\n",
      "          total_loss: 20.483217239379883\n",
      "          vf_explained_var: 0.1175333559513092\n",
      "          vf_loss: 20.517351150512695\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.88636363636364\n",
      "    ram_util_percent: 68.92727272727271\n",
      "  pid: 2677\n",
      "  policy_reward_max:\n",
      "    policy1: -36.5\n",
      "    policy2: 28.499999999999982\n",
      "  policy_reward_mean:\n",
      "    policy1: -40.745\n",
      "    policy2: -0.660999999999992\n",
      "  policy_reward_min:\n",
      "    policy1: -54.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13512870544926348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.058345022924319\n",
      "    mean_inference_ms: 2.149925429608323\n",
      "    mean_raw_obs_processing_ms: 0.28952017014256237\n",
      "  time_since_restore: 58.80850696563721\n",
      "  time_this_iter_s: 15.41672396659851\n",
      "  time_total_s: 58.80850696563721\n",
      "  timers:\n",
      "    learn_throughput: 441.753\n",
      "    learn_time_ms: 9054.825\n",
      "    load_throughput: 90384.256\n",
      "    load_time_ms: 44.255\n",
      "    sample_throughput: 719.315\n",
      "    sample_time_ms: 5560.848\n",
      "    update_time_ms: 4.767\n",
      "  timestamp: 1624524403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: '83919_00003'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00002:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-53\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 27.59999999999991\n",
      "  episode_reward_mean: 1.7430000000000065\n",
      "  episode_reward_min: -16.500000000000004\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: c49aef7b1b5542e194a1608b246ea2a9\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2454942464828491\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02204127609729767\n",
      "          model: {}\n",
      "          policy_loss: -0.05986357107758522\n",
      "          total_loss: 29.853612899780273\n",
      "          vf_explained_var: 0.3463425934314728\n",
      "          vf_loss: 29.906862258911133\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2331124544143677\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01736060343682766\n",
      "          model: {}\n",
      "          policy_loss: -0.05920691788196564\n",
      "          total_loss: 3.2174232006073\n",
      "          vf_explained_var: 0.33934932947158813\n",
      "          vf_loss: 3.2590525150299072\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.07999999999999\n",
      "    ram_util_percent: 61.86000000000001\n",
      "  pid: 2674\n",
      "  policy_reward_max:\n",
      "    policy1: 36.5\n",
      "    policy2: 4.299999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.4\n",
      "    policy2: -7.656999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -12.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13794494721605408\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05861953649363378\n",
      "    mean_inference_ms: 2.1377502243785567\n",
      "    mean_raw_obs_processing_ms: 0.28963978496130316\n",
      "  time_since_restore: 68.79273128509521\n",
      "  time_this_iter_s: 9.939868927001953\n",
      "  time_total_s: 68.79273128509521\n",
      "  timers:\n",
      "    learn_throughput: 466.637\n",
      "    learn_time_ms: 8571.98\n",
      "    load_throughput: 116098.873\n",
      "    load_time_ms: 34.453\n",
      "    sample_throughput: 782.176\n",
      "    sample_time_ms: 5113.937\n",
      "    update_time_ms: 5.748\n",
      "  timestamp: 1624524413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: '83919_00002'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>RUNNING   </td><td>192.168.0.179:2674</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         68.7927</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   1.743</td><td style=\"text-align: right;\">                27.6</td><td style=\"text-align: right;\">               -16.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>RUNNING   </td><td>192.168.0.179:2677</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         58.8085</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -41.406</td><td style=\"text-align: right;\">               -25.5</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         54.9213</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">   1.089</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -24  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         55.0482</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\"> -43.872</td><td style=\"text-align: right;\">               -15.3</td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_83919_00003:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-53\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -21.900000000000006\n",
      "  episode_reward_mean: -36.06600000000004\n",
      "  episode_reward_min: -46.500000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 777bea0709a34c368f94f4b08cbe8c9b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11249999701976776\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          model: {}\n",
      "          policy_loss: -0.0009185558883473277\n",
      "          total_loss: 146.60562133789062\n",
      "          vf_explained_var: 0.07484202086925507\n",
      "          vf_loss: 146.60653686523438\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.092926025390625\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012513847090303898\n",
      "          model: {}\n",
      "          policy_loss: -0.03175092115998268\n",
      "          total_loss: 32.38434600830078\n",
      "          vf_explained_var: 0.03248586133122444\n",
      "          vf_loss: 32.40342712402344\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.09333333333333\n",
      "    ram_util_percent: 61.853333333333346\n",
      "  pid: 2677\n",
      "  policy_reward_max:\n",
      "    policy1: -36.5\n",
      "    policy2: 35.09999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: -45.195\n",
      "    policy2: 9.129\n",
      "  policy_reward_min:\n",
      "    policy1: -57.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13505947328127552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.058292435672735045\n",
      "    mean_inference_ms: 2.1374865708771695\n",
      "    mean_raw_obs_processing_ms: 0.2901064345074678\n",
      "  time_since_restore: 68.77974581718445\n",
      "  time_this_iter_s: 9.971238851547241\n",
      "  time_total_s: 68.77974581718445\n",
      "  timers:\n",
      "    learn_throughput: 468.557\n",
      "    learn_time_ms: 8536.854\n",
      "    load_throughput: 109744.667\n",
      "    load_time_ms: 36.448\n",
      "    sample_throughput: 777.121\n",
      "    sample_time_ms: 5147.203\n",
      "    update_time_ms: 4.335\n",
      "  timestamp: 1624524413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: '83919_00003'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         54.9213</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">   1.089</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -24  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         55.0482</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\"> -43.872</td><td style=\"text-align: right;\">               -15.3</td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         68.7927</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   1.743</td><td style=\"text-align: right;\">                27.6</td><td style=\"text-align: right;\">               -16.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         68.7797</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> -36.066</td><td style=\"text-align: right;\">               -21.9</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 10:46:54,581\tINFO tune.py:549 -- Total run time: 91.50 seconds (90.98 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fbc14c9b970>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plugging in Ray Tune.\n",
    "# Note that this is the recommended way to run any experiments with RLlib.\n",
    "# Reasons:\n",
    "# - Tune allows you to do hyperparameter tuning in a user-friendly way\n",
    "#   and at large scale!\n",
    "# - Tune automatically allocates needed resources for the different\n",
    "#   hyperparam trials and experiment runs on a cluster.\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Running stuff with tune, we can re-use the exact\n",
    "# same config that we used when working with RLlib directly!\n",
    "tune_config = config.copy()\n",
    "\n",
    "# Let's add our first hyperparameter search via our config.\n",
    "# How about we try two different learning rates? Let's say 0.00005 and 0.5 (ouch!).\n",
    "tune_config[\"lr\"] = tune.grid_search([0.00005, 0.5])  # <- 0.5? again: ouch!\n",
    "tune_config[\"train_batch_size\"] = tune.grid_search([3000, 4000])\n",
    "\n",
    "# Now that we will run things \"automatically\" through tune, we have to\n",
    "# define one or more stopping criteria.\n",
    "# Tune will stop the run, once any single one of the criteria is matched (not all of them!).\n",
    "stop = {\n",
    "    # Note that the keys used here can be anything present in the above `rllib_trainer.train()` output dict.\n",
    "    \"training_iteration\": 5,\n",
    "    \"episode_reward_mean\": 20.0,\n",
    "}\n",
    "\n",
    "# \"PPO\" is a registered name that points to RLlib's PPOTrainer.\n",
    "# See `ray/rllib/agents/registry.py`\n",
    "\n",
    "# Run a simple experiment until one of the stopping criteria is met.\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=tune_config,\n",
    "    stop=stop,\n",
    "\n",
    "    # Note that no trainers will be returned from this call here.\n",
    "    # Tune will create n Trainers internally, run them in parallel and destroy them at the end.\n",
    "    # However, you can ...\n",
    "    checkpoint_at_end=True,  # ... create a checkpoint when done.\n",
    "    checkpoint_freq=10,  # ... create a checkpoint every 10 training iterations.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b886fb8-6ccd-4be2-80bb-fc0936808d11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Why did we use 6 CPUs in the tune run above (3 CPUs per trial)?\n",
    "\n",
    "PPO - by default - uses 2 \"rollout\" workers (`num_workers=2`). These are Ray Actors that have their own environment copy(ies) and step through those in parallel. On top of these two \"rollout\" workers, every Trainer in RLlib always also has a \"local\" worker, which - in case of PPO - handles the learning updates. This gives us 3 workers (2 rollout + 1 local learner), which require 3 CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a74ec7-a6c1-431d-83aa-35df56d93185",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise No 2\n",
    "\n",
    "<hr />\n",
    "\n",
    "Using the `tune_config` that we have built so far, let's run another `tune.run()`, but apply the following changes to our setup this time:\n",
    "- Setup only 1 learning rate under the \"lr\" config key. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Setup only 1 train batch size under the \"train_batch_size\" config key. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Set `num_workers` to 5, which will allow us to run more environment \"rollouts\" in parallel and to collect training batches more quickly.\n",
    "- Set the `num_envs_per_worker` config parameter to 5. This will clone our env on each rollout worker, and thus parallelize action computing forward passes through our neural networks.\n",
    "\n",
    "Other than that, use the exact same args as in our `tune.run()` call in the previous cell.\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff184330-4229-4476-a9e0-1fdbaed948d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2801)\u001b[0m 2021-06-24 10:51:43,603\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=2801)\u001b[0m 2021-06-24 10:51:43,604\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=2801)\u001b[0m 2021-06-24 10:51:43,604\tWARNING ppo.py:135 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=5 num_envs_per_worker=5 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 160.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=2801)\u001b[0m 2021-06-24 10:51:56,770\tINFO trainable.py:101 -- Trainable.setup took 13.167 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=2801)\u001b[0m 2021-06-24 10:51:56,770\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-52-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.999999999999936\n",
      "  episode_reward_mean: -8.483999999999995\n",
      "  episode_reward_min: -34.500000000000036\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 25\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3588711023330688\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027996812015771866\n",
      "          model: {}\n",
      "          policy_loss: -0.05669461190700531\n",
      "          total_loss: 44.654815673828125\n",
      "          vf_explained_var: 0.09232431650161743\n",
      "          vf_loss: 44.70591354370117\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3487696647644043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.039114002138376236\n",
      "          model: {}\n",
      "          policy_loss: -0.07051993161439896\n",
      "          total_loss: 2.040412425994873\n",
      "          vf_explained_var: 0.31341245770454407\n",
      "          vf_loss: 2.103109836578369\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.825\n",
      "    ram_util_percent: 68.1\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 28.0\n",
      "    policy2: -6.6999999999999815\n",
      "  policy_reward_mean:\n",
      "    policy1: 1.12\n",
      "    policy2: -9.603999999999981\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1803880892925381\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10065114276009318\n",
      "    mean_inference_ms: 1.8377259651326243\n",
      "    mean_raw_obs_processing_ms: 0.6592643927343143\n",
      "  time_since_restore: 5.336621046066284\n",
      "  time_this_iter_s: 5.336621046066284\n",
      "  time_total_s: 5.336621046066284\n",
      "  timers:\n",
      "    learn_throughput: 855.819\n",
      "    learn_time_ms: 4673.886\n",
      "    load_throughput: 37221.824\n",
      "    load_time_ms: 107.464\n",
      "    sample_throughput: 8551.875\n",
      "    sample_time_ms: 467.734\n",
      "    update_time_ms: 3.065\n",
      "  timestamp: 1624524722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.33662</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  -8.484</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-52-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999943\n",
      "  episode_reward_mean: -5.075999999999994\n",
      "  episode_reward_min: -37.50000000000004\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 100\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2776226997375488\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026900572702288628\n",
      "          model: {}\n",
      "          policy_loss: -0.07335038483142853\n",
      "          total_loss: 29.200061798095703\n",
      "          vf_explained_var: 0.20351383090019226\n",
      "          vf_loss: 29.261306762695312\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2615963220596313\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03076518326997757\n",
      "          model: {}\n",
      "          policy_loss: -0.06708455830812454\n",
      "          total_loss: 1.9222501516342163\n",
      "          vf_explained_var: 0.38434791564941406\n",
      "          vf_loss: 1.975490689277649\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.916666666666668\n",
      "    ram_util_percent: 67.98333333333333\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 32.5\n",
      "    policy2: -1.2000000000000042\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.945\n",
      "    policy2: -9.020999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -27.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18618382504208733\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1039765163277514\n",
      "    mean_inference_ms: 1.66533838792357\n",
      "    mean_raw_obs_processing_ms: 0.6739571082107836\n",
      "  time_since_restore: 14.441888809204102\n",
      "  time_this_iter_s: 4.50461483001709\n",
      "  time_total_s: 14.441888809204102\n",
      "  timers:\n",
      "    learn_throughput: 923.507\n",
      "    learn_time_ms: 4331.318\n",
      "    load_throughput: 106612.26\n",
      "    load_time_ms: 37.519\n",
      "    sample_throughput: 9743.61\n",
      "    sample_time_ms: 410.525\n",
      "    update_time_ms: 2.614\n",
      "  timestamp: 1624524731\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         14.4419</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -5.076</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-52-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.599999999999973\n",
      "  episode_reward_mean: -1.2689999999999875\n",
      "  episode_reward_min: -27.00000000000003\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 200\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2463394403457642\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015143339522182941\n",
      "          model: {}\n",
      "          policy_loss: -0.05386582016944885\n",
      "          total_loss: 24.41973876953125\n",
      "          vf_explained_var: 0.2661151587963104\n",
      "          vf_loss: 24.458274841308594\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2167445421218872\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017633827403187752\n",
      "          model: {}\n",
      "          policy_loss: -0.055553462356328964\n",
      "          total_loss: 2.983912706375122\n",
      "          vf_explained_var: 0.18852190673351288\n",
      "          vf_loss: 3.0216116905212402\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.42857142857143\n",
      "    ram_util_percent: 67.67142857142856\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 31.0\n",
      "    policy2: 4.300000000000015\n",
      "  policy_reward_mean:\n",
      "    policy1: 6.52\n",
      "    policy2: -7.788999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -17.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18667602886585313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10429512578409497\n",
      "    mean_inference_ms: 1.4869503541662628\n",
      "    mean_raw_obs_processing_ms: 0.6743472097051147\n",
      "  time_since_restore: 23.885329723358154\n",
      "  time_this_iter_s: 4.868424892425537\n",
      "  time_total_s: 23.885329723358154\n",
      "  timers:\n",
      "    learn_throughput: 924.738\n",
      "    learn_time_ms: 4325.549\n",
      "    load_throughput: 170199.201\n",
      "    load_time_ms: 23.502\n",
      "    sample_throughput: 9902.327\n",
      "    sample_time_ms: 403.945\n",
      "    update_time_ms: 2.584\n",
      "  timestamp: 1624524740\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         23.8853</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -1.269</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">                 -27</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-52-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999948\n",
      "  episode_reward_mean: 1.3830000000000056\n",
      "  episode_reward_min: -19.499999999999993\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 275\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2091331481933594\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016251813620328903\n",
      "          model: {}\n",
      "          policy_loss: -0.058222174644470215\n",
      "          total_loss: 26.836854934692383\n",
      "          vf_explained_var: 0.4003700613975525\n",
      "          vf_loss: 26.878620147705078\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.1863893270492554\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017240630462765694\n",
      "          model: {}\n",
      "          policy_loss: -0.054528843611478806\n",
      "          total_loss: 2.038210391998291\n",
      "          vf_explained_var: 0.28468000888824463\n",
      "          vf_loss: 2.0752835273742676\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.300000000000004\n",
      "    ram_util_percent: 67.7\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: 0.9999999999999948\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.985\n",
      "    policy2: -7.601999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -9.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18734152929110506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10459180652046932\n",
      "    mean_inference_ms: 1.4691107706448272\n",
      "    mean_raw_obs_processing_ms: 0.6764412724286976\n",
      "  time_since_restore: 33.146549701690674\n",
      "  time_this_iter_s: 4.601808786392212\n",
      "  time_total_s: 33.146549701690674\n",
      "  timers:\n",
      "    learn_throughput: 931.147\n",
      "    learn_time_ms: 4295.779\n",
      "    load_throughput: 228239.705\n",
      "    load_time_ms: 17.525\n",
      "    sample_throughput: 9936.871\n",
      "    sample_time_ms: 402.541\n",
      "    update_time_ms: 2.508\n",
      "  timestamp: 1624524750\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         33.1465</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   1.383</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-52-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.99999999999995\n",
      "  episode_reward_mean: 3.495000000000006\n",
      "  episode_reward_min: -12.599999999999982\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 350\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1770472526550293\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015331901609897614\n",
      "          model: {}\n",
      "          policy_loss: -0.05267131328582764\n",
      "          total_loss: 34.16590118408203\n",
      "          vf_explained_var: 0.384956032037735\n",
      "          vf_loss: 34.20304870605469\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.1497039794921875\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01865207776427269\n",
      "          model: {}\n",
      "          policy_loss: -0.05684083327651024\n",
      "          total_loss: 3.0013606548309326\n",
      "          vf_explained_var: 0.2907329499721527\n",
      "          vf_loss: 3.039316177368164\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.72857142857143\n",
      "    ram_util_percent: 67.7\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 37.0\n",
      "    policy2: 1.0000000000000133\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.8\n",
      "    policy2: -7.304999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -8.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18841656284174618\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10507025749676335\n",
      "    mean_inference_ms: 1.4641098677143014\n",
      "    mean_raw_obs_processing_ms: 0.6825804579829571\n",
      "  time_since_restore: 42.70558762550354\n",
      "  time_this_iter_s: 4.663714647293091\n",
      "  time_total_s: 42.70558762550354\n",
      "  timers:\n",
      "    learn_throughput: 927.491\n",
      "    learn_time_ms: 4312.712\n",
      "    load_throughput: 281992.134\n",
      "    load_time_ms: 14.185\n",
      "    sample_throughput: 9964.054\n",
      "    sample_time_ms: 401.443\n",
      "    update_time_ms: 2.459\n",
      "  timestamp: 1624524759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         42.7056</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   3.495</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">               -12.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-52-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.799999999999955\n",
      "  episode_reward_mean: 6.1229999999999976\n",
      "  episode_reward_min: -21.00000000000001\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 425\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1281288862228394\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017284387722611427\n",
      "          model: {}\n",
      "          policy_loss: -0.06095132231712341\n",
      "          total_loss: 31.74726104736328\n",
      "          vf_explained_var: 0.42851197719573975\n",
      "          vf_loss: 31.790712356567383\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.1163604259490967\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01815684698522091\n",
      "          model: {}\n",
      "          policy_loss: -0.05945218726992607\n",
      "          total_loss: 1.8503273725509644\n",
      "          vf_explained_var: 0.3132403790950775\n",
      "          vf_loss: 1.891395926475525\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.057142857142857\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 37.0\n",
      "    policy2: 1.0000000000000133\n",
      "  policy_reward_mean:\n",
      "    policy1: 13.395\n",
      "    policy2: -7.271999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -11.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18780611859259452\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10491313038838455\n",
      "    mean_inference_ms: 1.4540669132059962\n",
      "    mean_raw_obs_processing_ms: 0.681391593321603\n",
      "  time_since_restore: 51.90550756454468\n",
      "  time_this_iter_s: 4.553214073181152\n",
      "  time_total_s: 51.90550756454468\n",
      "  timers:\n",
      "    learn_throughput: 940.071\n",
      "    learn_time_ms: 4254.996\n",
      "    load_throughput: 1604538.595\n",
      "    load_time_ms: 2.493\n",
      "    sample_throughput: 10212.627\n",
      "    sample_time_ms: 391.672\n",
      "    update_time_ms: 2.361\n",
      "  timestamp: 1624524769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         51.9055</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   6.123</td><td style=\"text-align: right;\">                28.8</td><td style=\"text-align: right;\">                 -21</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-52-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.799999999999955\n",
      "  episode_reward_mean: 8.879999999999994\n",
      "  episode_reward_min: -11.099999999999996\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 500\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0732851028442383\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01739729940891266\n",
      "          model: {}\n",
      "          policy_loss: -0.05874611437320709\n",
      "          total_loss: 33.32643127441406\n",
      "          vf_explained_var: 0.38476911187171936\n",
      "          vf_loss: 33.36756134033203\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.0671054124832153\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017925459891557693\n",
      "          model: {}\n",
      "          policy_loss: -0.055113520473241806\n",
      "          total_loss: 2.705533742904663\n",
      "          vf_explained_var: 0.22145208716392517\n",
      "          vf_loss: 2.7424981594085693\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.75\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 36.5\n",
      "    policy2: -0.10000000000000081\n",
      "  policy_reward_mean:\n",
      "    policy1: 16.02\n",
      "    policy2: -7.139999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -11.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18843760427297632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10541449285624176\n",
      "    mean_inference_ms: 1.4498803333323103\n",
      "    mean_raw_obs_processing_ms: 0.6861005328261733\n",
      "  time_since_restore: 61.210946559906006\n",
      "  time_this_iter_s: 4.571083068847656\n",
      "  time_total_s: 61.210946559906006\n",
      "  timers:\n",
      "    learn_throughput: 936.448\n",
      "    learn_time_ms: 4271.461\n",
      "    load_throughput: 1592824.077\n",
      "    load_time_ms: 2.511\n",
      "    sample_throughput: 10117.043\n",
      "    sample_time_ms: 395.372\n",
      "    update_time_ms: 2.353\n",
      "  timestamp: 1624524778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         61.2109</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">    8.88</td><td style=\"text-align: right;\">                28.8</td><td style=\"text-align: right;\">               -11.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-53-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.499999999999915\n",
      "  episode_reward_mean: 12.90299999999998\n",
      "  episode_reward_min: -15.899999999999984\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 600\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.041711688041687\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01560800801962614\n",
      "          model: {}\n",
      "          policy_loss: -0.05298454314470291\n",
      "          total_loss: 45.75339126586914\n",
      "          vf_explained_var: 0.2644711434841156\n",
      "          vf_loss: 45.79058074951172\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.0430861711502075\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01725194975733757\n",
      "          model: {}\n",
      "          policy_loss: -0.050654247403144836\n",
      "          total_loss: 3.441502571105957\n",
      "          vf_explained_var: 0.1471896916627884\n",
      "          vf_loss: 3.47468900680542\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.416666666666668\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 44.5\n",
      "    policy2: 2.1000000000000045\n",
      "  policy_reward_mean:\n",
      "    policy1: 19.845\n",
      "    policy2: -6.941999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -7.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18899058675113153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1055464521529491\n",
      "    mean_inference_ms: 1.4461177659488147\n",
      "    mean_raw_obs_processing_ms: 0.6908022666487443\n",
      "  time_since_restore: 70.40173149108887\n",
      "  time_this_iter_s: 4.566832065582275\n",
      "  time_total_s: 70.40173149108887\n",
      "  timers:\n",
      "    learn_throughput: 942.383\n",
      "    learn_time_ms: 4244.56\n",
      "    load_throughput: 1610607.582\n",
      "    load_time_ms: 2.484\n",
      "    sample_throughput: 10070.809\n",
      "    sample_time_ms: 397.188\n",
      "    update_time_ms: 2.315\n",
      "  timestamp: 1624524787\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         70.4017</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  12.903</td><td style=\"text-align: right;\">                34.5</td><td style=\"text-align: right;\">               -15.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-53-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.49999999999991\n",
      "  episode_reward_mean: 16.301999999999964\n",
      "  episode_reward_min: -11.999999999999993\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 675\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9933565258979797\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015695845708251\n",
      "          model: {}\n",
      "          policy_loss: -0.05434282869100571\n",
      "          total_loss: 37.19157791137695\n",
      "          vf_explained_var: 0.33458060026168823\n",
      "          vf_loss: 37.230037689208984\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.028895378112793\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01564483903348446\n",
      "          model: {}\n",
      "          policy_loss: -0.048475589603185654\n",
      "          total_loss: 2.7131881713867188\n",
      "          vf_explained_var: 0.19221122562885284\n",
      "          vf_loss: 2.745824098587036\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.949999999999996\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 44.5\n",
      "    policy2: 4.299999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 22.76\n",
      "    policy2: -6.4579999999999895\n",
      "  policy_reward_min:\n",
      "    policy1: -3.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1883786260173379\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10529333037907357\n",
      "    mean_inference_ms: 1.4435778478117494\n",
      "    mean_raw_obs_processing_ms: 0.6881044391834817\n",
      "  time_since_restore: 79.5873703956604\n",
      "  time_this_iter_s: 4.66040301322937\n",
      "  time_total_s: 79.5873703956604\n",
      "  timers:\n",
      "    learn_throughput: 943.238\n",
      "    learn_time_ms: 4240.713\n",
      "    load_throughput: 1625242.519\n",
      "    load_time_ms: 2.461\n",
      "    sample_throughput: 10165.927\n",
      "    sample_time_ms: 393.471\n",
      "    update_time_ms: 2.337\n",
      "  timestamp: 1624524796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         79.5874</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  16.302</td><td style=\"text-align: right;\">                37.5</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-53-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.099999999999916\n",
      "  episode_reward_mean: 18.67799999999995\n",
      "  episode_reward_min: -6.899999999999993\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 750\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9335061311721802\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014588603749871254\n",
      "          model: {}\n",
      "          policy_loss: -0.05031830444931984\n",
      "          total_loss: 39.424007415771484\n",
      "          vf_explained_var: 0.3828079104423523\n",
      "          vf_loss: 39.459564208984375\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.9802654981613159\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015616741962730885\n",
      "          model: {}\n",
      "          policy_loss: -0.04722469300031662\n",
      "          total_loss: 2.228907823562622\n",
      "          vf_explained_var: 0.24936676025390625\n",
      "          vf_loss: 2.2603209018707275\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.300000000000004\n",
      "    ram_util_percent: 67.55\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 44.0\n",
      "    policy2: 2.0999999999999974\n",
      "  policy_reward_mean:\n",
      "    policy1: 25.015\n",
      "    policy2: -6.336999999999989\n",
      "  policy_reward_min:\n",
      "    policy1: -4.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18748910081041228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1048080348509544\n",
      "    mean_inference_ms: 1.4334617017471698\n",
      "    mean_raw_obs_processing_ms: 0.6833796065263681\n",
      "  time_since_restore: 88.62667441368103\n",
      "  time_this_iter_s: 4.409440040588379\n",
      "  time_total_s: 88.62667441368103\n",
      "  timers:\n",
      "    learn_throughput: 953.549\n",
      "    learn_time_ms: 4194.856\n",
      "    load_throughput: 1646374.627\n",
      "    load_time_ms: 2.43\n",
      "    sample_throughput: 10327.069\n",
      "    sample_time_ms: 387.332\n",
      "    update_time_ms: 2.352\n",
      "  timestamp: 1624524806\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         88.6267</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  18.678</td><td style=\"text-align: right;\">                35.1</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-53-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.89999999999991\n",
      "  episode_reward_mean: 21.638999999999932\n",
      "  episode_reward_min: -6.899999999999993\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 825\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8775365948677063\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01357054989784956\n",
      "          model: {}\n",
      "          policy_loss: -0.04833472520112991\n",
      "          total_loss: 49.6264533996582\n",
      "          vf_explained_var: 0.36908942461013794\n",
      "          vf_loss: 49.66104507446289\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.9530884623527527\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015996383503079414\n",
      "          model: {}\n",
      "          policy_loss: -0.04699886590242386\n",
      "          total_loss: 3.329789876937866\n",
      "          vf_explained_var: 0.21668118238449097\n",
      "          vf_loss: 3.3605921268463135\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.150000000000002\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 45.0\n",
      "    policy2: -0.10000000000000264\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.13\n",
      "    policy2: -6.490999999999989\n",
      "  policy_reward_min:\n",
      "    policy1: 0.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18634631266609225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10416675745069298\n",
      "    mean_inference_ms: 1.4216928559240705\n",
      "    mean_raw_obs_processing_ms: 0.6777996004040239\n",
      "  time_since_restore: 97.18751645088196\n",
      "  time_this_iter_s: 4.272667169570923\n",
      "  time_total_s: 97.18751645088196\n",
      "  timers:\n",
      "    learn_throughput: 966.883\n",
      "    learn_time_ms: 4137.007\n",
      "    load_throughput: 1625242.519\n",
      "    load_time_ms: 2.461\n",
      "    sample_throughput: 10491.682\n",
      "    sample_time_ms: 381.254\n",
      "    update_time_ms: 2.334\n",
      "  timestamp: 1624524814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         97.1875</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  21.639</td><td style=\"text-align: right;\">                39.9</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-53-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.89999999999991\n",
      "  episode_reward_mean: 24.497999999999923\n",
      "  episode_reward_min: -8.399999999999977\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 900\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.851176381111145\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012521426193416119\n",
      "          model: {}\n",
      "          policy_loss: -0.041859906166791916\n",
      "          total_loss: 60.12490463256836\n",
      "          vf_explained_var: 0.2846677005290985\n",
      "          vf_loss: 60.15409469604492\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.9234663248062134\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01742134988307953\n",
      "          model: {}\n",
      "          policy_loss: -0.053981851786375046\n",
      "          total_loss: 2.958487033843994\n",
      "          vf_explained_var: 0.17945560812950134\n",
      "          vf_loss: 2.9948301315307617\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.483333333333334\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 13.10000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.68\n",
      "    policy2: -5.1819999999999915\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1857012560592322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10376188801988728\n",
      "    mean_inference_ms: 1.4084494880366731\n",
      "    mean_raw_obs_processing_ms: 0.6736448800020446\n",
      "  time_since_restore: 105.73803853988647\n",
      "  time_this_iter_s: 4.271465063095093\n",
      "  time_total_s: 105.73803853988647\n",
      "  timers:\n",
      "    learn_throughput: 983.113\n",
      "    learn_time_ms: 4068.708\n",
      "    load_throughput: 1650309.952\n",
      "    load_time_ms: 2.424\n",
      "    sample_throughput: 10692.482\n",
      "    sample_time_ms: 374.095\n",
      "    update_time_ms: 2.311\n",
      "  timestamp: 1624524823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         105.738</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  24.498</td><td style=\"text-align: right;\">                39.9</td><td style=\"text-align: right;\">                -8.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-53-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 44.999999999999915\n",
      "  episode_reward_mean: 25.328999999999922\n",
      "  episode_reward_min: 4.199999999999992\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8086280822753906\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013364194892346859\n",
      "          model: {}\n",
      "          policy_loss: -0.04560176655650139\n",
      "          total_loss: 53.232032775878906\n",
      "          vf_explained_var: 0.3239186406135559\n",
      "          vf_loss: 53.26410675048828\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.8873233795166016\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014259649440646172\n",
      "          model: {}\n",
      "          policy_loss: -0.04232487455010414\n",
      "          total_loss: 3.13403058052063\n",
      "          vf_explained_var: 0.1824813038110733\n",
      "          vf_loss: 3.1619176864624023\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.91666666666667\n",
      "    ram_util_percent: 67.46666666666667\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 9.800000000000013\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.06\n",
      "    policy2: -4.730999999999992\n",
      "  policy_reward_min:\n",
      "    policy1: 8.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.184674308928809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10318004144356976\n",
      "    mean_inference_ms: 1.39836529335235\n",
      "    mean_raw_obs_processing_ms: 0.6694622787723356\n",
      "  time_since_restore: 114.38890051841736\n",
      "  time_this_iter_s: 4.338054895401001\n",
      "  time_total_s: 114.38890051841736\n",
      "  timers:\n",
      "    learn_throughput: 994.13\n",
      "    learn_time_ms: 4023.619\n",
      "    load_throughput: 1656649.024\n",
      "    load_time_ms: 2.415\n",
      "    sample_throughput: 10952.484\n",
      "    sample_time_ms: 365.214\n",
      "    update_time_ms: 2.288\n",
      "  timestamp: 1624524832\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         114.389</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  25.329</td><td style=\"text-align: right;\">                  45</td><td style=\"text-align: right;\">                 4.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-54-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.799999999999926\n",
      "  episode_reward_mean: 27.206999999999915\n",
      "  episode_reward_min: 3.2999999999999816\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1075\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.7687411308288574\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011752902530133724\n",
      "          model: {}\n",
      "          policy_loss: -0.03996681049466133\n",
      "          total_loss: 58.88492202758789\n",
      "          vf_explained_var: 0.3884660005569458\n",
      "          vf_loss: 58.912986755371094\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.8543674945831299\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013211368583142757\n",
      "          model: {}\n",
      "          policy_loss: -0.039770953357219696\n",
      "          total_loss: 2.8828179836273193\n",
      "          vf_explained_var: 0.1609441190958023\n",
      "          vf_loss: 2.9092118740081787\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.933333333333334\n",
      "    ram_util_percent: 67.55\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.5\n",
      "    policy2: 7.600000000000008\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.685\n",
      "    policy2: -4.477999999999993\n",
      "  policy_reward_min:\n",
      "    policy1: 10.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18363218680845148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1027035238997258\n",
      "    mean_inference_ms: 1.393164726951934\n",
      "    mean_raw_obs_processing_ms: 0.6660220386080498\n",
      "  time_since_restore: 123.23995542526245\n",
      "  time_this_iter_s: 4.397578001022339\n",
      "  time_total_s: 123.23995542526245\n",
      "  timers:\n",
      "    learn_throughput: 1001.233\n",
      "    learn_time_ms: 3995.073\n",
      "    load_throughput: 1668561.199\n",
      "    load_time_ms: 2.397\n",
      "    sample_throughput: 11096.273\n",
      "    sample_time_ms: 360.481\n",
      "    update_time_ms: 2.246\n",
      "  timestamp: 1624524840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">          123.24</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  27.207</td><td style=\"text-align: right;\">                46.8</td><td style=\"text-align: right;\">                 3.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-54-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.799999999999926\n",
      "  episode_reward_mean: 28.493999999999915\n",
      "  episode_reward_min: 3.2999999999999816\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1150\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.7267895340919495\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012627107091248035\n",
      "          model: {}\n",
      "          policy_loss: -0.042782749980688095\n",
      "          total_loss: 47.68418884277344\n",
      "          vf_explained_var: 0.45486703515052795\n",
      "          vf_loss: 47.71418762207031\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.8142289519309998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013674505054950714\n",
      "          model: {}\n",
      "          policy_loss: -0.04252731055021286\n",
      "          total_loss: 5.259716510772705\n",
      "          vf_explained_var: 0.198496013879776\n",
      "          vf_loss: 5.288397789001465\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.18333333333334\n",
      "    ram_util_percent: 67.7\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.5\n",
      "    policy2: 7.600000000000012\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.84\n",
      "    policy2: -4.34599999999999\n",
      "  policy_reward_min:\n",
      "    policy1: 10.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18306164406458794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10236325307053469\n",
      "    mean_inference_ms: 1.3879879538664681\n",
      "    mean_raw_obs_processing_ms: 0.6635427685139038\n",
      "  time_since_restore: 132.33677124977112\n",
      "  time_this_iter_s: 4.486896753311157\n",
      "  time_total_s: 132.33677124977112\n",
      "  timers:\n",
      "    learn_throughput: 999.891\n",
      "    learn_time_ms: 4000.437\n",
      "    load_throughput: 1684205.792\n",
      "    load_time_ms: 2.375\n",
      "    sample_throughput: 11079.835\n",
      "    sample_time_ms: 361.016\n",
      "    update_time_ms: 2.216\n",
      "  timestamp: 1624524850\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         132.337</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  28.494</td><td style=\"text-align: right;\">                46.8</td><td style=\"text-align: right;\">                 3.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-54-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.2999999999999\n",
      "  episode_reward_mean: 29.750999999999912\n",
      "  episode_reward_min: 12.899999999999961\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1225\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6840970516204834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01279220636934042\n",
      "          model: {}\n",
      "          policy_loss: -0.039032094180583954\n",
      "          total_loss: 65.87602233886719\n",
      "          vf_explained_var: 0.3669774830341339\n",
      "          vf_loss: 65.90210723876953\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.7689157724380493\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011909321881830692\n",
      "          model: {}\n",
      "          policy_loss: -0.031435973942279816\n",
      "          total_loss: 13.738896369934082\n",
      "          vf_explained_var: 0.19629336893558502\n",
      "          vf_loss: 13.758275032043457\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.88333333333333\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.0\n",
      "    policy2: 38.399999999999956\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.93\n",
      "    policy2: -2.1789999999999954\n",
      "  policy_reward_min:\n",
      "    policy1: -12.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18246295117520034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10209340125131484\n",
      "    mean_inference_ms: 1.3835809432719106\n",
      "    mean_raw_obs_processing_ms: 0.6611277603272261\n",
      "  time_since_restore: 141.03434944152832\n",
      "  time_this_iter_s: 4.309662103652954\n",
      "  time_total_s: 141.03434944152832\n",
      "  timers:\n",
      "    learn_throughput: 996.616\n",
      "    learn_time_ms: 4013.581\n",
      "    load_throughput: 1698631.757\n",
      "    load_time_ms: 2.355\n",
      "    sample_throughput: 11063.159\n",
      "    sample_time_ms: 361.56\n",
      "    update_time_ms: 2.236\n",
      "  timestamp: 1624524858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         141.034</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  29.751</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                12.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-54-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.2999999999999\n",
      "  episode_reward_mean: 29.228999999999917\n",
      "  episode_reward_min: 10.800000000000022\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1300\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.677323579788208\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011933168396353722\n",
      "          model: {}\n",
      "          policy_loss: -0.03782849758863449\n",
      "          total_loss: 71.7448501586914\n",
      "          vf_explained_var: 0.32996243238449097\n",
      "          vf_loss: 71.77059173583984\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.7344822883605957\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011026719585061073\n",
      "          model: {}\n",
      "          policy_loss: -0.03297652676701546\n",
      "          total_loss: 15.851787567138672\n",
      "          vf_explained_var: 0.29036301374435425\n",
      "          vf_loss: 15.873600006103516\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.28333333333333\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.0\n",
      "    policy2: 40.59999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.055\n",
      "    policy2: -0.8259999999999951\n",
      "  policy_reward_min:\n",
      "    policy1: -14.5\n",
      "    policy2: -8.899999999999983\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1821406783489237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10186239090810706\n",
      "    mean_inference_ms: 1.3771398381615092\n",
      "    mean_raw_obs_processing_ms: 0.6600469428380655\n",
      "  time_since_restore: 149.90831995010376\n",
      "  time_this_iter_s: 4.4346606731414795\n",
      "  time_total_s: 149.90831995010376\n",
      "  timers:\n",
      "    learn_throughput: 988.44\n",
      "    learn_time_ms: 4046.782\n",
      "    load_throughput: 1675576.862\n",
      "    load_time_ms: 2.387\n",
      "    sample_throughput: 11089.947\n",
      "    sample_time_ms: 360.687\n",
      "    update_time_ms: 2.223\n",
      "  timestamp: 1624524867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         149.908</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  29.229</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                10.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-54-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.49999999999989\n",
      "  episode_reward_mean: 29.945999999999913\n",
      "  episode_reward_min: 10.499999999999964\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1400\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6566652059555054\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012123221531510353\n",
      "          model: {}\n",
      "          policy_loss: -0.03603876009583473\n",
      "          total_loss: 62.44966506958008\n",
      "          vf_explained_var: 0.3267621695995331\n",
      "          vf_loss: 62.473426818847656\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.7197313904762268\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0124246496707201\n",
      "          model: {}\n",
      "          policy_loss: -0.038673412054777145\n",
      "          total_loss: 10.332758903503418\n",
      "          vf_explained_var: 0.19566388428211212\n",
      "          vf_loss: 10.35885238647461\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.81428571428571\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.5\n",
      "    policy2: 18.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.41\n",
      "    policy2: -1.4639999999999946\n",
      "  policy_reward_min:\n",
      "    policy1: -6.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18162015152075134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10154785230865754\n",
      "    mean_inference_ms: 1.371390107841683\n",
      "    mean_raw_obs_processing_ms: 0.6569029660770187\n",
      "  time_since_restore: 159.12935614585876\n",
      "  time_this_iter_s: 4.763045072555542\n",
      "  time_total_s: 159.12935614585876\n",
      "  timers:\n",
      "    learn_throughput: 974.719\n",
      "    learn_time_ms: 4103.747\n",
      "    load_throughput: 1669025.975\n",
      "    load_time_ms: 2.397\n",
      "    sample_throughput: 11090.224\n",
      "    sample_time_ms: 360.678\n",
      "    update_time_ms: 2.252\n",
      "  timestamp: 1624524877\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         159.129</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  29.946</td><td style=\"text-align: right;\">                46.5</td><td style=\"text-align: right;\">                10.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-54-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.49999999999989\n",
      "  episode_reward_mean: 30.701999999999916\n",
      "  episode_reward_min: 12.60000000000002\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1475\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6108099818229675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010560435242950916\n",
      "          model: {}\n",
      "          policy_loss: -0.03370382636785507\n",
      "          total_loss: 62.26347351074219\n",
      "          vf_explained_var: 0.4298911988735199\n",
      "          vf_loss: 62.2864875793457\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.6857420802116394\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01173960417509079\n",
      "          model: {}\n",
      "          policy_loss: -0.03361233323812485\n",
      "          total_loss: 12.483384132385254\n",
      "          vf_explained_var: 0.39778512716293335\n",
      "          vf_loss: 12.505107879638672\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.21428571428572\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 68.1\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.845\n",
      "    policy2: 0.8570000000000028\n",
      "  policy_reward_min:\n",
      "    policy1: -52.5\n",
      "    policy2: -8.899999999999984\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18029860669986683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10095788398440991\n",
      "    mean_inference_ms: 1.3662076310888964\n",
      "    mean_raw_obs_processing_ms: 0.6547402250669448\n",
      "  time_since_restore: 168.3749167919159\n",
      "  time_this_iter_s: 4.615001678466797\n",
      "  time_total_s: 168.3749167919159\n",
      "  timers:\n",
      "    learn_throughput: 964.668\n",
      "    learn_time_ms: 4146.505\n",
      "    load_throughput: 1668909.756\n",
      "    load_time_ms: 2.397\n",
      "    sample_throughput: 11198.077\n",
      "    sample_time_ms: 357.204\n",
      "    update_time_ms: 2.3\n",
      "  timestamp: 1624524886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         168.375</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">  30.702</td><td style=\"text-align: right;\">                46.5</td><td style=\"text-align: right;\">                12.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-54-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.29999999999991\n",
      "  episode_reward_mean: 31.331999999999912\n",
      "  episode_reward_min: 14.999999999999996\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1550\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5867441296577454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011585080996155739\n",
      "          model: {}\n",
      "          policy_loss: -0.03752788156270981\n",
      "          total_loss: 87.87799835205078\n",
      "          vf_explained_var: 0.4259708523750305\n",
      "          vf_loss: 87.90380859375\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.6529635787010193\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010162655264139175\n",
      "          model: {}\n",
      "          policy_loss: -0.02790335938334465\n",
      "          total_loss: 27.34839630126953\n",
      "          vf_explained_var: 0.2702203094959259\n",
      "          vf_loss: 27.36600685119629\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.800000000000004\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.5\n",
      "    policy2: 68.1\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.67\n",
      "    policy2: 3.662000000000002\n",
      "  policy_reward_min:\n",
      "    policy1: -52.5\n",
      "    policy2: -8.89999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17928594644545384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10047220614668334\n",
      "    mean_inference_ms: 1.3582128547479124\n",
      "    mean_raw_obs_processing_ms: 0.653696572549689\n",
      "  time_since_restore: 177.75175070762634\n",
      "  time_this_iter_s: 4.626401901245117\n",
      "  time_total_s: 177.75175070762634\n",
      "  timers:\n",
      "    learn_throughput: 957.284\n",
      "    learn_time_ms: 4178.488\n",
      "    load_throughput: 1634696.391\n",
      "    load_time_ms: 2.447\n",
      "    sample_throughput: 11330.472\n",
      "    sample_time_ms: 353.03\n",
      "    update_time_ms: 2.358\n",
      "  timestamp: 1624524895\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         177.752</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">  31.332</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-55-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 53.39999999999992\n",
      "  episode_reward_mean: 30.854999999999922\n",
      "  episode_reward_min: -1.1999999999999975\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1625\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.574537456035614\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009072775021195412\n",
      "          model: {}\n",
      "          policy_loss: -0.031153691932559013\n",
      "          total_loss: 108.16924285888672\n",
      "          vf_explained_var: 0.3942277729511261\n",
      "          vf_loss: 108.19120025634766\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.6456592679023743\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00929906778037548\n",
      "          model: {}\n",
      "          policy_loss: -0.026514235883951187\n",
      "          total_loss: 53.446678161621094\n",
      "          vf_explained_var: 0.3138171136379242\n",
      "          vf_loss: 53.463775634765625\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.442857142857136\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.5\n",
      "    policy2: 63.69999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 25.73\n",
      "    policy2: 5.125000000000003\n",
      "  policy_reward_min:\n",
      "    policy1: -43.0\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17880095410096353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10025251577377045\n",
      "    mean_inference_ms: 1.3546142309689861\n",
      "    mean_raw_obs_processing_ms: 0.6533497881970264\n",
      "  time_since_restore: 186.89528465270996\n",
      "  time_this_iter_s: 4.5814268589019775\n",
      "  time_total_s: 186.89528465270996\n",
      "  timers:\n",
      "    learn_throughput: 946.95\n",
      "    learn_time_ms: 4224.09\n",
      "    load_throughput: 1605782.542\n",
      "    load_time_ms: 2.491\n",
      "    sample_throughput: 11365.866\n",
      "    sample_time_ms: 351.931\n",
      "    update_time_ms: 2.374\n",
      "  timestamp: 1624524905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         186.895</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">  30.855</td><td style=\"text-align: right;\">                53.4</td><td style=\"text-align: right;\">                -1.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-55-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.29999999999989\n",
      "  episode_reward_mean: 29.840999999999926\n",
      "  episode_reward_min: 13.49999999999992\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1700\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5857378244400024\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010605060495436192\n",
      "          model: {}\n",
      "          policy_loss: -0.03383101895451546\n",
      "          total_loss: 84.67359924316406\n",
      "          vf_explained_var: 0.3525088131427765\n",
      "          vf_loss: 84.69670104980469\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.6111442446708679\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008278192020952702\n",
      "          model: {}\n",
      "          policy_loss: -0.023637862876057625\n",
      "          total_loss: 27.55168914794922\n",
      "          vf_explained_var: 0.33420389890670776\n",
      "          vf_loss: 27.566938400268555\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.583333333333332\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 68.1\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.465\n",
      "    policy2: 3.3760000000000026\n",
      "  policy_reward_min:\n",
      "    policy1: -49.5\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17833213203728213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09994354151370147\n",
      "    mean_inference_ms: 1.3465141548873236\n",
      "    mean_raw_obs_processing_ms: 0.6521050432711207\n",
      "  time_since_restore: 196.2537965774536\n",
      "  time_this_iter_s: 4.683165073394775\n",
      "  time_total_s: 196.2537965774536\n",
      "  timers:\n",
      "    learn_throughput: 935.965\n",
      "    learn_time_ms: 4273.665\n",
      "    load_throughput: 1625384.228\n",
      "    load_time_ms: 2.461\n",
      "    sample_throughput: 11405.318\n",
      "    sample_time_ms: 350.714\n",
      "    update_time_ms: 2.408\n",
      "  timestamp: 1624524914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         196.254</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  29.841</td><td style=\"text-align: right;\">                45.3</td><td style=\"text-align: right;\">                13.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-55-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.39999999999989\n",
      "  episode_reward_mean: 30.533999999999914\n",
      "  episode_reward_min: 6.900000000000023\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1800\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5727860331535339\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01140167098492384\n",
      "          model: {}\n",
      "          policy_loss: -0.033886298537254333\n",
      "          total_loss: 76.70872497558594\n",
      "          vf_explained_var: 0.3266090452671051\n",
      "          vf_loss: 76.73106384277344\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.5973948836326599\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009907253086566925\n",
      "          model: {}\n",
      "          policy_loss: -0.030402278527617455\n",
      "          total_loss: 17.8972110748291\n",
      "          vf_explained_var: 0.23843955993652344\n",
      "          vf_loss: 17.91758155822754\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.942857142857143\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 70.3\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.59\n",
      "    policy2: -0.05599999999999884\n",
      "  policy_reward_min:\n",
      "    policy1: -47.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17758484819333012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09956126524255851\n",
      "    mean_inference_ms: 1.3409209500094432\n",
      "    mean_raw_obs_processing_ms: 0.6510703829693041\n",
      "  time_since_restore: 205.49362444877625\n",
      "  time_this_iter_s: 4.626206874847412\n",
      "  time_total_s: 205.49362444877625\n",
      "  timers:\n",
      "    learn_throughput: 934.62\n",
      "    learn_time_ms: 4279.816\n",
      "    load_throughput: 1621472.712\n",
      "    load_time_ms: 2.467\n",
      "    sample_throughput: 11544.012\n",
      "    sample_time_ms: 346.5\n",
      "    update_time_ms: 2.408\n",
      "  timestamp: 1624524923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         205.494</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  30.534</td><td style=\"text-align: right;\">                47.4</td><td style=\"text-align: right;\">                 6.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-55-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.7999999999999\n",
      "  episode_reward_mean: 29.687999999999917\n",
      "  episode_reward_min: 14.099999999999909\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1875\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5390828251838684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009557900950312614\n",
      "          model: {}\n",
      "          policy_loss: -0.03181135281920433\n",
      "          total_loss: 55.03185272216797\n",
      "          vf_explained_var: 0.41093891859054565\n",
      "          vf_loss: 55.05398178100586\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.5633679628372192\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010973580181598663\n",
      "          model: {}\n",
      "          policy_loss: -0.03133324906229973\n",
      "          total_loss: 6.3301520347595215\n",
      "          vf_explained_var: 0.18075458705425262\n",
      "          vf_loss: 6.350374221801758\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.68571428571428\n",
      "    ram_util_percent: 67.54285714285716\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 57.0\n",
      "    policy2: 42.79999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.8\n",
      "    policy2: -1.1119999999999979\n",
      "  policy_reward_min:\n",
      "    policy1: -17.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17656440121038883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09914172942739845\n",
      "    mean_inference_ms: 1.3372868769742654\n",
      "    mean_raw_obs_processing_ms: 0.6492516964673379\n",
      "  time_since_restore: 214.4968774318695\n",
      "  time_this_iter_s: 4.512882947921753\n",
      "  time_total_s: 214.4968774318695\n",
      "  timers:\n",
      "    learn_throughput: 940.315\n",
      "    learn_time_ms: 4253.895\n",
      "    load_throughput: 1626771.128\n",
      "    load_time_ms: 2.459\n",
      "    sample_throughput: 11486.599\n",
      "    sample_time_ms: 348.232\n",
      "    update_time_ms: 2.423\n",
      "  timestamp: 1624524932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         214.497</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  29.688</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                14.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-55-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.7999999999999\n",
      "  episode_reward_mean: 30.25499999999991\n",
      "  episode_reward_min: 9.899999999999965\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1950\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5224382281303406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00951691810041666\n",
      "          model: {}\n",
      "          policy_loss: -0.031399473547935486\n",
      "          total_loss: 48.411170959472656\n",
      "          vf_explained_var: 0.5059530138969421\n",
      "          vf_loss: 48.43292999267578\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.5419241786003113\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010463923215866089\n",
      "          model: {}\n",
      "          policy_loss: -0.029458660632371902\n",
      "          total_loss: 5.945410251617432\n",
      "          vf_explained_var: 0.18880540132522583\n",
      "          vf_loss: 5.964276313781738\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.15714285714286\n",
      "    ram_util_percent: 67.45714285714287\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 42.79999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.85\n",
      "    policy2: -0.5949999999999991\n",
      "  policy_reward_min:\n",
      "    policy1: -17.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17597372494867525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09886227000727396\n",
      "    mean_inference_ms: 1.332893210748746\n",
      "    mean_raw_obs_processing_ms: 0.6486078104573967\n",
      "  time_since_restore: 223.80174946784973\n",
      "  time_this_iter_s: 4.73954701423645\n",
      "  time_total_s: 223.80174946784973\n",
      "  timers:\n",
      "    learn_throughput: 941.919\n",
      "    learn_time_ms: 4246.65\n",
      "    load_throughput: 1628713.608\n",
      "    load_time_ms: 2.456\n",
      "    sample_throughput: 11488.577\n",
      "    sample_time_ms: 348.172\n",
      "    update_time_ms: 2.408\n",
      "  timestamp: 1624524942\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         223.802</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  30.255</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-55-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 43.4999999999999\n",
      "  episode_reward_mean: 30.74399999999991\n",
      "  episode_reward_min: 7.499999999999952\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2025\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4954160153865814\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009320042096078396\n",
      "          model: {}\n",
      "          policy_loss: -0.029491031542420387\n",
      "          total_loss: 54.5516471862793\n",
      "          vf_explained_var: 0.4740511476993561\n",
      "          vf_loss: 54.57169723510742\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.5262089967727661\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011583560146391392\n",
      "          model: {}\n",
      "          policy_loss: -0.030432110652327538\n",
      "          total_loss: 5.1061296463012695\n",
      "          vf_explained_var: 0.14167535305023193\n",
      "          vf_loss: 5.124834060668945\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.800000000000004\n",
      "    ram_util_percent: 67.66666666666666\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 46.5\n",
      "    policy2: 68.10000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.69\n",
      "    policy2: 0.05400000000000119\n",
      "  policy_reward_min:\n",
      "    policy1: -49.5\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17578245906409393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09877540140541875\n",
      "    mean_inference_ms: 1.33154209785714\n",
      "    mean_raw_obs_processing_ms: 0.6487444873640281\n",
      "  time_since_restore: 233.28365230560303\n",
      "  time_this_iter_s: 4.794803142547607\n",
      "  time_total_s: 233.28365230560303\n",
      "  timers:\n",
      "    learn_throughput: 935.557\n",
      "    learn_time_ms: 4275.528\n",
      "    load_throughput: 1608522.943\n",
      "    load_time_ms: 2.487\n",
      "    sample_throughput: 11332.276\n",
      "    sample_time_ms: 352.974\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1624524951\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         233.284</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">  30.744</td><td style=\"text-align: right;\">                43.5</td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-56-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.899999999999906\n",
      "  episode_reward_mean: 32.897999999999904\n",
      "  episode_reward_min: 19.199999999999896\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2100\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.49919164180755615\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011284520849585533\n",
      "          model: {}\n",
      "          policy_loss: -0.03782995045185089\n",
      "          total_loss: 71.1130599975586\n",
      "          vf_explained_var: 0.34547683596611023\n",
      "          vf_loss: 71.13946533203125\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.506411075592041\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009664587676525116\n",
      "          model: {}\n",
      "          policy_loss: -0.026994599029421806\n",
      "          total_loss: 13.2347993850708\n",
      "          vf_explained_var: 0.3099183142185211\n",
      "          vf_loss: 13.252008438110352\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.285714285714285\n",
      "    ram_util_percent: 67.7\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 42.8\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.69\n",
      "    policy2: 0.2080000000000009\n",
      "  policy_reward_min:\n",
      "    policy1: -6.5\n",
      "    policy2: -8.899999999999984\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1762277608024443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09893365316129063\n",
      "    mean_inference_ms: 1.33129544796674\n",
      "    mean_raw_obs_processing_ms: 0.6502717371677855\n",
      "  time_since_restore: 242.82092332839966\n",
      "  time_this_iter_s: 4.808897018432617\n",
      "  time_total_s: 242.82092332839966\n",
      "  timers:\n",
      "    learn_throughput: 932.32\n",
      "    learn_time_ms: 4290.373\n",
      "    load_throughput: 1607675.192\n",
      "    load_time_ms: 2.488\n",
      "    sample_throughput: 11241.514\n",
      "    sample_time_ms: 355.824\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1624524961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         242.821</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  32.898</td><td style=\"text-align: right;\">                45.9</td><td style=\"text-align: right;\">                19.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-56-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.699999999999896\n",
      "  episode_reward_mean: 31.58399999999991\n",
      "  episode_reward_min: 12.299999999999967\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2200\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.46220383048057556\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008790775202214718\n",
      "          model: {}\n",
      "          policy_loss: -0.02926849201321602\n",
      "          total_loss: 59.04888153076172\n",
      "          vf_explained_var: 0.3530977964401245\n",
      "          vf_loss: 59.06924057006836\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.47448912262916565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00967141892760992\n",
      "          model: {}\n",
      "          policy_loss: -0.026778675615787506\n",
      "          total_loss: 16.0622615814209\n",
      "          vf_explained_var: 0.24281243979930878\n",
      "          vf_loss: 16.079246520996094\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.17142857142858\n",
      "    ram_util_percent: 67.65714285714286\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 31.799999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.21\n",
      "    policy2: 1.3740000000000014\n",
      "  policy_reward_min:\n",
      "    policy1: -14.0\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17625650377765006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09892738718371577\n",
      "    mean_inference_ms: 1.3315711464829545\n",
      "    mean_raw_obs_processing_ms: 0.6516239991338929\n",
      "  time_since_restore: 252.5169541835785\n",
      "  time_this_iter_s: 4.951074838638306\n",
      "  time_total_s: 252.5169541835785\n",
      "  timers:\n",
      "    learn_throughput: 924.658\n",
      "    learn_time_ms: 4325.924\n",
      "    load_throughput: 1569078.598\n",
      "    load_time_ms: 2.549\n",
      "    sample_throughput: 10940.662\n",
      "    sample_time_ms: 365.609\n",
      "    update_time_ms: 2.458\n",
      "  timestamp: 1624524971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         252.517</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  31.584</td><td style=\"text-align: right;\">                47.7</td><td style=\"text-align: right;\">                12.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-56-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.699999999999896\n",
      "  episode_reward_mean: 31.42799999999992\n",
      "  episode_reward_min: 11.699999999999946\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2275\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4620126485824585\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012693880125880241\n",
      "          model: {}\n",
      "          policy_loss: -0.03664546459913254\n",
      "          total_loss: 53.39973068237305\n",
      "          vf_explained_var: 0.49103453755378723\n",
      "          vf_loss: 53.42353057861328\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.4446917176246643\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0076546622440218925\n",
      "          model: {}\n",
      "          policy_loss: -0.02407807856798172\n",
      "          total_loss: 15.614127159118652\n",
      "          vf_explained_var: 0.23415397107601166\n",
      "          vf_loss: 15.630454063415527\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.633333333333336\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 38.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.79\n",
      "    policy2: 1.6380000000000026\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17589637478985246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0988247533089475\n",
      "    mean_inference_ms: 1.3320297421202079\n",
      "    mean_raw_obs_processing_ms: 0.6527021911366506\n",
      "  time_since_restore: 261.9815454483032\n",
      "  time_this_iter_s: 4.64864706993103\n",
      "  time_total_s: 261.9815454483032\n",
      "  timers:\n",
      "    learn_throughput: 915.638\n",
      "    learn_time_ms: 4368.538\n",
      "    load_throughput: 1536234.411\n",
      "    load_time_ms: 2.604\n",
      "    sample_throughput: 10839.39\n",
      "    sample_time_ms: 369.024\n",
      "    update_time_ms: 2.45\n",
      "  timestamp: 1624524980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         261.982</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  31.428</td><td style=\"text-align: right;\">                47.7</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-56-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.39999999999991\n",
      "  episode_reward_mean: 31.550999999999913\n",
      "  episode_reward_min: 15.29999999999998\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2350\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.42946454882621765\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00793248601257801\n",
      "          model: {}\n",
      "          policy_loss: -0.024854931980371475\n",
      "          total_loss: 84.31121063232422\n",
      "          vf_explained_var: 0.5293190479278564\n",
      "          vf_loss: 84.32803344726562\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.433282732963562\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006865167524665594\n",
      "          model: {}\n",
      "          policy_loss: -0.017328156158328056\n",
      "          total_loss: 47.22526168823242\n",
      "          vf_explained_var: 0.4216717481613159\n",
      "          vf_loss: 47.23564147949219\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.042857142857144\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.5\n",
      "    policy2: 69.2\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.065\n",
      "    policy2: 3.485999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -45.5\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17557125527363854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09869815704702964\n",
      "    mean_inference_ms: 1.3296510662921879\n",
      "    mean_raw_obs_processing_ms: 0.6529796767158977\n",
      "  time_since_restore: 271.2515106201172\n",
      "  time_this_iter_s: 4.534055948257446\n",
      "  time_total_s: 271.2515106201172\n",
      "  timers:\n",
      "    learn_throughput: 916.559\n",
      "    learn_time_ms: 4364.15\n",
      "    load_throughput: 1550030.119\n",
      "    load_time_ms: 2.581\n",
      "    sample_throughput: 10809.292\n",
      "    sample_time_ms: 370.052\n",
      "    update_time_ms: 2.456\n",
      "  timestamp: 1624524990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         271.252</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  31.551</td><td style=\"text-align: right;\">                47.4</td><td style=\"text-align: right;\">                15.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-56-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.39999999999991\n",
      "  episode_reward_mean: 30.452999999999914\n",
      "  episode_reward_min: 11.999999999999932\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2425\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.44497305154800415\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009061883203685284\n",
      "          model: {}\n",
      "          policy_loss: -0.027709050104022026\n",
      "          total_loss: 59.62226104736328\n",
      "          vf_explained_var: 0.46265682578086853\n",
      "          vf_loss: 59.64079284667969\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.42327675223350525\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008006012067198753\n",
      "          model: {}\n",
      "          policy_loss: -0.022609097883105278\n",
      "          total_loss: 17.18334197998047\n",
      "          vf_explained_var: 0.19386650621891022\n",
      "          vf_loss: 17.197847366333008\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.716666666666665\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 47.5\n",
      "    policy2: 64.8\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.76\n",
      "    policy2: 1.6929999999999996\n",
      "  policy_reward_min:\n",
      "    policy1: -39.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17500932116291967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09842208822863695\n",
      "    mean_inference_ms: 1.3252807925715338\n",
      "    mean_raw_obs_processing_ms: 0.651477967514704\n",
      "  time_since_restore: 280.06636095046997\n",
      "  time_this_iter_s: 4.381019115447998\n",
      "  time_total_s: 280.06636095046997\n",
      "  timers:\n",
      "    learn_throughput: 928.241\n",
      "    learn_time_ms: 4309.225\n",
      "    load_throughput: 1575784.125\n",
      "    load_time_ms: 2.538\n",
      "    sample_throughput: 11159.025\n",
      "    sample_time_ms: 358.454\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1624524998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         280.066</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">  30.453</td><td style=\"text-align: right;\">                47.4</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-56-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.19999999999992\n",
      "  episode_reward_mean: 31.199999999999914\n",
      "  episode_reward_min: 15.599999999999918\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2500\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.44672054052352905\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009240994229912758\n",
      "          model: {}\n",
      "          policy_loss: -0.030118603259325027\n",
      "          total_loss: 67.60299682617188\n",
      "          vf_explained_var: 0.3419913053512573\n",
      "          vf_loss: 67.62374877929688\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.4110371768474579\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0072503359988331795\n",
      "          model: {}\n",
      "          policy_loss: -0.019559714943170547\n",
      "          total_loss: 12.62038516998291\n",
      "          vf_explained_var: 0.28429844975471497\n",
      "          vf_loss: 12.63260555267334\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.733333333333334\n",
      "    ram_util_percent: 67.53333333333335\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 47.5\n",
      "    policy2: 54.89999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.825\n",
      "    policy2: 2.375\n",
      "  policy_reward_min:\n",
      "    policy1: -31.5\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17452660366999914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09808752717072554\n",
      "    mean_inference_ms: 1.3177242370899576\n",
      "    mean_raw_obs_processing_ms: 0.6494537959954042\n",
      "  time_since_restore: 288.8836178779602\n",
      "  time_this_iter_s: 4.481771945953369\n",
      "  time_total_s: 288.8836178779602\n",
      "  timers:\n",
      "    learn_throughput: 941.657\n",
      "    learn_time_ms: 4247.832\n",
      "    load_throughput: 1590317.737\n",
      "    load_time_ms: 2.515\n",
      "    sample_throughput: 11488.486\n",
      "    sample_time_ms: 348.175\n",
      "    update_time_ms: 2.391\n",
      "  timestamp: 1624525007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         288.884</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">    31.2</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                15.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-56-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.3999999999999\n",
      "  episode_reward_mean: 31.244999999999912\n",
      "  episode_reward_min: 3.9000000000000035\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2600\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.44735029339790344\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009497132152318954\n",
      "          model: {}\n",
      "          policy_loss: -0.029139278456568718\n",
      "          total_loss: 48.774818420410156\n",
      "          vf_explained_var: 0.4081743061542511\n",
      "          vf_loss: 48.79433822631836\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.38969045877456665\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008898007683455944\n",
      "          model: {}\n",
      "          policy_loss: -0.02757580764591694\n",
      "          total_loss: 7.056360244750977\n",
      "          vf_explained_var: 0.3804784417152405\n",
      "          vf_loss: 7.074925899505615\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.266666666666666\n",
      "    ram_util_percent: 67.53333333333333\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 50.5\n",
      "    policy2: 47.19999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.925\n",
      "    policy2: 2.3199999999999994\n",
      "  policy_reward_min:\n",
      "    policy1: -18.0\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17375568920463047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09768467195878187\n",
      "    mean_inference_ms: 1.3116161331978922\n",
      "    mean_raw_obs_processing_ms: 0.6474507671268843\n",
      "  time_since_restore: 297.8417479991913\n",
      "  time_this_iter_s: 4.376501083374023\n",
      "  time_total_s: 297.8417479991913\n",
      "  timers:\n",
      "    learn_throughput: 955.613\n",
      "    learn_time_ms: 4185.793\n",
      "    load_throughput: 1617190.172\n",
      "    load_time_ms: 2.473\n",
      "    sample_throughput: 11881.301\n",
      "    sample_time_ms: 336.663\n",
      "    update_time_ms: 2.385\n",
      "  timestamp: 1624525016\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         297.842</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">  31.245</td><td style=\"text-align: right;\">                50.4</td><td style=\"text-align: right;\">                 3.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-57-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.9999999999999\n",
      "  episode_reward_mean: 32.17799999999991\n",
      "  episode_reward_min: 6.8999999999999275\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2675\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4300624132156372\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009933280758559704\n",
      "          model: {}\n",
      "          policy_loss: -0.032282616943120956\n",
      "          total_loss: 53.065765380859375\n",
      "          vf_explained_var: 0.48740923404693604\n",
      "          vf_loss: 53.08799362182617\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.38092008233070374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008224285207688808\n",
      "          model: {}\n",
      "          policy_loss: -0.027118675410747528\n",
      "          total_loss: 9.986454010009766\n",
      "          vf_explained_var: 0.2789199650287628\n",
      "          vf_loss: 10.00524616241455\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.64285714285713\n",
      "    ram_util_percent: 67.51428571428572\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 47.0\n",
      "    policy2: 33.999999999999986\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.485\n",
      "    policy2: 1.6930000000000012\n",
      "  policy_reward_min:\n",
      "    policy1: -7.0\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17281868543018178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09729483142279637\n",
      "    mean_inference_ms: 1.3077588560450244\n",
      "    mean_raw_obs_processing_ms: 0.6455021412760367\n",
      "  time_since_restore: 306.8513181209564\n",
      "  time_this_iter_s: 4.57335901260376\n",
      "  time_total_s: 306.8513181209564\n",
      "  timers:\n",
      "    learn_throughput: 964.175\n",
      "    learn_time_ms: 4148.625\n",
      "    load_throughput: 1666307.394\n",
      "    load_time_ms: 2.401\n",
      "    sample_throughput: 12175.877\n",
      "    sample_time_ms: 328.518\n",
      "    update_time_ms: 2.361\n",
      "  timestamp: 1624525025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         306.851</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  32.178</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">                 6.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-57-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.3999999999999\n",
      "  episode_reward_mean: 32.69999999999992\n",
      "  episode_reward_min: 17.09999999999996\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2750\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.402817964553833\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008575302548706532\n",
      "          model: {}\n",
      "          policy_loss: -0.03071298636496067\n",
      "          total_loss: 56.4021110534668\n",
      "          vf_explained_var: 0.48138228058815\n",
      "          vf_loss: 56.42414093017578\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.35411858558654785\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008890919387340546\n",
      "          model: {}\n",
      "          policy_loss: -0.02767309360206127\n",
      "          total_loss: 7.4385457038879395\n",
      "          vf_explained_var: 0.17849840223789215\n",
      "          vf_loss: 7.457216739654541\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.128571428571426\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 33.999999999999986\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.38\n",
      "    policy2: 2.320000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -7.0\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17240615553435382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09707793987695801\n",
      "    mean_inference_ms: 1.3044485418359781\n",
      "    mean_raw_obs_processing_ms: 0.6446416175987246\n",
      "  time_since_restore: 316.2519519329071\n",
      "  time_this_iter_s: 4.739300966262817\n",
      "  time_total_s: 316.2519519329071\n",
      "  timers:\n",
      "    learn_throughput: 960.557\n",
      "    learn_time_ms: 4164.252\n",
      "    load_throughput: 1669790.097\n",
      "    load_time_ms: 2.396\n",
      "    sample_throughput: 12269.681\n",
      "    sample_time_ms: 326.007\n",
      "    update_time_ms: 2.341\n",
      "  timestamp: 1624525035\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         316.252</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">    32.7</td><td style=\"text-align: right;\">                47.4</td><td style=\"text-align: right;\">                17.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-57-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.1999999999999\n",
      "  episode_reward_mean: 33.91499999999991\n",
      "  episode_reward_min: 18.59999999999991\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2825\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.41297051310539246\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009383268654346466\n",
      "          model: {}\n",
      "          policy_loss: -0.027733489871025085\n",
      "          total_loss: 60.802494049072266\n",
      "          vf_explained_var: 0.5030959844589233\n",
      "          vf_loss: 60.82072448730469\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.3423934876918793\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007307581137865782\n",
      "          model: {}\n",
      "          policy_loss: -0.020343316718935966\n",
      "          total_loss: 14.309344291687012\n",
      "          vf_explained_var: 0.23174484074115753\n",
      "          vf_loss: 14.322286605834961\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.7\n",
      "    ram_util_percent: 67.51666666666667\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 46.0\n",
      "    policy2: 59.3\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.505\n",
      "    policy2: 4.41\n",
      "  policy_reward_min:\n",
      "    policy1: -35.0\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17204140353218406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09689323144427917\n",
      "    mean_inference_ms: 1.3014366637157426\n",
      "    mean_raw_obs_processing_ms: 0.6436429115643396\n",
      "  time_since_restore: 325.55698227882385\n",
      "  time_this_iter_s: 4.673952102661133\n",
      "  time_total_s: 325.55698227882385\n",
      "  timers:\n",
      "    learn_throughput: 950.007\n",
      "    learn_time_ms: 4210.497\n",
      "    load_throughput: 1670970.878\n",
      "    load_time_ms: 2.394\n",
      "    sample_throughput: 12167.42\n",
      "    sample_time_ms: 328.747\n",
      "    update_time_ms: 2.348\n",
      "  timestamp: 1624525044\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         325.557</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">  33.915</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-57-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.1999999999999\n",
      "  episode_reward_mean: 34.14899999999991\n",
      "  episode_reward_min: 14.999999999999938\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2900\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4195472002029419\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009106173180043697\n",
      "          model: {}\n",
      "          policy_loss: -0.03179044649004936\n",
      "          total_loss: 57.41215133666992\n",
      "          vf_explained_var: 0.36610138416290283\n",
      "          vf_loss: 57.434722900390625\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.3394518196582794\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007198273669928312\n",
      "          model: {}\n",
      "          policy_loss: -0.022834451869130135\n",
      "          total_loss: 21.24248504638672\n",
      "          vf_explained_var: 0.19909052550792694\n",
      "          vf_loss: 21.258031845092773\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_agent_steps_trained: 584000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.7\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 46.0\n",
      "    policy2: 27.399999999999963\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.815\n",
      "    policy2: 5.334\n",
      "  policy_reward_min:\n",
      "    policy1: 4.0\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17190357216416305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09674009231517014\n",
      "    mean_inference_ms: 1.2967153875328012\n",
      "    mean_raw_obs_processing_ms: 0.6426982917494761\n",
      "  time_since_restore: 334.4310200214386\n",
      "  time_this_iter_s: 4.339708089828491\n",
      "  time_total_s: 334.4310200214386\n",
      "  timers:\n",
      "    learn_throughput: 948.955\n",
      "    learn_time_ms: 4215.161\n",
      "    load_throughput: 1668577.794\n",
      "    load_time_ms: 2.397\n",
      "    sample_throughput: 12133.33\n",
      "    sample_time_ms: 329.67\n",
      "    update_time_ms: 2.357\n",
      "  timestamp: 1624525053\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         334.431</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  34.149</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-57-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.49999999999993\n",
      "  episode_reward_mean: 31.904999999999912\n",
      "  episode_reward_min: 9.899999999999986\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3000\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4204123318195343\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008523418568074703\n",
      "          model: {}\n",
      "          policy_loss: -0.0244015883654356\n",
      "          total_loss: 54.88983917236328\n",
      "          vf_explained_var: 0.3839063048362732\n",
      "          vf_loss: 54.90561294555664\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.3229769766330719\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009307971224188805\n",
      "          model: {}\n",
      "          policy_loss: -0.026552844792604446\n",
      "          total_loss: 10.16329288482666\n",
      "          vf_explained_var: 0.2718312740325928\n",
      "          vf_loss: 10.180419921875\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.93333333333334\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 42.800000000000004\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.285\n",
      "    policy2: 5.620000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -30.5\n",
      "    policy2: -8.89999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17124447856054645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0964053649118149\n",
      "    mean_inference_ms: 1.2912987649932899\n",
      "    mean_raw_obs_processing_ms: 0.6409644758347623\n",
      "  time_since_restore: 343.43498492240906\n",
      "  time_this_iter_s: 4.497093915939331\n",
      "  time_total_s: 343.43498492240906\n",
      "  timers:\n",
      "    learn_throughput: 947.907\n",
      "    learn_time_ms: 4219.825\n",
      "    load_throughput: 1703460.894\n",
      "    load_time_ms: 2.348\n",
      "    sample_throughput: 12135.74\n",
      "    sample_time_ms: 329.605\n",
      "    update_time_ms: 2.342\n",
      "  timestamp: 1624525062\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         343.435</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  31.905</td><td style=\"text-align: right;\">                46.5</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-57-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.1999999999999\n",
      "  episode_reward_mean: 32.76899999999992\n",
      "  episode_reward_min: 6.899999999999967\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3075\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3990224599838257\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010732964612543583\n",
      "          model: {}\n",
      "          policy_loss: -0.03383614122867584\n",
      "          total_loss: 50.62503433227539\n",
      "          vf_explained_var: 0.5071938633918762\n",
      "          vf_loss: 50.64799880981445\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.3109830319881439\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008884313516318798\n",
      "          model: {}\n",
      "          policy_loss: -0.025721069425344467\n",
      "          total_loss: 9.472145080566406\n",
      "          vf_explained_var: 0.4100339114665985\n",
      "          vf_loss: 9.488869667053223\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.583333333333336\n",
      "    ram_util_percent: 67.48333333333333\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 43.0\n",
      "    policy2: 63.7\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.005\n",
      "    policy2: 6.763999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -41.5\n",
      "    policy2: -5.599999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17075452193211496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0962422916610709\n",
      "    mean_inference_ms: 1.2911142463536456\n",
      "    mean_raw_obs_processing_ms: 0.6404889022639633\n",
      "  time_since_restore: 352.74889039993286\n",
      "  time_this_iter_s: 4.685326814651489\n",
      "  time_total_s: 352.74889039993286\n",
      "  timers:\n",
      "    learn_throughput: 942.477\n",
      "    learn_time_ms: 4244.136\n",
      "    load_throughput: 1690927.746\n",
      "    load_time_ms: 2.366\n",
      "    sample_throughput: 11918.569\n",
      "    sample_time_ms: 335.611\n",
      "    update_time_ms: 2.365\n",
      "  timestamp: 1624525072\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         352.749</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  32.769</td><td style=\"text-align: right;\">                46.2</td><td style=\"text-align: right;\">                 6.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-58-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.2999999999999\n",
      "  episode_reward_mean: 31.00799999999992\n",
      "  episode_reward_min: -6.300000000000022\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3150\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3911486268043518\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013790293596684933\n",
      "          model: {}\n",
      "          policy_loss: -0.03714088723063469\n",
      "          total_loss: 62.425235748291016\n",
      "          vf_explained_var: 0.5381317734718323\n",
      "          vf_loss: 62.44841003417969\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.30204886198043823\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005476077552884817\n",
      "          model: {}\n",
      "          policy_loss: -0.01377972774207592\n",
      "          total_loss: 22.101749420166016\n",
      "          vf_explained_var: 0.36000099778175354\n",
      "          vf_loss: 22.109983444213867\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_agent_steps_trained: 632000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.84285714285714\n",
      "    ram_util_percent: 67.42857142857143\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.5\n",
      "    policy2: 69.2\n",
      "  policy_reward_mean:\n",
      "    policy1: 25.025\n",
      "    policy2: 5.983000000000003\n",
      "  policy_reward_min:\n",
      "    policy1: -51.5\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17056293308368972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09614877839815492\n",
      "    mean_inference_ms: 1.2896295806624938\n",
      "    mean_raw_obs_processing_ms: 0.6403696403103235\n",
      "  time_since_restore: 362.20006942749023\n",
      "  time_this_iter_s: 4.737663984298706\n",
      "  time_total_s: 362.20006942749023\n",
      "  timers:\n",
      "    learn_throughput: 941.434\n",
      "    learn_time_ms: 4248.838\n",
      "    load_throughput: 1652113.836\n",
      "    load_time_ms: 2.421\n",
      "    sample_throughput: 11908.392\n",
      "    sample_time_ms: 335.898\n",
      "    update_time_ms: 2.393\n",
      "  timestamp: 1624525081\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">           362.2</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">  31.008</td><td style=\"text-align: right;\">                51.3</td><td style=\"text-align: right;\">                -6.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-58-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.89999999999991\n",
      "  episode_reward_mean: 32.60399999999992\n",
      "  episode_reward_min: 10.499999999999938\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3225\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.398970365524292\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007967749610543251\n",
      "          model: {}\n",
      "          policy_loss: -0.0246298648416996\n",
      "          total_loss: 43.50117111206055\n",
      "          vf_explained_var: 0.5558068156242371\n",
      "          vf_loss: 43.51772689819336\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2938200533390045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008505159057676792\n",
      "          model: {}\n",
      "          policy_loss: -0.023386728018522263\n",
      "          total_loss: 9.89889907836914\n",
      "          vf_explained_var: 0.23629705607891083\n",
      "          vf_loss: 9.913674354553223\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_agent_steps_trained: 648000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.416666666666664\n",
      "    ram_util_percent: 67.46666666666665\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 43.5\n",
      "    policy2: 69.2\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.26\n",
      "    policy2: 4.344000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -51.5\n",
      "    policy2: -5.6\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17042125125207058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09607902870797234\n",
      "    mean_inference_ms: 1.2883272931356244\n",
      "    mean_raw_obs_processing_ms: 0.6401121471297018\n",
      "  time_since_restore: 371.58276176452637\n",
      "  time_this_iter_s: 4.666711091995239\n",
      "  time_total_s: 371.58276176452637\n",
      "  timers:\n",
      "    learn_throughput: 940.948\n",
      "    learn_time_ms: 4251.031\n",
      "    load_throughput: 1647166.658\n",
      "    load_time_ms: 2.428\n",
      "    sample_throughput: 11716.605\n",
      "    sample_time_ms: 341.396\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1624525091\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         371.583</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  32.604</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">                10.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 664000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-58-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.799999999999905\n",
      "  episode_reward_mean: 32.879999999999924\n",
      "  episode_reward_min: 14.399999999999912\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3300\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4005577266216278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007322691846638918\n",
      "          model: {}\n",
      "          policy_loss: -0.02241870015859604\n",
      "          total_loss: 64.23497772216797\n",
      "          vf_explained_var: 0.40708014369010925\n",
      "          vf_loss: 64.2499771118164\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2735278010368347\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005859344732016325\n",
      "          model: {}\n",
      "          policy_loss: -0.01580343022942543\n",
      "          total_loss: 19.363710403442383\n",
      "          vf_explained_var: 0.284008651971817\n",
      "          vf_loss: 19.37358283996582\n",
      "    num_agent_steps_sampled: 664000\n",
      "    num_agent_steps_trained: 664000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.683333333333334\n",
      "    ram_util_percent: 67.36666666666667\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 45.5\n",
      "    policy2: 48.29999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.04\n",
      "    policy2: 5.840000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -19.5\n",
      "    policy2: -4.5000000000000036\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1707084336088747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09615585709317549\n",
      "    mean_inference_ms: 1.28728538686316\n",
      "    mean_raw_obs_processing_ms: 0.6405648891884765\n",
      "  time_since_restore: 380.6951198577881\n",
      "  time_this_iter_s: 4.501497030258179\n",
      "  time_total_s: 380.6951198577881\n",
      "  timers:\n",
      "    learn_throughput: 936.889\n",
      "    learn_time_ms: 4269.451\n",
      "    load_throughput: 1533103.908\n",
      "    load_time_ms: 2.609\n",
      "    sample_throughput: 11537.098\n",
      "    sample_time_ms: 346.708\n",
      "    update_time_ms: 2.414\n",
      "  timestamp: 1624525100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         380.695</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">   32.88</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                14.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 680000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-58-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.599999999999916\n",
      "  episode_reward_mean: 31.073999999999913\n",
      "  episode_reward_min: 7.800000000000004\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3400\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4029429256916046\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009470459073781967\n",
      "          model: {}\n",
      "          policy_loss: -0.030153706669807434\n",
      "          total_loss: 45.164791107177734\n",
      "          vf_explained_var: 0.4209146797657013\n",
      "          vf_loss: 45.18535614013672\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.29860323667526245\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010967005044221878\n",
      "          model: {}\n",
      "          policy_loss: -0.0294883344322443\n",
      "          total_loss: 8.07991886138916\n",
      "          vf_explained_var: 0.21126684546470642\n",
      "          vf_loss: 8.098301887512207\n",
      "    num_agent_steps_sampled: 680000\n",
      "    num_agent_steps_trained: 680000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.942857142857147\n",
      "    ram_util_percent: 67.39999999999999\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 24.09999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.39\n",
      "    policy2: 3.683999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17047871518806879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09604726506542438\n",
      "    mean_inference_ms: 1.2858178800352207\n",
      "    mean_raw_obs_processing_ms: 0.6402546355487043\n",
      "  time_since_restore: 389.8511800765991\n",
      "  time_this_iter_s: 4.5379109382629395\n",
      "  time_total_s: 389.8511800765991\n",
      "  timers:\n",
      "    learn_throughput: 934.477\n",
      "    learn_time_ms: 4280.467\n",
      "    load_throughput: 1503604.23\n",
      "    load_time_ms: 2.66\n",
      "    sample_throughput: 11398.167\n",
      "    sample_time_ms: 350.934\n",
      "    update_time_ms: 2.425\n",
      "  timestamp: 1624525109\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         389.851</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">  31.074</td><td style=\"text-align: right;\">                51.6</td><td style=\"text-align: right;\">                 7.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 696000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-58-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.899999999999906\n",
      "  episode_reward_mean: 31.28699999999991\n",
      "  episode_reward_min: 7.800000000000004\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3475\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3852783143520355\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007708707824349403\n",
      "          model: {}\n",
      "          policy_loss: -0.024134930223226547\n",
      "          total_loss: 40.1035041809082\n",
      "          vf_explained_var: 0.5831844806671143\n",
      "          vf_loss: 40.11983871459961\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.27408352494239807\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007624047342687845\n",
      "          model: {}\n",
      "          policy_loss: -0.022313129156827927\n",
      "          total_loss: 3.859591007232666\n",
      "          vf_explained_var: 0.2032572627067566\n",
      "          vf_loss: 3.8741848468780518\n",
      "    num_agent_steps_sampled: 696000\n",
      "    num_agent_steps_trained: 696000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.266666666666666\n",
      "    ram_util_percent: 67.36666666666667\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 15.299999999999983\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.725\n",
      "    policy2: 2.5620000000000003\n",
      "  policy_reward_min:\n",
      "    policy1: -2.0\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1699002878466159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09583906754889282\n",
      "    mean_inference_ms: 1.2846752089131994\n",
      "    mean_raw_obs_processing_ms: 0.6395322409641132\n",
      "  time_since_restore: 398.8821771144867\n",
      "  time_this_iter_s: 4.524286985397339\n",
      "  time_total_s: 398.8821771144867\n",
      "  timers:\n",
      "    learn_throughput: 939.858\n",
      "    learn_time_ms: 4255.961\n",
      "    load_throughput: 1509611.287\n",
      "    load_time_ms: 2.65\n",
      "    sample_throughput: 11522.679\n",
      "    sample_time_ms: 347.141\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1624525118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         398.882</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">  31.287</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">                 7.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 712000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-58-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.199999999999896\n",
      "  episode_reward_mean: 32.27399999999991\n",
      "  episode_reward_min: 4.799999999999987\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3550\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3577474355697632\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008355529978871346\n",
      "          model: {}\n",
      "          policy_loss: -0.024146242067217827\n",
      "          total_loss: 52.31120681762695\n",
      "          vf_explained_var: 0.5799316167831421\n",
      "          vf_loss: 52.32689666748047\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.25996536016464233\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006190388463437557\n",
      "          model: {}\n",
      "          policy_loss: -0.020133979618549347\n",
      "          total_loss: 14.794435501098633\n",
      "          vf_explained_var: 0.21265506744384766\n",
      "          vf_loss: 14.808300018310547\n",
      "    num_agent_steps_sampled: 712000\n",
      "    num_agent_steps_trained: 712000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.942857142857143\n",
      "    ram_util_percent: 67.3\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.5\n",
      "    policy2: 27.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.635\n",
      "    policy2: 2.639000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: 3.5\n",
      "    policy2: -6.699999999999987\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16960361917393954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09569407918735079\n",
      "    mean_inference_ms: 1.2823761335214587\n",
      "    mean_raw_obs_processing_ms: 0.6389605361248324\n",
      "  time_since_restore: 408.1360230445862\n",
      "  time_this_iter_s: 4.670665979385376\n",
      "  time_total_s: 408.1360230445862\n",
      "  timers:\n",
      "    learn_throughput: 943.806\n",
      "    learn_time_ms: 4238.161\n",
      "    load_throughput: 1549228.581\n",
      "    load_time_ms: 2.582\n",
      "    sample_throughput: 11583.36\n",
      "    sample_time_ms: 345.323\n",
      "    update_time_ms: 2.418\n",
      "  timestamp: 1624525127\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         408.136</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">  32.274</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                 4.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 728000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-58-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.199999999999896\n",
      "  episode_reward_mean: 31.472999999999914\n",
      "  episode_reward_min: 2.0999999999999424\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3625\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.33933261036872864\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007521684747189283\n",
      "          model: {}\n",
      "          policy_loss: -0.02475820481777191\n",
      "          total_loss: 61.76087188720703\n",
      "          vf_explained_var: 0.4999489486217499\n",
      "          vf_loss: 61.77800750732422\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.26676833629608154\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006703291088342667\n",
      "          model: {}\n",
      "          policy_loss: -0.021345200017094612\n",
      "          total_loss: 20.798843383789062\n",
      "          vf_explained_var: 0.3054805397987366\n",
      "          vf_loss: 20.813404083251953\n",
      "    num_agent_steps_sampled: 728000\n",
      "    num_agent_steps_trained: 728000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.94285714285714\n",
      "    ram_util_percent: 67.37142857142855\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.5\n",
      "    policy2: 32.899999999999956\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.075\n",
      "    policy2: 3.397999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -12.5\n",
      "    policy2: -6.699999999999983\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16949770324808824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09564261407259936\n",
      "    mean_inference_ms: 1.2814817052802965\n",
      "    mean_raw_obs_processing_ms: 0.6386385023445967\n",
      "  time_since_restore: 417.61429619789124\n",
      "  time_this_iter_s: 4.785260200500488\n",
      "  time_total_s: 417.61429619789124\n",
      "  timers:\n",
      "    learn_throughput: 941.746\n",
      "    learn_time_ms: 4247.43\n",
      "    load_throughput: 1551205.296\n",
      "    load_time_ms: 2.579\n",
      "    sample_throughput: 11573.235\n",
      "    sample_time_ms: 345.625\n",
      "    update_time_ms: 2.399\n",
      "  timestamp: 1624525137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         417.614</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">  31.473</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                 2.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 744000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-59-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.09999999999991\n",
      "  episode_reward_mean: 34.27499999999992\n",
      "  episode_reward_min: 7.7999999999999226\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3700\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3324660658836365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0072219776920974255\n",
      "          model: {}\n",
      "          policy_loss: -0.023308219388127327\n",
      "          total_loss: 54.98113250732422\n",
      "          vf_explained_var: 0.5101220011711121\n",
      "          vf_loss: 54.99712371826172\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.24780316650867462\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005509535316377878\n",
      "          model: {}\n",
      "          policy_loss: -0.017710553482174873\n",
      "          total_loss: 23.67041015625\n",
      "          vf_explained_var: 0.26758790016174316\n",
      "          vf_loss: 23.682540893554688\n",
      "    num_agent_steps_sampled: 744000\n",
      "    num_agent_steps_trained: 744000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.642857142857142\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.5\n",
      "    policy2: 57.099999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.83\n",
      "    policy2: 6.444999999999996\n",
      "  policy_reward_min:\n",
      "    policy1: -31.0\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16986541396108967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09575575731289802\n",
      "    mean_inference_ms: 1.2810667842111814\n",
      "    mean_raw_obs_processing_ms: 0.6392715210413445\n",
      "  time_since_restore: 427.2141447067261\n",
      "  time_this_iter_s: 4.7436957359313965\n",
      "  time_total_s: 427.2141447067261\n",
      "  timers:\n",
      "    learn_throughput: 931.574\n",
      "    learn_time_ms: 4293.81\n",
      "    load_throughput: 1646196.929\n",
      "    load_time_ms: 2.43\n",
      "    sample_throughput: 11496.324\n",
      "    sample_time_ms: 347.937\n",
      "    update_time_ms: 2.424\n",
      "  timestamp: 1624525147\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         427.214</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">  34.275</td><td style=\"text-align: right;\">                50.1</td><td style=\"text-align: right;\">                 7.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 760000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-59-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.09999999999991\n",
      "  episode_reward_mean: 33.47999999999991\n",
      "  episode_reward_min: 14.39999999999999\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3800\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3454279601573944\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0136724216863513\n",
      "          model: {}\n",
      "          policy_loss: -0.030320780351758003\n",
      "          total_loss: 43.55768966674805\n",
      "          vf_explained_var: 0.5542909502983093\n",
      "          vf_loss: 43.57417678833008\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.25329703092575073\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013872834853827953\n",
      "          model: {}\n",
      "          policy_loss: -0.027390142902731895\n",
      "          total_loss: 9.391124725341797\n",
      "          vf_explained_var: 0.3617810904979706\n",
      "          vf_loss: 9.411492347717285\n",
      "    num_agent_steps_sampled: 760000\n",
      "    num_agent_steps_trained: 760000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.357142857142854\n",
      "    ram_util_percent: 67.52857142857144\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 46.5\n",
      "    policy2: 60.399999999999984\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.07\n",
      "    policy2: 4.409999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -46.0\n",
      "    policy2: -4.499999999999987\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1697574349159968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09570751902238951\n",
      "    mean_inference_ms: 1.2803067846131495\n",
      "    mean_raw_obs_processing_ms: 0.6393324113163772\n",
      "  time_since_restore: 436.7044458389282\n",
      "  time_this_iter_s: 4.8013060092926025\n",
      "  time_total_s: 436.7044458389282\n",
      "  timers:\n",
      "    learn_throughput: 924.5\n",
      "    learn_time_ms: 4326.663\n",
      "    load_throughput: 1657516.474\n",
      "    load_time_ms: 2.413\n",
      "    sample_throughput: 11481.29\n",
      "    sample_time_ms: 348.393\n",
      "    update_time_ms: 2.433\n",
      "  timestamp: 1624525156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         436.704</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">   33.48</td><td style=\"text-align: right;\">                47.1</td><td style=\"text-align: right;\">                14.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 776000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-59-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.5999999999999\n",
      "  episode_reward_mean: 34.073999999999906\n",
      "  episode_reward_min: 1.4999999999999774\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3875\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3241569995880127\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0070323823019862175\n",
      "          model: {}\n",
      "          policy_loss: -0.02379133738577366\n",
      "          total_loss: 41.69140625\n",
      "          vf_explained_var: 0.6324723362922668\n",
      "          vf_loss: 41.70808792114258\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.27369552850723267\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019724808633327484\n",
      "          model: {}\n",
      "          policy_loss: -0.03195846453309059\n",
      "          total_loss: 4.269243240356445\n",
      "          vf_explained_var: 0.2059052288532257\n",
      "          vf_loss: 4.2912163734436035\n",
      "    num_agent_steps_sampled: 776000\n",
      "    num_agent_steps_trained: 776000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.82857142857143\n",
      "    ram_util_percent: 67.3\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 47.5\n",
      "    policy2: 27.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.93\n",
      "    policy2: 2.143999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -6.699999999999983\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1693767684273319\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09558947002912724\n",
      "    mean_inference_ms: 1.2806487270158868\n",
      "    mean_raw_obs_processing_ms: 0.6392242898622235\n",
      "  time_since_restore: 446.3455469608307\n",
      "  time_this_iter_s: 4.788665056228638\n",
      "  time_total_s: 446.3455469608307\n",
      "  timers:\n",
      "    learn_throughput: 912.606\n",
      "    learn_time_ms: 4383.053\n",
      "    load_throughput: 1602163.566\n",
      "    load_time_ms: 2.497\n",
      "    sample_throughput: 11336.268\n",
      "    sample_time_ms: 352.85\n",
      "    update_time_ms: 2.431\n",
      "  timestamp: 1624525166\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         446.346</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">  34.074</td><td style=\"text-align: right;\">                48.6</td><td style=\"text-align: right;\">                 1.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 792000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-59-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.59999999999991\n",
      "  episode_reward_mean: 35.078999999999915\n",
      "  episode_reward_min: 1.4999999999999774\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3950\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.29948750138282776\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006608506198972464\n",
      "          model: {}\n",
      "          policy_loss: -0.01910887286067009\n",
      "          total_loss: 37.096317291259766\n",
      "          vf_explained_var: 0.6903815865516663\n",
      "          vf_loss: 37.108734130859375\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.25495344400405884\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015316358767449856\n",
      "          model: {}\n",
      "          policy_loss: -0.029173873364925385\n",
      "          total_loss: 6.03762674331665\n",
      "          vf_explained_var: 0.2769571542739868\n",
      "          vf_loss: 6.059046745300293\n",
      "    num_agent_steps_sampled: 792000\n",
      "    num_agent_steps_trained: 792000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.12857142857143\n",
      "    ram_util_percent: 67.34285714285714\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 28.499999999999996\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.22\n",
      "    policy2: 2.858999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -6.699999999999983\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16932045314179325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0955634737263713\n",
      "    mean_inference_ms: 1.2801569921037457\n",
      "    mean_raw_obs_processing_ms: 0.6393585809782093\n",
      "  time_since_restore: 455.576580286026\n",
      "  time_this_iter_s: 4.645648002624512\n",
      "  time_total_s: 455.576580286026\n",
      "  timers:\n",
      "    learn_throughput: 913.618\n",
      "    learn_time_ms: 4378.197\n",
      "    load_throughput: 1557570.604\n",
      "    load_time_ms: 2.568\n",
      "    sample_throughput: 11256.439\n",
      "    sample_time_ms: 355.352\n",
      "    update_time_ms: 2.417\n",
      "  timestamp: 1624525175\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         455.577</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  35.079</td><td style=\"text-align: right;\">                48.6</td><td style=\"text-align: right;\">                 1.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 808000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-59-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 52.79999999999991\n",
      "  episode_reward_mean: 34.18499999999992\n",
      "  episode_reward_min: 15.899999999999894\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4025\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3147125542163849\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008687067776918411\n",
      "          model: {}\n",
      "          policy_loss: -0.027713540941476822\n",
      "          total_loss: 43.66475296020508\n",
      "          vf_explained_var: 0.644365668296814\n",
      "          vf_loss: 43.683677673339844\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.26116618514060974\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016135016456246376\n",
      "          model: {}\n",
      "          policy_loss: -0.031140219420194626\n",
      "          total_loss: 5.470158100128174\n",
      "          vf_explained_var: 0.3033427298069\n",
      "          vf_loss: 5.493129730224609\n",
      "    num_agent_steps_sampled: 808000\n",
      "    num_agent_steps_trained: 808000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.442857142857147\n",
      "    ram_util_percent: 67.47142857142858\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 39.49999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.03\n",
      "    policy2: 2.1549999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -17.0\n",
      "    policy2: -5.6\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16920087599966036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0955068764121377\n",
      "    mean_inference_ms: 1.2793581013523778\n",
      "    mean_raw_obs_processing_ms: 0.6389796904701863\n",
      "  time_since_restore: 464.71590518951416\n",
      "  time_this_iter_s: 4.563438892364502\n",
      "  time_total_s: 464.71590518951416\n",
      "  timers:\n",
      "    learn_throughput: 920.084\n",
      "    learn_time_ms: 4347.428\n",
      "    load_throughput: 1557281.452\n",
      "    load_time_ms: 2.569\n",
      "    sample_throughput: 11355.454\n",
      "    sample_time_ms: 352.254\n",
      "    update_time_ms: 2.421\n",
      "  timestamp: 1624525184\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         464.716</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">  34.185</td><td style=\"text-align: right;\">                52.8</td><td style=\"text-align: right;\">                15.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 824000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-59-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 52.79999999999991\n",
      "  episode_reward_mean: 34.57199999999991\n",
      "  episode_reward_min: 20.699999999999907\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4100\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.31303897500038147\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006495901849120855\n",
      "          model: {}\n",
      "          policy_loss: -0.0222522784024477\n",
      "          total_loss: 43.90480422973633\n",
      "          vf_explained_var: 0.6080071926116943\n",
      "          vf_loss: 43.92047882080078\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.27914172410964966\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01406316552311182\n",
      "          model: {}\n",
      "          policy_loss: -0.02581305429339409\n",
      "          total_loss: 6.742327690124512\n",
      "          vf_explained_var: 0.2200237512588501\n",
      "          vf_loss: 6.761022567749023\n",
      "    num_agent_steps_sampled: 824000\n",
      "    num_agent_steps_trained: 824000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.27142857142857\n",
      "    ram_util_percent: 67.47142857142858\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 48.3\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.9\n",
      "    policy2: 2.6719999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -16.5\n",
      "    policy2: -5.599999999999992\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1693307122604036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09549999759098457\n",
      "    mean_inference_ms: 1.277212206653976\n",
      "    mean_raw_obs_processing_ms: 0.6386464081299384\n",
      "  time_since_restore: 473.8085789680481\n",
      "  time_this_iter_s: 4.532634973526001\n",
      "  time_total_s: 473.8085789680481\n",
      "  timers:\n",
      "    learn_throughput: 930.2\n",
      "    learn_time_ms: 4300.152\n",
      "    load_throughput: 1564338.356\n",
      "    load_time_ms: 2.557\n",
      "    sample_throughput: 11466.394\n",
      "    sample_time_ms: 348.846\n",
      "    update_time_ms: 2.429\n",
      "  timestamp: 1624525194\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         473.809</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">  34.572</td><td style=\"text-align: right;\">                52.8</td><td style=\"text-align: right;\">                20.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 840000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-00-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.29999999999991\n",
      "  episode_reward_mean: 34.49099999999991\n",
      "  episode_reward_min: -33.000000000000036\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4200\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.30664438009262085\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008208895102143288\n",
      "          model: {}\n",
      "          policy_loss: -0.024369115009903908\n",
      "          total_loss: 42.06644058227539\n",
      "          vf_explained_var: 0.5696028470993042\n",
      "          vf_loss: 42.08250045776367\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.27642345428466797\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012947984039783478\n",
      "          model: {}\n",
      "          policy_loss: -0.030250122770667076\n",
      "          total_loss: 5.773519039154053\n",
      "          vf_explained_var: 0.24822552502155304\n",
      "          vf_loss: 5.7939372062683105\n",
      "    num_agent_steps_sampled: 840000\n",
      "    num_agent_steps_trained: 840000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.342857142857138\n",
      "    ram_util_percent: 67.67142857142856\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 19.69999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.82\n",
      "    policy2: 1.6709999999999994\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1691155636593995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09539451283200227\n",
      "    mean_inference_ms: 1.2759041083587082\n",
      "    mean_raw_obs_processing_ms: 0.6383063793854422\n",
      "  time_since_restore: 483.16644263267517\n",
      "  time_this_iter_s: 4.696215867996216\n",
      "  time_total_s: 483.16644263267517\n",
      "  timers:\n",
      "    learn_throughput: 932.468\n",
      "    learn_time_ms: 4289.689\n",
      "    load_throughput: 1539533.108\n",
      "    load_time_ms: 2.598\n",
      "    sample_throughput: 11559.3\n",
      "    sample_time_ms: 346.042\n",
      "    update_time_ms: 2.423\n",
      "  timestamp: 1624525203\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         483.166</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">  34.491</td><td style=\"text-align: right;\">                51.3</td><td style=\"text-align: right;\">                 -33</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 856000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-00-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.89999999999991\n",
      "  episode_reward_mean: 35.67899999999991\n",
      "  episode_reward_min: 4.200000000000008\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4275\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2875642478466034\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008471362292766571\n",
      "          model: {}\n",
      "          policy_loss: -0.025084299966692924\n",
      "          total_loss: 41.3225212097168\n",
      "          vf_explained_var: 0.6639360189437866\n",
      "          vf_loss: 41.33902359008789\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.25057777762413025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010779996402561665\n",
      "          model: {}\n",
      "          policy_loss: -0.026533858850598335\n",
      "          total_loss: 4.847839832305908\n",
      "          vf_explained_var: 0.28296810388565063\n",
      "          vf_loss: 4.8661885261535645\n",
      "    num_agent_steps_sampled: 856000\n",
      "    num_agent_steps_trained: 856000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.066666666666666\n",
      "    ram_util_percent: 67.7\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 19.69999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.315\n",
      "    policy2: 2.364\n",
      "  policy_reward_min:\n",
      "    policy1: -4.5\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16871101088186333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09525762543951694\n",
      "    mean_inference_ms: 1.276052111387794\n",
      "    mean_raw_obs_processing_ms: 0.6380145319149767\n",
      "  time_since_restore: 492.4290614128113\n",
      "  time_this_iter_s: 4.545041799545288\n",
      "  time_total_s: 492.4290614128113\n",
      "  timers:\n",
      "    learn_throughput: 940.561\n",
      "    learn_time_ms: 4252.782\n",
      "    load_throughput: 1551535.239\n",
      "    load_time_ms: 2.578\n",
      "    sample_throughput: 11584.138\n",
      "    sample_time_ms: 345.3\n",
      "    update_time_ms: 2.394\n",
      "  timestamp: 1624525212\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         492.429</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">  35.679</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">                 4.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 872000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-00-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.299999999999905\n",
      "  episode_reward_mean: 34.34699999999991\n",
      "  episode_reward_min: 4.200000000000008\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4350\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.27596229314804077\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007240485865622759\n",
      "          model: {}\n",
      "          policy_loss: -0.02397489733994007\n",
      "          total_loss: 38.14629364013672\n",
      "          vf_explained_var: 0.7113078832626343\n",
      "          vf_loss: 38.162940979003906\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2506893575191498\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009027007035911083\n",
      "          model: {}\n",
      "          policy_loss: -0.025447098538279533\n",
      "          total_loss: 5.984738349914551\n",
      "          vf_explained_var: 0.25098225474357605\n",
      "          vf_loss: 6.003331661224365\n",
      "    num_agent_steps_sampled: 872000\n",
      "    num_agent_steps_trained: 872000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.250000000000004\n",
      "    ram_util_percent: 67.46666666666667\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 45.5\n",
      "    policy2: 17.499999999999993\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.235\n",
      "    policy2: 3.111999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -4.5\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16851249351700176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09515590101843077\n",
      "    mean_inference_ms: 1.274560757574951\n",
      "    mean_raw_obs_processing_ms: 0.6376549417401882\n",
      "  time_since_restore: 501.48084831237793\n",
      "  time_this_iter_s: 4.541166067123413\n",
      "  time_total_s: 501.48084831237793\n",
      "  timers:\n",
      "    learn_throughput: 943.703\n",
      "    learn_time_ms: 4238.622\n",
      "    load_throughput: 1570165.278\n",
      "    load_time_ms: 2.548\n",
      "    sample_throughput: 11710.436\n",
      "    sample_time_ms: 341.576\n",
      "    update_time_ms: 2.401\n",
      "  timestamp: 1624525221\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         501.481</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">  34.347</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                 4.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 888000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-00-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.1999999999999\n",
      "  episode_reward_mean: 35.05499999999991\n",
      "  episode_reward_min: 12.29999999999995\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4425\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2788536250591278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007655322086066008\n",
      "          model: {}\n",
      "          policy_loss: -0.023204611614346504\n",
      "          total_loss: 33.8001708984375\n",
      "          vf_explained_var: 0.7406017184257507\n",
      "          vf_loss: 33.81562423706055\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.24452516436576843\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00971550028771162\n",
      "          model: {}\n",
      "          policy_loss: -0.021339574828743935\n",
      "          total_loss: 6.160281181335449\n",
      "          vf_explained_var: 0.24663661420345306\n",
      "          vf_loss: 6.174243450164795\n",
      "    num_agent_steps_sampled: 888000\n",
      "    num_agent_steps_trained: 888000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.18333333333333\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 27.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.48\n",
      "    policy2: 4.575\n",
      "  policy_reward_min:\n",
      "    policy1: -14.0\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16830283758131828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09504542384751836\n",
      "    mean_inference_ms: 1.2730309142901435\n",
      "    mean_raw_obs_processing_ms: 0.6369860706008386\n",
      "  time_since_restore: 510.4184880256653\n",
      "  time_this_iter_s: 4.499073028564453\n",
      "  time_total_s: 510.4184880256653\n",
      "  timers:\n",
      "    learn_throughput: 947.654\n",
      "    learn_time_ms: 4220.949\n",
      "    load_throughput: 1578022.16\n",
      "    load_time_ms: 2.535\n",
      "    sample_throughput: 11796.748\n",
      "    sample_time_ms: 339.076\n",
      "    update_time_ms: 2.387\n",
      "  timestamp: 1624525230\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         510.418</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">  35.055</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                12.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 904000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-00-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.1999999999999\n",
      "  episode_reward_mean: 35.94599999999991\n",
      "  episode_reward_min: 11.999999999999922\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4500\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2914312779903412\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006899161729961634\n",
      "          model: {}\n",
      "          policy_loss: -0.022130368277430534\n",
      "          total_loss: 52.92919158935547\n",
      "          vf_explained_var: 0.5927540063858032\n",
      "          vf_loss: 52.9443359375\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2550762891769409\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010542658157646656\n",
      "          model: {}\n",
      "          policy_loss: -0.025997217744588852\n",
      "          total_loss: 11.8694486618042\n",
      "          vf_explained_var: 0.22677062451839447\n",
      "          vf_loss: 11.887439727783203\n",
      "    num_agent_steps_sampled: 904000\n",
      "    num_agent_steps_trained: 904000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.185714285714283\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 47.0\n",
      "    policy2: 54.89999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.415\n",
      "    policy2: 4.530999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -39.0\n",
      "    policy2: -4.500000000000001\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16838334590617643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09500249533311741\n",
      "    mean_inference_ms: 1.2704020554867688\n",
      "    mean_raw_obs_processing_ms: 0.6364192548658499\n",
      "  time_since_restore: 519.2586629390717\n",
      "  time_this_iter_s: 4.469254970550537\n",
      "  time_total_s: 519.2586629390717\n",
      "  timers:\n",
      "    learn_throughput: 952.661\n",
      "    learn_time_ms: 4198.767\n",
      "    load_throughput: 1601735.278\n",
      "    load_time_ms: 2.497\n",
      "    sample_throughput: 11900.017\n",
      "    sample_time_ms: 336.134\n",
      "    update_time_ms: 2.342\n",
      "  timestamp: 1624525239\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         519.259</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">  35.946</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 920000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-00-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 53.99999999999991\n",
      "  episode_reward_mean: 37.07999999999991\n",
      "  episode_reward_min: 8.69999999999994\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4600\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.275748074054718\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006656138692051172\n",
      "          model: {}\n",
      "          policy_loss: -0.023216459900140762\n",
      "          total_loss: 37.434017181396484\n",
      "          vf_explained_var: 0.6401604413986206\n",
      "          vf_loss: 37.450496673583984\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2555607259273529\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010465813800692558\n",
      "          model: {}\n",
      "          policy_loss: -0.025538714602589607\n",
      "          total_loss: 5.787313461303711\n",
      "          vf_explained_var: 0.20743893086910248\n",
      "          vf_loss: 5.804904937744141\n",
      "    num_agent_steps_sampled: 920000\n",
      "    num_agent_steps_trained: 920000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.214285714285715\n",
      "    ram_util_percent: 67.55714285714286\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 48.299999999999976\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.0\n",
      "    policy2: 4.079999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -19.5\n",
      "    policy2: -7.799999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16811105622387154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09485366135285261\n",
      "    mean_inference_ms: 1.268469577293728\n",
      "    mean_raw_obs_processing_ms: 0.6357655176816865\n",
      "  time_since_restore: 528.4308788776398\n",
      "  time_this_iter_s: 4.65516209602356\n",
      "  time_total_s: 528.4308788776398\n",
      "  timers:\n",
      "    learn_throughput: 956.622\n",
      "    learn_time_ms: 4181.381\n",
      "    load_throughput: 1611442.951\n",
      "    load_time_ms: 2.482\n",
      "    sample_throughput: 11945.38\n",
      "    sample_time_ms: 334.857\n",
      "    update_time_ms: 2.347\n",
      "  timestamp: 1624525249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         528.431</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\">   37.08</td><td style=\"text-align: right;\">                  54</td><td style=\"text-align: right;\">                 8.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 936000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-00-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.399999999999906\n",
      "  episode_reward_mean: 35.49299999999991\n",
      "  episode_reward_min: 5.999999999999941\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4675\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2570823132991791\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006995275616645813\n",
      "          model: {}\n",
      "          policy_loss: -0.021396106109023094\n",
      "          total_loss: 33.54477310180664\n",
      "          vf_explained_var: 0.73509681224823\n",
      "          vf_loss: 33.559085845947266\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.25152504444122314\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010854723863303661\n",
      "          model: {}\n",
      "          policy_loss: -0.024281291291117668\n",
      "          total_loss: 4.403861045837402\n",
      "          vf_explained_var: 0.2721027135848999\n",
      "          vf_loss: 4.419899940490723\n",
      "    num_agent_steps_sampled: 936000\n",
      "    num_agent_steps_trained: 936000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.357142857142854\n",
      "    ram_util_percent: 67.52857142857142\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 50.5\n",
      "    policy2: 24.09999999999995\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.93\n",
      "    policy2: 3.5629999999999997\n",
      "  policy_reward_min:\n",
      "    policy1: -2.0\n",
      "    policy2: -7.799999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16770514092950872\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09471816886902763\n",
      "    mean_inference_ms: 1.2685831963953553\n",
      "    mean_raw_obs_processing_ms: 0.6354930835112463\n",
      "  time_since_restore: 537.4825577735901\n",
      "  time_this_iter_s: 4.5714380741119385\n",
      "  time_total_s: 537.4825577735901\n",
      "  timers:\n",
      "    learn_throughput: 961.499\n",
      "    learn_time_ms: 4160.169\n",
      "    load_throughput: 1647862.334\n",
      "    load_time_ms: 2.427\n",
      "    sample_throughput: 11941.727\n",
      "    sample_time_ms: 334.96\n",
      "    update_time_ms: 2.362\n",
      "  timestamp: 1624525258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 117\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         537.483</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\">  35.493</td><td style=\"text-align: right;\">                50.4</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 952000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-01-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 54.59999999999991\n",
      "  episode_reward_mean: 37.48199999999991\n",
      "  episode_reward_min: 12.599999999999913\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4750\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.24991481006145477\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00670434208586812\n",
      "          model: {}\n",
      "          policy_loss: -0.019589873030781746\n",
      "          total_loss: 43.31212615966797\n",
      "          vf_explained_var: 0.6904425024986267\n",
      "          vf_loss: 43.32492446899414\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.25124460458755493\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009829172864556313\n",
      "          model: {}\n",
      "          policy_loss: -0.0238055307418108\n",
      "          total_loss: 4.544461727142334\n",
      "          vf_explained_var: 0.29536816477775574\n",
      "          vf_loss: 4.560802936553955\n",
      "    num_agent_steps_sampled: 952000\n",
      "    num_agent_steps_trained: 952000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.000000000000004\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.5\n",
      "    policy2: 17.5\n",
      "  policy_reward_mean:\n",
      "    policy1: 35.195\n",
      "    policy2: 2.2869999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: 7.0\n",
      "    policy2: -7.799999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16762025325881846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09467300594403462\n",
      "    mean_inference_ms: 1.2678186749406328\n",
      "    mean_raw_obs_processing_ms: 0.6355390500940505\n",
      "  time_since_restore: 546.4841501712799\n",
      "  time_this_iter_s: 4.462865114212036\n",
      "  time_total_s: 546.4841501712799\n",
      "  timers:\n",
      "    learn_throughput: 963.176\n",
      "    learn_time_ms: 4152.926\n",
      "    load_throughput: 1615041.827\n",
      "    load_time_ms: 2.477\n",
      "    sample_throughput: 11866.55\n",
      "    sample_time_ms: 337.082\n",
      "    update_time_ms: 2.337\n",
      "  timestamp: 1624525267\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 119\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         546.484</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">  37.482</td><td style=\"text-align: right;\">                54.6</td><td style=\"text-align: right;\">                12.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 968000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-01-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 55.19999999999991\n",
      "  episode_reward_mean: 34.37099999999992\n",
      "  episode_reward_min: 12.29999999999992\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4825\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.24578088521957397\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005851077847182751\n",
      "          model: {}\n",
      "          policy_loss: -0.016021618619561195\n",
      "          total_loss: 43.45042037963867\n",
      "          vf_explained_var: 0.6269910931587219\n",
      "          vf_loss: 43.46051788330078\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.24350674450397491\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008587498217821121\n",
      "          model: {}\n",
      "          policy_loss: -0.019944751635193825\n",
      "          total_loss: 13.873799324035645\n",
      "          vf_explained_var: 0.21583668887615204\n",
      "          vf_loss: 13.887224197387695\n",
      "    num_agent_steps_sampled: 968000\n",
      "    num_agent_steps_trained: 968000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.628571428571426\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 26.29999999999995\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.565\n",
      "    policy2: 4.805999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: 1.0\n",
      "    policy2: -5.599999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16750453406751348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0946175005415162\n",
      "    mean_inference_ms: 1.2668566299866781\n",
      "    mean_raw_obs_processing_ms: 0.6352173506959053\n",
      "  time_since_restore: 555.4985430240631\n",
      "  time_this_iter_s: 4.489234924316406\n",
      "  time_total_s: 555.4985430240631\n",
      "  timers:\n",
      "    learn_throughput: 961.695\n",
      "    learn_time_ms: 4159.324\n",
      "    load_throughput: 1629077.35\n",
      "    load_time_ms: 2.455\n",
      "    sample_throughput: 11817.498\n",
      "    sample_time_ms: 338.481\n",
      "    update_time_ms: 2.325\n",
      "  timestamp: 1624525276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 121\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         555.499</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\">  34.371</td><td style=\"text-align: right;\">                55.2</td><td style=\"text-align: right;\">                12.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 984000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-01-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.6999999999999\n",
      "  episode_reward_mean: 33.48599999999991\n",
      "  episode_reward_min: 11.99999999999994\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4900\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2574528753757477\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007298425305634737\n",
      "          model: {}\n",
      "          policy_loss: -0.021373813971877098\n",
      "          total_loss: 40.427467346191406\n",
      "          vf_explained_var: 0.5343409776687622\n",
      "          vf_loss: 40.44144821166992\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.23988348245620728\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00858103483915329\n",
      "          model: {}\n",
      "          policy_loss: -0.020039953291416168\n",
      "          total_loss: 10.810911178588867\n",
      "          vf_explained_var: 0.2480611652135849\n",
      "          vf_loss: 10.824435234069824\n",
      "    num_agent_steps_sampled: 984000\n",
      "    num_agent_steps_trained: 984000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.25\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 47.5\n",
      "    policy2: 47.19999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 25.765\n",
      "    policy2: 7.721\n",
      "  policy_reward_min:\n",
      "    policy1: -22.0\n",
      "    policy2: -1.2000000000000055\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16765126242107936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09461431452956937\n",
      "    mean_inference_ms: 1.2648829889712863\n",
      "    mean_raw_obs_processing_ms: 0.6349177871428628\n",
      "  time_since_restore: 564.466803073883\n",
      "  time_this_iter_s: 4.442241907119751\n",
      "  time_total_s: 564.466803073883\n",
      "  timers:\n",
      "    learn_throughput: 958.91\n",
      "    learn_time_ms: 4171.405\n",
      "    load_throughput: 1623591.074\n",
      "    load_time_ms: 2.464\n",
      "    sample_throughput: 11792.269\n",
      "    sample_time_ms: 339.205\n",
      "    update_time_ms: 2.353\n",
      "  timestamp: 1624525285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 123\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         564.467</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\">  33.486</td><td style=\"text-align: right;\">                50.7</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1000000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-01-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 57.5999999999999\n",
      "  episode_reward_mean: 34.322999999999915\n",
      "  episode_reward_min: 4.199999999999958\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5000\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.27957624197006226\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006263553164899349\n",
      "          model: {}\n",
      "          policy_loss: -0.01936435140669346\n",
      "          total_loss: 66.02657318115234\n",
      "          vf_explained_var: 0.4892638027667999\n",
      "          vf_loss: 66.03959655761719\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.23123636841773987\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007088086102157831\n",
      "          model: {}\n",
      "          policy_loss: -0.014691291376948357\n",
      "          total_loss: 24.81045150756836\n",
      "          vf_explained_var: 0.31815969944000244\n",
      "          vf_loss: 24.819759368896484\n",
      "    num_agent_steps_sampled: 1000000\n",
      "    num_agent_steps_trained: 1000000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.866666666666664\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 42.79999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.525\n",
      "    policy2: 7.797999999999997\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -4.500000000000003\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16745686897660847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09451227550448504\n",
      "    mean_inference_ms: 1.2635640256006022\n",
      "    mean_raw_obs_processing_ms: 0.6344767491443603\n",
      "  time_since_restore: 573.4366993904114\n",
      "  time_this_iter_s: 4.412427186965942\n",
      "  time_total_s: 573.4366993904114\n",
      "  timers:\n",
      "    learn_throughput: 963.901\n",
      "    learn_time_ms: 4149.805\n",
      "    load_throughput: 1642634.918\n",
      "    load_time_ms: 2.435\n",
      "    sample_throughput: 11737.534\n",
      "    sample_time_ms: 340.787\n",
      "    update_time_ms: 2.331\n",
      "  timestamp: 1624525294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 125\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         573.437</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\">  34.323</td><td style=\"text-align: right;\">                57.6</td><td style=\"text-align: right;\">                 4.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1016000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-01-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 57.5999999999999\n",
      "  episode_reward_mean: 35.429999999999914\n",
      "  episode_reward_min: 8.099999999999937\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5075\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2499667853116989\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006388661451637745\n",
      "          model: {}\n",
      "          policy_loss: -0.022287975996732712\n",
      "          total_loss: 52.9800910949707\n",
      "          vf_explained_var: 0.6502175331115723\n",
      "          vf_loss: 52.995914459228516\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.21969854831695557\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007198038976639509\n",
      "          model: {}\n",
      "          policy_loss: -0.012781202793121338\n",
      "          total_loss: 17.514198303222656\n",
      "          vf_explained_var: 0.3583277761936188\n",
      "          vf_loss: 17.521512985229492\n",
      "    num_agent_steps_sampled: 1016000\n",
      "    num_agent_steps_trained: 1016000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.75\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 40.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.38\n",
      "    policy2: 7.049999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -19.5\n",
      "    policy2: -4.500000000000003\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1669284375130022\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09431330711299162\n",
      "    mean_inference_ms: 1.2626482251479096\n",
      "    mean_raw_obs_processing_ms: 0.6337353749880996\n",
      "  time_since_restore: 582.254784822464\n",
      "  time_this_iter_s: 4.420592308044434\n",
      "  time_total_s: 582.254784822464\n",
      "  timers:\n",
      "    learn_throughput: 967.776\n",
      "    learn_time_ms: 4133.187\n",
      "    load_throughput: 1647312.216\n",
      "    load_time_ms: 2.428\n",
      "    sample_throughput: 11971.921\n",
      "    sample_time_ms: 334.115\n",
      "    update_time_ms: 2.289\n",
      "  timestamp: 1624525303\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 127\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         582.255</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">   35.43</td><td style=\"text-align: right;\">                57.6</td><td style=\"text-align: right;\">                 8.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1032000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-01-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.5999999999999\n",
      "  episode_reward_mean: 35.06099999999991\n",
      "  episode_reward_min: 8.099999999999937\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5150\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.24276036024093628\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005688406992703676\n",
      "          model: {}\n",
      "          policy_loss: -0.015478793531656265\n",
      "          total_loss: 42.58594512939453\n",
      "          vf_explained_var: 0.6779042482376099\n",
      "          vf_loss: 42.59566116333008\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.20040692389011383\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0060876160860061646\n",
      "          model: {}\n",
      "          policy_loss: -0.014645088464021683\n",
      "          total_loss: 10.825906753540039\n",
      "          vf_explained_var: 0.3160204589366913\n",
      "          vf_loss: 10.835929870605469\n",
      "    num_agent_steps_sampled: 1032000\n",
      "    num_agent_steps_trained: 1032000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.700000000000006\n",
      "    ram_util_percent: 67.52857142857142\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 40.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.44\n",
      "    policy2: 6.621\n",
      "  policy_reward_min:\n",
      "    policy1: -15.5\n",
      "    policy2: -5.599999999999991\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.166731006807423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09420355021530291\n",
      "    mean_inference_ms: 1.261301246383061\n",
      "    mean_raw_obs_processing_ms: 0.6332086454615714\n",
      "  time_since_restore: 591.180380821228\n",
      "  time_this_iter_s: 4.450028896331787\n",
      "  time_total_s: 591.180380821228\n",
      "  timers:\n",
      "    learn_throughput: 969.512\n",
      "    learn_time_ms: 4125.786\n",
      "    load_throughput: 1694137.795\n",
      "    load_time_ms: 2.361\n",
      "    sample_throughput: 11973.357\n",
      "    sample_time_ms: 334.075\n",
      "    update_time_ms: 2.297\n",
      "  timestamp: 1624525312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 129\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">          591.18</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\">  35.061</td><td style=\"text-align: right;\">                51.6</td><td style=\"text-align: right;\">                 8.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1048000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.5999999999999\n",
      "  episode_reward_mean: 35.74499999999991\n",
      "  episode_reward_min: 3.9000000000000195\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5225\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.23224569857120514\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00638602813705802\n",
      "          model: {}\n",
      "          policy_loss: -0.017932292073965073\n",
      "          total_loss: 48.474735260009766\n",
      "          vf_explained_var: 0.6207736730575562\n",
      "          vf_loss: 48.4862060546875\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.20422615110874176\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008029193617403507\n",
      "          model: {}\n",
      "          policy_loss: -0.01926053687930107\n",
      "          total_loss: 12.014561653137207\n",
      "          vf_explained_var: 0.3136391341686249\n",
      "          vf_loss: 12.02772331237793\n",
      "    num_agent_steps_sampled: 1048000\n",
      "    num_agent_steps_trained: 1048000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.03333333333333\n",
      "    ram_util_percent: 67.76666666666667\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 40.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.685\n",
      "    policy2: 6.059999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -15.0\n",
      "    policy2: -5.599999999999982\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16656889743994344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09412068552821509\n",
      "    mean_inference_ms: 1.2602316795829502\n",
      "    mean_raw_obs_processing_ms: 0.6326418441284624\n",
      "  time_since_restore: 599.9205775260925\n",
      "  time_this_iter_s: 4.339865684509277\n",
      "  time_total_s: 599.9205775260925\n",
      "  timers:\n",
      "    learn_throughput: 975.164\n",
      "    learn_time_ms: 4101.876\n",
      "    load_throughput: 1674055.419\n",
      "    load_time_ms: 2.389\n",
      "    sample_throughput: 12102.317\n",
      "    sample_time_ms: 330.515\n",
      "    update_time_ms: 2.315\n",
      "  timestamp: 1624525321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 131\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         599.921</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\">  35.745</td><td style=\"text-align: right;\">                51.6</td><td style=\"text-align: right;\">                 3.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1064000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.5999999999999\n",
      "  episode_reward_mean: 35.042999999999914\n",
      "  episode_reward_min: 3.9000000000000195\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5300\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.23823557794094086\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005204926244914532\n",
      "          model: {}\n",
      "          policy_loss: -0.016981763765215874\n",
      "          total_loss: 48.62496566772461\n",
      "          vf_explained_var: 0.586117148399353\n",
      "          vf_loss: 48.636680603027344\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.19742310047149658\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005584673024713993\n",
      "          model: {}\n",
      "          policy_loss: -0.015445487573742867\n",
      "          total_loss: 11.617486000061035\n",
      "          vf_explained_var: 0.3385823667049408\n",
      "          vf_loss: 11.628690719604492\n",
      "    num_agent_steps_sampled: 1064000\n",
      "    num_agent_steps_trained: 1064000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.485714285714288\n",
      "    ram_util_percent: 67.8\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 40.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.16\n",
      "    policy2: 4.882999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -20.5\n",
      "    policy2: -5.599999999999982\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16661107769303318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09407704657771063\n",
      "    mean_inference_ms: 1.2576710531460729\n",
      "    mean_raw_obs_processing_ms: 0.6319843871097126\n",
      "  time_since_restore: 608.7232522964478\n",
      "  time_this_iter_s: 4.456153631210327\n",
      "  time_total_s: 608.7232522964478\n",
      "  timers:\n",
      "    learn_throughput: 978.458\n",
      "    learn_time_ms: 4088.065\n",
      "    load_throughput: 1678023.644\n",
      "    load_time_ms: 2.384\n",
      "    sample_throughput: 12202.07\n",
      "    sample_time_ms: 327.813\n",
      "    update_time_ms: 2.305\n",
      "  timestamp: 1624525329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 133\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         608.723</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\">  35.043</td><td style=\"text-align: right;\">                51.6</td><td style=\"text-align: right;\">                 3.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1080000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.49999999999991\n",
      "  episode_reward_mean: 37.77899999999991\n",
      "  episode_reward_min: 23.69999999999991\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5400\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.25082454085350037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005223363172262907\n",
      "          model: {}\n",
      "          policy_loss: -0.014521021395921707\n",
      "          total_loss: 39.5062141418457\n",
      "          vf_explained_var: 0.628329873085022\n",
      "          vf_loss: 39.51543426513672\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.20017002522945404\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00696710217744112\n",
      "          model: {}\n",
      "          policy_loss: -0.01815933920443058\n",
      "          total_loss: 9.493572235107422\n",
      "          vf_explained_var: 0.3221750259399414\n",
      "          vf_loss: 9.506441116333008\n",
      "    num_agent_steps_sampled: 1080000\n",
      "    num_agent_steps_trained: 1080000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.17142857142857\n",
      "    ram_util_percent: 67.62857142857142\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 56.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.94\n",
      "    policy2: 4.8389999999999995\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -4.5000000000000036\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16640224279635774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09397688308229955\n",
      "    mean_inference_ms: 1.2561870014349423\n",
      "    mean_raw_obs_processing_ms: 0.631457447670295\n",
      "  time_since_restore: 617.7466673851013\n",
      "  time_this_iter_s: 4.543848991394043\n",
      "  time_total_s: 617.7466673851013\n",
      "  timers:\n",
      "    learn_throughput: 977.291\n",
      "    learn_time_ms: 4092.948\n",
      "    load_throughput: 1684865.429\n",
      "    load_time_ms: 2.374\n",
      "    sample_throughput: 12184.554\n",
      "    sample_time_ms: 328.284\n",
      "    update_time_ms: 2.321\n",
      "  timestamp: 1624525339\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 135\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         617.747</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\">  37.779</td><td style=\"text-align: right;\">                49.5</td><td style=\"text-align: right;\">                23.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1096000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.29999999999991\n",
      "  episode_reward_mean: 37.56599999999991\n",
      "  episode_reward_min: 22.499999999999908\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5475\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.223600372672081\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011608666740357876\n",
      "          model: {}\n",
      "          policy_loss: -0.022319426760077477\n",
      "          total_loss: 37.95277404785156\n",
      "          vf_explained_var: 0.759234607219696\n",
      "          vf_loss: 37.96921920776367\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.21290934085845947\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010500269010663033\n",
      "          model: {}\n",
      "          policy_loss: -0.022729966789484024\n",
      "          total_loss: 8.242344856262207\n",
      "          vf_explained_var: 0.34098178148269653\n",
      "          vf_loss: 8.257102012634277\n",
      "    num_agent_steps_sampled: 1096000\n",
      "    num_agent_steps_trained: 1096000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.599999999999998\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 47.5\n",
      "    policy2: 52.7\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.485\n",
      "    policy2: 5.080999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16599873319983566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0938461195930208\n",
      "    mean_inference_ms: 1.256214849749199\n",
      "    mean_raw_obs_processing_ms: 0.6310999747996031\n",
      "  time_since_restore: 626.9609663486481\n",
      "  time_this_iter_s: 4.500033140182495\n",
      "  time_total_s: 626.9609663486481\n",
      "  timers:\n",
      "    learn_throughput: 968.626\n",
      "    learn_time_ms: 4129.56\n",
      "    load_throughput: 1687848.692\n",
      "    load_time_ms: 2.37\n",
      "    sample_throughput: 12073.542\n",
      "    sample_time_ms: 331.303\n",
      "    update_time_ms: 2.342\n",
      "  timestamp: 1624525348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 137\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         626.961</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\">  37.566</td><td style=\"text-align: right;\">                51.3</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.29999999999991\n",
      "  episode_reward_mean: 37.658999999999914\n",
      "  episode_reward_min: 22.499999999999908\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5500\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.23901215195655823\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009055706672370434\n",
      "          model: {}\n",
      "          policy_loss: -0.020144682377576828\n",
      "          total_loss: 51.1915397644043\n",
      "          vf_explained_var: 0.5503359436988831\n",
      "          vf_loss: 51.207096099853516\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2099953442811966\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010106624104082584\n",
      "          model: {}\n",
      "          policy_loss: -0.027311472222208977\n",
      "          total_loss: 3.3611810207366943\n",
      "          vf_explained_var: 0.23576776683330536\n",
      "          vf_loss: 3.3808176517486572\n",
      "    num_agent_steps_sampled: 1104000\n",
      "    num_agent_steps_trained: 1104000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.38888888888889\n",
      "    ram_util_percent: 71.05555555555556\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 52.7\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.875\n",
      "    policy2: 4.783999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16624891386525412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09389999329612943\n",
      "    mean_inference_ms: 1.2551231975081258\n",
      "    mean_raw_obs_processing_ms: 0.6310298140876249\n",
      "  time_since_restore: 633.011801481247\n",
      "  time_this_iter_s: 6.050835132598877\n",
      "  time_total_s: 633.011801481247\n",
      "  timers:\n",
      "    learn_throughput: 933.293\n",
      "    learn_time_ms: 4285.9\n",
      "    load_throughput: 1683597.355\n",
      "    load_time_ms: 2.376\n",
      "    sample_throughput: 12048.246\n",
      "    sample_time_ms: 331.999\n",
      "    update_time_ms: 2.37\n",
      "  timestamp: 1624525354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 138\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         633.012</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\">  37.659</td><td style=\"text-align: right;\">                51.3</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.99999999999991\n",
      "  episode_reward_mean: 38.29799999999991\n",
      "  episode_reward_min: 20.09999999999993\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5600\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.24074704945087433\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010952427051961422\n",
      "          model: {}\n",
      "          policy_loss: -0.022378819063305855\n",
      "          total_loss: 40.17567443847656\n",
      "          vf_explained_var: 0.6254825592041016\n",
      "          vf_loss: 40.1925163269043\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.22088049352169037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008209168910980225\n",
      "          model: {}\n",
      "          policy_loss: -0.019686808809638023\n",
      "          total_loss: 3.0470938682556152\n",
      "          vf_explained_var: 0.29467371106147766\n",
      "          vf_loss: 3.0605461597442627\n",
      "    num_agent_steps_sampled: 1120000\n",
      "    num_agent_steps_trained: 1120000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.35\n",
      "    ram_util_percent: 71.9125\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 40.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 36.0\n",
      "    policy2: 2.2979999999999996\n",
      "  policy_reward_min:\n",
      "    policy1: -4.0\n",
      "    policy2: -8.89999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16648058689285533\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09402363054787802\n",
      "    mean_inference_ms: 1.2588027788366718\n",
      "    mean_raw_obs_processing_ms: 0.6321547492237665\n",
      "  time_since_restore: 643.3540205955505\n",
      "  time_this_iter_s: 5.685152053833008\n",
      "  time_total_s: 643.3540205955505\n",
      "  timers:\n",
      "    learn_throughput: 908.932\n",
      "    learn_time_ms: 4400.77\n",
      "    load_throughput: 1248592.756\n",
      "    load_time_ms: 3.204\n",
      "    sample_throughput: 10975.714\n",
      "    sample_time_ms: 364.441\n",
      "    update_time_ms: 2.604\n",
      "  timestamp: 1624525364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 140\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         643.354</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\">  38.298</td><td style=\"text-align: right;\">                  51</td><td style=\"text-align: right;\">                20.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.99999999999991\n",
      "  episode_reward_mean: 38.28899999999991\n",
      "  episode_reward_min: 16.499999999999932\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5625\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2217584103345871\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009751363657414913\n",
      "          model: {}\n",
      "          policy_loss: -0.01812124252319336\n",
      "          total_loss: 32.837345123291016\n",
      "          vf_explained_var: 0.7351718544960022\n",
      "          vf_loss: 32.85053253173828\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.20305748283863068\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007782900705933571\n",
      "          model: {}\n",
      "          policy_loss: -0.021022338420152664\n",
      "          total_loss: 4.760045528411865\n",
      "          vf_explained_var: 0.2392144650220871\n",
      "          vf_loss: 4.775158405303955\n",
      "    num_agent_steps_sampled: 1128000\n",
      "    num_agent_steps_trained: 1128000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.2125\n",
      "    ram_util_percent: 70.75\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 40.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 36.035\n",
      "    policy2: 2.2539999999999996\n",
      "  policy_reward_min:\n",
      "    policy1: -4.0\n",
      "    policy2: -8.89999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1663300781698021\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09402545071179975\n",
      "    mean_inference_ms: 1.2616507065537608\n",
      "    mean_raw_obs_processing_ms: 0.6326667168559605\n",
      "  time_since_restore: 648.4982075691223\n",
      "  time_this_iter_s: 5.144186973571777\n",
      "  time_total_s: 648.4982075691223\n",
      "  timers:\n",
      "    learn_throughput: 892.799\n",
      "    learn_time_ms: 4480.291\n",
      "    load_throughput: 1248444.097\n",
      "    load_time_ms: 3.204\n",
      "    sample_throughput: 10949.48\n",
      "    sample_time_ms: 365.314\n",
      "    update_time_ms: 2.617\n",
      "  timestamp: 1624525370\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 141\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         648.498</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\">  38.289</td><td style=\"text-align: right;\">                  51</td><td style=\"text-align: right;\">                16.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.699999999999896\n",
      "  episode_reward_mean: 38.963999999999906\n",
      "  episode_reward_min: 8.999999999999956\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5675\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.21710826456546783\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012960165739059448\n",
      "          model: {}\n",
      "          policy_loss: -0.026138702407479286\n",
      "          total_loss: 35.3424186706543\n",
      "          vf_explained_var: 0.7727813720703125\n",
      "          vf_loss: 35.362003326416016\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.20458777248859406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008599617518484592\n",
      "          model: {}\n",
      "          policy_loss: -0.027774294838309288\n",
      "          total_loss: 4.549069881439209\n",
      "          vf_explained_var: 0.273370623588562\n",
      "          vf_loss: 4.570313930511475\n",
      "    num_agent_steps_sampled: 1136000\n",
      "    num_agent_steps_trained: 1136000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.9375\n",
      "    ram_util_percent: 70.91250000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 15.300000000000004\n",
      "  policy_reward_mean:\n",
      "    policy1: 37.15\n",
      "    policy2: 1.8139999999999992\n",
      "  policy_reward_min:\n",
      "    policy1: 8.0\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1664238881562613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09407757634066793\n",
      "    mean_inference_ms: 1.263366590942911\n",
      "    mean_raw_obs_processing_ms: 0.6331355598252393\n",
      "  time_since_restore: 654.5321345329285\n",
      "  time_this_iter_s: 6.033926963806152\n",
      "  time_total_s: 654.5321345329285\n",
      "  timers:\n",
      "    learn_throughput: 860.777\n",
      "    learn_time_ms: 4646.962\n",
      "    load_throughput: 1234499.312\n",
      "    load_time_ms: 3.24\n",
      "    sample_throughput: 10891.727\n",
      "    sample_time_ms: 367.251\n",
      "    update_time_ms: 2.636\n",
      "  timestamp: 1624525376\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         654.532</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\">  38.964</td><td style=\"text-align: right;\">                50.7</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-03-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.59999999999991\n",
      "  episode_reward_mean: 38.780999999999906\n",
      "  episode_reward_min: 8.999999999999956\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5700\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.22592586278915405\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011361761949956417\n",
      "          model: {}\n",
      "          policy_loss: -0.023921573534607887\n",
      "          total_loss: 44.94271469116211\n",
      "          vf_explained_var: 0.5940679311752319\n",
      "          vf_loss: 44.96088790893555\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.202354297041893\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007086870726197958\n",
      "          model: {}\n",
      "          policy_loss: -0.02121191844344139\n",
      "          total_loss: 11.456941604614258\n",
      "          vf_explained_var: 0.325809121131897\n",
      "          vf_loss: 11.472771644592285\n",
      "    num_agent_steps_sampled: 1144000\n",
      "    num_agent_steps_trained: 1144000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.2125\n",
      "    ram_util_percent: 70.69999999999999\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 42.79999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 36.285\n",
      "    policy2: 2.4959999999999996\n",
      "  policy_reward_min:\n",
      "    policy1: -8.0\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16674619072999608\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09416750311459499\n",
      "    mean_inference_ms: 1.262882094880298\n",
      "    mean_raw_obs_processing_ms: 0.6333106003182898\n",
      "  time_since_restore: 659.9292094707489\n",
      "  time_this_iter_s: 5.397074937820435\n",
      "  time_total_s: 659.9292094707489\n",
      "  timers:\n",
      "    learn_throughput: 845.654\n",
      "    learn_time_ms: 4730.067\n",
      "    load_throughput: 1220391.929\n",
      "    load_time_ms: 3.278\n",
      "    sample_throughput: 10577.472\n",
      "    sample_time_ms: 378.162\n",
      "    update_time_ms: 2.647\n",
      "  timestamp: 1624525381\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 143\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         659.929</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\">  38.781</td><td style=\"text-align: right;\">                51.6</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 52.79999999999992\n",
      "  episode_reward_mean: 36.83399999999992\n",
      "  episode_reward_min: 8.999999999999956\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5750\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.19761675596237183\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010123346000909805\n",
      "          model: {}\n",
      "          policy_loss: -0.020393038168549538\n",
      "          total_loss: 56.9569091796875\n",
      "          vf_explained_var: 0.6359074115753174\n",
      "          vf_loss: 56.9721794128418\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.19509486854076385\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00623331731185317\n",
      "          model: {}\n",
      "          policy_loss: -0.015097827650606632\n",
      "          total_loss: 26.778621673583984\n",
      "          vf_explained_var: 0.28644058108329773\n",
      "          vf_loss: 26.788990020751953\n",
      "    num_agent_steps_sampled: 1152000\n",
      "    num_agent_steps_trained: 1152000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.82857142857143\n",
      "    ram_util_percent: 70.77142857142857\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 45.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.04\n",
      "    policy2: 3.7939999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -24.0\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1665028130485986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09411770633725623\n",
      "    mean_inference_ms: 1.2643209961636324\n",
      "    mean_raw_obs_processing_ms: 0.6336044589642214\n",
      "  time_since_restore: 664.9656484127045\n",
      "  time_this_iter_s: 5.036438941955566\n",
      "  time_total_s: 664.9656484127045\n",
      "  timers:\n",
      "    learn_throughput: 835.856\n",
      "    learn_time_ms: 4785.511\n",
      "    load_throughput: 1205857.501\n",
      "    load_time_ms: 3.317\n",
      "    sample_throughput: 10574.106\n",
      "    sample_time_ms: 378.283\n",
      "    update_time_ms: 2.667\n",
      "  timestamp: 1624525386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 144\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         664.966</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\">  36.834</td><td style=\"text-align: right;\">                52.8</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-03-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.2999999999999\n",
      "  episode_reward_mean: 37.94099999999992\n",
      "  episode_reward_min: 11.700000000000003\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5825\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.20331810414791107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013396869413554668\n",
      "          model: {}\n",
      "          policy_loss: -0.024723723530769348\n",
      "          total_loss: 52.11165237426758\n",
      "          vf_explained_var: 0.6604058742523193\n",
      "          vf_loss: 52.129600524902344\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.18580752611160278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007885206490755081\n",
      "          model: {}\n",
      "          policy_loss: -0.015923289582133293\n",
      "          total_loss: 25.498579025268555\n",
      "          vf_explained_var: 0.2031383514404297\n",
      "          vf_loss: 25.508516311645508\n",
      "    num_agent_steps_sampled: 1168000\n",
      "    num_agent_steps_trained: 1168000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.914285714285715\n",
      "    ram_util_percent: 70.3\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.5\n",
      "    policy2: 59.3\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.505\n",
      "    policy2: 8.435999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -29.0\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16653285377698615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0941350906257862\n",
      "    mean_inference_ms: 1.2647005179564241\n",
      "    mean_raw_obs_processing_ms: 0.6337583894533371\n",
      "  time_since_restore: 674.5527865886688\n",
      "  time_this_iter_s: 4.716874122619629\n",
      "  time_total_s: 674.5527865886688\n",
      "  timers:\n",
      "    learn_throughput: 830.622\n",
      "    learn_time_ms: 4815.668\n",
      "    load_throughput: 1186029.394\n",
      "    load_time_ms: 3.373\n",
      "    sample_throughput: 10499.199\n",
      "    sample_time_ms: 380.981\n",
      "    update_time_ms: 2.677\n",
      "  timestamp: 1624525396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 146\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         674.553</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\">  37.941</td><td style=\"text-align: right;\">                51.3</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-03-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 52.19999999999991\n",
      "  episode_reward_mean: 38.666999999999916\n",
      "  episode_reward_min: 21.299999999999933\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5900\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.21368350088596344\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011372498236596584\n",
      "          model: {}\n",
      "          policy_loss: -0.019039282575249672\n",
      "          total_loss: 60.702919006347656\n",
      "          vf_explained_var: 0.5923455357551575\n",
      "          vf_loss: 60.7161979675293\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.1883404403924942\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00579369580373168\n",
      "          model: {}\n",
      "          policy_loss: -0.009316980838775635\n",
      "          total_loss: 27.108476638793945\n",
      "          vf_explained_var: 0.2842789590358734\n",
      "          vf_loss: 27.1133975982666\n",
      "    num_agent_steps_sampled: 1184000\n",
      "    num_agent_steps_trained: 1184000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.0\n",
      "    ram_util_percent: 70.25714285714285\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 48.3\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.09\n",
      "    policy2: 6.576999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -15.0\n",
      "    policy2: -6.6999999999999895\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1667599541608395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09417481718806592\n",
      "    mean_inference_ms: 1.2632780472914509\n",
      "    mean_raw_obs_processing_ms: 0.6336828355038147\n",
      "  time_since_restore: 683.4815049171448\n",
      "  time_this_iter_s: 4.451847076416016\n",
      "  time_total_s: 683.4815049171448\n",
      "  timers:\n",
      "    learn_throughput: 859.369\n",
      "    learn_time_ms: 4654.578\n",
      "    load_throughput: 1174990.265\n",
      "    load_time_ms: 3.404\n",
      "    sample_throughput: 10518.673\n",
      "    sample_time_ms: 380.276\n",
      "    update_time_ms: 2.632\n",
      "  timestamp: 1624525405\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         683.482</td><td style=\"text-align: right;\">592000</td><td style=\"text-align: right;\">  38.667</td><td style=\"text-align: right;\">                52.2</td><td style=\"text-align: right;\">                21.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-03-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 53.99999999999991\n",
      "  episode_reward_mean: 38.91299999999992\n",
      "  episode_reward_min: 15.599999999999943\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6000\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2088169902563095\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013482782989740372\n",
      "          model: {}\n",
      "          policy_loss: -0.02733447402715683\n",
      "          total_loss: 46.111915588378906\n",
      "          vf_explained_var: 0.656093180179596\n",
      "          vf_loss: 46.132423400878906\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.19980944693088531\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005913416389375925\n",
      "          model: {}\n",
      "          policy_loss: -0.014601672068238258\n",
      "          total_loss: 8.434819221496582\n",
      "          vf_explained_var: 0.28599923849105835\n",
      "          vf_loss: 8.444929122924805\n",
      "    num_agent_steps_sampled: 1200000\n",
      "    num_agent_steps_trained: 1200000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.799999999999997\n",
      "    ram_util_percent: 69.88333333333334\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 54.0\n",
      "    policy2: 43.9\n",
      "  policy_reward_mean:\n",
      "    policy1: 34.58\n",
      "    policy2: 4.332999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -16.0\n",
      "    policy2: -5.599999999999984\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16651803327911288\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09404636908923124\n",
      "    mean_inference_ms: 1.2613873014260135\n",
      "    mean_raw_obs_processing_ms: 0.6330015225839984\n",
      "  time_since_restore: 692.3037040233612\n",
      "  time_this_iter_s: 4.360272169113159\n",
      "  time_total_s: 692.3037040233612\n",
      "  timers:\n",
      "    learn_throughput: 881.167\n",
      "    learn_time_ms: 4539.436\n",
      "    load_throughput: 1558771.729\n",
      "    load_time_ms: 2.566\n",
      "    sample_throughput: 11583.935\n",
      "    sample_time_ms: 345.306\n",
      "    update_time_ms: 2.39\n",
      "  timestamp: 1624525414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 150\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         692.304</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\">  38.913</td><td style=\"text-align: right;\">                  54</td><td style=\"text-align: right;\">                15.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 54.299999999999905\n",
      "  episode_reward_mean: 39.91199999999991\n",
      "  episode_reward_min: 19.499999999999908\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6075\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2036026567220688\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011194639839231968\n",
      "          model: {}\n",
      "          policy_loss: -0.021081706508994102\n",
      "          total_loss: 42.769195556640625\n",
      "          vf_explained_var: 0.7206318378448486\n",
      "          vf_loss: 42.78461837768555\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.21334820985794067\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007823157124221325\n",
      "          model: {}\n",
      "          policy_loss: -0.020087214186787605\n",
      "          total_loss: 4.281621932983398\n",
      "          vf_explained_var: 0.3642756938934326\n",
      "          vf_loss: 4.295769214630127\n",
      "    num_agent_steps_sampled: 1216000\n",
      "    num_agent_steps_trained: 1216000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.51666666666667\n",
      "    ram_util_percent: 68.75\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 37.3\n",
      "  policy_reward_mean:\n",
      "    policy1: 37.24\n",
      "    policy2: 2.671999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -16.0\n",
      "    policy2: -6.6999999999999895\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16600044668778524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09385819585905754\n",
      "    mean_inference_ms: 1.2605885402097374\n",
      "    mean_raw_obs_processing_ms: 0.6323115375757706\n",
      "  time_since_restore: 701.1891367435455\n",
      "  time_this_iter_s: 4.493305683135986\n",
      "  time_total_s: 701.1891367435455\n",
      "  timers:\n",
      "    learn_throughput: 927.251\n",
      "    learn_time_ms: 4313.828\n",
      "    load_throughput: 1546828.935\n",
      "    load_time_ms: 2.586\n",
      "    sample_throughput: 11705.267\n",
      "    sample_time_ms: 341.727\n",
      "    update_time_ms: 2.364\n",
      "  timestamp: 1624525423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 152\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         701.189</td><td style=\"text-align: right;\">608000</td><td style=\"text-align: right;\">  39.912</td><td style=\"text-align: right;\">                54.3</td><td style=\"text-align: right;\">                19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "\n",
    "# Solution to Exercise #2\n",
    "\n",
    "# Run for longer this time (100 iterations) and try to reach 40.0 reward (sum of both agents).\n",
    "stop = {\n",
    "    \"training_iteration\": 200,  # we have the 15min break now to run this many iterations\n",
    "    \"episode_reward_mean\": 60.0,  # sum of both agents' rewards. Probably won't reach it, but we should try nevertheless :)\n",
    "}\n",
    "\n",
    "# tune_config.update({\n",
    "# ???\n",
    "# })\n",
    "\n",
    "# analysis = tune.run(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d282a-4ad1-4d5f-9dec-00afb8154048",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 15 min break :)\n",
    "------------------\n",
    "\n",
    "\n",
    "(while the above experiment is running (and hopefully learning))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82057-6b4c-4075-bd32-93c3426a1700",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How do we extract any checkpoint from a trial of a tune.run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160e4d0-8feb-411d-a457-dfc10d50e909",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The previous tune.run (the one we did before the exercise) returned an Analysis object, from which we can access any checkpoint\n",
    "# (given we set checkpoint_freq or checkpoint_at_end to reasonable values) like so:\n",
    "print(analysis)\n",
    "# Get all trials.\n",
    "trials = analysis.trials\n",
    "# Assuming, the first trial was the best, we'd like to extract this trial's best checkpoint \"\":\n",
    "best_checkpoint = analysis.get_best_checkpoint(trial=trials[0], mode=\"max\")\n",
    "print(f\"Found best checkpoint for trial #2: {best_checkpoint}\")\n",
    "\n",
    "# Undo the grid-search config, which RLlib doesn't understand.\n",
    "rllib_config = tune_config.copy()\n",
    "rllib_config[\"lr\"] = 0.00005\n",
    "rllib_config[\"train_batch_size\"] = 4000\n",
    "\n",
    "# Restore a RLlib Trainer from the checkpoint.\n",
    "new_trainer = PPOTrainer(config=rllib_config)\n",
    "new_trainer.restore(best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e104d-72f8-4b80-bf9a-2f8cbd25d9cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "with out:\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        a1 = new_trainer.compute_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "        a2 = new_trainer.compute_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "        actions = {\"agent1\": a1, \"agent2\": a2}\n",
    "        obs, rewards, dones, _ = env.step(actions)\n",
    "\n",
    "        out.clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.07)\n",
    "\n",
    "        if dones[\"agent1\"] is True:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3f56e-c4e1-4503-9ce5-589e826d1e5a",
   "metadata": {},
   "source": [
    "## Let's talk about customization options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f940b-697c-4d1d-af28-1d174331dc3c",
   "metadata": {},
   "source": [
    "### Deep Dive: How do we customize RLlib's RL loop?\n",
    "\n",
    "RLlib offers a callbacks API that allows you to add custom behavior to\n",
    "all major events during the environment sampling- and learning process.\n",
    "\n",
    "**Our problem:** So far, we can only see standard stats, such as rewards, episode lengths, etc..\n",
    "This does not give us enough insights sometimes into important questions, such as: How many times\n",
    "have both agents collided? or How many times has agent1 discovered a new field?\n",
    "\n",
    "In the following cell, we will create custom callback \"hooks\" that will allow us to\n",
    "add these stats to the returned metrics dict, and which will therefore be displayed in tensorboard!\n",
    "\n",
    "For that we will override RLlib's DefaultCallbacks class and implement the\n",
    "`on_episode_start`, `on_episode_step`, and `on_episode_end` methods therein:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb9733-01bc-426b-ad57-7983fc7db8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the DefaultCallbacks with your own and implement any methods (hooks)\n",
    "# that you need.\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(self,\n",
    "                         *,\n",
    "                         worker,\n",
    "                         base_env,\n",
    "                         policies,\n",
    "                         episode: MultiAgentEpisode,\n",
    "                         env_index,\n",
    "                         **kwargs):\n",
    "        # We will use the `MultiAgentEpisode` object being passed into\n",
    "        # all episode-related callbacks. It comes with a user_data property (dict),\n",
    "        # which we can write arbitrary data into.\n",
    "\n",
    "        # At the end of an episode, we'll transfer that data into the `hist_data`, and `custom_metrics`\n",
    "        # properties to make sure our custom data is displayed in TensorBoard.\n",
    "\n",
    "        # The episode is starting:\n",
    "        # Set per-episode object to capture, which states (observations)\n",
    "        # have been visited by agent1.\n",
    "        episode.user_data[\"new_fields_discovered\"] = 0\n",
    "        # Set per-episode agent2-blocks counter (how many times has agent2 blocked agent1?).\n",
    "        episode.user_data[\"num_collisions\"] = 0\n",
    "\n",
    "    def on_episode_step(self,\n",
    "                        *,\n",
    "                        worker,\n",
    "                        base_env,\n",
    "                        episode: MultiAgentEpisode,\n",
    "                        env_index,\n",
    "                        **kwargs):\n",
    "        # Get both rewards.\n",
    "        ag1_r = episode.prev_reward_for(\"agent1\")\n",
    "        ag2_r = episode.prev_reward_for(\"agent2\")\n",
    "\n",
    "        # Agent1 discovered a new field.\n",
    "        if ag1_r == 1.0:\n",
    "            episode.user_data[\"new_fields_discovered\"] += 1\n",
    "        # Collision.\n",
    "        elif ag2_r == 1.0:\n",
    "            episode.user_data[\"num_collisions\"] += 1\n",
    "\n",
    "    def on_episode_end(self,\n",
    "                       *,\n",
    "                       worker,\n",
    "                       base_env,\n",
    "                       policies,\n",
    "                       episode: MultiAgentEpisode,\n",
    "                       env_index,\n",
    "                       **kwargs):\n",
    "        # Episode is done:\n",
    "        # Write scalar values (sum over rewards) to `custom_metrics` and\n",
    "        # time-series data (rewards per time step) to `hist_data`.\n",
    "        # Both will be visible then in TensorBoard.\n",
    "        episode.custom_metrics[\"new_fields_discovered\"] = episode.user_data[\"new_fields_discovered\"]\n",
    "        episode.custom_metrics[\"num_collisions\"] = episode.user_data[\"num_collisions\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2fe8eb-c52f-4a26-9067-96ad9fe160a4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up our config to point to our new custom callbacks class:\n",
    "config = {\n",
    "    \"env\": MultiAgentArena,\n",
    "    \"callbacks\": MyCallbacks,  # by default, this would point to `rllib.agents.callbacks.DefaultCallbacks`, which does nothing.\n",
    "}\n",
    "\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,\n",
    "    stop={\"training_iteration\": 20},\n",
    "    checkpoint_at_end=True,\n",
    "    # If you'd like to restore the tune run from an existing checkpoint file, you can do the following:\n",
    "    #restore=\"/Users/sven/ray_results/PPO/PPO_MultiAgentArena_fd451_00000_0_2021-05-25_15-13-26/checkpoint_000010/checkpoint-10\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa6a24",
   "metadata": {},
   "source": [
    "### Let's check tensorboard for the new custom metrics!\n",
    "\n",
    "1. Head over to the Anyscale project view and click on the \"TensorBoard\" butten:\n",
    "\n",
    "<img src=\"images/tensorboard_button.png\" width=1000>\n",
    "\n",
    "Alternatively - if you ran this locally on your own machine:\n",
    "\n",
    "1. Head over to ~/ray_results/PPO/PPO_MultiAgentArena_[some key]_00000_0_[date]_[time]/\n",
    "1. In that directory, you should see a `event.out....` file.\n",
    "1. Run `tensorboard --logdir .` and head to https://localhost:6006\n",
    "\n",
    "<img src=\"images/tensorboard.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac90dc-097d-4f10-b5ea-c4c1167f1f3a",
   "metadata": {},
   "source": [
    "### Deep Dive: Providing your custom Models in tf or torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5516d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "\n",
    "tf1, tf, tf_version = try_import_tf()\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "# Custom Neural Network Models.\n",
    "class MyKerasModel(TFModelV2):\n",
    "    \"\"\"Custom model for policy gradient algorithms.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        super(MyKerasModel, self).__init__(obs_space, action_space,\n",
    "                                           num_outputs, model_config, name)\n",
    "        \n",
    "        # Keras Input layer.\n",
    "        self.inputs = tf.keras.layers.Input(\n",
    "            shape=obs_space.shape, name=\"observations\")\n",
    "\n",
    "        # Hidden layer (shared by action logits outputs and value output).\n",
    "        layer_1 = tf.keras.layers.Dense(\n",
    "            16,\n",
    "            name=\"layer1\",\n",
    "            activation=tf.nn.relu)(self.inputs)\n",
    "        \n",
    "        # Action logits output.\n",
    "        logits = tf.keras.layers.Dense(\n",
    "            num_outputs,\n",
    "            name=\"out\",\n",
    "            activation=None)(layer_1)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        value_out = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            name=\"value\",\n",
    "            activation=None)(layer_1)\n",
    "\n",
    "        # The actual Keras model:\n",
    "        self.base_model = tf.keras.Model(self.inputs,\n",
    "                                         [logits, value_out])\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers and calculate the \"value\"\n",
    "        # of the observation and store it for when `value_function` is called.\n",
    "        logits, self.cur_value = self.base_model(input_dict[\"obs\"])\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return tf.reshape(self.cur_value, [-1])\n",
    "\n",
    "\n",
    "class MyTorchModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.device = torch.device(\"cuda\"\n",
    "                                   if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Hidden layer (shared by action logits outputs and value output).\n",
    "        self.layer_1 = nn.Linear(obs_space.shape[0], 16).to(self.device)\n",
    "\n",
    "        # Action logits output.\n",
    "        self.layer_out = nn.Linear(16, num_outputs).to(self.device)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        self.value_branch = nn.Linear(16, 1).to(self.device)\n",
    "        self.cur_value = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers.\n",
    "        layer_1_out = self.layer_1(input_dict[\"obs\"])\n",
    "        logits = self.layer_out(layer_1_out)\n",
    "\n",
    "        # Calculate the \"value\" of the observation and store it for\n",
    "        # when `value_function` is called.\n",
    "        self.cur_value = self.value_branch(layer_1_out).squeeze(1)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return self.cur_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "controversial-repair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-output=(<tf.Tensor 'model_6/out/BiasAdd:0' shape=(1, 2) dtype=float32>, [])\n"
     ]
    }
   ],
   "source": [
    "# Do a quick test on the custom model classes.\n",
    "test_model_tf = MyKerasModel(\n",
    "    obs_space=gym.spaces.Box(-1.0, 1.0, (2, )),\n",
    "    action_space=None,\n",
    "    num_outputs=2,\n",
    "    model_config={},\n",
    "    name=\"MyModel\",\n",
    ")\n",
    "\n",
    "print(\"TF-output={}\".format(test_model_tf({\"obs\": np.array([[0.5, 0.5]])})))\n",
    "\n",
    "# For PyTorch, you can do:\n",
    "#test_model_torch = MyTorchModel(\n",
    "#    obs_space=gym.spaces.Box(-1.0, 1.0, (2, )),\n",
    "#    action_space=None,\n",
    "#    num_outputs=2,\n",
    "#    model_config={},\n",
    "#    name=\"MyModel\",\n",
    "#)\n",
    "#print(\"Torch-output={}\".format(test_model_torch({\"obs\": torch.from_numpy(np.array([[0.5, 0.5]], dtype=np.float32))})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2237526a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-bab76b26568f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set up our custom model and re-run the experiment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m config.update({\n\u001b[0m\u001b[1;32m      3\u001b[0m     \"model\": {\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m\"custom_model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMyKerasModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \"custom_model_config\": {\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up our custom model and re-run the experiment.\n",
    "config.update({\n",
    "    \"model\": {\n",
    "        \"custom_model\": MyKerasModel,\n",
    "        \"custom_model_config\": {\n",
    "            #\"layers\": [128, 128],\n",
    "        },\n",
    "    },\n",
    "    # Revert these to single trials (and use those hyperparams that performed well in our Exercise #2).\n",
    "    \"lr\": 0.0005,\n",
    "    \"train_batch_size\": 2000,\n",
    "})\n",
    "\n",
    "tune.run(\"PPO\", config=config, stop=stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85ad95",
   "metadata": {},
   "source": [
    "### Deep Dive: A closer look at RLlib's APIs and components\n",
    "#### (Depending on time left and amount of questions having been accumulated :)\n",
    "\n",
    "We already took a quick look inside an RLlib Trainer object and extracted its Policy(ies) and the Policy's model (neural network). Here is a much more detailed overview of what's inside a Trainer object.\n",
    "\n",
    "At the core is the so-called `WorkerSet` sitting under `Trainer.workers`. A WorkerSet is a group of `RolloutWorker` (`rllib.evaluation.rollout_worker.py`) objects that always consists of a \"local worker\" (`Trainer.workers.local_worker()`) and n \"remote workers\" (`Trainer.workers.remote_workers()`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f37549",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_structure.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d72883",
   "metadata": {},
   "source": [
    "### Scaling RLlib\n",
    "\n",
    "Scaling RLlib works by parallelizing the \"jobs\" that the remote `RolloutWorkers` do. In a vanilla RL algorithm, like PPO, DQN, and many others, the `@ray.remote` labeled RolloutWorkers in the figure above are responsible for interacting with one or more environments and thereby collecting experiences. Observations are produced by the environment, actions are then computed by the Policy(ies) copy located on the remote worker and sent to the environment in order to produce yet another observation. This cycle is repeated endlessly and only sometimes interrupted to send experience batches (\"train batches\") of a certain size to the \"local worker\". There these batches are used to call `Policy.learn_on_batch()`, which performs a loss calculation, followed by a model weights update, and a subsequent weights broadcast back to all the remote workers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {},
   "source": [
    "## Time for Q&A\n",
    "\n",
    "...\n",
    "\n",
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful.\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials.git\">github repo of this tutorial</a>.\n",
    "- <a href=\"https://docs.ray.io/en/master/rllib.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on RLlib.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13abd3-3ac0-490d-8040-63110e26677a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b385f-1353-433a-a14b-5be17a732297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
