{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "# Hands-on RL with Ray’s RLlib\n",
    "## A beginner’s tutorial for working with multi-agent environments, models, and algorithms\n",
    "\n",
    "<img src=\"images/pitfall.jpg\" width=250> <img src=\"images/tesla.jpg\" width=254> <img src=\"images/forklifts.jpg\" width=169> <img src=\"images/robots.jpg\" width=252> <img src=\"images/dota2.jpg\" width=213>\n",
    "\n",
    "### Overview\n",
    "“Hands-on RL with Ray’s RLlib” is a beginners tutorial for working with reinforcement learning (RL) environments, models, and algorithms using Ray’s RLlib library. RLlib offers high scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL) before proceeding to RLlib (multi- and single-agent) environments, neural network models, hyperparameter tuning, debugging, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo.\n",
    "\n",
    "### Intended Audience\n",
    "* Python programmers who want to get started with reinforcement learning and RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "To get this very notebook up and running on your local machine, you can follow these steps here:\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Linux):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install ray[rllib]\n",
    "$ pip install tensorflow  # <- either one works!\n",
    "$ pip install torch  # <- either one works!\n",
    "$ pip install jupyterlab\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install cmake \"ray[rllib]\"\n",
    "$ pip install tensorflow  # <- either one works!\n",
    "$ pip install torch  # <- either one works!\n",
    "$ pip install jupyterlab\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install ray[rllib]\n",
    "$ pip install [tensorflow|torch]  # <- either one works!\n",
    "$ pip install jupyterlab\n",
    "$ conda install pywin32\n",
    "```\n",
    "\n",
    "Also, for Win10 Atari support, we have to install atari_py from a different source (gym does not support Atari envs on Windows).\n",
    "\n",
    "```\n",
    "$ pip install git+https://github.com/Kojoley/atari-py.git\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and why RLlib?\n",
    "* Core concepts of RLlib: Environments, Trainers, Policies, and Models.\n",
    "* How to configure, hyperparameter-tune, and parallelize RLlib.\n",
    "* RLlib debugging best practices.\n",
    "\n",
    "### Tutorial Outline\n",
    "1. RL and RLlib in a nutshell.\n",
    "1. Defining an RL-solvable problem: Our first environment.\n",
    "1. **Exercise No.1**: Environment loop.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. Picking an algorithm and training our first RLlib Trainer.\n",
    "1. Configurations and hyperparameters - Easy tuning with Ray Tune.\n",
    "1. Fixing our experiment's config - Going multi-agent.\n",
    "1. The \"infinite laptop\": Quick intro into how to use RLlib with Anyscale's product.\n",
    "1. **Exercise No.2**: Run your own Ray RLlib+Tune experiment)\n",
    "1. Neural network models - Provide your custom models using tf.keras or torch.nn.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. Deeper dive into RLlib's parallelization architecture.\n",
    "1. Specifying different compute resources and parallelization options through our config.\n",
    "1. \"Hacking in\": Using callbacks to customize the RL loop and generate our own metrics.\n",
    "1. **Exercise No.3**: Write your own custom callback.\n",
    "1. \"Hacking in (part II)\" - Debugging with RLlib and PyCharm.\n",
    "1. Checking on the \"infinite laptop\" - Did RLlib learn to solve the problem?\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n",
    "\n",
    "<img src=\"images/unity3d_blog_post.png\" width=400>\n",
    "\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-yorkshire",
   "metadata": {},
   "source": [
    "## The RL cycle\n",
    "\n",
    "<img src=\"images/rl-cycle.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62744730",
   "metadata": {},
   "source": [
    "### Coding/defining our \"problem\" via an RL environment.\n",
    "\n",
    "We will use the following (adversarial) multi-agent environment\n",
    "throughout this tutorial to demonstrate a large fraction of RLlib's\n",
    "APIs, features, and customization options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb35116-efda-4799-8bae-e96d7775a0d1",
   "metadata": {},
   "source": [
    "<img src=\"images/environment.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe753-d7e0-4de1-b937-160507f75ed8",
   "metadata": {},
   "source": [
    "### A word or two on Spaces:\n",
    "\n",
    "Spaces are used in ML to describe what possible/valid values inputs and outputs of a neural network can have.\n",
    "\n",
    "RL environments also use them to describe what their valid observations and actions are.\n",
    "\n",
    "Spaces are usually defined by their shape (e.g. 84x84x3 RGB images) and datatype (e.g. uint8 for RGB values between 0 and 255).\n",
    "However, spaces could also be composed of other spaces (see Tuple or Dict spaces) or could be simply discrete with n fixed possible values\n",
    "(represented by integers). For example, in our game, where each agent can only go up/down/left/right, the action space would be `Discrete(4)`\n",
    "(no datatype, no shape needs to be defined here). Our observation space will be `MultiDiscrete([n, m])`, where n is the position of the agent observing and m is the position of the opposing agent, so if agent1 starts in the upper left corner and agent2 starts in the bottom right corner, agent1's observation would be: `[0, 63]` (in an 8 x 8 grid) and agent2's observation would be `[63, 0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e4135-98ed-4e65-9e26-66f340747529",
   "metadata": {},
   "source": [
    "<img src=\"images/spaces.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6925507-0210-49d2-9e68-5d1e1157ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________\n",
      "|.         |\n",
      "|1         |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|         2|\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "\n",
      "R1= 1.0\n",
      "R2=-0.1\n",
      "\n",
      "Agent1's x/y position=[1, 0]\n",
      "Agent2's x/y position=[8, 9]\n",
      "Env timesteps=1\n"
     ]
    }
   ],
   "source": [
    "# Let's code our multi-agent environment.\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "class MultiAgentArena(MultiAgentEnv):\n",
    "    def __init__(self, config=None):\n",
    "        config = config or {}\n",
    "        # Dimensions of the grid.\n",
    "        self.width = config.get(\"width\", 10)\n",
    "        self.height = config.get(\"height\", 10)\n",
    "\n",
    "        # End an episode after this many timesteps.\n",
    "        self.timestep_limit = config.get(\"ts\", 100)\n",
    "\n",
    "        self.observation_space = MultiDiscrete([self.width * self.height,\n",
    "                                                self.width * self.height])\n",
    "        # 0=up, 1=right, 2=down, 3=left.\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        # Reset env.\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Returns initial observation of next(!) episode.\"\"\"\n",
    "        # Row-major coords.\n",
    "        self.agent1_pos = [0, 0]  # upper left corner\n",
    "        self.agent2_pos = [self.height - 1, self.width - 1]  # lower bottom corner\n",
    "\n",
    "        # Accumulated rewards in this episode.\n",
    "        self.agent1_R = 0.0\n",
    "        self.agent2_R = 0.0\n",
    "\n",
    "        # Reset agent1's visited fields.\n",
    "        self.agent1_visited_fields = set([tuple(self.agent1_pos)])\n",
    "\n",
    "        # How many timesteps have we done in this episode.\n",
    "        self.timesteps = 0\n",
    "\n",
    "        # Return the initial observation in the new episode.\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: dict):\n",
    "        \"\"\"\n",
    "        Returns (next observation, rewards, dones, infos) after having taken the given actions.\n",
    "        \n",
    "        e.g.\n",
    "        `action={\"agent1\": action_for_agent1, \"agent2\": action_for_agent2}`\n",
    "        \"\"\"\n",
    "        \n",
    "        # increase our time steps counter by 1.\n",
    "        self.timesteps += 1\n",
    "        # An episode is \"done\" when we reach the time step limit.\n",
    "        is_done = self.timesteps >= self.timestep_limit\n",
    "\n",
    "        # Agent2 always moves first.\n",
    "        # events = [collision|agent1_new_field]\n",
    "        events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        events |= self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "\n",
    "        # Useful for rendering.\n",
    "        self.collision = \"collision\" in events\n",
    "            \n",
    "        # Get observations (based on new agent positions).\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Determine rewards based on the collected events:\n",
    "        r1 = -1.0 if \"collision\" in events else 1.0 if \"agent1_new_field\" in events else -0.5\n",
    "        r2 = 1.0 if \"collision\" in events else -0.1\n",
    "\n",
    "        self.agent1_R += r1\n",
    "        self.agent2_R += r2\n",
    "        \n",
    "        rewards = {\n",
    "            \"agent1\": r1,\n",
    "            \"agent2\": r2,\n",
    "        }\n",
    "\n",
    "        # Generate a `done` dict (per-agent and total).\n",
    "        dones = {\n",
    "            \"agent1\": is_done,\n",
    "            \"agent2\": is_done,\n",
    "            # special `__all__` key indicates that the episode is done for all agents.\n",
    "            \"__all__\": is_done,\n",
    "        }\n",
    "\n",
    "        return obs, rewards, dones, {}  # <- info dict (not needed here).\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns obs dict (agent name to discrete-pos tuple) using each\n",
    "        agent's current x/y-positions.\n",
    "        \"\"\"\n",
    "        ag1_discrete_pos = self.agent1_pos[0] * self.width + \\\n",
    "            (self.agent1_pos[1] % self.width)\n",
    "        ag2_discrete_pos = self.agent2_pos[0] * self.width + \\\n",
    "            (self.agent2_pos[1] % self.width)\n",
    "        return {\n",
    "            \"agent1\": np.array([ag1_discrete_pos, ag2_discrete_pos]),\n",
    "            \"agent2\": np.array([ag2_discrete_pos, ag1_discrete_pos]),\n",
    "        }\n",
    "\n",
    "    def _move(self, coords, action, is_agent1):\n",
    "        \"\"\"\n",
    "        Moves an agent (agent1 iff is_agent1=True, else agent2) from `coords` (x/y) using the\n",
    "        given action (0=up, 1=right, etc..) and returns a resulting events dict:\n",
    "        Agent1: \"new\" when entering a new field. \"bumped\" when having been bumped into by agent2.\n",
    "        Agent2: \"bumped\" when bumping into agent1 (agent1 then gets -1.0).\n",
    "        \"\"\"\n",
    "        orig_coords = coords[:]\n",
    "        # Change the row: 0=up (-1), 2=down (+1)\n",
    "        coords[0] += -1 if action == 0 else 1 if action == 2 else 0\n",
    "        # Change the column: 1=right (+1), 3=left (-1)\n",
    "        coords[1] += 1 if action == 1 else -1 if action == 3 else 0\n",
    "\n",
    "        # Solve collisions.\n",
    "        # Make sure, we don't end up on the other agent's position.\n",
    "        # If yes, don't move (we are blocked).\n",
    "        if (is_agent1 and coords == self.agent2_pos) or (not is_agent1 and coords == self.agent1_pos):\n",
    "            coords[0], coords[1] = orig_coords\n",
    "            # Agent2 blocked agent1 (agent1 tried to run into agent2)\n",
    "            # OR Agent2 bumped into agent1 (agent2 tried to run into agent1)\n",
    "            return {\"collision\"}\n",
    "\n",
    "        # No agent blocking -> check walls.\n",
    "        if coords[0] < 0:\n",
    "            coords[0] = 0\n",
    "        elif coords[0] >= self.height:\n",
    "            coords[0] = self.height - 1\n",
    "        if coords[1] < 0:\n",
    "            coords[1] = 0\n",
    "        elif coords[1] >= self.width:\n",
    "            coords[1] = self.width - 1\n",
    "\n",
    "        # If agent1 -> \"new\" if new tile covered.\n",
    "        if is_agent1 and not tuple(coords) in self.agent1_visited_fields:\n",
    "            self.agent1_visited_fields.add(tuple(coords))\n",
    "            return {\"agent1_new_field\"}\n",
    "        # No new tile for agent1.\n",
    "        return set()\n",
    "\n",
    "    def render(self, mode=None):\n",
    "        print(\"_\" * (self.width + 2))\n",
    "        for r in range(self.height):\n",
    "            print(\"|\", end=\"\")\n",
    "            for c in range(self.width):\n",
    "                field = r * self.width + c % self.width\n",
    "                if self.agent1_pos == [r, c]:\n",
    "                    print(\"1\", end=\"\")\n",
    "                elif self.agent2_pos == [r, c]:\n",
    "                    print(\"2\", end=\"\")\n",
    "                elif (r, c) in self.agent1_visited_fields:\n",
    "                    print(\".\", end=\"\")\n",
    "                else:\n",
    "                    print(\" \", end=\"\")\n",
    "            print(\"|\")\n",
    "        print(\"‾\" * (self.width + 2))\n",
    "        print(f\"{'!!Collision!!' if self.collision else ''}\")\n",
    "        print(\"R1={: .1f}\".format(self.agent1_R))\n",
    "        print(\"R2={: .1f}\".format(self.agent2_R))\n",
    "        print()\n",
    "\n",
    "\n",
    "env = MultiAgentArena()\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# Agent1 will move down, Agent2 moves up.\n",
    "obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(\"Agent1's x/y position={}\".format(env.agent1_pos))\n",
    "print(\"Agent2's x/y position={}\".format(env.agent2_pos))\n",
    "print(\"Env timesteps={}\".format(env.timesteps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-sussex",
   "metadata": {},
   "source": [
    "## Exercise No 1\n",
    "\n",
    "<hr />\n",
    "\n",
    "<img src=\"images/exercise1.png\" width=400>\n",
    "\n",
    "In the cell above, we performed a `reset()` and a single `step()` call. To walk through an entire episode, one would normally call `step()` repeatedly (with different actions) until the returned `done` dict has the \"agent1\" or \"agent2\" (or \"__all__\") key set to True. Your task is to write an \"environment loop\" that runs for exactly one episode using our `MultiAgentArena` class.\n",
    "\n",
    "Follow these instructions here to get this done.\n",
    "\n",
    "1. Create an env object.\n",
    "1. `reset` your environment to get the first (initial) observation.\n",
    "1. Compute the actions for \"agent1\" and \"agent2\" calling `DummyTrainer.compute_action([obs])` twice and putting the results into an action dict to be passed into `step()`, just like it's done in the above cell (where we do a single `step()`).\n",
    "1. Repeat this, `step`ing through an entire episode.\n",
    "1. When an episode is done, `step()` will return a done dict with key `__all__` set to True.\n",
    "1. If you feel, this is way too easy for you ;) , try to extract each agent's reward, sum it up over the episode and - at the end of the episode - print out each agent's accumulated reward (also called the \"return\" of an episode).\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_agent1=3\n",
      "action_agent2=1\n",
      "\n",
      "action_agent1=0\n",
      "action_agent2=1\n",
      "\n",
      "action_agent1=0\n",
      "action_agent2=2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DummyTrainer:\n",
    "    \"\"\"Dummy Trainer class used in Exercise #1.\n",
    "\n",
    "    Use its `compute_action` method to get a new action for one of the agents,\n",
    "    given the agent's observation (a single discrete value encoding the field\n",
    "    the agent is currently in).\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_action(self, single_agent_obs=None):\n",
    "        # Returns a random action for a single agent.\n",
    "        return np.random.randint(4)  # Discrete(4) -> return rand int between 0 and 3 (incl. 3).\n",
    "\n",
    "dummy_trainer = DummyTrainer()\n",
    "# Check, whether it's working.\n",
    "for _ in range(3):\n",
    "    # Get action for agent1 (providing agent1's and agent2's positions).\n",
    "    print(\"action_agent1={}\".format(dummy_trainer.compute_action(np.array([0, 99]))))\n",
    "\n",
    "    # Get action for agent2 (providing agent2's and agent1's positions).\n",
    "    print(\"action_agent2={}\".format(dummy_trainer.compute_action(np.array([99, 0]))))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa8a1d-7d82-4b79-b2d4-3ce7ffa6fae1",
   "metadata": {},
   "source": [
    "Write your solution code into this cell here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b12373b-4b71-4ee9-a7a1-13077a59840b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2d4608bd894cb5ad4a233671f1a0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "\n",
    "# Leave the following as-is. It'll help us with rendering the env in this very cell's output.\n",
    "import time\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "with out:\n",
    "\n",
    "    # Solution to Exercise #1:\n",
    "    # Start coding here inside this `with`-block:\n",
    "    # 1) Reset the env.\n",
    "    obs = env.reset()  # start new episode\n",
    "\n",
    "    # 2) Enter an infinite while loop (to step through the episode).\n",
    "    while env.timesteps < 100:\n",
    "        # 3) Calculate both agents' actions individually, using dummy_trainer.compute_action([individual agent's obs])\n",
    "        a1 = dummy_trainer.compute_action(obs[\"agent1\"])\n",
    "        a2 = dummy_trainer.compute_action(obs[\"agent2\"])\n",
    "\n",
    "        # 4) Compile the actions dict from both individual actions.\n",
    "        actions = {\n",
    "            \"agent1\": a1, \"agent2\": a2,\n",
    "        }\n",
    "\n",
    "        # 5) Send the actions dict to the env's `step()` method to receive: obs, rewards, dones, info dicts\n",
    "        obs, rewards, dones, _ = env.step(actions)\n",
    "\n",
    "        # 6) We'll do this together: Render the env.\n",
    "        # Don't write any code here (skip directly to 7).\n",
    "        out.clear_output(wait=True)\n",
    "        time.sleep(0.05)\n",
    "        env.render()\n",
    "\n",
    "        # 7) Check, whether the episde is done, if yes, break out of the while loop.\n",
    "        if dones[\"agent1\"] is True:\n",
    "            break\n",
    "\n",
    "# 8) Run it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4196a5-7e7a-442a-8100-96bc7393c59d",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 15 min break :)\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
   "metadata": {},
   "source": [
    "### And now for something completely different:\n",
    "#### Plugging in RLlib!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd830b90-5762-4d22-8fa9-0abf0777a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 10:42:20,283\tINFO services.py:1272 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.179',\n",
       " 'raylet_ip_address': '192.168.0.179',\n",
       " 'redis_address': '192.168.0.179:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-06-24_10-42-18_600631_2196/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-06-24_10-42-18_600631_2196/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-06-24_10-42-18_600631_2196',\n",
       " 'metrics_export_port': 63988,\n",
       " 'node_id': 'fb379f8d24eba01da8c8027447994ca7c9754701cb243183c55e28d7'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import ray\n",
    "\n",
    "# Start a new instance of Ray (when running this tutorial locally) or\n",
    "# connect to an already running one (when running this tutorial through Anyscale).\n",
    "\n",
    "ray.init()  # Hear the engine humming? ;)\n",
    "\n",
    "# In case you encounter the following error during our tutorial: `RuntimeError: Maybe you called ray.init twice by accident?`\n",
    "# Try: `ray.shutdown() + ray.init()` or `ray.init(ignore_reinit_error=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
   "metadata": {},
   "source": [
    "### Picking an RLlib algorithm - We'll use PPO throughout this tutorial (one-size-fits-all-kind-of-algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_algos.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bcc1116-a14c-4479-87c0-6ece58ab0464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 10:42:23,897\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-06-24 10:42:23,898\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-06-24 10:42:33,238\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPO"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "# We use the PPO algorithm here b/c its very flexible wrt its supported\n",
    "# action spaces and model types and b/c it learns well almost any problem.\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "# Specify a very simple config, defining our environment and some environment\n",
    "# options (see environment.py).\n",
    "config = {\n",
    "    \"env\": MultiAgentArena,  # \"my_env\" <- if we previously have registered the env with `tune.register_env(\"[name]\", lambda config: [returns env object])`.\n",
    "    \"env_config\": {\n",
    "        \"config\": {\n",
    "            \"width\": 10,\n",
    "            \"height\": 10,\n",
    "            \"ts\": 100,\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # !PyTorch users!\n",
    "    #\"framework\": \"torch\",  # If users have chosen to install torch instead of tf.\n",
    "\n",
    "    \"create_env_on_driver\": True,\n",
    "}\n",
    "# Instantiate the Trainer object using above config.\n",
    "rllib_trainer = PPOTrainer(config=config)\n",
    "rllib_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ae150-c0a3-477f-8d78-d0a34f147958",
   "metadata": {},
   "source": [
    "### Ready to train with RLlib's PPO algorithm\n",
    "\n",
    "That's it, we are ready to train.\n",
    "Calling `Trainer.train()` will execute a single \"training iteration\".\n",
    "\n",
    "One iteration for most algos involves:\n",
    "\n",
    "1) sampling from the environment(s)\n",
    "2) using the sampled data (observations, actions taken, rewards) to update the policy model (neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f6c94d4-6871-4d20-81af-3d4081f05f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_timesteps_total': 4000,\n",
      " 'custom_metrics': {},\n",
      " 'date': '2021-06-24_10-42-36',\n",
      " 'done': False,\n",
      " 'episode_len_mean': 100.0,\n",
      " 'episode_media': {},\n",
      " 'episode_reward_max': 14.999999999999998,\n",
      " 'episode_reward_mean': -9.104999999999999,\n",
      " 'episode_reward_min': -34.50000000000005,\n",
      " 'episodes_this_iter': 20,\n",
      " 'episodes_total': 20,\n",
      " 'experiment_id': '74285469b30e44cdbdfb8f3ba86dfb6c',\n",
      " 'hist_stats': {'episode_lengths': [100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100],\n",
      "                'episode_reward': [-30.000000000000036,\n",
      "                                   -10.499999999999996,\n",
      "                                   -14.999999999999986,\n",
      "                                   -11.399999999999993,\n",
      "                                   -5.999999999999982,\n",
      "                                   4.500000000000023,\n",
      "                                   -5.999999999999986,\n",
      "                                   -13.499999999999991,\n",
      "                                   1.4999999999999978,\n",
      "                                   -16.499999999999993,\n",
      "                                   -6.899999999999999,\n",
      "                                   -15.00000000000002,\n",
      "                                   -15.000000000000009,\n",
      "                                   -20.39999999999999,\n",
      "                                   4.500000000000025,\n",
      "                                   14.999999999999998,\n",
      "                                   -34.50000000000005,\n",
      "                                   5.700000000000026,\n",
      "                                   -9.899999999999991,\n",
      "                                   -2.699999999999981]},\n",
      " 'hostname': 'Svens-MacBook-Pro.local',\n",
      " 'info': {'learner': {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224,\n",
      "                                                           'cur_lr': 4.999999873689376e-05,\n",
      "                                                           'entropy': 1.3686056,\n",
      "                                                           'entropy_coeff': 0.0,\n",
      "                                                           'kl': 0.018066175,\n",
      "                                                           'model': {},\n",
      "                                                           'policy_loss': -0.043953087,\n",
      "                                                           'total_loss': 20.284836,\n",
      "                                                           'vf_explained_var': 0.10961369,\n",
      "                                                           'vf_loss': 20.325178}}},\n",
      "          'num_agent_steps_sampled': 4000,\n",
      "          'num_agent_steps_trained': 4000,\n",
      "          'num_steps_sampled': 4000,\n",
      "          'num_steps_trained': 4000},\n",
      " 'iterations_since_restore': 1,\n",
      " 'node_ip': '192.168.0.179',\n",
      " 'num_healthy_workers': 2,\n",
      " 'off_policy_estimator': {},\n",
      " 'perf': {'cpu_util_percent': 28.82, 'ram_util_percent': 65.97999999999999},\n",
      " 'pid': 2196,\n",
      " 'policy_reward_max': {},\n",
      " 'policy_reward_mean': {},\n",
      " 'policy_reward_min': {},\n",
      " 'sampler_perf': {'mean_action_processing_ms': 0.05331537225744226,\n",
      "                  'mean_env_render_ms': 0.0,\n",
      "                  'mean_env_wait_ms': 0.028410515227875154,\n",
      "                  'mean_inference_ms': 0.5918417300854053,\n",
      "                  'mean_raw_obs_processing_ms': 0.13975651709588022},\n",
      " 'time_since_restore': 3.4800119400024414,\n",
      " 'time_this_iter_s': 3.4800119400024414,\n",
      " 'time_total_s': 3.4800119400024414,\n",
      " 'timers': {'learn_throughput': 1539.993,\n",
      "            'learn_time_ms': 2597.414,\n",
      "            'load_throughput': 109748.975,\n",
      "            'load_time_ms': 36.447,\n",
      "            'sample_throughput': 4764.843,\n",
      "            'sample_time_ms': 839.482,\n",
      "            'update_time_ms': 1.433},\n",
      " 'timestamp': 1624524156,\n",
      " 'timesteps_since_restore': 0,\n",
      " 'timesteps_total': 4000,\n",
      " 'training_iteration': 1}\n"
     ]
    }
   ],
   "source": [
    "results = rllib_trainer.train()\n",
    "\n",
    "# Delete the config from the results for clarity.\n",
    "# Only the stats will remain, then.\n",
    "del results[\"config\"]\n",
    "# Pretty print the stats.\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96f682-fc7d-46a0-b136-f5d62cd7ad67",
   "metadata": {},
   "source": [
    "### Going from single policy (RLlib's default) to multi-policy:\n",
    "\n",
    "So far, our experiment has been ill-configured, because both\n",
    "agents, which should behave differently due to their different\n",
    "tasks and reward functions, learn the same policy: the \"default_policy\",\n",
    "which RLlib always provides if you don't configure anything else.\n",
    "Remember that RLlib does not know at Trainer setup time, how many and which agents\n",
    "the environment will \"produce\". Agent control (adding agents, removing them, terminating\n",
    "episodes for agents) is entirely in the Env's hands.\n",
    "Let's fix our single policy problem and introduce the \"multiagent\" API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13900163-f520-40f1-87be-d759760bd3a5",
   "metadata": {},
   "source": [
    "<img src=\"images/from_single_agent_to_multi_agent.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a813b988-e40f-4890-8c9c-f5f7d0f49cc9",
   "metadata": {},
   "source": [
    "In order to turn on RLlib's multi-agent functionality, we need two things:\n",
    "\n",
    "1. A policy mapping function, mapping agent IDs (e.g. a string like \"agent1\", produced by the environment in the returned observation/rewards/dones-dicts) to a policy ID (another string, e.g. \"policy1\", which is under our control).\n",
    "1. A policies definition dict, mapping policy IDs (e.g. \"policy1\") to 4-tuples consisting of 1) policy class (None for using the default class), 2) observation space, 3) action space, and 4) config overrides (empty dict for no overrides and using the Trainer's main config dict).\n",
    "\n",
    "Let's take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dff7017-f1b9-41e8-94fd-266bbe56cf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'create_env_on_driver': True,\n",
      " 'env': <class '__main__.MultiAgentArena'>,\n",
      " 'env_config': {'config': {'height': 10, 'ts': 100, 'width': 10}},\n",
      " 'multiagent': {'policies': {'policy1': (None,\n",
      "                                         MultiDiscrete([100 100]),\n",
      "                                         Discrete(4),\n",
      "                                         {}),\n",
      "                             'policy2': (None,\n",
      "                                         MultiDiscrete([100 100]),\n",
      "                                         Discrete(4),\n",
      "                                         {'lr': 0.0002})},\n",
      "                'policy_mapping_fn': <function policy_mapping_fn at 0x7fbc3a8b2af0>}}\n",
      "\n",
      "agent1 is now mapped to policy1\n",
      "agent2 is now mapped to policy2\n"
     ]
    }
   ],
   "source": [
    "# Define the policies definition dict:\n",
    "# Each policy in there is defined by its ID (key) mapping to a 4-tuple (value):\n",
    "# - Policy class (None for using the \"default\" class, e.g. PPOTFPolicy for PPO+tf or PPOTorchPolicy for PPO+torch).\n",
    "# - obs-space (we get this directly from our already created env object).\n",
    "# - act-space (we get this directly from our already created env object).\n",
    "# - config-overrides dict (leave empty for using the Trainer's config as-is)\n",
    "policies = {\n",
    "    \"policy1\": (None, env.observation_space, env.action_space, {}),\n",
    "    \"policy2\": (None, env.observation_space, env.action_space, {\"lr\": 0.0002}),\n",
    "}\n",
    "# Note that now we won't have a \"default_policy\" anymore, just \"policy1\" and \"policy2\".\n",
    "\n",
    "# Define an agent->policy mapping function.\n",
    "# Which agents (defined by the environment) use which policies (defined by us)?\n",
    "# The mapping here is M (agents) -> N (policies), where M >= N.\n",
    "def policy_mapping_fn(agent_id: str):\n",
    "    # Make sure agent ID is valid.\n",
    "    assert agent_id in [\"agent1\", \"agent2\"], f\"ERROR: invalid agent ID {agent_id}!\"\n",
    "    # Map agent1 to policy1, and agent2 to policy2.\n",
    "    return \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
    "\n",
    "# We could - if we wanted - specify, which policies should be learnt (by default, RLlib learns all).\n",
    "# Non-learnt policies will be frozen and not updated:\n",
    "# policies_to_train = [\"policy1\", \"policy2\"]\n",
    "\n",
    "# Adding the above to our config.\n",
    "config.update({\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        # We'll leave this empty: Means, we train both policy1 and policy2.\n",
    "        # \"policies_to_train\": policies_to_train,\n",
    "    },\n",
    "})\n",
    "\n",
    "pprint.pprint(config)\n",
    "print()\n",
    "print(f\"agent1 is now mapped to {policy_mapping_fn('agent1')}\")\n",
    "print(f\"agent2 is now mapped to {policy_mapping_fn('agent2')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "646f8800-941b-43cb-a924-622af6788aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 10:42:49,739\tINFO trainable.py:101 -- Trainable.setup took 12.972 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-06-24 10:42:49,740\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPO"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate our Trainer (we cannot just change the config on-the-fly).\n",
    "rllib_trainer.stop()\n",
    "\n",
    "# Using our updated (now multiagent!) config dict.\n",
    "rllib_trainer = PPOTrainer(config=config)\n",
    "rllib_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {},
   "source": [
    "Now that we are setup correctly with two policies as per our \"multiagent\" config, let's call `train()` on the new Trainer several times (what about 10 times?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17ae724d-71cc-422b-96cb-3dc9faa2d111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=1: R(\"return\")=-8.587500000000002\n",
      "Iteration=2: R(\"return\")=-5.167499999999999\n",
      "Iteration=3: R(\"return\")=-4.460999999999993\n",
      "Iteration=4: R(\"return\")=-2.3789999999999885\n",
      "Iteration=5: R(\"return\")=-1.7069999999999876\n",
      "Iteration=6: R(\"return\")=-0.26699999999998636\n",
      "Iteration=7: R(\"return\")=1.4850000000000128\n",
      "Iteration=8: R(\"return\")=2.2110000000000114\n",
      "Iteration=9: R(\"return\")=3.3690000000000078\n",
      "Iteration=10: R(\"return\")=3.6810000000000063\n"
     ]
    }
   ],
   "source": [
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "# Move on once you see (agent1 + agent2) episode rewards of 10.0 or more.\n",
    "for _ in range(10):\n",
    "    results = rllib_trainer.train()\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R(\\\"return\\\")={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "365ef0d7-9977-4d9d-9fa5-ffaa7c111b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=11: R1=11.675 R2=-6.655999999999988\n",
      "Iteration=12: R1=13.175 R2=-6.325999999999986\n",
      "Iteration=13: R1=12.185 R2=-5.929999999999989\n",
      "Iteration=14: R1=12.515 R2=-5.995999999999988\n",
      "Iteration=15: R1=12.13 R2=-5.610999999999991\n",
      "Iteration=16: R1=13.305 R2=-5.3249999999999895\n",
      "Iteration=17: R1=14.36 R2=-5.2699999999999925\n",
      "Iteration=18: R1=15.555 R2=-5.78699999999999\n",
      "Iteration=19: R1=16.79 R2=-5.698999999999991\n",
      "Iteration=20: R1=17.795 R2=-5.66599999999999\n"
     ]
    }
   ],
   "source": [
    "# Do another loop, but this time, we will print out each policies' individual rewards.\n",
    "for _ in range(10):\n",
    "    results = rllib_trainer.train()\n",
    "    r1 = results['policy_reward_mean']['policy1']\n",
    "    r2 = results['policy_reward_mean']['policy2']\n",
    "    r = r1 + r2\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R(\\\"return\\\")={r} R1={r1} R2={r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80ad33-a55b-4e18-857b-b884eedda0a4",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK! (<-- we will not do these during the tutorial, but feel free to try these cells by yourself)\n",
    "\n",
    "Use the above solution of Exercise #1 and replace our `dummy_trainer` in that solution\n",
    "with the now trained `rllib_trainer`. You should see a better performance of the two agents.\n",
    "\n",
    "However, keep in mind that we are mostly training agent1 as we only trian a single policy and agent1\n",
    "is the \"easier\" one to collect high rewards with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK!\n",
    "\n",
    "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using Policies and their model(s) inside our Trainer object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aff679e8-74b4-4603-9d5c-4cc0c6ebe45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our (only!) Policy right now is: <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fbc409bd160>\n",
      "Our Policy's observation space is: Box(-1.0, 1.0, (200,), float32)\n",
      "Our Policy's action space is: Discrete(4)\n",
      "sampled action=3\n"
     ]
    }
   ],
   "source": [
    "# Let's actually \"look inside\" our Trainer to see what's in there.\n",
    "from ray.rllib.utils.numpy import softmax\n",
    "\n",
    "# To get to one of the policies inside the Trainer, use `Trainer.get_policy([policy ID])`:\n",
    "policy = rllib_trainer.get_policy(\"policy1\")\n",
    "print(f\"Our (only!) Policy right now is: {policy}\")\n",
    "\n",
    "# To get to the model inside any policy, do:\n",
    "model = policy.model\n",
    "#print(f\"Our Policy's model is: {model}\")\n",
    "\n",
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\")\n",
    "\n",
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = np.array([policy.observation_space.sample()])\n",
    "# Alternatively for PyTorch:\n",
    "#import torch\n",
    "#obs = torch.from_numpy(obs)\n",
    "\n",
    "# Get the action logits (as tf tensor).\n",
    "# If you are using torch, you would get a torch tensor here.\n",
    "logits, _ = model({\"obs\": obs})\n",
    "logits\n",
    "\n",
    "# Numpyize the tensor by running `logits` through the Policy's own tf.Session.\n",
    "logits_np = policy.get_session().run(logits)\n",
    "# For torch, you can simply do: `logits_np = logits.detach().cpu().numpy()`.\n",
    "\n",
    "# Convert logits into action probabilities and remove the B=1.\n",
    "action_probs = np.squeeze(softmax(logits_np))\n",
    "\n",
    "# Sample an action, using the probabilities.\n",
    "action = np.random.choice([0, 1, 2, 3], p=action_probs)\n",
    "\n",
    "# Print out the action.\n",
    "print(f\"sampled action={action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd66c3-f07a-4795-84ea-6b232ba6a047",
   "metadata": {},
   "source": [
    "### Saving and restoring a trained Trainer.\n",
    "Currently, `rllib_trainer` is in an already trained state.\n",
    "It holds optimized weights in its Policy's model that allow it to act\n",
    "already somewhat smart in our environment when given an observation.\n",
    "\n",
    "However, if we closed this notebook right now, all the effort would have been for nothing.\n",
    "Let's therefore save the state of our trainer to disk for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57eae1e4-3cc4-4282-9a83-bc374bdad978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer (at iteration 20 was saved in '/Users/sven/ray_results/PPO_MultiAgentArena_2021-06-24_10-42-36tjwkrob9/checkpoint_000020/checkpoint-20'!\n",
      "The checkpoint directory contains the following files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['checkpoint-20', 'checkpoint-20.tune_metadata', '.is_checkpoint']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the `Trainer.save()` method to create a checkpoint.\n",
    "checkpoint_file = rllib_trainer.save()\n",
    "print(f\"Trainer (at iteration {rllib_trainer.iteration} was saved in '{checkpoint_file}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "import os\n",
    "os.listdir(os.path.dirname(checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e0ab-2c10-469a-97b1-4aadf1a1ec97",
   "metadata": {},
   "source": [
    "### Restoring and evaluating a Trainer\n",
    "In the following cell, we'll learn how to restore a saved Trainer from a checkpoint file.\n",
    "\n",
    "We'll also evaluate a completely new Trainer (should act more or less randomly) vs an already trained one (the one we just restored from the created checkpoint file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74ceedb9-c225-46f2-ad1d-f902c81d3256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 10:45:16,711\tINFO trainable.py:101 -- Trainable.setup took 13.248 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-06-24 10:45:16,713\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating new trainer: R=-7.379999999999993\n",
      "Before restoring: Trainer is at iteration=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 10:45:19,816\tINFO trainable.py:377 -- Restored on 192.168.0.179 from checkpoint: /Users/sven/ray_results/PPO_MultiAgentArena_2021-06-24_10-42-36tjwkrob9/checkpoint_000020/checkpoint-20\n",
      "2021-06-24 10:45:19,817\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': None, '_time_total': 132.94485211372375, '_episodes_total': 800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After restoring: Trainer is at iteration=20\n",
      "Evaluating restored trainer: R=15.284999999999972\n"
     ]
    }
   ],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_trainer = PPOTrainer(config=config)\n",
    "# Evaluate the new trainer (this should yield random results).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "print(f\"Before restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "new_trainer.restore(checkpoint_file)\n",
    "print(f\"After restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "\n",
    "# Evaluate again (this should yield results we saw after having trained our saved agent).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating restored trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de603d14-f0cb-4363-a72b-8f147c094071",
   "metadata": {},
   "source": [
    "In order to release all resources from a Trainer, you can use a Trainer's `stop()` method.\n",
    "You should definitley run this cell as it frees resources that we'll need later in this tutorial, when we'll do parallel hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "737dca4f-942f-4fda-abcc-0052263a103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rllib_trainer.stop()\n",
    "new_trainer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c1e4c-cb02-4719-ac5a-0106172a6c6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Moving stuff to the professional level: RLlib in connection w/ Ray Tune\n",
    "\n",
    "Running any experiments through Ray Tune is the recommended way of doing things with RLlib. If you look at our\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/rllib/examples\">examples scripts folder</a>, you will see that almost all of the scripts use Ray Tune to run the particular RLlib workload demonstrated in each script.\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdacebb-d27f-4174-9002-35c5657f146c",
   "metadata": {
    "tags": []
   },
   "source": [
    "When setting up hyperparameter sweeps for Tune, we'll do this in our already familiar config dict.\n",
    "\n",
    "So let's take a quick look at our PPO algo's default config to understand, which hyperparameters we may want to play around with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1b32582-52bd-4585-9009-2f877a0723a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO's default config is:\n",
      "{'_fake_gpus': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': True,\n",
      " 'clip_param': 0.3,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': 180,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'eager_tracing': False,\n",
      " 'entropy_coeff': 0.0,\n",
      " 'entropy_coeff_schedule': None,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'env_task_fn': None,\n",
      " 'evaluation_config': {},\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': 10,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'evaluation_parallel_to_training': False,\n",
      " 'exploration_config': {'type': 'StochasticSampling'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'tf',\n",
      " 'gamma': 0.99,\n",
      " 'grad_clip': None,\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'kl_coeff': 0.2,\n",
      " 'kl_target': 0.01,\n",
      " 'lambda': 1.0,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'lr': 5e-05,\n",
      " 'lr_schedule': None,\n",
      " 'metrics_smoothing_episodes': 100,\n",
      " 'min_iter_time_s': 0,\n",
      " 'model': {'_time_major': False,\n",
      "           '_use_default_native_models': False,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'num_framestacks': 'auto',\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': False,\n",
      "           'zero_mean': True},\n",
      " 'monitor': -1,\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_mapping_fn': None,\n",
      "                'replay_mode': 'independent'},\n",
      " 'no_done_at_end': False,\n",
      " 'normalize_actions': False,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_sgd_iter': 30,\n",
      " 'num_workers': 2,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'record_env': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'rollout_fragment_length': 200,\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'seed': None,\n",
      " 'sgd_minibatch_size': 128,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'shuffle_sequences': True,\n",
      " 'simple_optimizer': -1,\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 0,\n",
      " 'train_batch_size': 4000,\n",
      " 'use_critic': True,\n",
      " 'use_gae': True,\n",
      " 'vf_clip_param': 10.0,\n",
      " 'vf_loss_coeff': 1.0,\n",
      " 'vf_share_layers': -1}\n"
     ]
    }
   ],
   "source": [
    "# Configuration dicts and Ray Tune.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# PPO algorithm:\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as PPO_DEFAULT_CONFIG\n",
    "print(f\"PPO's default config is:\")\n",
    "pprint.pprint(PPO_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint.pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint.pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded886cc-436e-46cd-8fea-d68af8b41236",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's do a very simple grid-search over two learning rates with tune.run().\n",
    "\n",
    "In particular, we will try the learning rates 0.00005 and 0.5 using `tune.grid_search([...])`\n",
    "inside our config dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5063991e-173b-49be-a4e7-467e2e18321a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2675)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=2675)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=2675)\u001b[0m 2021-06-24 10:45:29,349\tWARNING ppo.py:135 -- `train_batch_size` (3000) cannot be achieved with your other settings (num_workers=2 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1500.\n",
      "\u001b[2m\u001b[36m(pid=2677)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=2677)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=2670)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=2670)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=2670)\u001b[0m 2021-06-24 10:45:29,349\tWARNING ppo.py:135 -- `train_batch_size` (3000) cannot be achieved with your other settings (num_workers=2 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1500.\n",
      "\u001b[2m\u001b[36m(pid=2674)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=2674)\u001b[0m 2021-06-24 10:45:29,348\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=2675)\u001b[0m 2021-06-24 10:45:44,763\tINFO trainable.py:101 -- Trainable.setup took 15.416 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=2675)\u001b[0m 2021-06-24 10:45:44,764\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=2677)\u001b[0m 2021-06-24 10:45:44,776\tINFO trainable.py:101 -- Trainable.setup took 15.429 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=2677)\u001b[0m 2021-06-24 10:45:44,776\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=2670)\u001b[0m 2021-06-24 10:45:44,779\tINFO trainable.py:101 -- Trainable.setup took 15.432 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=2670)\u001b[0m 2021-06-24 10:45:44,780\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=2674)\u001b[0m 2021-06-24 10:45:44,765\tINFO trainable.py:101 -- Trainable.setup took 15.418 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=2674)\u001b[0m 2021-06-24 10:45:44,766\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_83919_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-45-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.000000000000014\n",
      "  episode_reward_mean: -8.909999999999993\n",
      "  episode_reward_min: -34.500000000000036\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 30\n",
      "  experiment_id: 43c5f1bf54a446d6ac106c951f38d022\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3664278984069824\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02037949115037918\n",
      "          model: {}\n",
      "          policy_loss: -0.05178745463490486\n",
      "          total_loss: 53.435001373291016\n",
      "          vf_explained_var: 0.13389408588409424\n",
      "          vf_loss: 53.48271560668945\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3454874753952026\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04217240586876869\n",
      "          model: {}\n",
      "          policy_loss: -0.08426131308078766\n",
      "          total_loss: 1.8325474262237549\n",
      "          vf_explained_var: 0.41607698798179626\n",
      "          vf_loss: 1.9083740711212158\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.22\n",
      "    ram_util_percent: 68.74\n",
      "  pid: 2675\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -7.799999999999981\n",
      "  policy_reward_mean:\n",
      "    policy1: 0.8333333333333334\n",
      "    policy2: -9.743333333333316\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11689038692832711\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05074622708904511\n",
      "    mean_inference_ms: 1.8129266952372325\n",
      "    mean_raw_obs_processing_ms: 0.2514038937318969\n",
      "  time_since_restore: 10.05943489074707\n",
      "  time_this_iter_s: 10.05943489074707\n",
      "  time_total_s: 10.05943489074707\n",
      "  timers:\n",
      "    learn_throughput: 469.142\n",
      "    learn_time_ms: 6394.648\n",
      "    load_throughput: 23594.744\n",
      "    load_time_ms: 127.147\n",
      "    sample_throughput: 882.612\n",
      "    sample_time_ms: 3399.003\n",
      "    update_time_ms: 5.25\n",
      "  timestamp: 1624524354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: '83919_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>RUNNING </td><td>192.168.0.179:2675</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.0594</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">   -8.91</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_83919_00001:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-45-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.000000000000016\n",
      "  episode_reward_mean: -5.909999999999995\n",
      "  episode_reward_min: -34.50000000000003\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 30\n",
      "  experiment_id: 38bdf32c865b44ec9dfdfa051cfd3bca\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.078101746737957\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 18.170473098754883\n",
      "          model: {}\n",
      "          policy_loss: 0.47826075553894043\n",
      "          total_loss: 56.070186614990234\n",
      "          vf_explained_var: 0.009776771068572998\n",
      "          vf_loss: 51.957828521728516\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3435875177383423\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.044070493429899216\n",
      "          model: {}\n",
      "          policy_loss: -0.09309946000576019\n",
      "          total_loss: 1.7173503637313843\n",
      "          vf_explained_var: 0.4769733250141144\n",
      "          vf_loss: 1.8016357421875\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.26666666666666\n",
      "    ram_util_percent: 68.74666666666666\n",
      "  pid: 2670\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -4.499999999999982\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.1\n",
      "    policy2: -9.00999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11567764485541542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05062598534062415\n",
      "    mean_inference_ms: 1.8227679343480891\n",
      "    mean_raw_obs_processing_ms: 0.24832319530306624\n",
      "  time_since_restore: 10.054849863052368\n",
      "  time_this_iter_s: 10.054849863052368\n",
      "  time_total_s: 10.054849863052368\n",
      "  timers:\n",
      "    learn_throughput: 468.629\n",
      "    learn_time_ms: 6401.65\n",
      "    load_throughput: 24602.476\n",
      "    load_time_ms: 121.939\n",
      "    sample_throughput: 882.943\n",
      "    sample_time_ms: 3397.727\n",
      "    update_time_ms: 6.46\n",
      "  timestamp: 1624524354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: '83919_00001'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00002:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-45-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.700000000000021\n",
      "  episode_reward_mean: -11.864999999999995\n",
      "  episode_reward_min: -33.000000000000036\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: c49aef7b1b5542e194a1608b246ea2a9\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3629132509231567\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02385992743074894\n",
      "          model: {}\n",
      "          policy_loss: -0.04925725609064102\n",
      "          total_loss: 30.79827308654785\n",
      "          vf_explained_var: 0.08894110471010208\n",
      "          vf_loss: 30.842761993408203\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3493726253509521\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03826555982232094\n",
      "          model: {}\n",
      "          policy_loss: -0.07087726891040802\n",
      "          total_loss: 2.229379177093506\n",
      "          vf_explained_var: 0.3409762978553772\n",
      "          vf_loss: 2.2926034927368164\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.05263157894737\n",
      "    ram_util_percent: 68.77894736842106\n",
      "  pid: 2674\n",
      "  policy_reward_max:\n",
      "    policy1: 13.5\n",
      "    policy2: -4.499999999999985\n",
      "  policy_reward_mean:\n",
      "    policy1: -2.6625\n",
      "    policy2: -9.202499999999981\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1123193381489187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04853444478322363\n",
      "    mean_inference_ms: 1.760626184767571\n",
      "    mean_raw_obs_processing_ms: 0.23792303543815246\n",
      "  time_since_restore: 13.129771947860718\n",
      "  time_this_iter_s: 13.129771947860718\n",
      "  time_total_s: 13.129771947860718\n",
      "  timers:\n",
      "    learn_throughput: 475.88\n",
      "    learn_time_ms: 8405.473\n",
      "    load_throughput: 27262.782\n",
      "    load_time_ms: 146.72\n",
      "    sample_throughput: 899.984\n",
      "    sample_time_ms: 4444.524\n",
      "    update_time_ms: 5.544\n",
      "  timestamp: 1624524357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '83919_00002'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00003:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-45-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.000000000000021\n",
      "  episode_reward_mean: -9.817499999999999\n",
      "  episode_reward_min: -34.50000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 777bea0709a34c368f94f4b08cbe8c9b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.10365462303161621\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 15.364961624145508\n",
      "          model: {}\n",
      "          policy_loss: 0.45147833228111267\n",
      "          total_loss: 57.60213851928711\n",
      "          vf_explained_var: -0.010712865740060806\n",
      "          vf_loss: 54.07766342163086\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3498151302337646\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03807845339179039\n",
      "          model: {}\n",
      "          policy_loss: -0.06896308064460754\n",
      "          total_loss: 1.8189435005187988\n",
      "          vf_explained_var: 0.4419904053211212\n",
      "          vf_loss: 1.8802908658981323\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.64210526315791\n",
      "    ram_util_percent: 68.77894736842106\n",
      "  pid: 2677\n",
      "  policy_reward_max:\n",
      "    policy1: 22.0\n",
      "    policy2: -3.3999999999999906\n",
      "  policy_reward_mean:\n",
      "    policy1: -0.5875\n",
      "    policy2: -9.22999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11233381483925395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04932631378707619\n",
      "    mean_inference_ms: 1.789251248399238\n",
      "    mean_raw_obs_processing_ms: 0.2394804413589104\n",
      "  time_since_restore: 13.130576133728027\n",
      "  time_this_iter_s: 13.130576133728027\n",
      "  time_total_s: 13.130576133728027\n",
      "  timers:\n",
      "    learn_throughput: 479.719\n",
      "    learn_time_ms: 8338.208\n",
      "    load_throughput: 25797.092\n",
      "    load_time_ms: 155.056\n",
      "    sample_throughput: 886.902\n",
      "    sample_time_ms: 4510.082\n",
      "    update_time_ms: 4.399\n",
      "  timestamp: 1624524357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '83919_00003'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.000000000000016\n",
      "  episode_reward_mean: -25.160000000000025\n",
      "  episode_reward_min: -48.00000000000008\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 60\n",
      "  experiment_id: 38bdf32c865b44ec9dfdfa051cfd3bca\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.04965648800134659\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.443717122077942\n",
      "          model: {}\n",
      "          policy_loss: 0.056447744369506836\n",
      "          total_loss: 169.8813018798828\n",
      "          vf_explained_var: -0.16828449070453644\n",
      "          vf_loss: 169.3917694091797\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3145511150360107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027634384110569954\n",
      "          model: {}\n",
      "          policy_loss: -0.05799044668674469\n",
      "          total_loss: 1.9219611883163452\n",
      "          vf_explained_var: 0.47436726093292236\n",
      "          vf_loss: 1.97166109085083\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.10714285714288\n",
      "    ram_util_percent: 69.05714285714288\n",
      "  pid: 2670\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -4.499999999999982\n",
      "  policy_reward_mean:\n",
      "    policy1: -16.058333333333334\n",
      "    policy2: -9.101666666666649\n",
      "  policy_reward_min:\n",
      "    policy1: -38.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12051659211884705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0528642218110237\n",
      "    mean_inference_ms: 1.8961515698803892\n",
      "    mean_raw_obs_processing_ms: 0.25930044203067726\n",
      "  time_since_restore: 19.843579053878784\n",
      "  time_this_iter_s: 9.788729190826416\n",
      "  time_total_s: 19.843579053878784\n",
      "  timers:\n",
      "    learn_throughput: 490.272\n",
      "    learn_time_ms: 6119.057\n",
      "    load_throughput: 47593.319\n",
      "    load_time_ms: 63.034\n",
      "    sample_throughput: 818.742\n",
      "    sample_time_ms: 3664.158\n",
      "    update_time_ms: 7.123\n",
      "  timestamp: 1624524364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: '83919_00001'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>RUNNING </td><td>192.168.0.179:2675</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.0594</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\"> -8.91  </td><td style=\"text-align: right;\">                15  </td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>RUNNING </td><td>192.168.0.179:2670</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         19.8436</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">-25.16  </td><td style=\"text-align: right;\">                15  </td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>RUNNING </td><td>192.168.0.179:2674</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.1298</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-11.865 </td><td style=\"text-align: right;\">                 5.7</td><td style=\"text-align: right;\">               -33  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>RUNNING </td><td>192.168.0.179:2677</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.1306</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -9.8175</td><td style=\"text-align: right;\">                12  </td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_83919_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.000000000000014\n",
      "  episode_reward_mean: -5.68499999999999\n",
      "  episode_reward_min: -34.500000000000036\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 60\n",
      "  experiment_id: 43c5f1bf54a446d6ac106c951f38d022\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3404359817504883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019646519795060158\n",
      "          model: {}\n",
      "          policy_loss: -0.05948462709784508\n",
      "          total_loss: 29.73061752319336\n",
      "          vf_explained_var: 0.20065376162528992\n",
      "          vf_loss: 29.78420639038086\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3029606342315674\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03657953068614006\n",
      "          model: {}\n",
      "          policy_loss: -0.08567504584789276\n",
      "          total_loss: 2.230103015899658\n",
      "          vf_explained_var: 0.38956695795059204\n",
      "          vf_loss: 2.3048043251037598\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.12142857142857\n",
      "    ram_util_percent: 69.05714285714288\n",
      "  pid: 2675\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -2.3000000000000016\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.6\n",
      "    policy2: -9.28499999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12243444409471098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05305036450166201\n",
      "    mean_inference_ms: 1.8900885114154373\n",
      "    mean_raw_obs_processing_ms: 0.26302880406828194\n",
      "  time_since_restore: 20.00106167793274\n",
      "  time_this_iter_s: 9.941626787185669\n",
      "  time_total_s: 20.00106167793274\n",
      "  timers:\n",
      "    learn_throughput: 486.496\n",
      "    learn_time_ms: 6166.543\n",
      "    load_throughput: 45949.869\n",
      "    load_time_ms: 65.289\n",
      "    sample_throughput: 812.428\n",
      "    sample_time_ms: 3692.633\n",
      "    update_time_ms: 4.975\n",
      "  timestamp: 1624524364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: '83919_00000'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00002:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.999999999999943\n",
      "  episode_reward_mean: -5.5387499999999905\n",
      "  episode_reward_min: -33.000000000000036\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: c49aef7b1b5542e194a1608b246ea2a9\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3350809812545776\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018097825348377228\n",
      "          model: {}\n",
      "          policy_loss: -0.047358158975839615\n",
      "          total_loss: 30.488140106201172\n",
      "          vf_explained_var: 0.20678654313087463\n",
      "          vf_loss: 30.53006935119629\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3102316856384277\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03630093112587929\n",
      "          model: {}\n",
      "          policy_loss: -0.07370398193597794\n",
      "          total_loss: 3.1660776138305664\n",
      "          vf_explained_var: 0.3064298629760742\n",
      "          vf_loss: 3.228891372680664\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.01578947368421\n",
      "    ram_util_percent: 69.15789473684212\n",
      "  pid: 2674\n",
      "  policy_reward_max:\n",
      "    policy1: 31.0\n",
      "    policy2: -1.1999999999999866\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.03125\n",
      "    policy2: -8.569999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11715029688576059\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0504215349344249\n",
      "    mean_inference_ms: 1.8516685061844107\n",
      "    mean_raw_obs_processing_ms: 0.2470169052622661\n",
      "  time_since_restore: 26.6622576713562\n",
      "  time_this_iter_s: 13.532485723495483\n",
      "  time_total_s: 26.6622576713562\n",
      "  timers:\n",
      "    learn_throughput: 480.501\n",
      "    learn_time_ms: 8324.648\n",
      "    load_throughput: 52482.02\n",
      "    load_time_ms: 76.217\n",
      "    sample_throughput: 823.96\n",
      "    sample_time_ms: 4854.602\n",
      "    update_time_ms: 6.82\n",
      "  timestamp: 1624524371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: '83919_00002'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>RUNNING </td><td>192.168.0.179:2675</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         20.0011</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\"> -5.685  </td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>RUNNING </td><td>192.168.0.179:2670</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         19.8436</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">-25.16   </td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>RUNNING </td><td>192.168.0.179:2674</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         26.6623</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> -5.53875</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">               -33  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>RUNNING </td><td>192.168.0.179:2677</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.1306</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -9.8175 </td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_83919_00003:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.000000000000021\n",
      "  episode_reward_mean: -27.71625000000003\n",
      "  episode_reward_min: -46.500000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 777bea0709a34c368f94f4b08cbe8c9b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.011390337720513344\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.7135323286056519\n",
      "          model: {}\n",
      "          policy_loss: 0.028235359117388725\n",
      "          total_loss: 87.91030883789062\n",
      "          vf_explained_var: 0.04217272251844406\n",
      "          vf_loss: 87.36800384521484\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3063417673110962\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028647078201174736\n",
      "          model: {}\n",
      "          policy_loss: -0.048121437430381775\n",
      "          total_loss: 2.3983969688415527\n",
      "          vf_explained_var: 0.438218891620636\n",
      "          vf_loss: 2.4379241466522217\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.05\n",
      "    ram_util_percent: 69.16000000000001\n",
      "  pid: 2677\n",
      "  policy_reward_max:\n",
      "    policy1: 22.0\n",
      "    policy2: -1.2\n",
      "  policy_reward_mean:\n",
      "    policy1: -18.9125\n",
      "    policy2: -8.803749999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -40.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11705511535269011\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.051375631702234616\n",
      "    mean_inference_ms: 1.8818053188675505\n",
      "    mean_raw_obs_processing_ms: 0.25049687105924917\n",
      "  time_since_restore: 26.892842054367065\n",
      "  time_this_iter_s: 13.762265920639038\n",
      "  time_total_s: 26.892842054367065\n",
      "  timers:\n",
      "    learn_throughput: 480.15\n",
      "    learn_time_ms: 8330.735\n",
      "    load_throughput: 49495.131\n",
      "    load_time_ms: 80.816\n",
      "    sample_throughput: 805.941\n",
      "    sample_time_ms: 4963.141\n",
      "    update_time_ms: 5.102\n",
      "  timestamp: 1624524371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: '83919_00003'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999945\n",
      "  episode_reward_mean: -3.733333333333324\n",
      "  episode_reward_min: -34.500000000000036\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 90\n",
      "  experiment_id: 43c5f1bf54a446d6ac106c951f38d022\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3026436567306519\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02130306325852871\n",
      "          model: {}\n",
      "          policy_loss: -0.06467930227518082\n",
      "          total_loss: 39.7680778503418\n",
      "          vf_explained_var: 0.3046334385871887\n",
      "          vf_loss: 39.826358795166016\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2618191242218018\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03267037868499756\n",
      "          model: {}\n",
      "          policy_loss: -0.0773257315158844\n",
      "          total_loss: 2.8779094219207764\n",
      "          vf_explained_var: 0.39631444215774536\n",
      "          vf_loss: 2.940533399581909\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.64375\n",
      "    ram_util_percent: 69.2\n",
      "  pid: 2675\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: 2.100000000000002\n",
      "  policy_reward_mean:\n",
      "    policy1: 4.983333333333333\n",
      "    policy2: -8.71666666666665\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12719069821097068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.054957949531456586\n",
      "    mean_inference_ms: 1.962962074043572\n",
      "    mean_raw_obs_processing_ms: 0.27342046464026004\n",
      "  time_since_restore: 31.078620672225952\n",
      "  time_this_iter_s: 11.077558994293213\n",
      "  time_total_s: 31.078620672225952\n",
      "  timers:\n",
      "    learn_throughput: 475.052\n",
      "    learn_time_ms: 6315.098\n",
      "    load_throughput: 66745.767\n",
      "    load_time_ms: 44.947\n",
      "    sample_throughput: 760.883\n",
      "    sample_time_ms: 3942.79\n",
      "    update_time_ms: 5.264\n",
      "  timestamp: 1624524375\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 3\n",
      "  trial_id: '83919_00000'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00001:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.000000000000016\n",
      "  episode_reward_mean: -31.26000000000004\n",
      "  episode_reward_min: -48.00000000000008\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 90\n",
      "  experiment_id: 38bdf32c865b44ec9dfdfa051cfd3bca\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0488428920507431\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.7631802558898926\n",
      "          model: {}\n",
      "          policy_loss: 0.07700446993112564\n",
      "          total_loss: 56.744136810302734\n",
      "          vf_explained_var: 0.06207181140780449\n",
      "          vf_loss: 55.4237060546875\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.273618221282959\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027872901409864426\n",
      "          model: {}\n",
      "          policy_loss: -0.06016816198825836\n",
      "          total_loss: 2.4626219272613525\n",
      "          vf_explained_var: 0.5540760159492493\n",
      "          vf_loss: 2.5102474689483643\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.0375\n",
      "    ram_util_percent: 69.2\n",
      "  pid: 2670\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: 4.299999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: -22.738888888888887\n",
      "    policy2: -8.521111111111093\n",
      "  policy_reward_min:\n",
      "    policy1: -43.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12567760495290625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0549953162051934\n",
      "    mean_inference_ms: 1.9768780640657528\n",
      "    mean_raw_obs_processing_ms: 0.27164816813410947\n",
      "  time_since_restore: 31.025849103927612\n",
      "  time_this_iter_s: 11.182270050048828\n",
      "  time_total_s: 31.025849103927612\n",
      "  timers:\n",
      "    learn_throughput: 479.399\n",
      "    learn_time_ms: 6257.838\n",
      "    load_throughput: 68822.514\n",
      "    load_time_ms: 43.59\n",
      "    sample_throughput: 752.804\n",
      "    sample_time_ms: 3985.099\n",
      "    update_time_ms: 6.56\n",
      "  timestamp: 1624524376\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 3\n",
      "  trial_id: '83919_00001'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00002:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.699999999999918\n",
      "  episode_reward_mean: -2.08799999999999\n",
      "  episode_reward_min: -33.000000000000036\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: c49aef7b1b5542e194a1608b246ea2a9\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3165600299835205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017024165019392967\n",
      "          model: {}\n",
      "          policy_loss: -0.04819672927260399\n",
      "          total_loss: 27.994075775146484\n",
      "          vf_explained_var: 0.43264496326446533\n",
      "          vf_loss: 28.037168502807617\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2749582529067993\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026819780468940735\n",
      "          model: {}\n",
      "          policy_loss: -0.061643145978450775\n",
      "          total_loss: 2.8891725540161133\n",
      "          vf_explained_var: 0.27856311202049255\n",
      "          vf_loss: 2.938746452331543\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.02916666666667\n",
      "    ram_util_percent: 69.47083333333333\n",
      "  pid: 2674\n",
      "  policy_reward_max:\n",
      "    policy1: 34.5\n",
      "    policy2: -0.0999999999999881\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.69\n",
      "    policy2: -7.777999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12735953995346633\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05454683190509087\n",
      "    mean_inference_ms: 2.0021295381334543\n",
      "    mean_raw_obs_processing_ms: 0.2671495726129366\n",
      "  time_since_restore: 43.40005970001221\n",
      "  time_this_iter_s: 16.737802028656006\n",
      "  time_total_s: 43.40005970001221\n",
      "  timers:\n",
      "    learn_throughput: 448.993\n",
      "    learn_time_ms: 8908.826\n",
      "    load_throughput: 76056.679\n",
      "    load_time_ms: 52.592\n",
      "    sample_throughput: 734.11\n",
      "    sample_time_ms: 5448.772\n",
      "    update_time_ms: 7.195\n",
      "  timestamp: 1624524388\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: '83919_00002'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>RUNNING </td><td>192.168.0.179:2675</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         31.0786</td><td style=\"text-align: right;\"> 9000</td><td style=\"text-align: right;\"> -3.73333</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>RUNNING </td><td>192.168.0.179:2670</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         31.0258</td><td style=\"text-align: right;\"> 9000</td><td style=\"text-align: right;\">-31.26   </td><td style=\"text-align: right;\">                15  </td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>RUNNING </td><td>192.168.0.179:2674</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         43.4001</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -2.088  </td><td style=\"text-align: right;\">                26.7</td><td style=\"text-align: right;\">               -33  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>RUNNING </td><td>192.168.0.179:2677</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         26.8928</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">-27.7163 </td><td style=\"text-align: right;\">                12  </td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_83919_00003:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.699999999999969\n",
      "  episode_reward_mean: -37.944000000000045\n",
      "  episode_reward_min: -46.500000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 777bea0709a34c368f94f4b08cbe8c9b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.331962483699158e-09\n",
      "          model: {}\n",
      "          policy_loss: -0.0007407463272102177\n",
      "          total_loss: 84.66165924072266\n",
      "          vf_explained_var: 0.10813942551612854\n",
      "          vf_loss: 84.66240692138672\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2471791505813599\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02472626231610775\n",
      "          model: {}\n",
      "          policy_loss: -0.0469743013381958\n",
      "          total_loss: 6.413702487945557\n",
      "          vf_explained_var: 0.30558180809020996\n",
      "          vf_loss: 6.449549674987793\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.6\n",
      "    ram_util_percent: 69.47826086956523\n",
      "  pid: 2677\n",
      "  policy_reward_max:\n",
      "    policy1: 19.5\n",
      "    policy2: 8.699999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: -31.255\n",
      "    policy2: -6.688999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -45.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12622916369021578\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0548617613642206\n",
      "    mean_inference_ms: 2.018634722977499\n",
      "    mean_raw_obs_processing_ms: 0.2694390828811978\n",
      "  time_since_restore: 43.391782999038696\n",
      "  time_this_iter_s: 16.49894094467163\n",
      "  time_total_s: 43.391782999038696\n",
      "  timers:\n",
      "    learn_throughput: 449.228\n",
      "    learn_time_ms: 8904.17\n",
      "    load_throughput: 70716.738\n",
      "    load_time_ms: 56.564\n",
      "    sample_throughput: 733.875\n",
      "    sample_time_ms: 5450.517\n",
      "    update_time_ms: 5.206\n",
      "  timestamp: 1624524388\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: '83919_00003'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999945\n",
      "  episode_reward_mean: -0.7859999999999892\n",
      "  episode_reward_min: -24.00000000000005\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 120\n",
      "  experiment_id: 43c5f1bf54a446d6ac106c951f38d022\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.280354380607605\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019933432340621948\n",
      "          model: {}\n",
      "          policy_loss: -0.06500809639692307\n",
      "          total_loss: 26.117982864379883\n",
      "          vf_explained_var: 0.4396723806858063\n",
      "          vf_loss: 26.174026489257812\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2293742895126343\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02492038533091545\n",
      "          model: {}\n",
      "          policy_loss: -0.06643515825271606\n",
      "          total_loss: 3.5759403705596924\n",
      "          vf_explained_var: 0.3846901059150696\n",
      "          vf_loss: 3.625553846359253\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.44117647058823\n",
      "    ram_util_percent: 69.58823529411765\n",
      "  pid: 2675\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: 9.800000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.135\n",
      "    policy2: -7.920999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -17.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1329172032885749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.057550090204053175\n",
      "    mean_inference_ms: 2.0580447193036906\n",
      "    mean_raw_obs_processing_ms: 0.28657437187765616\n",
      "  time_since_restore: 43.349509716033936\n",
      "  time_this_iter_s: 12.270889043807983\n",
      "  time_total_s: 43.349509716033936\n",
      "  timers:\n",
      "    learn_throughput: 448.515\n",
      "    learn_time_ms: 6688.734\n",
      "    load_throughput: 86618.012\n",
      "    load_time_ms: 34.635\n",
      "    sample_throughput: 737.537\n",
      "    sample_time_ms: 4067.593\n",
      "    update_time_ms: 5.465\n",
      "  timestamp: 1624524388\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 4\n",
      "  trial_id: '83919_00000'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.600000000000026\n",
      "  episode_reward_mean: -40.51200000000005\n",
      "  episode_reward_min: -48.00000000000008\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 120\n",
      "  experiment_id: 38bdf32c865b44ec9dfdfa051cfd3bca\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0018394350772723556\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029386624693870544\n",
      "          model: {}\n",
      "          policy_loss: -0.00014596334949601442\n",
      "          total_loss: 126.78753662109375\n",
      "          vf_explained_var: 0.1086287572979927\n",
      "          vf_loss: 126.76785278320312\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2640140056610107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015236832201480865\n",
      "          model: {}\n",
      "          policy_loss: -0.04080624878406525\n",
      "          total_loss: 17.182188034057617\n",
      "          vf_explained_var: 0.3381587862968445\n",
      "          vf_loss: 17.212709426879883\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.41666666666669\n",
      "    ram_util_percent: 69.58888888888889\n",
      "  pid: 2670\n",
      "  policy_reward_max:\n",
      "    policy1: 15.5\n",
      "    policy2: 47.19999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: -32.69\n",
      "    policy2: -7.821999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -62.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13219237169798656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05778962328583804\n",
      "    mean_inference_ms: 2.0796389559956636\n",
      "    mean_raw_obs_processing_ms: 0.2860392304493828\n",
      "  time_since_restore: 43.41788196563721\n",
      "  time_this_iter_s: 12.392032861709595\n",
      "  time_total_s: 43.41788196563721\n",
      "  timers:\n",
      "    learn_throughput: 451.267\n",
      "    learn_time_ms: 6647.946\n",
      "    load_throughput: 89237.663\n",
      "    load_time_ms: 33.618\n",
      "    sample_throughput: 726.997\n",
      "    sample_time_ms: 4126.564\n",
      "    update_time_ms: 6.747\n",
      "  timestamp: 1624524388\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 4\n",
      "  trial_id: '83919_00001'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-40\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999945\n",
      "  episode_reward_mean: 1.0890000000000097\n",
      "  episode_reward_min: -24.00000000000005\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 150\n",
      "  experiment_id: 43c5f1bf54a446d6ac106c951f38d022\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.251846432685852\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016989026218652725\n",
      "          model: {}\n",
      "          policy_loss: -0.059918273240327835\n",
      "          total_loss: 25.520841598510742\n",
      "          vf_explained_var: 0.3757430911064148\n",
      "          vf_loss: 25.57311248779297\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2176501750946045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0200651865452528\n",
      "          model: {}\n",
      "          policy_loss: -0.06906390190124512\n",
      "          total_loss: 2.931469202041626\n",
      "          vf_explained_var: 0.3495412766933441\n",
      "          vf_loss: 2.980217218399048\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.21176470588236\n",
      "    ram_util_percent: 69.78823529411764\n",
      "  pid: 2675\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: 9.800000000000006\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.46\n",
      "    policy2: -7.370999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -17.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13770736103379852\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.059650622142528906\n",
      "    mean_inference_ms: 2.1370365550005266\n",
      "    mean_raw_obs_processing_ms: 0.2975132662660366\n",
      "  time_since_restore: 54.92128086090088\n",
      "  time_this_iter_s: 11.571771144866943\n",
      "  time_total_s: 54.92128086090088\n",
      "  timers:\n",
      "    learn_throughput: 437.844\n",
      "    learn_time_ms: 6851.757\n",
      "    load_throughput: 105406.416\n",
      "    load_time_ms: 28.461\n",
      "    sample_throughput: 738.239\n",
      "    sample_time_ms: 4063.727\n",
      "    update_time_ms: 5.697\n",
      "  timestamp: 1624524400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 5\n",
      "  trial_id: '83919_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>RUNNING </td><td>192.168.0.179:2675</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         54.9213</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">   1.089</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -24  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>RUNNING </td><td>192.168.0.179:2670</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         43.4179</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -40.512</td><td style=\"text-align: right;\">                 6.6</td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>RUNNING </td><td>192.168.0.179:2674</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         43.4001</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -2.088</td><td style=\"text-align: right;\">                26.7</td><td style=\"text-align: right;\">               -33  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>RUNNING </td><td>192.168.0.179:2677</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         43.3918</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -37.944</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_83919_00001:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-40\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -15.299999999999994\n",
      "  episode_reward_mean: -43.872000000000064\n",
      "  episode_reward_min: -48.00000000000008\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 150\n",
      "  experiment_id: 38bdf32c865b44ec9dfdfa051cfd3bca\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.004095461219549179\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.16092397272586823\n",
      "          model: {}\n",
      "          policy_loss: 0.051739536225795746\n",
      "          total_loss: 95.20844268798828\n",
      "          vf_explained_var: 0.10867179930210114\n",
      "          vf_loss: 94.99378204345703\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2013071775436401\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02281377837061882\n",
      "          model: {}\n",
      "          policy_loss: -0.046474866569042206\n",
      "          total_loss: 9.4592866897583\n",
      "          vf_explained_var: 0.3502912223339081\n",
      "          vf_loss: 9.490362167358398\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.3\n",
      "    ram_util_percent: 69.79375\n",
      "  pid: 2670\n",
      "  policy_reward_max:\n",
      "    policy1: -29.0\n",
      "    policy2: 47.19999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: -37.48\n",
      "    policy2: -6.391999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -62.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13779135145873325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06025668216811887\n",
      "    mean_inference_ms: 2.166517606009812\n",
      "    mean_raw_obs_processing_ms: 0.29902887312290694\n",
      "  time_since_restore: 55.04816913604736\n",
      "  time_this_iter_s: 11.630287170410156\n",
      "  time_total_s: 55.04816913604736\n",
      "  timers:\n",
      "    learn_throughput: 439.589\n",
      "    learn_time_ms: 6824.557\n",
      "    load_throughput: 107811.824\n",
      "    load_time_ms: 27.826\n",
      "    sample_throughput: 728.669\n",
      "    sample_time_ms: 4117.095\n",
      "    update_time_ms: 6.546\n",
      "  timestamp: 1624524400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 5\n",
      "  trial_id: '83919_00001'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00002:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 27.59999999999991\n",
      "  episode_reward_mean: 2.1210000000000075\n",
      "  episode_reward_min: -19.500000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: c49aef7b1b5542e194a1608b246ea2a9\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2858389616012573\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019233517348766327\n",
      "          model: {}\n",
      "          policy_loss: -0.05634910985827446\n",
      "          total_loss: 26.89621925354004\n",
      "          vf_explained_var: 0.3860708475112915\n",
      "          vf_loss: 26.946792602539062\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.256722092628479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0231766514480114\n",
      "          model: {}\n",
      "          policy_loss: -0.06373517960309982\n",
      "          total_loss: 2.837135076522827\n",
      "          vf_explained_var: 0.2912205457687378\n",
      "          vf_loss: 2.885225534439087\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.99545454545456\n",
      "    ram_util_percent: 68.90909090909089\n",
      "  pid: 2674\n",
      "  policy_reward_max:\n",
      "    policy1: 36.5\n",
      "    policy2: 0.9999999999999974\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.635\n",
      "    policy2: -7.513999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -15.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13762167797970593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.058437466686413364\n",
      "    mean_inference_ms: 2.144904586647178\n",
      "    mean_raw_obs_processing_ms: 0.2882661375597641\n",
      "  time_since_restore: 58.85286235809326\n",
      "  time_this_iter_s: 15.452802658081055\n",
      "  time_total_s: 58.85286235809326\n",
      "  timers:\n",
      "    learn_throughput: 439.829\n",
      "    learn_time_ms: 9094.451\n",
      "    load_throughput: 95701.235\n",
      "    load_time_ms: 41.797\n",
      "    sample_throughput: 723.112\n",
      "    sample_time_ms: 5531.65\n",
      "    update_time_ms: 6.431\n",
      "  timestamp: 1624524403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: '83919_00002'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00003:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -25.500000000000014\n",
      "  episode_reward_mean: -41.40600000000004\n",
      "  episode_reward_min: -46.500000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 777bea0709a34c368f94f4b08cbe8c9b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          model: {}\n",
      "          policy_loss: 0.0024662308860570192\n",
      "          total_loss: 108.67005157470703\n",
      "          vf_explained_var: 0.08102112263441086\n",
      "          vf_loss: 108.66757202148438\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.163246512413025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02405012585222721\n",
      "          model: {}\n",
      "          policy_loss: -0.050368454307317734\n",
      "          total_loss: 20.483217239379883\n",
      "          vf_explained_var: 0.1175333559513092\n",
      "          vf_loss: 20.517351150512695\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.88636363636364\n",
      "    ram_util_percent: 68.92727272727271\n",
      "  pid: 2677\n",
      "  policy_reward_max:\n",
      "    policy1: -36.5\n",
      "    policy2: 28.499999999999982\n",
      "  policy_reward_mean:\n",
      "    policy1: -40.745\n",
      "    policy2: -0.660999999999992\n",
      "  policy_reward_min:\n",
      "    policy1: -54.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13512870544926348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.058345022924319\n",
      "    mean_inference_ms: 2.149925429608323\n",
      "    mean_raw_obs_processing_ms: 0.28952017014256237\n",
      "  time_since_restore: 58.80850696563721\n",
      "  time_this_iter_s: 15.41672396659851\n",
      "  time_total_s: 58.80850696563721\n",
      "  timers:\n",
      "    learn_throughput: 441.753\n",
      "    learn_time_ms: 9054.825\n",
      "    load_throughput: 90384.256\n",
      "    load_time_ms: 44.255\n",
      "    sample_throughput: 719.315\n",
      "    sample_time_ms: 5560.848\n",
      "    update_time_ms: 4.767\n",
      "  timestamp: 1624524403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: '83919_00003'\n",
      "  \n",
      "Result for PPO_MultiAgentArena_83919_00002:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-53\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 27.59999999999991\n",
      "  episode_reward_mean: 1.7430000000000065\n",
      "  episode_reward_min: -16.500000000000004\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: c49aef7b1b5542e194a1608b246ea2a9\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2454942464828491\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02204127609729767\n",
      "          model: {}\n",
      "          policy_loss: -0.05986357107758522\n",
      "          total_loss: 29.853612899780273\n",
      "          vf_explained_var: 0.3463425934314728\n",
      "          vf_loss: 29.906862258911133\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2331124544143677\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01736060343682766\n",
      "          model: {}\n",
      "          policy_loss: -0.05920691788196564\n",
      "          total_loss: 3.2174232006073\n",
      "          vf_explained_var: 0.33934932947158813\n",
      "          vf_loss: 3.2590525150299072\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.07999999999999\n",
      "    ram_util_percent: 61.86000000000001\n",
      "  pid: 2674\n",
      "  policy_reward_max:\n",
      "    policy1: 36.5\n",
      "    policy2: 4.299999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.4\n",
      "    policy2: -7.656999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -12.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13794494721605408\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05861953649363378\n",
      "    mean_inference_ms: 2.1377502243785567\n",
      "    mean_raw_obs_processing_ms: 0.28963978496130316\n",
      "  time_since_restore: 68.79273128509521\n",
      "  time_this_iter_s: 9.939868927001953\n",
      "  time_total_s: 68.79273128509521\n",
      "  timers:\n",
      "    learn_throughput: 466.637\n",
      "    learn_time_ms: 8571.98\n",
      "    load_throughput: 116098.873\n",
      "    load_time_ms: 34.453\n",
      "    sample_throughput: 782.176\n",
      "    sample_time_ms: 5113.937\n",
      "    update_time_ms: 5.748\n",
      "  timestamp: 1624524413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: '83919_00002'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>RUNNING   </td><td>192.168.0.179:2674</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         68.7927</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   1.743</td><td style=\"text-align: right;\">                27.6</td><td style=\"text-align: right;\">               -16.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>RUNNING   </td><td>192.168.0.179:2677</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         58.8085</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -41.406</td><td style=\"text-align: right;\">               -25.5</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         54.9213</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">   1.089</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -24  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         55.0482</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\"> -43.872</td><td style=\"text-align: right;\">               -15.3</td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_83919_00003:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-46-53\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -21.900000000000006\n",
      "  episode_reward_mean: -36.06600000000004\n",
      "  episode_reward_min: -46.500000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 777bea0709a34c368f94f4b08cbe8c9b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11249999701976776\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          model: {}\n",
      "          policy_loss: -0.0009185558883473277\n",
      "          total_loss: 146.60562133789062\n",
      "          vf_explained_var: 0.07484202086925507\n",
      "          vf_loss: 146.60653686523438\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.092926025390625\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012513847090303898\n",
      "          model: {}\n",
      "          policy_loss: -0.03175092115998268\n",
      "          total_loss: 32.38434600830078\n",
      "          vf_explained_var: 0.03248586133122444\n",
      "          vf_loss: 32.40342712402344\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.09333333333333\n",
      "    ram_util_percent: 61.853333333333346\n",
      "  pid: 2677\n",
      "  policy_reward_max:\n",
      "    policy1: -36.5\n",
      "    policy2: 35.09999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: -45.195\n",
      "    policy2: 9.129\n",
      "  policy_reward_min:\n",
      "    policy1: -57.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13505947328127552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.058292435672735045\n",
      "    mean_inference_ms: 2.1374865708771695\n",
      "    mean_raw_obs_processing_ms: 0.2901064345074678\n",
      "  time_since_restore: 68.77974581718445\n",
      "  time_this_iter_s: 9.971238851547241\n",
      "  time_total_s: 68.77974581718445\n",
      "  timers:\n",
      "    learn_throughput: 468.557\n",
      "    learn_time_ms: 8536.854\n",
      "    load_throughput: 109744.667\n",
      "    load_time_ms: 36.448\n",
      "    sample_throughput: 777.121\n",
      "    sample_time_ms: 5147.203\n",
      "    update_time_ms: 4.335\n",
      "  timestamp: 1624524413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: '83919_00003'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         54.9213</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">   1.089</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -24  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         55.0482</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\"> -43.872</td><td style=\"text-align: right;\">               -15.3</td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         68.7927</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   1.743</td><td style=\"text-align: right;\">                27.6</td><td style=\"text-align: right;\">               -16.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_83919_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         68.7797</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> -36.066</td><td style=\"text-align: right;\">               -21.9</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 10:46:54,581\tINFO tune.py:549 -- Total run time: 91.50 seconds (90.98 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fbc14c9b970>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plugging in Ray Tune.\n",
    "# Note that this is the recommended way to run any experiments with RLlib.\n",
    "# Reasons:\n",
    "# - Tune allows you to do hyperparameter tuning in a user-friendly way\n",
    "#   and at large scale!\n",
    "# - Tune automatically allocates needed resources for the different\n",
    "#   hyperparam trials and experiment runs on a cluster.\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Running stuff with tune, we can re-use the exact\n",
    "# same config that we used when working with RLlib directly!\n",
    "tune_config = config.copy()\n",
    "\n",
    "# Let's add our first hyperparameter search via our config.\n",
    "# How about we try two different learning rates? Let's say 0.00005 and 0.5 (ouch!).\n",
    "tune_config[\"lr\"] = tune.grid_search([0.00005, 0.5])  # <- 0.5? again: ouch!\n",
    "tune_config[\"train_batch_size\"] = tune.grid_search([3000, 4000])\n",
    "\n",
    "# Now that we will run things \"automatically\" through tune, we have to\n",
    "# define one or more stopping criteria.\n",
    "# Tune will stop the run, once any single one of the criteria is matched (not all of them!).\n",
    "stop = {\n",
    "    # Note that the keys used here can be anything present in the above `rllib_trainer.train()` output dict.\n",
    "    \"training_iteration\": 5,\n",
    "    \"episode_reward_mean\": 20.0,\n",
    "}\n",
    "\n",
    "# \"PPO\" is a registered name that points to RLlib's PPOTrainer.\n",
    "# See `ray/rllib/agents/registry.py`\n",
    "\n",
    "# Run a simple experiment until one of the stopping criteria is met.\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=tune_config,\n",
    "    stop=stop,\n",
    "\n",
    "    # Note that no trainers will be returned from this call here.\n",
    "    # Tune will create n Trainers internally, run them in parallel and destroy them at the end.\n",
    "    # However, you can ...\n",
    "    checkpoint_at_end=True,  # ... create a checkpoint when done.\n",
    "    checkpoint_freq=10,  # ... create a checkpoint every 10 training iterations.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b886fb8-6ccd-4be2-80bb-fc0936808d11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Why did we use 6 CPUs in the tune run above (3 CPUs per trial)?\n",
    "\n",
    "PPO - by default - uses 2 \"rollout\" workers (`num_workers=2`). These are Ray Actors that have their own environment copy(ies) and step through those in parallel. On top of these two \"rollout\" workers, every Trainer in RLlib always also has a \"local\" worker, which - in case of PPO - handles the learning updates. This gives us 3 workers (2 rollout + 1 local learner), which require 3 CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a74ec7-a6c1-431d-83aa-35df56d93185",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise No 2\n",
    "\n",
    "<hr />\n",
    "\n",
    "Using the `tune_config` that we have built so far, let's run another `tune.run()`, but apply the following changes to our setup this time:\n",
    "- Setup only 1 learning rate under the \"lr\" config key. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Setup only 1 train batch size under the \"train_batch_size\" config key. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Set `num_workers` to 5, which will allow us to run more environment \"rollouts\" in parallel and to collect training batches more quickly.\n",
    "- Set the `num_envs_per_worker` config parameter to 5. This will clone our env on each rollout worker, and thus parallelize action computing forward passes through our neural networks.\n",
    "\n",
    "Other than that, use the exact same args as in our `tune.run()` call in the previous cell.\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff184330-4229-4476-a9e0-1fdbaed948d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2801)\u001b[0m 2021-06-24 10:51:43,603\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=2801)\u001b[0m 2021-06-24 10:51:43,604\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=2801)\u001b[0m 2021-06-24 10:51:43,604\tWARNING ppo.py:135 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=5 num_envs_per_worker=5 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 160.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=2801)\u001b[0m 2021-06-24 10:51:56,770\tINFO trainable.py:101 -- Trainable.setup took 13.167 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=2801)\u001b[0m 2021-06-24 10:51:56,770\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-52-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.999999999999936\n",
      "  episode_reward_mean: -8.483999999999995\n",
      "  episode_reward_min: -34.500000000000036\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 25\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3588711023330688\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027996812015771866\n",
      "          model: {}\n",
      "          policy_loss: -0.05669461190700531\n",
      "          total_loss: 44.654815673828125\n",
      "          vf_explained_var: 0.09232431650161743\n",
      "          vf_loss: 44.70591354370117\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3487696647644043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.039114002138376236\n",
      "          model: {}\n",
      "          policy_loss: -0.07051993161439896\n",
      "          total_loss: 2.040412425994873\n",
      "          vf_explained_var: 0.31341245770454407\n",
      "          vf_loss: 2.103109836578369\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.825\n",
      "    ram_util_percent: 68.1\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 28.0\n",
      "    policy2: -6.6999999999999815\n",
      "  policy_reward_mean:\n",
      "    policy1: 1.12\n",
      "    policy2: -9.603999999999981\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1803880892925381\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10065114276009318\n",
      "    mean_inference_ms: 1.8377259651326243\n",
      "    mean_raw_obs_processing_ms: 0.6592643927343143\n",
      "  time_since_restore: 5.336621046066284\n",
      "  time_this_iter_s: 5.336621046066284\n",
      "  time_total_s: 5.336621046066284\n",
      "  timers:\n",
      "    learn_throughput: 855.819\n",
      "    learn_time_ms: 4673.886\n",
      "    load_throughput: 37221.824\n",
      "    load_time_ms: 107.464\n",
      "    sample_throughput: 8551.875\n",
      "    sample_time_ms: 467.734\n",
      "    update_time_ms: 3.065\n",
      "  timestamp: 1624524722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.33662</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  -8.484</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">               -34.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-52-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999943\n",
      "  episode_reward_mean: -5.075999999999994\n",
      "  episode_reward_min: -37.50000000000004\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 100\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2776226997375488\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026900572702288628\n",
      "          model: {}\n",
      "          policy_loss: -0.07335038483142853\n",
      "          total_loss: 29.200061798095703\n",
      "          vf_explained_var: 0.20351383090019226\n",
      "          vf_loss: 29.261306762695312\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2615963220596313\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03076518326997757\n",
      "          model: {}\n",
      "          policy_loss: -0.06708455830812454\n",
      "          total_loss: 1.9222501516342163\n",
      "          vf_explained_var: 0.38434791564941406\n",
      "          vf_loss: 1.975490689277649\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.916666666666668\n",
      "    ram_util_percent: 67.98333333333333\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 32.5\n",
      "    policy2: -1.2000000000000042\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.945\n",
      "    policy2: -9.020999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -27.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18618382504208733\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1039765163277514\n",
      "    mean_inference_ms: 1.66533838792357\n",
      "    mean_raw_obs_processing_ms: 0.6739571082107836\n",
      "  time_since_restore: 14.441888809204102\n",
      "  time_this_iter_s: 4.50461483001709\n",
      "  time_total_s: 14.441888809204102\n",
      "  timers:\n",
      "    learn_throughput: 923.507\n",
      "    learn_time_ms: 4331.318\n",
      "    load_throughput: 106612.26\n",
      "    load_time_ms: 37.519\n",
      "    sample_throughput: 9743.61\n",
      "    sample_time_ms: 410.525\n",
      "    update_time_ms: 2.614\n",
      "  timestamp: 1624524731\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         14.4419</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -5.076</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-52-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.599999999999973\n",
      "  episode_reward_mean: -1.2689999999999875\n",
      "  episode_reward_min: -27.00000000000003\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 200\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2463394403457642\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015143339522182941\n",
      "          model: {}\n",
      "          policy_loss: -0.05386582016944885\n",
      "          total_loss: 24.41973876953125\n",
      "          vf_explained_var: 0.2661151587963104\n",
      "          vf_loss: 24.458274841308594\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2167445421218872\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017633827403187752\n",
      "          model: {}\n",
      "          policy_loss: -0.055553462356328964\n",
      "          total_loss: 2.983912706375122\n",
      "          vf_explained_var: 0.18852190673351288\n",
      "          vf_loss: 3.0216116905212402\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.42857142857143\n",
      "    ram_util_percent: 67.67142857142856\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 31.0\n",
      "    policy2: 4.300000000000015\n",
      "  policy_reward_mean:\n",
      "    policy1: 6.52\n",
      "    policy2: -7.788999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -17.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18667602886585313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10429512578409497\n",
      "    mean_inference_ms: 1.4869503541662628\n",
      "    mean_raw_obs_processing_ms: 0.6743472097051147\n",
      "  time_since_restore: 23.885329723358154\n",
      "  time_this_iter_s: 4.868424892425537\n",
      "  time_total_s: 23.885329723358154\n",
      "  timers:\n",
      "    learn_throughput: 924.738\n",
      "    learn_time_ms: 4325.549\n",
      "    load_throughput: 170199.201\n",
      "    load_time_ms: 23.502\n",
      "    sample_throughput: 9902.327\n",
      "    sample_time_ms: 403.945\n",
      "    update_time_ms: 2.584\n",
      "  timestamp: 1624524740\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         23.8853</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -1.269</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">                 -27</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-52-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.099999999999948\n",
      "  episode_reward_mean: 1.3830000000000056\n",
      "  episode_reward_min: -19.499999999999993\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 275\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2091331481933594\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016251813620328903\n",
      "          model: {}\n",
      "          policy_loss: -0.058222174644470215\n",
      "          total_loss: 26.836854934692383\n",
      "          vf_explained_var: 0.4003700613975525\n",
      "          vf_loss: 26.878620147705078\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.1863893270492554\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017240630462765694\n",
      "          model: {}\n",
      "          policy_loss: -0.054528843611478806\n",
      "          total_loss: 2.038210391998291\n",
      "          vf_explained_var: 0.28468000888824463\n",
      "          vf_loss: 2.0752835273742676\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.300000000000004\n",
      "    ram_util_percent: 67.7\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 32.0\n",
      "    policy2: 0.9999999999999948\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.985\n",
      "    policy2: -7.601999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -9.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18734152929110506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10459180652046932\n",
      "    mean_inference_ms: 1.4691107706448272\n",
      "    mean_raw_obs_processing_ms: 0.6764412724286976\n",
      "  time_since_restore: 33.146549701690674\n",
      "  time_this_iter_s: 4.601808786392212\n",
      "  time_total_s: 33.146549701690674\n",
      "  timers:\n",
      "    learn_throughput: 931.147\n",
      "    learn_time_ms: 4295.779\n",
      "    load_throughput: 228239.705\n",
      "    load_time_ms: 17.525\n",
      "    sample_throughput: 9936.871\n",
      "    sample_time_ms: 402.541\n",
      "    update_time_ms: 2.508\n",
      "  timestamp: 1624524750\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         33.1465</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   1.383</td><td style=\"text-align: right;\">                23.1</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-52-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.99999999999995\n",
      "  episode_reward_mean: 3.495000000000006\n",
      "  episode_reward_min: -12.599999999999982\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 350\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1770472526550293\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015331901609897614\n",
      "          model: {}\n",
      "          policy_loss: -0.05267131328582764\n",
      "          total_loss: 34.16590118408203\n",
      "          vf_explained_var: 0.384956032037735\n",
      "          vf_loss: 34.20304870605469\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.1497039794921875\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01865207776427269\n",
      "          model: {}\n",
      "          policy_loss: -0.05684083327651024\n",
      "          total_loss: 3.0013606548309326\n",
      "          vf_explained_var: 0.2907329499721527\n",
      "          vf_loss: 3.039316177368164\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.72857142857143\n",
      "    ram_util_percent: 67.7\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 37.0\n",
      "    policy2: 1.0000000000000133\n",
      "  policy_reward_mean:\n",
      "    policy1: 10.8\n",
      "    policy2: -7.304999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -8.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18841656284174618\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10507025749676335\n",
      "    mean_inference_ms: 1.4641098677143014\n",
      "    mean_raw_obs_processing_ms: 0.6825804579829571\n",
      "  time_since_restore: 42.70558762550354\n",
      "  time_this_iter_s: 4.663714647293091\n",
      "  time_total_s: 42.70558762550354\n",
      "  timers:\n",
      "    learn_throughput: 927.491\n",
      "    learn_time_ms: 4312.712\n",
      "    load_throughput: 281992.134\n",
      "    load_time_ms: 14.185\n",
      "    sample_throughput: 9964.054\n",
      "    sample_time_ms: 401.443\n",
      "    update_time_ms: 2.459\n",
      "  timestamp: 1624524759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         42.7056</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   3.495</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">               -12.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-52-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.799999999999955\n",
      "  episode_reward_mean: 6.1229999999999976\n",
      "  episode_reward_min: -21.00000000000001\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 425\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1281288862228394\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017284387722611427\n",
      "          model: {}\n",
      "          policy_loss: -0.06095132231712341\n",
      "          total_loss: 31.74726104736328\n",
      "          vf_explained_var: 0.42851197719573975\n",
      "          vf_loss: 31.790712356567383\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.1163604259490967\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01815684698522091\n",
      "          model: {}\n",
      "          policy_loss: -0.05945218726992607\n",
      "          total_loss: 1.8503273725509644\n",
      "          vf_explained_var: 0.3132403790950775\n",
      "          vf_loss: 1.891395926475525\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.057142857142857\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 37.0\n",
      "    policy2: 1.0000000000000133\n",
      "  policy_reward_mean:\n",
      "    policy1: 13.395\n",
      "    policy2: -7.271999999999988\n",
      "  policy_reward_min:\n",
      "    policy1: -11.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18780611859259452\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10491313038838455\n",
      "    mean_inference_ms: 1.4540669132059962\n",
      "    mean_raw_obs_processing_ms: 0.681391593321603\n",
      "  time_since_restore: 51.90550756454468\n",
      "  time_this_iter_s: 4.553214073181152\n",
      "  time_total_s: 51.90550756454468\n",
      "  timers:\n",
      "    learn_throughput: 940.071\n",
      "    learn_time_ms: 4254.996\n",
      "    load_throughput: 1604538.595\n",
      "    load_time_ms: 2.493\n",
      "    sample_throughput: 10212.627\n",
      "    sample_time_ms: 391.672\n",
      "    update_time_ms: 2.361\n",
      "  timestamp: 1624524769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         51.9055</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   6.123</td><td style=\"text-align: right;\">                28.8</td><td style=\"text-align: right;\">                 -21</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-52-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.799999999999955\n",
      "  episode_reward_mean: 8.879999999999994\n",
      "  episode_reward_min: -11.099999999999996\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 500\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0732851028442383\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01739729940891266\n",
      "          model: {}\n",
      "          policy_loss: -0.05874611437320709\n",
      "          total_loss: 33.32643127441406\n",
      "          vf_explained_var: 0.38476911187171936\n",
      "          vf_loss: 33.36756134033203\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.0671054124832153\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017925459891557693\n",
      "          model: {}\n",
      "          policy_loss: -0.055113520473241806\n",
      "          total_loss: 2.705533742904663\n",
      "          vf_explained_var: 0.22145208716392517\n",
      "          vf_loss: 2.7424981594085693\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.75\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 36.5\n",
      "    policy2: -0.10000000000000081\n",
      "  policy_reward_mean:\n",
      "    policy1: 16.02\n",
      "    policy2: -7.139999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -11.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18843760427297632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10541449285624176\n",
      "    mean_inference_ms: 1.4498803333323103\n",
      "    mean_raw_obs_processing_ms: 0.6861005328261733\n",
      "  time_since_restore: 61.210946559906006\n",
      "  time_this_iter_s: 4.571083068847656\n",
      "  time_total_s: 61.210946559906006\n",
      "  timers:\n",
      "    learn_throughput: 936.448\n",
      "    learn_time_ms: 4271.461\n",
      "    load_throughput: 1592824.077\n",
      "    load_time_ms: 2.511\n",
      "    sample_throughput: 10117.043\n",
      "    sample_time_ms: 395.372\n",
      "    update_time_ms: 2.353\n",
      "  timestamp: 1624524778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         61.2109</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">    8.88</td><td style=\"text-align: right;\">                28.8</td><td style=\"text-align: right;\">               -11.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-53-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.499999999999915\n",
      "  episode_reward_mean: 12.90299999999998\n",
      "  episode_reward_min: -15.899999999999984\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 600\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.041711688041687\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01560800801962614\n",
      "          model: {}\n",
      "          policy_loss: -0.05298454314470291\n",
      "          total_loss: 45.75339126586914\n",
      "          vf_explained_var: 0.2644711434841156\n",
      "          vf_loss: 45.79058074951172\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.0430861711502075\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01725194975733757\n",
      "          model: {}\n",
      "          policy_loss: -0.050654247403144836\n",
      "          total_loss: 3.441502571105957\n",
      "          vf_explained_var: 0.1471896916627884\n",
      "          vf_loss: 3.47468900680542\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.416666666666668\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 44.5\n",
      "    policy2: 2.1000000000000045\n",
      "  policy_reward_mean:\n",
      "    policy1: 19.845\n",
      "    policy2: -6.941999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -7.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18899058675113153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1055464521529491\n",
      "    mean_inference_ms: 1.4461177659488147\n",
      "    mean_raw_obs_processing_ms: 0.6908022666487443\n",
      "  time_since_restore: 70.40173149108887\n",
      "  time_this_iter_s: 4.566832065582275\n",
      "  time_total_s: 70.40173149108887\n",
      "  timers:\n",
      "    learn_throughput: 942.383\n",
      "    learn_time_ms: 4244.56\n",
      "    load_throughput: 1610607.582\n",
      "    load_time_ms: 2.484\n",
      "    sample_throughput: 10070.809\n",
      "    sample_time_ms: 397.188\n",
      "    update_time_ms: 2.315\n",
      "  timestamp: 1624524787\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         70.4017</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  12.903</td><td style=\"text-align: right;\">                34.5</td><td style=\"text-align: right;\">               -15.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-53-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.49999999999991\n",
      "  episode_reward_mean: 16.301999999999964\n",
      "  episode_reward_min: -11.999999999999993\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 675\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9933565258979797\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015695845708251\n",
      "          model: {}\n",
      "          policy_loss: -0.05434282869100571\n",
      "          total_loss: 37.19157791137695\n",
      "          vf_explained_var: 0.33458060026168823\n",
      "          vf_loss: 37.230037689208984\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.028895378112793\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01564483903348446\n",
      "          model: {}\n",
      "          policy_loss: -0.048475589603185654\n",
      "          total_loss: 2.7131881713867188\n",
      "          vf_explained_var: 0.19221122562885284\n",
      "          vf_loss: 2.745824098587036\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.949999999999996\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 44.5\n",
      "    policy2: 4.299999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 22.76\n",
      "    policy2: -6.4579999999999895\n",
      "  policy_reward_min:\n",
      "    policy1: -3.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1883786260173379\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10529333037907357\n",
      "    mean_inference_ms: 1.4435778478117494\n",
      "    mean_raw_obs_processing_ms: 0.6881044391834817\n",
      "  time_since_restore: 79.5873703956604\n",
      "  time_this_iter_s: 4.66040301322937\n",
      "  time_total_s: 79.5873703956604\n",
      "  timers:\n",
      "    learn_throughput: 943.238\n",
      "    learn_time_ms: 4240.713\n",
      "    load_throughput: 1625242.519\n",
      "    load_time_ms: 2.461\n",
      "    sample_throughput: 10165.927\n",
      "    sample_time_ms: 393.471\n",
      "    update_time_ms: 2.337\n",
      "  timestamp: 1624524796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         79.5874</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  16.302</td><td style=\"text-align: right;\">                37.5</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-53-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.099999999999916\n",
      "  episode_reward_mean: 18.67799999999995\n",
      "  episode_reward_min: -6.899999999999993\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 750\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9335061311721802\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014588603749871254\n",
      "          model: {}\n",
      "          policy_loss: -0.05031830444931984\n",
      "          total_loss: 39.424007415771484\n",
      "          vf_explained_var: 0.3828079104423523\n",
      "          vf_loss: 39.459564208984375\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.9802654981613159\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015616741962730885\n",
      "          model: {}\n",
      "          policy_loss: -0.04722469300031662\n",
      "          total_loss: 2.228907823562622\n",
      "          vf_explained_var: 0.24936676025390625\n",
      "          vf_loss: 2.2603209018707275\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.300000000000004\n",
      "    ram_util_percent: 67.55\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 44.0\n",
      "    policy2: 2.0999999999999974\n",
      "  policy_reward_mean:\n",
      "    policy1: 25.015\n",
      "    policy2: -6.336999999999989\n",
      "  policy_reward_min:\n",
      "    policy1: -4.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18748910081041228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1048080348509544\n",
      "    mean_inference_ms: 1.4334617017471698\n",
      "    mean_raw_obs_processing_ms: 0.6833796065263681\n",
      "  time_since_restore: 88.62667441368103\n",
      "  time_this_iter_s: 4.409440040588379\n",
      "  time_total_s: 88.62667441368103\n",
      "  timers:\n",
      "    learn_throughput: 953.549\n",
      "    learn_time_ms: 4194.856\n",
      "    load_throughput: 1646374.627\n",
      "    load_time_ms: 2.43\n",
      "    sample_throughput: 10327.069\n",
      "    sample_time_ms: 387.332\n",
      "    update_time_ms: 2.352\n",
      "  timestamp: 1624524806\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         88.6267</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  18.678</td><td style=\"text-align: right;\">                35.1</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-53-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.89999999999991\n",
      "  episode_reward_mean: 21.638999999999932\n",
      "  episode_reward_min: -6.899999999999993\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 825\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8775365948677063\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01357054989784956\n",
      "          model: {}\n",
      "          policy_loss: -0.04833472520112991\n",
      "          total_loss: 49.6264533996582\n",
      "          vf_explained_var: 0.36908942461013794\n",
      "          vf_loss: 49.66104507446289\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.9530884623527527\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015996383503079414\n",
      "          model: {}\n",
      "          policy_loss: -0.04699886590242386\n",
      "          total_loss: 3.329789876937866\n",
      "          vf_explained_var: 0.21668118238449097\n",
      "          vf_loss: 3.3605921268463135\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.150000000000002\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 45.0\n",
      "    policy2: -0.10000000000000264\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.13\n",
      "    policy2: -6.490999999999989\n",
      "  policy_reward_min:\n",
      "    policy1: 0.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18634631266609225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10416675745069298\n",
      "    mean_inference_ms: 1.4216928559240705\n",
      "    mean_raw_obs_processing_ms: 0.6777996004040239\n",
      "  time_since_restore: 97.18751645088196\n",
      "  time_this_iter_s: 4.272667169570923\n",
      "  time_total_s: 97.18751645088196\n",
      "  timers:\n",
      "    learn_throughput: 966.883\n",
      "    learn_time_ms: 4137.007\n",
      "    load_throughput: 1625242.519\n",
      "    load_time_ms: 2.461\n",
      "    sample_throughput: 10491.682\n",
      "    sample_time_ms: 381.254\n",
      "    update_time_ms: 2.334\n",
      "  timestamp: 1624524814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         97.1875</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  21.639</td><td style=\"text-align: right;\">                39.9</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-53-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.89999999999991\n",
      "  episode_reward_mean: 24.497999999999923\n",
      "  episode_reward_min: -8.399999999999977\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 900\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.851176381111145\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012521426193416119\n",
      "          model: {}\n",
      "          policy_loss: -0.041859906166791916\n",
      "          total_loss: 60.12490463256836\n",
      "          vf_explained_var: 0.2846677005290985\n",
      "          vf_loss: 60.15409469604492\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.9234663248062134\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01742134988307953\n",
      "          model: {}\n",
      "          policy_loss: -0.053981851786375046\n",
      "          total_loss: 2.958487033843994\n",
      "          vf_explained_var: 0.17945560812950134\n",
      "          vf_loss: 2.9948301315307617\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.483333333333334\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 13.10000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.68\n",
      "    policy2: -5.1819999999999915\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1857012560592322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10376188801988728\n",
      "    mean_inference_ms: 1.4084494880366731\n",
      "    mean_raw_obs_processing_ms: 0.6736448800020446\n",
      "  time_since_restore: 105.73803853988647\n",
      "  time_this_iter_s: 4.271465063095093\n",
      "  time_total_s: 105.73803853988647\n",
      "  timers:\n",
      "    learn_throughput: 983.113\n",
      "    learn_time_ms: 4068.708\n",
      "    load_throughput: 1650309.952\n",
      "    load_time_ms: 2.424\n",
      "    sample_throughput: 10692.482\n",
      "    sample_time_ms: 374.095\n",
      "    update_time_ms: 2.311\n",
      "  timestamp: 1624524823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         105.738</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  24.498</td><td style=\"text-align: right;\">                39.9</td><td style=\"text-align: right;\">                -8.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-53-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 44.999999999999915\n",
      "  episode_reward_mean: 25.328999999999922\n",
      "  episode_reward_min: 4.199999999999992\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8086280822753906\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013364194892346859\n",
      "          model: {}\n",
      "          policy_loss: -0.04560176655650139\n",
      "          total_loss: 53.232032775878906\n",
      "          vf_explained_var: 0.3239186406135559\n",
      "          vf_loss: 53.26410675048828\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.8873233795166016\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014259649440646172\n",
      "          model: {}\n",
      "          policy_loss: -0.04232487455010414\n",
      "          total_loss: 3.13403058052063\n",
      "          vf_explained_var: 0.1824813038110733\n",
      "          vf_loss: 3.1619176864624023\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.91666666666667\n",
      "    ram_util_percent: 67.46666666666667\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 9.800000000000013\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.06\n",
      "    policy2: -4.730999999999992\n",
      "  policy_reward_min:\n",
      "    policy1: 8.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.184674308928809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10318004144356976\n",
      "    mean_inference_ms: 1.39836529335235\n",
      "    mean_raw_obs_processing_ms: 0.6694622787723356\n",
      "  time_since_restore: 114.38890051841736\n",
      "  time_this_iter_s: 4.338054895401001\n",
      "  time_total_s: 114.38890051841736\n",
      "  timers:\n",
      "    learn_throughput: 994.13\n",
      "    learn_time_ms: 4023.619\n",
      "    load_throughput: 1656649.024\n",
      "    load_time_ms: 2.415\n",
      "    sample_throughput: 10952.484\n",
      "    sample_time_ms: 365.214\n",
      "    update_time_ms: 2.288\n",
      "  timestamp: 1624524832\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         114.389</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  25.329</td><td style=\"text-align: right;\">                  45</td><td style=\"text-align: right;\">                 4.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-54-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.799999999999926\n",
      "  episode_reward_mean: 27.206999999999915\n",
      "  episode_reward_min: 3.2999999999999816\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1075\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.7687411308288574\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011752902530133724\n",
      "          model: {}\n",
      "          policy_loss: -0.03996681049466133\n",
      "          total_loss: 58.88492202758789\n",
      "          vf_explained_var: 0.3884660005569458\n",
      "          vf_loss: 58.912986755371094\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.8543674945831299\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013211368583142757\n",
      "          model: {}\n",
      "          policy_loss: -0.039770953357219696\n",
      "          total_loss: 2.8828179836273193\n",
      "          vf_explained_var: 0.1609441190958023\n",
      "          vf_loss: 2.9092118740081787\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.933333333333334\n",
      "    ram_util_percent: 67.55\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.5\n",
      "    policy2: 7.600000000000008\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.685\n",
      "    policy2: -4.477999999999993\n",
      "  policy_reward_min:\n",
      "    policy1: 10.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18363218680845148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1027035238997258\n",
      "    mean_inference_ms: 1.393164726951934\n",
      "    mean_raw_obs_processing_ms: 0.6660220386080498\n",
      "  time_since_restore: 123.23995542526245\n",
      "  time_this_iter_s: 4.397578001022339\n",
      "  time_total_s: 123.23995542526245\n",
      "  timers:\n",
      "    learn_throughput: 1001.233\n",
      "    learn_time_ms: 3995.073\n",
      "    load_throughput: 1668561.199\n",
      "    load_time_ms: 2.397\n",
      "    sample_throughput: 11096.273\n",
      "    sample_time_ms: 360.481\n",
      "    update_time_ms: 2.246\n",
      "  timestamp: 1624524840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">          123.24</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  27.207</td><td style=\"text-align: right;\">                46.8</td><td style=\"text-align: right;\">                 3.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-54-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.799999999999926\n",
      "  episode_reward_mean: 28.493999999999915\n",
      "  episode_reward_min: 3.2999999999999816\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1150\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.7267895340919495\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012627107091248035\n",
      "          model: {}\n",
      "          policy_loss: -0.042782749980688095\n",
      "          total_loss: 47.68418884277344\n",
      "          vf_explained_var: 0.45486703515052795\n",
      "          vf_loss: 47.71418762207031\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.8142289519309998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013674505054950714\n",
      "          model: {}\n",
      "          policy_loss: -0.04252731055021286\n",
      "          total_loss: 5.259716510772705\n",
      "          vf_explained_var: 0.198496013879776\n",
      "          vf_loss: 5.288397789001465\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.18333333333334\n",
      "    ram_util_percent: 67.7\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.5\n",
      "    policy2: 7.600000000000012\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.84\n",
      "    policy2: -4.34599999999999\n",
      "  policy_reward_min:\n",
      "    policy1: 10.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18306164406458794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10236325307053469\n",
      "    mean_inference_ms: 1.3879879538664681\n",
      "    mean_raw_obs_processing_ms: 0.6635427685139038\n",
      "  time_since_restore: 132.33677124977112\n",
      "  time_this_iter_s: 4.486896753311157\n",
      "  time_total_s: 132.33677124977112\n",
      "  timers:\n",
      "    learn_throughput: 999.891\n",
      "    learn_time_ms: 4000.437\n",
      "    load_throughput: 1684205.792\n",
      "    load_time_ms: 2.375\n",
      "    sample_throughput: 11079.835\n",
      "    sample_time_ms: 361.016\n",
      "    update_time_ms: 2.216\n",
      "  timestamp: 1624524850\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         132.337</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  28.494</td><td style=\"text-align: right;\">                46.8</td><td style=\"text-align: right;\">                 3.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-54-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.2999999999999\n",
      "  episode_reward_mean: 29.750999999999912\n",
      "  episode_reward_min: 12.899999999999961\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1225\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6840970516204834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01279220636934042\n",
      "          model: {}\n",
      "          policy_loss: -0.039032094180583954\n",
      "          total_loss: 65.87602233886719\n",
      "          vf_explained_var: 0.3669774830341339\n",
      "          vf_loss: 65.90210723876953\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.7689157724380493\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011909321881830692\n",
      "          model: {}\n",
      "          policy_loss: -0.031435973942279816\n",
      "          total_loss: 13.738896369934082\n",
      "          vf_explained_var: 0.19629336893558502\n",
      "          vf_loss: 13.758275032043457\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.88333333333333\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.0\n",
      "    policy2: 38.399999999999956\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.93\n",
      "    policy2: -2.1789999999999954\n",
      "  policy_reward_min:\n",
      "    policy1: -12.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18246295117520034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10209340125131484\n",
      "    mean_inference_ms: 1.3835809432719106\n",
      "    mean_raw_obs_processing_ms: 0.6611277603272261\n",
      "  time_since_restore: 141.03434944152832\n",
      "  time_this_iter_s: 4.309662103652954\n",
      "  time_total_s: 141.03434944152832\n",
      "  timers:\n",
      "    learn_throughput: 996.616\n",
      "    learn_time_ms: 4013.581\n",
      "    load_throughput: 1698631.757\n",
      "    load_time_ms: 2.355\n",
      "    sample_throughput: 11063.159\n",
      "    sample_time_ms: 361.56\n",
      "    update_time_ms: 2.236\n",
      "  timestamp: 1624524858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         141.034</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  29.751</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                12.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-54-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.2999999999999\n",
      "  episode_reward_mean: 29.228999999999917\n",
      "  episode_reward_min: 10.800000000000022\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1300\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.677323579788208\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011933168396353722\n",
      "          model: {}\n",
      "          policy_loss: -0.03782849758863449\n",
      "          total_loss: 71.7448501586914\n",
      "          vf_explained_var: 0.32996243238449097\n",
      "          vf_loss: 71.77059173583984\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.7344822883605957\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011026719585061073\n",
      "          model: {}\n",
      "          policy_loss: -0.03297652676701546\n",
      "          total_loss: 15.851787567138672\n",
      "          vf_explained_var: 0.29036301374435425\n",
      "          vf_loss: 15.873600006103516\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.28333333333333\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.0\n",
      "    policy2: 40.59999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.055\n",
      "    policy2: -0.8259999999999951\n",
      "  policy_reward_min:\n",
      "    policy1: -14.5\n",
      "    policy2: -8.899999999999983\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1821406783489237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10186239090810706\n",
      "    mean_inference_ms: 1.3771398381615092\n",
      "    mean_raw_obs_processing_ms: 0.6600469428380655\n",
      "  time_since_restore: 149.90831995010376\n",
      "  time_this_iter_s: 4.4346606731414795\n",
      "  time_total_s: 149.90831995010376\n",
      "  timers:\n",
      "    learn_throughput: 988.44\n",
      "    learn_time_ms: 4046.782\n",
      "    load_throughput: 1675576.862\n",
      "    load_time_ms: 2.387\n",
      "    sample_throughput: 11089.947\n",
      "    sample_time_ms: 360.687\n",
      "    update_time_ms: 2.223\n",
      "  timestamp: 1624524867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         149.908</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  29.229</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                10.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-54-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.49999999999989\n",
      "  episode_reward_mean: 29.945999999999913\n",
      "  episode_reward_min: 10.499999999999964\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1400\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6566652059555054\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012123221531510353\n",
      "          model: {}\n",
      "          policy_loss: -0.03603876009583473\n",
      "          total_loss: 62.44966506958008\n",
      "          vf_explained_var: 0.3267621695995331\n",
      "          vf_loss: 62.473426818847656\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.7197313904762268\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0124246496707201\n",
      "          model: {}\n",
      "          policy_loss: -0.038673412054777145\n",
      "          total_loss: 10.332758903503418\n",
      "          vf_explained_var: 0.19566388428211212\n",
      "          vf_loss: 10.35885238647461\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.81428571428571\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.5\n",
      "    policy2: 18.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.41\n",
      "    policy2: -1.4639999999999946\n",
      "  policy_reward_min:\n",
      "    policy1: -6.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18162015152075134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10154785230865754\n",
      "    mean_inference_ms: 1.371390107841683\n",
      "    mean_raw_obs_processing_ms: 0.6569029660770187\n",
      "  time_since_restore: 159.12935614585876\n",
      "  time_this_iter_s: 4.763045072555542\n",
      "  time_total_s: 159.12935614585876\n",
      "  timers:\n",
      "    learn_throughput: 974.719\n",
      "    learn_time_ms: 4103.747\n",
      "    load_throughput: 1669025.975\n",
      "    load_time_ms: 2.397\n",
      "    sample_throughput: 11090.224\n",
      "    sample_time_ms: 360.678\n",
      "    update_time_ms: 2.252\n",
      "  timestamp: 1624524877\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         159.129</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  29.946</td><td style=\"text-align: right;\">                46.5</td><td style=\"text-align: right;\">                10.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-54-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.49999999999989\n",
      "  episode_reward_mean: 30.701999999999916\n",
      "  episode_reward_min: 12.60000000000002\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1475\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6108099818229675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010560435242950916\n",
      "          model: {}\n",
      "          policy_loss: -0.03370382636785507\n",
      "          total_loss: 62.26347351074219\n",
      "          vf_explained_var: 0.4298911988735199\n",
      "          vf_loss: 62.2864875793457\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.6857420802116394\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01173960417509079\n",
      "          model: {}\n",
      "          policy_loss: -0.03361233323812485\n",
      "          total_loss: 12.483384132385254\n",
      "          vf_explained_var: 0.39778512716293335\n",
      "          vf_loss: 12.505107879638672\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.21428571428572\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 68.1\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.845\n",
      "    policy2: 0.8570000000000028\n",
      "  policy_reward_min:\n",
      "    policy1: -52.5\n",
      "    policy2: -8.899999999999984\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18029860669986683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10095788398440991\n",
      "    mean_inference_ms: 1.3662076310888964\n",
      "    mean_raw_obs_processing_ms: 0.6547402250669448\n",
      "  time_since_restore: 168.3749167919159\n",
      "  time_this_iter_s: 4.615001678466797\n",
      "  time_total_s: 168.3749167919159\n",
      "  timers:\n",
      "    learn_throughput: 964.668\n",
      "    learn_time_ms: 4146.505\n",
      "    load_throughput: 1668909.756\n",
      "    load_time_ms: 2.397\n",
      "    sample_throughput: 11198.077\n",
      "    sample_time_ms: 357.204\n",
      "    update_time_ms: 2.3\n",
      "  timestamp: 1624524886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         168.375</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">  30.702</td><td style=\"text-align: right;\">                46.5</td><td style=\"text-align: right;\">                12.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-54-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.29999999999991\n",
      "  episode_reward_mean: 31.331999999999912\n",
      "  episode_reward_min: 14.999999999999996\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1550\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5867441296577454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011585080996155739\n",
      "          model: {}\n",
      "          policy_loss: -0.03752788156270981\n",
      "          total_loss: 87.87799835205078\n",
      "          vf_explained_var: 0.4259708523750305\n",
      "          vf_loss: 87.90380859375\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.6529635787010193\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010162655264139175\n",
      "          model: {}\n",
      "          policy_loss: -0.02790335938334465\n",
      "          total_loss: 27.34839630126953\n",
      "          vf_explained_var: 0.2702203094959259\n",
      "          vf_loss: 27.36600685119629\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.800000000000004\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.5\n",
      "    policy2: 68.1\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.67\n",
      "    policy2: 3.662000000000002\n",
      "  policy_reward_min:\n",
      "    policy1: -52.5\n",
      "    policy2: -8.89999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17928594644545384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10047220614668334\n",
      "    mean_inference_ms: 1.3582128547479124\n",
      "    mean_raw_obs_processing_ms: 0.653696572549689\n",
      "  time_since_restore: 177.75175070762634\n",
      "  time_this_iter_s: 4.626401901245117\n",
      "  time_total_s: 177.75175070762634\n",
      "  timers:\n",
      "    learn_throughput: 957.284\n",
      "    learn_time_ms: 4178.488\n",
      "    load_throughput: 1634696.391\n",
      "    load_time_ms: 2.447\n",
      "    sample_throughput: 11330.472\n",
      "    sample_time_ms: 353.03\n",
      "    update_time_ms: 2.358\n",
      "  timestamp: 1624524895\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         177.752</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">  31.332</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-55-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 53.39999999999992\n",
      "  episode_reward_mean: 30.854999999999922\n",
      "  episode_reward_min: -1.1999999999999975\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1625\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.574537456035614\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009072775021195412\n",
      "          model: {}\n",
      "          policy_loss: -0.031153691932559013\n",
      "          total_loss: 108.16924285888672\n",
      "          vf_explained_var: 0.3942277729511261\n",
      "          vf_loss: 108.19120025634766\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.6456592679023743\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00929906778037548\n",
      "          model: {}\n",
      "          policy_loss: -0.026514235883951187\n",
      "          total_loss: 53.446678161621094\n",
      "          vf_explained_var: 0.3138171136379242\n",
      "          vf_loss: 53.463775634765625\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.442857142857136\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.5\n",
      "    policy2: 63.69999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 25.73\n",
      "    policy2: 5.125000000000003\n",
      "  policy_reward_min:\n",
      "    policy1: -43.0\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17880095410096353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10025251577377045\n",
      "    mean_inference_ms: 1.3546142309689861\n",
      "    mean_raw_obs_processing_ms: 0.6533497881970264\n",
      "  time_since_restore: 186.89528465270996\n",
      "  time_this_iter_s: 4.5814268589019775\n",
      "  time_total_s: 186.89528465270996\n",
      "  timers:\n",
      "    learn_throughput: 946.95\n",
      "    learn_time_ms: 4224.09\n",
      "    load_throughput: 1605782.542\n",
      "    load_time_ms: 2.491\n",
      "    sample_throughput: 11365.866\n",
      "    sample_time_ms: 351.931\n",
      "    update_time_ms: 2.374\n",
      "  timestamp: 1624524905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         186.895</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">  30.855</td><td style=\"text-align: right;\">                53.4</td><td style=\"text-align: right;\">                -1.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-55-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.29999999999989\n",
      "  episode_reward_mean: 29.840999999999926\n",
      "  episode_reward_min: 13.49999999999992\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1700\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5857378244400024\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010605060495436192\n",
      "          model: {}\n",
      "          policy_loss: -0.03383101895451546\n",
      "          total_loss: 84.67359924316406\n",
      "          vf_explained_var: 0.3525088131427765\n",
      "          vf_loss: 84.69670104980469\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.6111442446708679\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008278192020952702\n",
      "          model: {}\n",
      "          policy_loss: -0.023637862876057625\n",
      "          total_loss: 27.55168914794922\n",
      "          vf_explained_var: 0.33420389890670776\n",
      "          vf_loss: 27.566938400268555\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.583333333333332\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 68.1\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.465\n",
      "    policy2: 3.3760000000000026\n",
      "  policy_reward_min:\n",
      "    policy1: -49.5\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17833213203728213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09994354151370147\n",
      "    mean_inference_ms: 1.3465141548873236\n",
      "    mean_raw_obs_processing_ms: 0.6521050432711207\n",
      "  time_since_restore: 196.2537965774536\n",
      "  time_this_iter_s: 4.683165073394775\n",
      "  time_total_s: 196.2537965774536\n",
      "  timers:\n",
      "    learn_throughput: 935.965\n",
      "    learn_time_ms: 4273.665\n",
      "    load_throughput: 1625384.228\n",
      "    load_time_ms: 2.461\n",
      "    sample_throughput: 11405.318\n",
      "    sample_time_ms: 350.714\n",
      "    update_time_ms: 2.408\n",
      "  timestamp: 1624524914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         196.254</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  29.841</td><td style=\"text-align: right;\">                45.3</td><td style=\"text-align: right;\">                13.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-55-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.39999999999989\n",
      "  episode_reward_mean: 30.533999999999914\n",
      "  episode_reward_min: 6.900000000000023\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1800\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5727860331535339\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01140167098492384\n",
      "          model: {}\n",
      "          policy_loss: -0.033886298537254333\n",
      "          total_loss: 76.70872497558594\n",
      "          vf_explained_var: 0.3266090452671051\n",
      "          vf_loss: 76.73106384277344\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.5973948836326599\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009907253086566925\n",
      "          model: {}\n",
      "          policy_loss: -0.030402278527617455\n",
      "          total_loss: 17.8972110748291\n",
      "          vf_explained_var: 0.23843955993652344\n",
      "          vf_loss: 17.91758155822754\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.942857142857143\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 70.3\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.59\n",
      "    policy2: -0.05599999999999884\n",
      "  policy_reward_min:\n",
      "    policy1: -47.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17758484819333012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09956126524255851\n",
      "    mean_inference_ms: 1.3409209500094432\n",
      "    mean_raw_obs_processing_ms: 0.6510703829693041\n",
      "  time_since_restore: 205.49362444877625\n",
      "  time_this_iter_s: 4.626206874847412\n",
      "  time_total_s: 205.49362444877625\n",
      "  timers:\n",
      "    learn_throughput: 934.62\n",
      "    learn_time_ms: 4279.816\n",
      "    load_throughput: 1621472.712\n",
      "    load_time_ms: 2.467\n",
      "    sample_throughput: 11544.012\n",
      "    sample_time_ms: 346.5\n",
      "    update_time_ms: 2.408\n",
      "  timestamp: 1624524923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         205.494</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  30.534</td><td style=\"text-align: right;\">                47.4</td><td style=\"text-align: right;\">                 6.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-55-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.7999999999999\n",
      "  episode_reward_mean: 29.687999999999917\n",
      "  episode_reward_min: 14.099999999999909\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1875\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5390828251838684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009557900950312614\n",
      "          model: {}\n",
      "          policy_loss: -0.03181135281920433\n",
      "          total_loss: 55.03185272216797\n",
      "          vf_explained_var: 0.41093891859054565\n",
      "          vf_loss: 55.05398178100586\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.5633679628372192\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010973580181598663\n",
      "          model: {}\n",
      "          policy_loss: -0.03133324906229973\n",
      "          total_loss: 6.3301520347595215\n",
      "          vf_explained_var: 0.18075458705425262\n",
      "          vf_loss: 6.350374221801758\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.68571428571428\n",
      "    ram_util_percent: 67.54285714285716\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 57.0\n",
      "    policy2: 42.79999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.8\n",
      "    policy2: -1.1119999999999979\n",
      "  policy_reward_min:\n",
      "    policy1: -17.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17656440121038883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09914172942739845\n",
      "    mean_inference_ms: 1.3372868769742654\n",
      "    mean_raw_obs_processing_ms: 0.6492516964673379\n",
      "  time_since_restore: 214.4968774318695\n",
      "  time_this_iter_s: 4.512882947921753\n",
      "  time_total_s: 214.4968774318695\n",
      "  timers:\n",
      "    learn_throughput: 940.315\n",
      "    learn_time_ms: 4253.895\n",
      "    load_throughput: 1626771.128\n",
      "    load_time_ms: 2.459\n",
      "    sample_throughput: 11486.599\n",
      "    sample_time_ms: 348.232\n",
      "    update_time_ms: 2.423\n",
      "  timestamp: 1624524932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         214.497</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  29.688</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                14.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-55-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.7999999999999\n",
      "  episode_reward_mean: 30.25499999999991\n",
      "  episode_reward_min: 9.899999999999965\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1950\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5224382281303406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00951691810041666\n",
      "          model: {}\n",
      "          policy_loss: -0.031399473547935486\n",
      "          total_loss: 48.411170959472656\n",
      "          vf_explained_var: 0.5059530138969421\n",
      "          vf_loss: 48.43292999267578\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.5419241786003113\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010463923215866089\n",
      "          model: {}\n",
      "          policy_loss: -0.029458660632371902\n",
      "          total_loss: 5.945410251617432\n",
      "          vf_explained_var: 0.18880540132522583\n",
      "          vf_loss: 5.964276313781738\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.15714285714286\n",
      "    ram_util_percent: 67.45714285714287\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 42.79999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.85\n",
      "    policy2: -0.5949999999999991\n",
      "  policy_reward_min:\n",
      "    policy1: -17.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17597372494867525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09886227000727396\n",
      "    mean_inference_ms: 1.332893210748746\n",
      "    mean_raw_obs_processing_ms: 0.6486078104573967\n",
      "  time_since_restore: 223.80174946784973\n",
      "  time_this_iter_s: 4.73954701423645\n",
      "  time_total_s: 223.80174946784973\n",
      "  timers:\n",
      "    learn_throughput: 941.919\n",
      "    learn_time_ms: 4246.65\n",
      "    load_throughput: 1628713.608\n",
      "    load_time_ms: 2.456\n",
      "    sample_throughput: 11488.577\n",
      "    sample_time_ms: 348.172\n",
      "    update_time_ms: 2.408\n",
      "  timestamp: 1624524942\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         223.802</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  30.255</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-55-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 43.4999999999999\n",
      "  episode_reward_mean: 30.74399999999991\n",
      "  episode_reward_min: 7.499999999999952\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2025\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4954160153865814\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009320042096078396\n",
      "          model: {}\n",
      "          policy_loss: -0.029491031542420387\n",
      "          total_loss: 54.5516471862793\n",
      "          vf_explained_var: 0.4740511476993561\n",
      "          vf_loss: 54.57169723510742\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.5262089967727661\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011583560146391392\n",
      "          model: {}\n",
      "          policy_loss: -0.030432110652327538\n",
      "          total_loss: 5.1061296463012695\n",
      "          vf_explained_var: 0.14167535305023193\n",
      "          vf_loss: 5.124834060668945\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.800000000000004\n",
      "    ram_util_percent: 67.66666666666666\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 46.5\n",
      "    policy2: 68.10000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.69\n",
      "    policy2: 0.05400000000000119\n",
      "  policy_reward_min:\n",
      "    policy1: -49.5\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17578245906409393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09877540140541875\n",
      "    mean_inference_ms: 1.33154209785714\n",
      "    mean_raw_obs_processing_ms: 0.6487444873640281\n",
      "  time_since_restore: 233.28365230560303\n",
      "  time_this_iter_s: 4.794803142547607\n",
      "  time_total_s: 233.28365230560303\n",
      "  timers:\n",
      "    learn_throughput: 935.557\n",
      "    learn_time_ms: 4275.528\n",
      "    load_throughput: 1608522.943\n",
      "    load_time_ms: 2.487\n",
      "    sample_throughput: 11332.276\n",
      "    sample_time_ms: 352.974\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1624524951\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         233.284</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">  30.744</td><td style=\"text-align: right;\">                43.5</td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-56-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.899999999999906\n",
      "  episode_reward_mean: 32.897999999999904\n",
      "  episode_reward_min: 19.199999999999896\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2100\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.49919164180755615\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011284520849585533\n",
      "          model: {}\n",
      "          policy_loss: -0.03782995045185089\n",
      "          total_loss: 71.1130599975586\n",
      "          vf_explained_var: 0.34547683596611023\n",
      "          vf_loss: 71.13946533203125\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.506411075592041\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009664587676525116\n",
      "          model: {}\n",
      "          policy_loss: -0.026994599029421806\n",
      "          total_loss: 13.2347993850708\n",
      "          vf_explained_var: 0.3099183142185211\n",
      "          vf_loss: 13.252008438110352\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.285714285714285\n",
      "    ram_util_percent: 67.7\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 42.8\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.69\n",
      "    policy2: 0.2080000000000009\n",
      "  policy_reward_min:\n",
      "    policy1: -6.5\n",
      "    policy2: -8.899999999999984\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1762277608024443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09893365316129063\n",
      "    mean_inference_ms: 1.33129544796674\n",
      "    mean_raw_obs_processing_ms: 0.6502717371677855\n",
      "  time_since_restore: 242.82092332839966\n",
      "  time_this_iter_s: 4.808897018432617\n",
      "  time_total_s: 242.82092332839966\n",
      "  timers:\n",
      "    learn_throughput: 932.32\n",
      "    learn_time_ms: 4290.373\n",
      "    load_throughput: 1607675.192\n",
      "    load_time_ms: 2.488\n",
      "    sample_throughput: 11241.514\n",
      "    sample_time_ms: 355.824\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1624524961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         242.821</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  32.898</td><td style=\"text-align: right;\">                45.9</td><td style=\"text-align: right;\">                19.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-56-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.699999999999896\n",
      "  episode_reward_mean: 31.58399999999991\n",
      "  episode_reward_min: 12.299999999999967\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2200\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.46220383048057556\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008790775202214718\n",
      "          model: {}\n",
      "          policy_loss: -0.02926849201321602\n",
      "          total_loss: 59.04888153076172\n",
      "          vf_explained_var: 0.3530977964401245\n",
      "          vf_loss: 59.06924057006836\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.47448912262916565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00967141892760992\n",
      "          model: {}\n",
      "          policy_loss: -0.026778675615787506\n",
      "          total_loss: 16.0622615814209\n",
      "          vf_explained_var: 0.24281243979930878\n",
      "          vf_loss: 16.079246520996094\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.17142857142858\n",
      "    ram_util_percent: 67.65714285714286\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 31.799999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.21\n",
      "    policy2: 1.3740000000000014\n",
      "  policy_reward_min:\n",
      "    policy1: -14.0\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17625650377765006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09892738718371577\n",
      "    mean_inference_ms: 1.3315711464829545\n",
      "    mean_raw_obs_processing_ms: 0.6516239991338929\n",
      "  time_since_restore: 252.5169541835785\n",
      "  time_this_iter_s: 4.951074838638306\n",
      "  time_total_s: 252.5169541835785\n",
      "  timers:\n",
      "    learn_throughput: 924.658\n",
      "    learn_time_ms: 4325.924\n",
      "    load_throughput: 1569078.598\n",
      "    load_time_ms: 2.549\n",
      "    sample_throughput: 10940.662\n",
      "    sample_time_ms: 365.609\n",
      "    update_time_ms: 2.458\n",
      "  timestamp: 1624524971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         252.517</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  31.584</td><td style=\"text-align: right;\">                47.7</td><td style=\"text-align: right;\">                12.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-56-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.699999999999896\n",
      "  episode_reward_mean: 31.42799999999992\n",
      "  episode_reward_min: 11.699999999999946\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2275\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4620126485824585\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012693880125880241\n",
      "          model: {}\n",
      "          policy_loss: -0.03664546459913254\n",
      "          total_loss: 53.39973068237305\n",
      "          vf_explained_var: 0.49103453755378723\n",
      "          vf_loss: 53.42353057861328\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.4446917176246643\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0076546622440218925\n",
      "          model: {}\n",
      "          policy_loss: -0.02407807856798172\n",
      "          total_loss: 15.614127159118652\n",
      "          vf_explained_var: 0.23415397107601166\n",
      "          vf_loss: 15.630454063415527\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.633333333333336\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 38.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.79\n",
      "    policy2: 1.6380000000000026\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17589637478985246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0988247533089475\n",
      "    mean_inference_ms: 1.3320297421202079\n",
      "    mean_raw_obs_processing_ms: 0.6527021911366506\n",
      "  time_since_restore: 261.9815454483032\n",
      "  time_this_iter_s: 4.64864706993103\n",
      "  time_total_s: 261.9815454483032\n",
      "  timers:\n",
      "    learn_throughput: 915.638\n",
      "    learn_time_ms: 4368.538\n",
      "    load_throughput: 1536234.411\n",
      "    load_time_ms: 2.604\n",
      "    sample_throughput: 10839.39\n",
      "    sample_time_ms: 369.024\n",
      "    update_time_ms: 2.45\n",
      "  timestamp: 1624524980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         261.982</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  31.428</td><td style=\"text-align: right;\">                47.7</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-56-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.39999999999991\n",
      "  episode_reward_mean: 31.550999999999913\n",
      "  episode_reward_min: 15.29999999999998\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2350\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.42946454882621765\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00793248601257801\n",
      "          model: {}\n",
      "          policy_loss: -0.024854931980371475\n",
      "          total_loss: 84.31121063232422\n",
      "          vf_explained_var: 0.5293190479278564\n",
      "          vf_loss: 84.32803344726562\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.433282732963562\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006865167524665594\n",
      "          model: {}\n",
      "          policy_loss: -0.017328156158328056\n",
      "          total_loss: 47.22526168823242\n",
      "          vf_explained_var: 0.4216717481613159\n",
      "          vf_loss: 47.23564147949219\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.042857142857144\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.5\n",
      "    policy2: 69.2\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.065\n",
      "    policy2: 3.485999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -45.5\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17557125527363854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09869815704702964\n",
      "    mean_inference_ms: 1.3296510662921879\n",
      "    mean_raw_obs_processing_ms: 0.6529796767158977\n",
      "  time_since_restore: 271.2515106201172\n",
      "  time_this_iter_s: 4.534055948257446\n",
      "  time_total_s: 271.2515106201172\n",
      "  timers:\n",
      "    learn_throughput: 916.559\n",
      "    learn_time_ms: 4364.15\n",
      "    load_throughput: 1550030.119\n",
      "    load_time_ms: 2.581\n",
      "    sample_throughput: 10809.292\n",
      "    sample_time_ms: 370.052\n",
      "    update_time_ms: 2.456\n",
      "  timestamp: 1624524990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         271.252</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  31.551</td><td style=\"text-align: right;\">                47.4</td><td style=\"text-align: right;\">                15.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-56-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.39999999999991\n",
      "  episode_reward_mean: 30.452999999999914\n",
      "  episode_reward_min: 11.999999999999932\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2425\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.44497305154800415\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009061883203685284\n",
      "          model: {}\n",
      "          policy_loss: -0.027709050104022026\n",
      "          total_loss: 59.62226104736328\n",
      "          vf_explained_var: 0.46265682578086853\n",
      "          vf_loss: 59.64079284667969\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.42327675223350525\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008006012067198753\n",
      "          model: {}\n",
      "          policy_loss: -0.022609097883105278\n",
      "          total_loss: 17.18334197998047\n",
      "          vf_explained_var: 0.19386650621891022\n",
      "          vf_loss: 17.197847366333008\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.716666666666665\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 47.5\n",
      "    policy2: 64.8\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.76\n",
      "    policy2: 1.6929999999999996\n",
      "  policy_reward_min:\n",
      "    policy1: -39.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17500932116291967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09842208822863695\n",
      "    mean_inference_ms: 1.3252807925715338\n",
      "    mean_raw_obs_processing_ms: 0.651477967514704\n",
      "  time_since_restore: 280.06636095046997\n",
      "  time_this_iter_s: 4.381019115447998\n",
      "  time_total_s: 280.06636095046997\n",
      "  timers:\n",
      "    learn_throughput: 928.241\n",
      "    learn_time_ms: 4309.225\n",
      "    load_throughput: 1575784.125\n",
      "    load_time_ms: 2.538\n",
      "    sample_throughput: 11159.025\n",
      "    sample_time_ms: 358.454\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1624524998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         280.066</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">  30.453</td><td style=\"text-align: right;\">                47.4</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-56-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.19999999999992\n",
      "  episode_reward_mean: 31.199999999999914\n",
      "  episode_reward_min: 15.599999999999918\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2500\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.44672054052352905\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009240994229912758\n",
      "          model: {}\n",
      "          policy_loss: -0.030118603259325027\n",
      "          total_loss: 67.60299682617188\n",
      "          vf_explained_var: 0.3419913053512573\n",
      "          vf_loss: 67.62374877929688\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.4110371768474579\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0072503359988331795\n",
      "          model: {}\n",
      "          policy_loss: -0.019559714943170547\n",
      "          total_loss: 12.62038516998291\n",
      "          vf_explained_var: 0.28429844975471497\n",
      "          vf_loss: 12.63260555267334\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.733333333333334\n",
      "    ram_util_percent: 67.53333333333335\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 47.5\n",
      "    policy2: 54.89999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.825\n",
      "    policy2: 2.375\n",
      "  policy_reward_min:\n",
      "    policy1: -31.5\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17452660366999914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09808752717072554\n",
      "    mean_inference_ms: 1.3177242370899576\n",
      "    mean_raw_obs_processing_ms: 0.6494537959954042\n",
      "  time_since_restore: 288.8836178779602\n",
      "  time_this_iter_s: 4.481771945953369\n",
      "  time_total_s: 288.8836178779602\n",
      "  timers:\n",
      "    learn_throughput: 941.657\n",
      "    learn_time_ms: 4247.832\n",
      "    load_throughput: 1590317.737\n",
      "    load_time_ms: 2.515\n",
      "    sample_throughput: 11488.486\n",
      "    sample_time_ms: 348.175\n",
      "    update_time_ms: 2.391\n",
      "  timestamp: 1624525007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         288.884</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">    31.2</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                15.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-56-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.3999999999999\n",
      "  episode_reward_mean: 31.244999999999912\n",
      "  episode_reward_min: 3.9000000000000035\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2600\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.44735029339790344\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009497132152318954\n",
      "          model: {}\n",
      "          policy_loss: -0.029139278456568718\n",
      "          total_loss: 48.774818420410156\n",
      "          vf_explained_var: 0.4081743061542511\n",
      "          vf_loss: 48.79433822631836\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.38969045877456665\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008898007683455944\n",
      "          model: {}\n",
      "          policy_loss: -0.02757580764591694\n",
      "          total_loss: 7.056360244750977\n",
      "          vf_explained_var: 0.3804784417152405\n",
      "          vf_loss: 7.074925899505615\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.266666666666666\n",
      "    ram_util_percent: 67.53333333333333\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 50.5\n",
      "    policy2: 47.19999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.925\n",
      "    policy2: 2.3199999999999994\n",
      "  policy_reward_min:\n",
      "    policy1: -18.0\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17375568920463047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09768467195878187\n",
      "    mean_inference_ms: 1.3116161331978922\n",
      "    mean_raw_obs_processing_ms: 0.6474507671268843\n",
      "  time_since_restore: 297.8417479991913\n",
      "  time_this_iter_s: 4.376501083374023\n",
      "  time_total_s: 297.8417479991913\n",
      "  timers:\n",
      "    learn_throughput: 955.613\n",
      "    learn_time_ms: 4185.793\n",
      "    load_throughput: 1617190.172\n",
      "    load_time_ms: 2.473\n",
      "    sample_throughput: 11881.301\n",
      "    sample_time_ms: 336.663\n",
      "    update_time_ms: 2.385\n",
      "  timestamp: 1624525016\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         297.842</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">  31.245</td><td style=\"text-align: right;\">                50.4</td><td style=\"text-align: right;\">                 3.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-57-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.9999999999999\n",
      "  episode_reward_mean: 32.17799999999991\n",
      "  episode_reward_min: 6.8999999999999275\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2675\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4300624132156372\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009933280758559704\n",
      "          model: {}\n",
      "          policy_loss: -0.032282616943120956\n",
      "          total_loss: 53.065765380859375\n",
      "          vf_explained_var: 0.48740923404693604\n",
      "          vf_loss: 53.08799362182617\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.38092008233070374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008224285207688808\n",
      "          model: {}\n",
      "          policy_loss: -0.027118675410747528\n",
      "          total_loss: 9.986454010009766\n",
      "          vf_explained_var: 0.2789199650287628\n",
      "          vf_loss: 10.00524616241455\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.64285714285713\n",
      "    ram_util_percent: 67.51428571428572\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 47.0\n",
      "    policy2: 33.999999999999986\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.485\n",
      "    policy2: 1.6930000000000012\n",
      "  policy_reward_min:\n",
      "    policy1: -7.0\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17281868543018178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09729483142279637\n",
      "    mean_inference_ms: 1.3077588560450244\n",
      "    mean_raw_obs_processing_ms: 0.6455021412760367\n",
      "  time_since_restore: 306.8513181209564\n",
      "  time_this_iter_s: 4.57335901260376\n",
      "  time_total_s: 306.8513181209564\n",
      "  timers:\n",
      "    learn_throughput: 964.175\n",
      "    learn_time_ms: 4148.625\n",
      "    load_throughput: 1666307.394\n",
      "    load_time_ms: 2.401\n",
      "    sample_throughput: 12175.877\n",
      "    sample_time_ms: 328.518\n",
      "    update_time_ms: 2.361\n",
      "  timestamp: 1624525025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         306.851</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  32.178</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">                 6.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-57-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.3999999999999\n",
      "  episode_reward_mean: 32.69999999999992\n",
      "  episode_reward_min: 17.09999999999996\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2750\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.402817964553833\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008575302548706532\n",
      "          model: {}\n",
      "          policy_loss: -0.03071298636496067\n",
      "          total_loss: 56.4021110534668\n",
      "          vf_explained_var: 0.48138228058815\n",
      "          vf_loss: 56.42414093017578\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.35411858558654785\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008890919387340546\n",
      "          model: {}\n",
      "          policy_loss: -0.02767309360206127\n",
      "          total_loss: 7.4385457038879395\n",
      "          vf_explained_var: 0.17849840223789215\n",
      "          vf_loss: 7.457216739654541\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.128571428571426\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 33.999999999999986\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.38\n",
      "    policy2: 2.320000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -7.0\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17240615553435382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09707793987695801\n",
      "    mean_inference_ms: 1.3044485418359781\n",
      "    mean_raw_obs_processing_ms: 0.6446416175987246\n",
      "  time_since_restore: 316.2519519329071\n",
      "  time_this_iter_s: 4.739300966262817\n",
      "  time_total_s: 316.2519519329071\n",
      "  timers:\n",
      "    learn_throughput: 960.557\n",
      "    learn_time_ms: 4164.252\n",
      "    load_throughput: 1669790.097\n",
      "    load_time_ms: 2.396\n",
      "    sample_throughput: 12269.681\n",
      "    sample_time_ms: 326.007\n",
      "    update_time_ms: 2.341\n",
      "  timestamp: 1624525035\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         316.252</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">    32.7</td><td style=\"text-align: right;\">                47.4</td><td style=\"text-align: right;\">                17.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-57-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.1999999999999\n",
      "  episode_reward_mean: 33.91499999999991\n",
      "  episode_reward_min: 18.59999999999991\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2825\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.41297051310539246\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009383268654346466\n",
      "          model: {}\n",
      "          policy_loss: -0.027733489871025085\n",
      "          total_loss: 60.802494049072266\n",
      "          vf_explained_var: 0.5030959844589233\n",
      "          vf_loss: 60.82072448730469\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.3423934876918793\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007307581137865782\n",
      "          model: {}\n",
      "          policy_loss: -0.020343316718935966\n",
      "          total_loss: 14.309344291687012\n",
      "          vf_explained_var: 0.23174484074115753\n",
      "          vf_loss: 14.322286605834961\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.7\n",
      "    ram_util_percent: 67.51666666666667\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 46.0\n",
      "    policy2: 59.3\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.505\n",
      "    policy2: 4.41\n",
      "  policy_reward_min:\n",
      "    policy1: -35.0\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17204140353218406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09689323144427917\n",
      "    mean_inference_ms: 1.3014366637157426\n",
      "    mean_raw_obs_processing_ms: 0.6436429115643396\n",
      "  time_since_restore: 325.55698227882385\n",
      "  time_this_iter_s: 4.673952102661133\n",
      "  time_total_s: 325.55698227882385\n",
      "  timers:\n",
      "    learn_throughput: 950.007\n",
      "    learn_time_ms: 4210.497\n",
      "    load_throughput: 1670970.878\n",
      "    load_time_ms: 2.394\n",
      "    sample_throughput: 12167.42\n",
      "    sample_time_ms: 328.747\n",
      "    update_time_ms: 2.348\n",
      "  timestamp: 1624525044\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         325.557</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">  33.915</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                18.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-57-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.1999999999999\n",
      "  episode_reward_mean: 34.14899999999991\n",
      "  episode_reward_min: 14.999999999999938\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2900\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4195472002029419\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009106173180043697\n",
      "          model: {}\n",
      "          policy_loss: -0.03179044649004936\n",
      "          total_loss: 57.41215133666992\n",
      "          vf_explained_var: 0.36610138416290283\n",
      "          vf_loss: 57.434722900390625\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.3394518196582794\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007198273669928312\n",
      "          model: {}\n",
      "          policy_loss: -0.022834451869130135\n",
      "          total_loss: 21.24248504638672\n",
      "          vf_explained_var: 0.19909052550792694\n",
      "          vf_loss: 21.258031845092773\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_agent_steps_trained: 584000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.7\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 46.0\n",
      "    policy2: 27.399999999999963\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.815\n",
      "    policy2: 5.334\n",
      "  policy_reward_min:\n",
      "    policy1: 4.0\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17190357216416305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09674009231517014\n",
      "    mean_inference_ms: 1.2967153875328012\n",
      "    mean_raw_obs_processing_ms: 0.6426982917494761\n",
      "  time_since_restore: 334.4310200214386\n",
      "  time_this_iter_s: 4.339708089828491\n",
      "  time_total_s: 334.4310200214386\n",
      "  timers:\n",
      "    learn_throughput: 948.955\n",
      "    learn_time_ms: 4215.161\n",
      "    load_throughput: 1668577.794\n",
      "    load_time_ms: 2.397\n",
      "    sample_throughput: 12133.33\n",
      "    sample_time_ms: 329.67\n",
      "    update_time_ms: 2.357\n",
      "  timestamp: 1624525053\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         334.431</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  34.149</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-57-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.49999999999993\n",
      "  episode_reward_mean: 31.904999999999912\n",
      "  episode_reward_min: 9.899999999999986\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3000\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4204123318195343\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008523418568074703\n",
      "          model: {}\n",
      "          policy_loss: -0.0244015883654356\n",
      "          total_loss: 54.88983917236328\n",
      "          vf_explained_var: 0.3839063048362732\n",
      "          vf_loss: 54.90561294555664\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.3229769766330719\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009307971224188805\n",
      "          model: {}\n",
      "          policy_loss: -0.026552844792604446\n",
      "          total_loss: 10.16329288482666\n",
      "          vf_explained_var: 0.2718312740325928\n",
      "          vf_loss: 10.180419921875\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.93333333333334\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 42.800000000000004\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.285\n",
      "    policy2: 5.620000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -30.5\n",
      "    policy2: -8.89999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17124447856054645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0964053649118149\n",
      "    mean_inference_ms: 1.2912987649932899\n",
      "    mean_raw_obs_processing_ms: 0.6409644758347623\n",
      "  time_since_restore: 343.43498492240906\n",
      "  time_this_iter_s: 4.497093915939331\n",
      "  time_total_s: 343.43498492240906\n",
      "  timers:\n",
      "    learn_throughput: 947.907\n",
      "    learn_time_ms: 4219.825\n",
      "    load_throughput: 1703460.894\n",
      "    load_time_ms: 2.348\n",
      "    sample_throughput: 12135.74\n",
      "    sample_time_ms: 329.605\n",
      "    update_time_ms: 2.342\n",
      "  timestamp: 1624525062\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         343.435</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  31.905</td><td style=\"text-align: right;\">                46.5</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-57-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 46.1999999999999\n",
      "  episode_reward_mean: 32.76899999999992\n",
      "  episode_reward_min: 6.899999999999967\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3075\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3990224599838257\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010732964612543583\n",
      "          model: {}\n",
      "          policy_loss: -0.03383614122867584\n",
      "          total_loss: 50.62503433227539\n",
      "          vf_explained_var: 0.5071938633918762\n",
      "          vf_loss: 50.64799880981445\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.3109830319881439\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008884313516318798\n",
      "          model: {}\n",
      "          policy_loss: -0.025721069425344467\n",
      "          total_loss: 9.472145080566406\n",
      "          vf_explained_var: 0.4100339114665985\n",
      "          vf_loss: 9.488869667053223\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.583333333333336\n",
      "    ram_util_percent: 67.48333333333333\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 43.0\n",
      "    policy2: 63.7\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.005\n",
      "    policy2: 6.763999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -41.5\n",
      "    policy2: -5.599999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17075452193211496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0962422916610709\n",
      "    mean_inference_ms: 1.2911142463536456\n",
      "    mean_raw_obs_processing_ms: 0.6404889022639633\n",
      "  time_since_restore: 352.74889039993286\n",
      "  time_this_iter_s: 4.685326814651489\n",
      "  time_total_s: 352.74889039993286\n",
      "  timers:\n",
      "    learn_throughput: 942.477\n",
      "    learn_time_ms: 4244.136\n",
      "    load_throughput: 1690927.746\n",
      "    load_time_ms: 2.366\n",
      "    sample_throughput: 11918.569\n",
      "    sample_time_ms: 335.611\n",
      "    update_time_ms: 2.365\n",
      "  timestamp: 1624525072\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         352.749</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  32.769</td><td style=\"text-align: right;\">                46.2</td><td style=\"text-align: right;\">                 6.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-58-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.2999999999999\n",
      "  episode_reward_mean: 31.00799999999992\n",
      "  episode_reward_min: -6.300000000000022\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3150\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3911486268043518\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013790293596684933\n",
      "          model: {}\n",
      "          policy_loss: -0.03714088723063469\n",
      "          total_loss: 62.425235748291016\n",
      "          vf_explained_var: 0.5381317734718323\n",
      "          vf_loss: 62.44841003417969\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.30204886198043823\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005476077552884817\n",
      "          model: {}\n",
      "          policy_loss: -0.01377972774207592\n",
      "          total_loss: 22.101749420166016\n",
      "          vf_explained_var: 0.36000099778175354\n",
      "          vf_loss: 22.109983444213867\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_agent_steps_trained: 632000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.84285714285714\n",
      "    ram_util_percent: 67.42857142857143\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.5\n",
      "    policy2: 69.2\n",
      "  policy_reward_mean:\n",
      "    policy1: 25.025\n",
      "    policy2: 5.983000000000003\n",
      "  policy_reward_min:\n",
      "    policy1: -51.5\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17056293308368972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09614877839815492\n",
      "    mean_inference_ms: 1.2896295806624938\n",
      "    mean_raw_obs_processing_ms: 0.6403696403103235\n",
      "  time_since_restore: 362.20006942749023\n",
      "  time_this_iter_s: 4.737663984298706\n",
      "  time_total_s: 362.20006942749023\n",
      "  timers:\n",
      "    learn_throughput: 941.434\n",
      "    learn_time_ms: 4248.838\n",
      "    load_throughput: 1652113.836\n",
      "    load_time_ms: 2.421\n",
      "    sample_throughput: 11908.392\n",
      "    sample_time_ms: 335.898\n",
      "    update_time_ms: 2.393\n",
      "  timestamp: 1624525081\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">           362.2</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">  31.008</td><td style=\"text-align: right;\">                51.3</td><td style=\"text-align: right;\">                -6.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-58-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.89999999999991\n",
      "  episode_reward_mean: 32.60399999999992\n",
      "  episode_reward_min: 10.499999999999938\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3225\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.398970365524292\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007967749610543251\n",
      "          model: {}\n",
      "          policy_loss: -0.0246298648416996\n",
      "          total_loss: 43.50117111206055\n",
      "          vf_explained_var: 0.5558068156242371\n",
      "          vf_loss: 43.51772689819336\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2938200533390045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008505159057676792\n",
      "          model: {}\n",
      "          policy_loss: -0.023386728018522263\n",
      "          total_loss: 9.89889907836914\n",
      "          vf_explained_var: 0.23629705607891083\n",
      "          vf_loss: 9.913674354553223\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_agent_steps_trained: 648000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.416666666666664\n",
      "    ram_util_percent: 67.46666666666665\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 43.5\n",
      "    policy2: 69.2\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.26\n",
      "    policy2: 4.344000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -51.5\n",
      "    policy2: -5.6\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17042125125207058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09607902870797234\n",
      "    mean_inference_ms: 1.2883272931356244\n",
      "    mean_raw_obs_processing_ms: 0.6401121471297018\n",
      "  time_since_restore: 371.58276176452637\n",
      "  time_this_iter_s: 4.666711091995239\n",
      "  time_total_s: 371.58276176452637\n",
      "  timers:\n",
      "    learn_throughput: 940.948\n",
      "    learn_time_ms: 4251.031\n",
      "    load_throughput: 1647166.658\n",
      "    load_time_ms: 2.428\n",
      "    sample_throughput: 11716.605\n",
      "    sample_time_ms: 341.396\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1624525091\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         371.583</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  32.604</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">                10.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 664000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-58-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.799999999999905\n",
      "  episode_reward_mean: 32.879999999999924\n",
      "  episode_reward_min: 14.399999999999912\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3300\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4005577266216278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007322691846638918\n",
      "          model: {}\n",
      "          policy_loss: -0.02241870015859604\n",
      "          total_loss: 64.23497772216797\n",
      "          vf_explained_var: 0.40708014369010925\n",
      "          vf_loss: 64.2499771118164\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2735278010368347\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005859344732016325\n",
      "          model: {}\n",
      "          policy_loss: -0.01580343022942543\n",
      "          total_loss: 19.363710403442383\n",
      "          vf_explained_var: 0.284008651971817\n",
      "          vf_loss: 19.37358283996582\n",
      "    num_agent_steps_sampled: 664000\n",
      "    num_agent_steps_trained: 664000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.683333333333334\n",
      "    ram_util_percent: 67.36666666666667\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 45.5\n",
      "    policy2: 48.29999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.04\n",
      "    policy2: 5.840000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -19.5\n",
      "    policy2: -4.5000000000000036\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1707084336088747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09615585709317549\n",
      "    mean_inference_ms: 1.28728538686316\n",
      "    mean_raw_obs_processing_ms: 0.6405648891884765\n",
      "  time_since_restore: 380.6951198577881\n",
      "  time_this_iter_s: 4.501497030258179\n",
      "  time_total_s: 380.6951198577881\n",
      "  timers:\n",
      "    learn_throughput: 936.889\n",
      "    learn_time_ms: 4269.451\n",
      "    load_throughput: 1533103.908\n",
      "    load_time_ms: 2.609\n",
      "    sample_throughput: 11537.098\n",
      "    sample_time_ms: 346.708\n",
      "    update_time_ms: 2.414\n",
      "  timestamp: 1624525100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         380.695</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">   32.88</td><td style=\"text-align: right;\">                49.8</td><td style=\"text-align: right;\">                14.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 680000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-58-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.599999999999916\n",
      "  episode_reward_mean: 31.073999999999913\n",
      "  episode_reward_min: 7.800000000000004\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3400\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4029429256916046\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009470459073781967\n",
      "          model: {}\n",
      "          policy_loss: -0.030153706669807434\n",
      "          total_loss: 45.164791107177734\n",
      "          vf_explained_var: 0.4209146797657013\n",
      "          vf_loss: 45.18535614013672\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.29860323667526245\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010967005044221878\n",
      "          model: {}\n",
      "          policy_loss: -0.0294883344322443\n",
      "          total_loss: 8.07991886138916\n",
      "          vf_explained_var: 0.21126684546470642\n",
      "          vf_loss: 8.098301887512207\n",
      "    num_agent_steps_sampled: 680000\n",
      "    num_agent_steps_trained: 680000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.942857142857147\n",
      "    ram_util_percent: 67.39999999999999\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 24.09999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.39\n",
      "    policy2: 3.683999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17047871518806879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09604726506542438\n",
      "    mean_inference_ms: 1.2858178800352207\n",
      "    mean_raw_obs_processing_ms: 0.6402546355487043\n",
      "  time_since_restore: 389.8511800765991\n",
      "  time_this_iter_s: 4.5379109382629395\n",
      "  time_total_s: 389.8511800765991\n",
      "  timers:\n",
      "    learn_throughput: 934.477\n",
      "    learn_time_ms: 4280.467\n",
      "    load_throughput: 1503604.23\n",
      "    load_time_ms: 2.66\n",
      "    sample_throughput: 11398.167\n",
      "    sample_time_ms: 350.934\n",
      "    update_time_ms: 2.425\n",
      "  timestamp: 1624525109\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         389.851</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">  31.074</td><td style=\"text-align: right;\">                51.6</td><td style=\"text-align: right;\">                 7.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 696000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-58-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.899999999999906\n",
      "  episode_reward_mean: 31.28699999999991\n",
      "  episode_reward_min: 7.800000000000004\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3475\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3852783143520355\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007708707824349403\n",
      "          model: {}\n",
      "          policy_loss: -0.024134930223226547\n",
      "          total_loss: 40.1035041809082\n",
      "          vf_explained_var: 0.5831844806671143\n",
      "          vf_loss: 40.11983871459961\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.27408352494239807\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007624047342687845\n",
      "          model: {}\n",
      "          policy_loss: -0.022313129156827927\n",
      "          total_loss: 3.859591007232666\n",
      "          vf_explained_var: 0.2032572627067566\n",
      "          vf_loss: 3.8741848468780518\n",
      "    num_agent_steps_sampled: 696000\n",
      "    num_agent_steps_trained: 696000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.266666666666666\n",
      "    ram_util_percent: 67.36666666666667\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 15.299999999999983\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.725\n",
      "    policy2: 2.5620000000000003\n",
      "  policy_reward_min:\n",
      "    policy1: -2.0\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1699002878466159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09583906754889282\n",
      "    mean_inference_ms: 1.2846752089131994\n",
      "    mean_raw_obs_processing_ms: 0.6395322409641132\n",
      "  time_since_restore: 398.8821771144867\n",
      "  time_this_iter_s: 4.524286985397339\n",
      "  time_total_s: 398.8821771144867\n",
      "  timers:\n",
      "    learn_throughput: 939.858\n",
      "    learn_time_ms: 4255.961\n",
      "    load_throughput: 1509611.287\n",
      "    load_time_ms: 2.65\n",
      "    sample_throughput: 11522.679\n",
      "    sample_time_ms: 347.141\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1624525118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         398.882</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">  31.287</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">                 7.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 712000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-58-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.199999999999896\n",
      "  episode_reward_mean: 32.27399999999991\n",
      "  episode_reward_min: 4.799999999999987\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3550\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3577474355697632\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008355529978871346\n",
      "          model: {}\n",
      "          policy_loss: -0.024146242067217827\n",
      "          total_loss: 52.31120681762695\n",
      "          vf_explained_var: 0.5799316167831421\n",
      "          vf_loss: 52.32689666748047\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.25996536016464233\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006190388463437557\n",
      "          model: {}\n",
      "          policy_loss: -0.020133979618549347\n",
      "          total_loss: 14.794435501098633\n",
      "          vf_explained_var: 0.21265506744384766\n",
      "          vf_loss: 14.808300018310547\n",
      "    num_agent_steps_sampled: 712000\n",
      "    num_agent_steps_trained: 712000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.942857142857143\n",
      "    ram_util_percent: 67.3\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.5\n",
      "    policy2: 27.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.635\n",
      "    policy2: 2.639000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: 3.5\n",
      "    policy2: -6.699999999999987\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16960361917393954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09569407918735079\n",
      "    mean_inference_ms: 1.2823761335214587\n",
      "    mean_raw_obs_processing_ms: 0.6389605361248324\n",
      "  time_since_restore: 408.1360230445862\n",
      "  time_this_iter_s: 4.670665979385376\n",
      "  time_total_s: 408.1360230445862\n",
      "  timers:\n",
      "    learn_throughput: 943.806\n",
      "    learn_time_ms: 4238.161\n",
      "    load_throughput: 1549228.581\n",
      "    load_time_ms: 2.582\n",
      "    sample_throughput: 11583.36\n",
      "    sample_time_ms: 345.323\n",
      "    update_time_ms: 2.418\n",
      "  timestamp: 1624525127\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         408.136</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">  32.274</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                 4.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 728000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-58-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.199999999999896\n",
      "  episode_reward_mean: 31.472999999999914\n",
      "  episode_reward_min: 2.0999999999999424\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3625\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.33933261036872864\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007521684747189283\n",
      "          model: {}\n",
      "          policy_loss: -0.02475820481777191\n",
      "          total_loss: 61.76087188720703\n",
      "          vf_explained_var: 0.4999489486217499\n",
      "          vf_loss: 61.77800750732422\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.26676833629608154\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006703291088342667\n",
      "          model: {}\n",
      "          policy_loss: -0.021345200017094612\n",
      "          total_loss: 20.798843383789062\n",
      "          vf_explained_var: 0.3054805397987366\n",
      "          vf_loss: 20.813404083251953\n",
      "    num_agent_steps_sampled: 728000\n",
      "    num_agent_steps_trained: 728000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.94285714285714\n",
      "    ram_util_percent: 67.37142857142855\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.5\n",
      "    policy2: 32.899999999999956\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.075\n",
      "    policy2: 3.397999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -12.5\n",
      "    policy2: -6.699999999999983\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16949770324808824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09564261407259936\n",
      "    mean_inference_ms: 1.2814817052802965\n",
      "    mean_raw_obs_processing_ms: 0.6386385023445967\n",
      "  time_since_restore: 417.61429619789124\n",
      "  time_this_iter_s: 4.785260200500488\n",
      "  time_total_s: 417.61429619789124\n",
      "  timers:\n",
      "    learn_throughput: 941.746\n",
      "    learn_time_ms: 4247.43\n",
      "    load_throughput: 1551205.296\n",
      "    load_time_ms: 2.579\n",
      "    sample_throughput: 11573.235\n",
      "    sample_time_ms: 345.625\n",
      "    update_time_ms: 2.399\n",
      "  timestamp: 1624525137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         417.614</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">  31.473</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                 2.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 744000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-59-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.09999999999991\n",
      "  episode_reward_mean: 34.27499999999992\n",
      "  episode_reward_min: 7.7999999999999226\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3700\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3324660658836365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0072219776920974255\n",
      "          model: {}\n",
      "          policy_loss: -0.023308219388127327\n",
      "          total_loss: 54.98113250732422\n",
      "          vf_explained_var: 0.5101220011711121\n",
      "          vf_loss: 54.99712371826172\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.24780316650867462\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005509535316377878\n",
      "          model: {}\n",
      "          policy_loss: -0.017710553482174873\n",
      "          total_loss: 23.67041015625\n",
      "          vf_explained_var: 0.26758790016174316\n",
      "          vf_loss: 23.682540893554688\n",
      "    num_agent_steps_sampled: 744000\n",
      "    num_agent_steps_trained: 744000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.642857142857142\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.5\n",
      "    policy2: 57.099999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 27.83\n",
      "    policy2: 6.444999999999996\n",
      "  policy_reward_min:\n",
      "    policy1: -31.0\n",
      "    policy2: -8.899999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16986541396108967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09575575731289802\n",
      "    mean_inference_ms: 1.2810667842111814\n",
      "    mean_raw_obs_processing_ms: 0.6392715210413445\n",
      "  time_since_restore: 427.2141447067261\n",
      "  time_this_iter_s: 4.7436957359313965\n",
      "  time_total_s: 427.2141447067261\n",
      "  timers:\n",
      "    learn_throughput: 931.574\n",
      "    learn_time_ms: 4293.81\n",
      "    load_throughput: 1646196.929\n",
      "    load_time_ms: 2.43\n",
      "    sample_throughput: 11496.324\n",
      "    sample_time_ms: 347.937\n",
      "    update_time_ms: 2.424\n",
      "  timestamp: 1624525147\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         427.214</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">  34.275</td><td style=\"text-align: right;\">                50.1</td><td style=\"text-align: right;\">                 7.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 760000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-59-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.09999999999991\n",
      "  episode_reward_mean: 33.47999999999991\n",
      "  episode_reward_min: 14.39999999999999\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3800\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3454279601573944\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0136724216863513\n",
      "          model: {}\n",
      "          policy_loss: -0.030320780351758003\n",
      "          total_loss: 43.55768966674805\n",
      "          vf_explained_var: 0.5542909502983093\n",
      "          vf_loss: 43.57417678833008\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.25329703092575073\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013872834853827953\n",
      "          model: {}\n",
      "          policy_loss: -0.027390142902731895\n",
      "          total_loss: 9.391124725341797\n",
      "          vf_explained_var: 0.3617810904979706\n",
      "          vf_loss: 9.411492347717285\n",
      "    num_agent_steps_sampled: 760000\n",
      "    num_agent_steps_trained: 760000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.357142857142854\n",
      "    ram_util_percent: 67.52857142857144\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 46.5\n",
      "    policy2: 60.399999999999984\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.07\n",
      "    policy2: 4.409999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -46.0\n",
      "    policy2: -4.499999999999987\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1697574349159968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09570751902238951\n",
      "    mean_inference_ms: 1.2803067846131495\n",
      "    mean_raw_obs_processing_ms: 0.6393324113163772\n",
      "  time_since_restore: 436.7044458389282\n",
      "  time_this_iter_s: 4.8013060092926025\n",
      "  time_total_s: 436.7044458389282\n",
      "  timers:\n",
      "    learn_throughput: 924.5\n",
      "    learn_time_ms: 4326.663\n",
      "    load_throughput: 1657516.474\n",
      "    load_time_ms: 2.413\n",
      "    sample_throughput: 11481.29\n",
      "    sample_time_ms: 348.393\n",
      "    update_time_ms: 2.433\n",
      "  timestamp: 1624525156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         436.704</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">   33.48</td><td style=\"text-align: right;\">                47.1</td><td style=\"text-align: right;\">                14.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 776000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-59-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.5999999999999\n",
      "  episode_reward_mean: 34.073999999999906\n",
      "  episode_reward_min: 1.4999999999999774\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3875\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3241569995880127\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0070323823019862175\n",
      "          model: {}\n",
      "          policy_loss: -0.02379133738577366\n",
      "          total_loss: 41.69140625\n",
      "          vf_explained_var: 0.6324723362922668\n",
      "          vf_loss: 41.70808792114258\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.27369552850723267\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019724808633327484\n",
      "          model: {}\n",
      "          policy_loss: -0.03195846453309059\n",
      "          total_loss: 4.269243240356445\n",
      "          vf_explained_var: 0.2059052288532257\n",
      "          vf_loss: 4.2912163734436035\n",
      "    num_agent_steps_sampled: 776000\n",
      "    num_agent_steps_trained: 776000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.82857142857143\n",
      "    ram_util_percent: 67.3\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 47.5\n",
      "    policy2: 27.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.93\n",
      "    policy2: 2.143999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -6.699999999999983\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1693767684273319\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09558947002912724\n",
      "    mean_inference_ms: 1.2806487270158868\n",
      "    mean_raw_obs_processing_ms: 0.6392242898622235\n",
      "  time_since_restore: 446.3455469608307\n",
      "  time_this_iter_s: 4.788665056228638\n",
      "  time_total_s: 446.3455469608307\n",
      "  timers:\n",
      "    learn_throughput: 912.606\n",
      "    learn_time_ms: 4383.053\n",
      "    load_throughput: 1602163.566\n",
      "    load_time_ms: 2.497\n",
      "    sample_throughput: 11336.268\n",
      "    sample_time_ms: 352.85\n",
      "    update_time_ms: 2.431\n",
      "  timestamp: 1624525166\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         446.346</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">  34.074</td><td style=\"text-align: right;\">                48.6</td><td style=\"text-align: right;\">                 1.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 792000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-59-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.59999999999991\n",
      "  episode_reward_mean: 35.078999999999915\n",
      "  episode_reward_min: 1.4999999999999774\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3950\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.29948750138282776\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006608506198972464\n",
      "          model: {}\n",
      "          policy_loss: -0.01910887286067009\n",
      "          total_loss: 37.096317291259766\n",
      "          vf_explained_var: 0.6903815865516663\n",
      "          vf_loss: 37.108734130859375\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.25495344400405884\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015316358767449856\n",
      "          model: {}\n",
      "          policy_loss: -0.029173873364925385\n",
      "          total_loss: 6.03762674331665\n",
      "          vf_explained_var: 0.2769571542739868\n",
      "          vf_loss: 6.059046745300293\n",
      "    num_agent_steps_sampled: 792000\n",
      "    num_agent_steps_trained: 792000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.12857142857143\n",
      "    ram_util_percent: 67.34285714285714\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 28.499999999999996\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.22\n",
      "    policy2: 2.858999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -5.0\n",
      "    policy2: -6.699999999999983\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16932045314179325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0955634737263713\n",
      "    mean_inference_ms: 1.2801569921037457\n",
      "    mean_raw_obs_processing_ms: 0.6393585809782093\n",
      "  time_since_restore: 455.576580286026\n",
      "  time_this_iter_s: 4.645648002624512\n",
      "  time_total_s: 455.576580286026\n",
      "  timers:\n",
      "    learn_throughput: 913.618\n",
      "    learn_time_ms: 4378.197\n",
      "    load_throughput: 1557570.604\n",
      "    load_time_ms: 2.568\n",
      "    sample_throughput: 11256.439\n",
      "    sample_time_ms: 355.352\n",
      "    update_time_ms: 2.417\n",
      "  timestamp: 1624525175\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         455.577</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  35.079</td><td style=\"text-align: right;\">                48.6</td><td style=\"text-align: right;\">                 1.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 808000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-59-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 52.79999999999991\n",
      "  episode_reward_mean: 34.18499999999992\n",
      "  episode_reward_min: 15.899999999999894\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4025\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3147125542163849\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008687067776918411\n",
      "          model: {}\n",
      "          policy_loss: -0.027713540941476822\n",
      "          total_loss: 43.66475296020508\n",
      "          vf_explained_var: 0.644365668296814\n",
      "          vf_loss: 43.683677673339844\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.26116618514060974\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016135016456246376\n",
      "          model: {}\n",
      "          policy_loss: -0.031140219420194626\n",
      "          total_loss: 5.470158100128174\n",
      "          vf_explained_var: 0.3033427298069\n",
      "          vf_loss: 5.493129730224609\n",
      "    num_agent_steps_sampled: 808000\n",
      "    num_agent_steps_trained: 808000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.442857142857147\n",
      "    ram_util_percent: 67.47142857142858\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 39.49999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.03\n",
      "    policy2: 2.1549999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -17.0\n",
      "    policy2: -5.6\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16920087599966036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0955068764121377\n",
      "    mean_inference_ms: 1.2793581013523778\n",
      "    mean_raw_obs_processing_ms: 0.6389796904701863\n",
      "  time_since_restore: 464.71590518951416\n",
      "  time_this_iter_s: 4.563438892364502\n",
      "  time_total_s: 464.71590518951416\n",
      "  timers:\n",
      "    learn_throughput: 920.084\n",
      "    learn_time_ms: 4347.428\n",
      "    load_throughput: 1557281.452\n",
      "    load_time_ms: 2.569\n",
      "    sample_throughput: 11355.454\n",
      "    sample_time_ms: 352.254\n",
      "    update_time_ms: 2.421\n",
      "  timestamp: 1624525184\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         464.716</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">  34.185</td><td style=\"text-align: right;\">                52.8</td><td style=\"text-align: right;\">                15.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 824000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_10-59-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 52.79999999999991\n",
      "  episode_reward_mean: 34.57199999999991\n",
      "  episode_reward_min: 20.699999999999907\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4100\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.31303897500038147\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006495901849120855\n",
      "          model: {}\n",
      "          policy_loss: -0.0222522784024477\n",
      "          total_loss: 43.90480422973633\n",
      "          vf_explained_var: 0.6080071926116943\n",
      "          vf_loss: 43.92047882080078\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.27914172410964966\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01406316552311182\n",
      "          model: {}\n",
      "          policy_loss: -0.02581305429339409\n",
      "          total_loss: 6.742327690124512\n",
      "          vf_explained_var: 0.2200237512588501\n",
      "          vf_loss: 6.761022567749023\n",
      "    num_agent_steps_sampled: 824000\n",
      "    num_agent_steps_trained: 824000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.27142857142857\n",
      "    ram_util_percent: 67.47142857142858\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 48.3\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.9\n",
      "    policy2: 2.6719999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -16.5\n",
      "    policy2: -5.599999999999992\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1693307122604036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09549999759098457\n",
      "    mean_inference_ms: 1.277212206653976\n",
      "    mean_raw_obs_processing_ms: 0.6386464081299384\n",
      "  time_since_restore: 473.8085789680481\n",
      "  time_this_iter_s: 4.532634973526001\n",
      "  time_total_s: 473.8085789680481\n",
      "  timers:\n",
      "    learn_throughput: 930.2\n",
      "    learn_time_ms: 4300.152\n",
      "    load_throughput: 1564338.356\n",
      "    load_time_ms: 2.557\n",
      "    sample_throughput: 11466.394\n",
      "    sample_time_ms: 348.846\n",
      "    update_time_ms: 2.429\n",
      "  timestamp: 1624525194\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         473.809</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">  34.572</td><td style=\"text-align: right;\">                52.8</td><td style=\"text-align: right;\">                20.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 840000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-00-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.29999999999991\n",
      "  episode_reward_mean: 34.49099999999991\n",
      "  episode_reward_min: -33.000000000000036\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4200\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.30664438009262085\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008208895102143288\n",
      "          model: {}\n",
      "          policy_loss: -0.024369115009903908\n",
      "          total_loss: 42.06644058227539\n",
      "          vf_explained_var: 0.5696028470993042\n",
      "          vf_loss: 42.08250045776367\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.27642345428466797\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012947984039783478\n",
      "          model: {}\n",
      "          policy_loss: -0.030250122770667076\n",
      "          total_loss: 5.773519039154053\n",
      "          vf_explained_var: 0.24822552502155304\n",
      "          vf_loss: 5.7939372062683105\n",
      "    num_agent_steps_sampled: 840000\n",
      "    num_agent_steps_trained: 840000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.342857142857138\n",
      "    ram_util_percent: 67.67142857142856\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 19.69999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.82\n",
      "    policy2: 1.6709999999999994\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1691155636593995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09539451283200227\n",
      "    mean_inference_ms: 1.2759041083587082\n",
      "    mean_raw_obs_processing_ms: 0.6383063793854422\n",
      "  time_since_restore: 483.16644263267517\n",
      "  time_this_iter_s: 4.696215867996216\n",
      "  time_total_s: 483.16644263267517\n",
      "  timers:\n",
      "    learn_throughput: 932.468\n",
      "    learn_time_ms: 4289.689\n",
      "    load_throughput: 1539533.108\n",
      "    load_time_ms: 2.598\n",
      "    sample_throughput: 11559.3\n",
      "    sample_time_ms: 346.042\n",
      "    update_time_ms: 2.423\n",
      "  timestamp: 1624525203\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         483.166</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">  34.491</td><td style=\"text-align: right;\">                51.3</td><td style=\"text-align: right;\">                 -33</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 856000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-00-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.89999999999991\n",
      "  episode_reward_mean: 35.67899999999991\n",
      "  episode_reward_min: 4.200000000000008\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4275\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2875642478466034\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008471362292766571\n",
      "          model: {}\n",
      "          policy_loss: -0.025084299966692924\n",
      "          total_loss: 41.3225212097168\n",
      "          vf_explained_var: 0.6639360189437866\n",
      "          vf_loss: 41.33902359008789\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.25057777762413025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010779996402561665\n",
      "          model: {}\n",
      "          policy_loss: -0.026533858850598335\n",
      "          total_loss: 4.847839832305908\n",
      "          vf_explained_var: 0.28296810388565063\n",
      "          vf_loss: 4.8661885261535645\n",
      "    num_agent_steps_sampled: 856000\n",
      "    num_agent_steps_trained: 856000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.066666666666666\n",
      "    ram_util_percent: 67.7\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 19.69999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.315\n",
      "    policy2: 2.364\n",
      "  policy_reward_min:\n",
      "    policy1: -4.5\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16871101088186333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09525762543951694\n",
      "    mean_inference_ms: 1.276052111387794\n",
      "    mean_raw_obs_processing_ms: 0.6380145319149767\n",
      "  time_since_restore: 492.4290614128113\n",
      "  time_this_iter_s: 4.545041799545288\n",
      "  time_total_s: 492.4290614128113\n",
      "  timers:\n",
      "    learn_throughput: 940.561\n",
      "    learn_time_ms: 4252.782\n",
      "    load_throughput: 1551535.239\n",
      "    load_time_ms: 2.578\n",
      "    sample_throughput: 11584.138\n",
      "    sample_time_ms: 345.3\n",
      "    update_time_ms: 2.394\n",
      "  timestamp: 1624525212\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         492.429</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">  35.679</td><td style=\"text-align: right;\">                48.9</td><td style=\"text-align: right;\">                 4.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 872000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-00-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 48.299999999999905\n",
      "  episode_reward_mean: 34.34699999999991\n",
      "  episode_reward_min: 4.200000000000008\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4350\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.27596229314804077\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007240485865622759\n",
      "          model: {}\n",
      "          policy_loss: -0.02397489733994007\n",
      "          total_loss: 38.14629364013672\n",
      "          vf_explained_var: 0.7113078832626343\n",
      "          vf_loss: 38.162940979003906\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2506893575191498\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009027007035911083\n",
      "          model: {}\n",
      "          policy_loss: -0.025447098538279533\n",
      "          total_loss: 5.984738349914551\n",
      "          vf_explained_var: 0.25098225474357605\n",
      "          vf_loss: 6.003331661224365\n",
      "    num_agent_steps_sampled: 872000\n",
      "    num_agent_steps_trained: 872000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.250000000000004\n",
      "    ram_util_percent: 67.46666666666667\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 45.5\n",
      "    policy2: 17.499999999999993\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.235\n",
      "    policy2: 3.111999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -4.5\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16851249351700176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09515590101843077\n",
      "    mean_inference_ms: 1.274560757574951\n",
      "    mean_raw_obs_processing_ms: 0.6376549417401882\n",
      "  time_since_restore: 501.48084831237793\n",
      "  time_this_iter_s: 4.541166067123413\n",
      "  time_total_s: 501.48084831237793\n",
      "  timers:\n",
      "    learn_throughput: 943.703\n",
      "    learn_time_ms: 4238.622\n",
      "    load_throughput: 1570165.278\n",
      "    load_time_ms: 2.548\n",
      "    sample_throughput: 11710.436\n",
      "    sample_time_ms: 341.576\n",
      "    update_time_ms: 2.401\n",
      "  timestamp: 1624525221\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         501.481</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">  34.347</td><td style=\"text-align: right;\">                48.3</td><td style=\"text-align: right;\">                 4.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 888000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-00-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.1999999999999\n",
      "  episode_reward_mean: 35.05499999999991\n",
      "  episode_reward_min: 12.29999999999995\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4425\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2788536250591278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007655322086066008\n",
      "          model: {}\n",
      "          policy_loss: -0.023204611614346504\n",
      "          total_loss: 33.8001708984375\n",
      "          vf_explained_var: 0.7406017184257507\n",
      "          vf_loss: 33.81562423706055\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.24452516436576843\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00971550028771162\n",
      "          model: {}\n",
      "          policy_loss: -0.021339574828743935\n",
      "          total_loss: 6.160281181335449\n",
      "          vf_explained_var: 0.24663661420345306\n",
      "          vf_loss: 6.174243450164795\n",
      "    num_agent_steps_sampled: 888000\n",
      "    num_agent_steps_trained: 888000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.18333333333333\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 27.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.48\n",
      "    policy2: 4.575\n",
      "  policy_reward_min:\n",
      "    policy1: -14.0\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16830283758131828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09504542384751836\n",
      "    mean_inference_ms: 1.2730309142901435\n",
      "    mean_raw_obs_processing_ms: 0.6369860706008386\n",
      "  time_since_restore: 510.4184880256653\n",
      "  time_this_iter_s: 4.499073028564453\n",
      "  time_total_s: 510.4184880256653\n",
      "  timers:\n",
      "    learn_throughput: 947.654\n",
      "    learn_time_ms: 4220.949\n",
      "    load_throughput: 1578022.16\n",
      "    load_time_ms: 2.535\n",
      "    sample_throughput: 11796.748\n",
      "    sample_time_ms: 339.076\n",
      "    update_time_ms: 2.387\n",
      "  timestamp: 1624525230\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         510.418</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">  35.055</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                12.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 904000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-00-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.1999999999999\n",
      "  episode_reward_mean: 35.94599999999991\n",
      "  episode_reward_min: 11.999999999999922\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4500\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2914312779903412\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006899161729961634\n",
      "          model: {}\n",
      "          policy_loss: -0.022130368277430534\n",
      "          total_loss: 52.92919158935547\n",
      "          vf_explained_var: 0.5927540063858032\n",
      "          vf_loss: 52.9443359375\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2550762891769409\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010542658157646656\n",
      "          model: {}\n",
      "          policy_loss: -0.025997217744588852\n",
      "          total_loss: 11.8694486618042\n",
      "          vf_explained_var: 0.22677062451839447\n",
      "          vf_loss: 11.887439727783203\n",
      "    num_agent_steps_sampled: 904000\n",
      "    num_agent_steps_trained: 904000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.185714285714283\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 47.0\n",
      "    policy2: 54.89999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.415\n",
      "    policy2: 4.530999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -39.0\n",
      "    policy2: -4.500000000000001\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16838334590617643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09500249533311741\n",
      "    mean_inference_ms: 1.2704020554867688\n",
      "    mean_raw_obs_processing_ms: 0.6364192548658499\n",
      "  time_since_restore: 519.2586629390717\n",
      "  time_this_iter_s: 4.469254970550537\n",
      "  time_total_s: 519.2586629390717\n",
      "  timers:\n",
      "    learn_throughput: 952.661\n",
      "    learn_time_ms: 4198.767\n",
      "    load_throughput: 1601735.278\n",
      "    load_time_ms: 2.497\n",
      "    sample_throughput: 11900.017\n",
      "    sample_time_ms: 336.134\n",
      "    update_time_ms: 2.342\n",
      "  timestamp: 1624525239\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         519.259</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">  35.946</td><td style=\"text-align: right;\">                49.2</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 920000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-00-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 53.99999999999991\n",
      "  episode_reward_mean: 37.07999999999991\n",
      "  episode_reward_min: 8.69999999999994\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4600\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.275748074054718\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006656138692051172\n",
      "          model: {}\n",
      "          policy_loss: -0.023216459900140762\n",
      "          total_loss: 37.434017181396484\n",
      "          vf_explained_var: 0.6401604413986206\n",
      "          vf_loss: 37.450496673583984\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2555607259273529\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010465813800692558\n",
      "          model: {}\n",
      "          policy_loss: -0.025538714602589607\n",
      "          total_loss: 5.787313461303711\n",
      "          vf_explained_var: 0.20743893086910248\n",
      "          vf_loss: 5.804904937744141\n",
      "    num_agent_steps_sampled: 920000\n",
      "    num_agent_steps_trained: 920000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.214285714285715\n",
      "    ram_util_percent: 67.55714285714286\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 48.299999999999976\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.0\n",
      "    policy2: 4.079999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -19.5\n",
      "    policy2: -7.799999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16811105622387154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09485366135285261\n",
      "    mean_inference_ms: 1.268469577293728\n",
      "    mean_raw_obs_processing_ms: 0.6357655176816865\n",
      "  time_since_restore: 528.4308788776398\n",
      "  time_this_iter_s: 4.65516209602356\n",
      "  time_total_s: 528.4308788776398\n",
      "  timers:\n",
      "    learn_throughput: 956.622\n",
      "    learn_time_ms: 4181.381\n",
      "    load_throughput: 1611442.951\n",
      "    load_time_ms: 2.482\n",
      "    sample_throughput: 11945.38\n",
      "    sample_time_ms: 334.857\n",
      "    update_time_ms: 2.347\n",
      "  timestamp: 1624525249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         528.431</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\">   37.08</td><td style=\"text-align: right;\">                  54</td><td style=\"text-align: right;\">                 8.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 936000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-00-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.399999999999906\n",
      "  episode_reward_mean: 35.49299999999991\n",
      "  episode_reward_min: 5.999999999999941\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4675\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2570823132991791\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006995275616645813\n",
      "          model: {}\n",
      "          policy_loss: -0.021396106109023094\n",
      "          total_loss: 33.54477310180664\n",
      "          vf_explained_var: 0.73509681224823\n",
      "          vf_loss: 33.559085845947266\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.25152504444122314\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010854723863303661\n",
      "          model: {}\n",
      "          policy_loss: -0.024281291291117668\n",
      "          total_loss: 4.403861045837402\n",
      "          vf_explained_var: 0.2721027135848999\n",
      "          vf_loss: 4.419899940490723\n",
      "    num_agent_steps_sampled: 936000\n",
      "    num_agent_steps_trained: 936000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.357142857142854\n",
      "    ram_util_percent: 67.52857142857142\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 50.5\n",
      "    policy2: 24.09999999999995\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.93\n",
      "    policy2: 3.5629999999999997\n",
      "  policy_reward_min:\n",
      "    policy1: -2.0\n",
      "    policy2: -7.799999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16770514092950872\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09471816886902763\n",
      "    mean_inference_ms: 1.2685831963953553\n",
      "    mean_raw_obs_processing_ms: 0.6354930835112463\n",
      "  time_since_restore: 537.4825577735901\n",
      "  time_this_iter_s: 4.5714380741119385\n",
      "  time_total_s: 537.4825577735901\n",
      "  timers:\n",
      "    learn_throughput: 961.499\n",
      "    learn_time_ms: 4160.169\n",
      "    load_throughput: 1647862.334\n",
      "    load_time_ms: 2.427\n",
      "    sample_throughput: 11941.727\n",
      "    sample_time_ms: 334.96\n",
      "    update_time_ms: 2.362\n",
      "  timestamp: 1624525258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 117\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         537.483</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\">  35.493</td><td style=\"text-align: right;\">                50.4</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 952000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-01-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 54.59999999999991\n",
      "  episode_reward_mean: 37.48199999999991\n",
      "  episode_reward_min: 12.599999999999913\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 4750\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.24991481006145477\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00670434208586812\n",
      "          model: {}\n",
      "          policy_loss: -0.019589873030781746\n",
      "          total_loss: 43.31212615966797\n",
      "          vf_explained_var: 0.6904425024986267\n",
      "          vf_loss: 43.32492446899414\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.25124460458755493\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009829172864556313\n",
      "          model: {}\n",
      "          policy_loss: -0.0238055307418108\n",
      "          total_loss: 4.544461727142334\n",
      "          vf_explained_var: 0.29536816477775574\n",
      "          vf_loss: 4.560802936553955\n",
      "    num_agent_steps_sampled: 952000\n",
      "    num_agent_steps_trained: 952000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.000000000000004\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.5\n",
      "    policy2: 17.5\n",
      "  policy_reward_mean:\n",
      "    policy1: 35.195\n",
      "    policy2: 2.2869999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: 7.0\n",
      "    policy2: -7.799999999999986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16762025325881846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09467300594403462\n",
      "    mean_inference_ms: 1.2678186749406328\n",
      "    mean_raw_obs_processing_ms: 0.6355390500940505\n",
      "  time_since_restore: 546.4841501712799\n",
      "  time_this_iter_s: 4.462865114212036\n",
      "  time_total_s: 546.4841501712799\n",
      "  timers:\n",
      "    learn_throughput: 963.176\n",
      "    learn_time_ms: 4152.926\n",
      "    load_throughput: 1615041.827\n",
      "    load_time_ms: 2.477\n",
      "    sample_throughput: 11866.55\n",
      "    sample_time_ms: 337.082\n",
      "    update_time_ms: 2.337\n",
      "  timestamp: 1624525267\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 119\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         546.484</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">  37.482</td><td style=\"text-align: right;\">                54.6</td><td style=\"text-align: right;\">                12.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 968000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-01-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 55.19999999999991\n",
      "  episode_reward_mean: 34.37099999999992\n",
      "  episode_reward_min: 12.29999999999992\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4825\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.24578088521957397\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005851077847182751\n",
      "          model: {}\n",
      "          policy_loss: -0.016021618619561195\n",
      "          total_loss: 43.45042037963867\n",
      "          vf_explained_var: 0.6269910931587219\n",
      "          vf_loss: 43.46051788330078\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.24350674450397491\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008587498217821121\n",
      "          model: {}\n",
      "          policy_loss: -0.019944751635193825\n",
      "          total_loss: 13.873799324035645\n",
      "          vf_explained_var: 0.21583668887615204\n",
      "          vf_loss: 13.887224197387695\n",
      "    num_agent_steps_sampled: 968000\n",
      "    num_agent_steps_trained: 968000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.628571428571426\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 26.29999999999995\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.565\n",
      "    policy2: 4.805999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: 1.0\n",
      "    policy2: -5.599999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16750453406751348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0946175005415162\n",
      "    mean_inference_ms: 1.2668566299866781\n",
      "    mean_raw_obs_processing_ms: 0.6352173506959053\n",
      "  time_since_restore: 555.4985430240631\n",
      "  time_this_iter_s: 4.489234924316406\n",
      "  time_total_s: 555.4985430240631\n",
      "  timers:\n",
      "    learn_throughput: 961.695\n",
      "    learn_time_ms: 4159.324\n",
      "    load_throughput: 1629077.35\n",
      "    load_time_ms: 2.455\n",
      "    sample_throughput: 11817.498\n",
      "    sample_time_ms: 338.481\n",
      "    update_time_ms: 2.325\n",
      "  timestamp: 1624525276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 121\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         555.499</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\">  34.371</td><td style=\"text-align: right;\">                55.2</td><td style=\"text-align: right;\">                12.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 984000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-01-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.6999999999999\n",
      "  episode_reward_mean: 33.48599999999991\n",
      "  episode_reward_min: 11.99999999999994\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4900\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2574528753757477\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007298425305634737\n",
      "          model: {}\n",
      "          policy_loss: -0.021373813971877098\n",
      "          total_loss: 40.427467346191406\n",
      "          vf_explained_var: 0.5343409776687622\n",
      "          vf_loss: 40.44144821166992\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.23988348245620728\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00858103483915329\n",
      "          model: {}\n",
      "          policy_loss: -0.020039953291416168\n",
      "          total_loss: 10.810911178588867\n",
      "          vf_explained_var: 0.2480611652135849\n",
      "          vf_loss: 10.824435234069824\n",
      "    num_agent_steps_sampled: 984000\n",
      "    num_agent_steps_trained: 984000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.25\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 47.5\n",
      "    policy2: 47.19999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 25.765\n",
      "    policy2: 7.721\n",
      "  policy_reward_min:\n",
      "    policy1: -22.0\n",
      "    policy2: -1.2000000000000055\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16765126242107936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09461431452956937\n",
      "    mean_inference_ms: 1.2648829889712863\n",
      "    mean_raw_obs_processing_ms: 0.6349177871428628\n",
      "  time_since_restore: 564.466803073883\n",
      "  time_this_iter_s: 4.442241907119751\n",
      "  time_total_s: 564.466803073883\n",
      "  timers:\n",
      "    learn_throughput: 958.91\n",
      "    learn_time_ms: 4171.405\n",
      "    load_throughput: 1623591.074\n",
      "    load_time_ms: 2.464\n",
      "    sample_throughput: 11792.269\n",
      "    sample_time_ms: 339.205\n",
      "    update_time_ms: 2.353\n",
      "  timestamp: 1624525285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 123\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         564.467</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\">  33.486</td><td style=\"text-align: right;\">                50.7</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1000000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-01-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 57.5999999999999\n",
      "  episode_reward_mean: 34.322999999999915\n",
      "  episode_reward_min: 4.199999999999958\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5000\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.27957624197006226\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006263553164899349\n",
      "          model: {}\n",
      "          policy_loss: -0.01936435140669346\n",
      "          total_loss: 66.02657318115234\n",
      "          vf_explained_var: 0.4892638027667999\n",
      "          vf_loss: 66.03959655761719\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.23123636841773987\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007088086102157831\n",
      "          model: {}\n",
      "          policy_loss: -0.014691291376948357\n",
      "          total_loss: 24.81045150756836\n",
      "          vf_explained_var: 0.31815969944000244\n",
      "          vf_loss: 24.819759368896484\n",
      "    num_agent_steps_sampled: 1000000\n",
      "    num_agent_steps_trained: 1000000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.866666666666664\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 42.79999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 26.525\n",
      "    policy2: 7.797999999999997\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -4.500000000000003\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16745686897660847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09451227550448504\n",
      "    mean_inference_ms: 1.2635640256006022\n",
      "    mean_raw_obs_processing_ms: 0.6344767491443603\n",
      "  time_since_restore: 573.4366993904114\n",
      "  time_this_iter_s: 4.412427186965942\n",
      "  time_total_s: 573.4366993904114\n",
      "  timers:\n",
      "    learn_throughput: 963.901\n",
      "    learn_time_ms: 4149.805\n",
      "    load_throughput: 1642634.918\n",
      "    load_time_ms: 2.435\n",
      "    sample_throughput: 11737.534\n",
      "    sample_time_ms: 340.787\n",
      "    update_time_ms: 2.331\n",
      "  timestamp: 1624525294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 125\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         573.437</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\">  34.323</td><td style=\"text-align: right;\">                57.6</td><td style=\"text-align: right;\">                 4.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1016000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-01-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 57.5999999999999\n",
      "  episode_reward_mean: 35.429999999999914\n",
      "  episode_reward_min: 8.099999999999937\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5075\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2499667853116989\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006388661451637745\n",
      "          model: {}\n",
      "          policy_loss: -0.022287975996732712\n",
      "          total_loss: 52.9800910949707\n",
      "          vf_explained_var: 0.6502175331115723\n",
      "          vf_loss: 52.995914459228516\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.21969854831695557\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007198038976639509\n",
      "          model: {}\n",
      "          policy_loss: -0.012781202793121338\n",
      "          total_loss: 17.514198303222656\n",
      "          vf_explained_var: 0.3583277761936188\n",
      "          vf_loss: 17.521512985229492\n",
      "    num_agent_steps_sampled: 1016000\n",
      "    num_agent_steps_trained: 1016000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.75\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 40.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.38\n",
      "    policy2: 7.049999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -19.5\n",
      "    policy2: -4.500000000000003\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1669284375130022\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09431330711299162\n",
      "    mean_inference_ms: 1.2626482251479096\n",
      "    mean_raw_obs_processing_ms: 0.6337353749880996\n",
      "  time_since_restore: 582.254784822464\n",
      "  time_this_iter_s: 4.420592308044434\n",
      "  time_total_s: 582.254784822464\n",
      "  timers:\n",
      "    learn_throughput: 967.776\n",
      "    learn_time_ms: 4133.187\n",
      "    load_throughput: 1647312.216\n",
      "    load_time_ms: 2.428\n",
      "    sample_throughput: 11971.921\n",
      "    sample_time_ms: 334.115\n",
      "    update_time_ms: 2.289\n",
      "  timestamp: 1624525303\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 127\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         582.255</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">   35.43</td><td style=\"text-align: right;\">                57.6</td><td style=\"text-align: right;\">                 8.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1032000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-01-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.5999999999999\n",
      "  episode_reward_mean: 35.06099999999991\n",
      "  episode_reward_min: 8.099999999999937\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5150\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.24276036024093628\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005688406992703676\n",
      "          model: {}\n",
      "          policy_loss: -0.015478793531656265\n",
      "          total_loss: 42.58594512939453\n",
      "          vf_explained_var: 0.6779042482376099\n",
      "          vf_loss: 42.59566116333008\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.20040692389011383\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0060876160860061646\n",
      "          model: {}\n",
      "          policy_loss: -0.014645088464021683\n",
      "          total_loss: 10.825906753540039\n",
      "          vf_explained_var: 0.3160204589366913\n",
      "          vf_loss: 10.835929870605469\n",
      "    num_agent_steps_sampled: 1032000\n",
      "    num_agent_steps_trained: 1032000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.700000000000006\n",
      "    ram_util_percent: 67.52857142857142\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 40.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 28.44\n",
      "    policy2: 6.621\n",
      "  policy_reward_min:\n",
      "    policy1: -15.5\n",
      "    policy2: -5.599999999999991\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.166731006807423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09420355021530291\n",
      "    mean_inference_ms: 1.261301246383061\n",
      "    mean_raw_obs_processing_ms: 0.6332086454615714\n",
      "  time_since_restore: 591.180380821228\n",
      "  time_this_iter_s: 4.450028896331787\n",
      "  time_total_s: 591.180380821228\n",
      "  timers:\n",
      "    learn_throughput: 969.512\n",
      "    learn_time_ms: 4125.786\n",
      "    load_throughput: 1694137.795\n",
      "    load_time_ms: 2.361\n",
      "    sample_throughput: 11973.357\n",
      "    sample_time_ms: 334.075\n",
      "    update_time_ms: 2.297\n",
      "  timestamp: 1624525312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 129\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">          591.18</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\">  35.061</td><td style=\"text-align: right;\">                51.6</td><td style=\"text-align: right;\">                 8.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1048000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.5999999999999\n",
      "  episode_reward_mean: 35.74499999999991\n",
      "  episode_reward_min: 3.9000000000000195\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5225\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.23224569857120514\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00638602813705802\n",
      "          model: {}\n",
      "          policy_loss: -0.017932292073965073\n",
      "          total_loss: 48.474735260009766\n",
      "          vf_explained_var: 0.6207736730575562\n",
      "          vf_loss: 48.4862060546875\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.20422615110874176\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008029193617403507\n",
      "          model: {}\n",
      "          policy_loss: -0.01926053687930107\n",
      "          total_loss: 12.014561653137207\n",
      "          vf_explained_var: 0.3136391341686249\n",
      "          vf_loss: 12.02772331237793\n",
      "    num_agent_steps_sampled: 1048000\n",
      "    num_agent_steps_trained: 1048000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.03333333333333\n",
      "    ram_util_percent: 67.76666666666667\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 40.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.685\n",
      "    policy2: 6.059999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -15.0\n",
      "    policy2: -5.599999999999982\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16656889743994344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09412068552821509\n",
      "    mean_inference_ms: 1.2602316795829502\n",
      "    mean_raw_obs_processing_ms: 0.6326418441284624\n",
      "  time_since_restore: 599.9205775260925\n",
      "  time_this_iter_s: 4.339865684509277\n",
      "  time_total_s: 599.9205775260925\n",
      "  timers:\n",
      "    learn_throughput: 975.164\n",
      "    learn_time_ms: 4101.876\n",
      "    load_throughput: 1674055.419\n",
      "    load_time_ms: 2.389\n",
      "    sample_throughput: 12102.317\n",
      "    sample_time_ms: 330.515\n",
      "    update_time_ms: 2.315\n",
      "  timestamp: 1624525321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 131\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         599.921</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\">  35.745</td><td style=\"text-align: right;\">                51.6</td><td style=\"text-align: right;\">                 3.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1064000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.5999999999999\n",
      "  episode_reward_mean: 35.042999999999914\n",
      "  episode_reward_min: 3.9000000000000195\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5300\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.23823557794094086\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005204926244914532\n",
      "          model: {}\n",
      "          policy_loss: -0.016981763765215874\n",
      "          total_loss: 48.62496566772461\n",
      "          vf_explained_var: 0.586117148399353\n",
      "          vf_loss: 48.636680603027344\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.19742310047149658\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005584673024713993\n",
      "          model: {}\n",
      "          policy_loss: -0.015445487573742867\n",
      "          total_loss: 11.617486000061035\n",
      "          vf_explained_var: 0.3385823667049408\n",
      "          vf_loss: 11.628690719604492\n",
      "    num_agent_steps_sampled: 1064000\n",
      "    num_agent_steps_trained: 1064000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.485714285714288\n",
      "    ram_util_percent: 67.8\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.5\n",
      "    policy2: 40.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.16\n",
      "    policy2: 4.882999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -20.5\n",
      "    policy2: -5.599999999999982\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16661107769303318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09407704657771063\n",
      "    mean_inference_ms: 1.2576710531460729\n",
      "    mean_raw_obs_processing_ms: 0.6319843871097126\n",
      "  time_since_restore: 608.7232522964478\n",
      "  time_this_iter_s: 4.456153631210327\n",
      "  time_total_s: 608.7232522964478\n",
      "  timers:\n",
      "    learn_throughput: 978.458\n",
      "    learn_time_ms: 4088.065\n",
      "    load_throughput: 1678023.644\n",
      "    load_time_ms: 2.384\n",
      "    sample_throughput: 12202.07\n",
      "    sample_time_ms: 327.813\n",
      "    update_time_ms: 2.305\n",
      "  timestamp: 1624525329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 133\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         608.723</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\">  35.043</td><td style=\"text-align: right;\">                51.6</td><td style=\"text-align: right;\">                 3.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1080000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.49999999999991\n",
      "  episode_reward_mean: 37.77899999999991\n",
      "  episode_reward_min: 23.69999999999991\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5400\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.25082454085350037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005223363172262907\n",
      "          model: {}\n",
      "          policy_loss: -0.014521021395921707\n",
      "          total_loss: 39.5062141418457\n",
      "          vf_explained_var: 0.628329873085022\n",
      "          vf_loss: 39.51543426513672\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.20017002522945404\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00696710217744112\n",
      "          model: {}\n",
      "          policy_loss: -0.01815933920443058\n",
      "          total_loss: 9.493572235107422\n",
      "          vf_explained_var: 0.3221750259399414\n",
      "          vf_loss: 9.506441116333008\n",
      "    num_agent_steps_sampled: 1080000\n",
      "    num_agent_steps_trained: 1080000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.17142857142857\n",
      "    ram_util_percent: 67.62857142857142\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 48.5\n",
      "    policy2: 56.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.94\n",
      "    policy2: 4.8389999999999995\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -4.5000000000000036\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16640224279635774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09397688308229955\n",
      "    mean_inference_ms: 1.2561870014349423\n",
      "    mean_raw_obs_processing_ms: 0.631457447670295\n",
      "  time_since_restore: 617.7466673851013\n",
      "  time_this_iter_s: 4.543848991394043\n",
      "  time_total_s: 617.7466673851013\n",
      "  timers:\n",
      "    learn_throughput: 977.291\n",
      "    learn_time_ms: 4092.948\n",
      "    load_throughput: 1684865.429\n",
      "    load_time_ms: 2.374\n",
      "    sample_throughput: 12184.554\n",
      "    sample_time_ms: 328.284\n",
      "    update_time_ms: 2.321\n",
      "  timestamp: 1624525339\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 135\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         617.747</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\">  37.779</td><td style=\"text-align: right;\">                49.5</td><td style=\"text-align: right;\">                23.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1096000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.29999999999991\n",
      "  episode_reward_mean: 37.56599999999991\n",
      "  episode_reward_min: 22.499999999999908\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5475\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.223600372672081\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011608666740357876\n",
      "          model: {}\n",
      "          policy_loss: -0.022319426760077477\n",
      "          total_loss: 37.95277404785156\n",
      "          vf_explained_var: 0.759234607219696\n",
      "          vf_loss: 37.96921920776367\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.21290934085845947\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010500269010663033\n",
      "          model: {}\n",
      "          policy_loss: -0.022729966789484024\n",
      "          total_loss: 8.242344856262207\n",
      "          vf_explained_var: 0.34098178148269653\n",
      "          vf_loss: 8.257102012634277\n",
      "    num_agent_steps_sampled: 1096000\n",
      "    num_agent_steps_trained: 1096000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.599999999999998\n",
      "    ram_util_percent: 67.60000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 47.5\n",
      "    policy2: 52.7\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.485\n",
      "    policy2: 5.080999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16599873319983566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0938461195930208\n",
      "    mean_inference_ms: 1.256214849749199\n",
      "    mean_raw_obs_processing_ms: 0.6310999747996031\n",
      "  time_since_restore: 626.9609663486481\n",
      "  time_this_iter_s: 4.500033140182495\n",
      "  time_total_s: 626.9609663486481\n",
      "  timers:\n",
      "    learn_throughput: 968.626\n",
      "    learn_time_ms: 4129.56\n",
      "    load_throughput: 1687848.692\n",
      "    load_time_ms: 2.37\n",
      "    sample_throughput: 12073.542\n",
      "    sample_time_ms: 331.303\n",
      "    update_time_ms: 2.342\n",
      "  timestamp: 1624525348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 137\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         626.961</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\">  37.566</td><td style=\"text-align: right;\">                51.3</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.29999999999991\n",
      "  episode_reward_mean: 37.658999999999914\n",
      "  episode_reward_min: 22.499999999999908\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5500\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.23901215195655823\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009055706672370434\n",
      "          model: {}\n",
      "          policy_loss: -0.020144682377576828\n",
      "          total_loss: 51.1915397644043\n",
      "          vf_explained_var: 0.5503359436988831\n",
      "          vf_loss: 51.207096099853516\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2099953442811966\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010106624104082584\n",
      "          model: {}\n",
      "          policy_loss: -0.027311472222208977\n",
      "          total_loss: 3.3611810207366943\n",
      "          vf_explained_var: 0.23576776683330536\n",
      "          vf_loss: 3.3808176517486572\n",
      "    num_agent_steps_sampled: 1104000\n",
      "    num_agent_steps_trained: 1104000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.38888888888889\n",
      "    ram_util_percent: 71.05555555555556\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 48.0\n",
      "    policy2: 52.7\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.875\n",
      "    policy2: 4.783999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -24.5\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16624891386525412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09389999329612943\n",
      "    mean_inference_ms: 1.2551231975081258\n",
      "    mean_raw_obs_processing_ms: 0.6310298140876249\n",
      "  time_since_restore: 633.011801481247\n",
      "  time_this_iter_s: 6.050835132598877\n",
      "  time_total_s: 633.011801481247\n",
      "  timers:\n",
      "    learn_throughput: 933.293\n",
      "    learn_time_ms: 4285.9\n",
      "    load_throughput: 1683597.355\n",
      "    load_time_ms: 2.376\n",
      "    sample_throughput: 12048.246\n",
      "    sample_time_ms: 331.999\n",
      "    update_time_ms: 2.37\n",
      "  timestamp: 1624525354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 138\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         633.012</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\">  37.659</td><td style=\"text-align: right;\">                51.3</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.99999999999991\n",
      "  episode_reward_mean: 38.29799999999991\n",
      "  episode_reward_min: 20.09999999999993\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5600\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.24074704945087433\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010952427051961422\n",
      "          model: {}\n",
      "          policy_loss: -0.022378819063305855\n",
      "          total_loss: 40.17567443847656\n",
      "          vf_explained_var: 0.6254825592041016\n",
      "          vf_loss: 40.1925163269043\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.22088049352169037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008209168910980225\n",
      "          model: {}\n",
      "          policy_loss: -0.019686808809638023\n",
      "          total_loss: 3.0470938682556152\n",
      "          vf_explained_var: 0.29467371106147766\n",
      "          vf_loss: 3.0605461597442627\n",
      "    num_agent_steps_sampled: 1120000\n",
      "    num_agent_steps_trained: 1120000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.35\n",
      "    ram_util_percent: 71.9125\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 40.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 36.0\n",
      "    policy2: 2.2979999999999996\n",
      "  policy_reward_min:\n",
      "    policy1: -4.0\n",
      "    policy2: -8.89999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16648058689285533\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09402363054787802\n",
      "    mean_inference_ms: 1.2588027788366718\n",
      "    mean_raw_obs_processing_ms: 0.6321547492237665\n",
      "  time_since_restore: 643.3540205955505\n",
      "  time_this_iter_s: 5.685152053833008\n",
      "  time_total_s: 643.3540205955505\n",
      "  timers:\n",
      "    learn_throughput: 908.932\n",
      "    learn_time_ms: 4400.77\n",
      "    load_throughput: 1248592.756\n",
      "    load_time_ms: 3.204\n",
      "    sample_throughput: 10975.714\n",
      "    sample_time_ms: 364.441\n",
      "    update_time_ms: 2.604\n",
      "  timestamp: 1624525364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 140\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         643.354</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\">  38.298</td><td style=\"text-align: right;\">                  51</td><td style=\"text-align: right;\">                20.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.99999999999991\n",
      "  episode_reward_mean: 38.28899999999991\n",
      "  episode_reward_min: 16.499999999999932\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5625\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2217584103345871\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009751363657414913\n",
      "          model: {}\n",
      "          policy_loss: -0.01812124252319336\n",
      "          total_loss: 32.837345123291016\n",
      "          vf_explained_var: 0.7351718544960022\n",
      "          vf_loss: 32.85053253173828\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.20305748283863068\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007782900705933571\n",
      "          model: {}\n",
      "          policy_loss: -0.021022338420152664\n",
      "          total_loss: 4.760045528411865\n",
      "          vf_explained_var: 0.2392144650220871\n",
      "          vf_loss: 4.775158405303955\n",
      "    num_agent_steps_sampled: 1128000\n",
      "    num_agent_steps_trained: 1128000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.2125\n",
      "    ram_util_percent: 70.75\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 40.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 36.035\n",
      "    policy2: 2.2539999999999996\n",
      "  policy_reward_min:\n",
      "    policy1: -4.0\n",
      "    policy2: -8.89999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1663300781698021\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09402545071179975\n",
      "    mean_inference_ms: 1.2616507065537608\n",
      "    mean_raw_obs_processing_ms: 0.6326667168559605\n",
      "  time_since_restore: 648.4982075691223\n",
      "  time_this_iter_s: 5.144186973571777\n",
      "  time_total_s: 648.4982075691223\n",
      "  timers:\n",
      "    learn_throughput: 892.799\n",
      "    learn_time_ms: 4480.291\n",
      "    load_throughput: 1248444.097\n",
      "    load_time_ms: 3.204\n",
      "    sample_throughput: 10949.48\n",
      "    sample_time_ms: 365.314\n",
      "    update_time_ms: 2.617\n",
      "  timestamp: 1624525370\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 141\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         648.498</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\">  38.289</td><td style=\"text-align: right;\">                  51</td><td style=\"text-align: right;\">                16.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-02-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 50.699999999999896\n",
      "  episode_reward_mean: 38.963999999999906\n",
      "  episode_reward_min: 8.999999999999956\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5675\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.21710826456546783\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012960165739059448\n",
      "          model: {}\n",
      "          policy_loss: -0.026138702407479286\n",
      "          total_loss: 35.3424186706543\n",
      "          vf_explained_var: 0.7727813720703125\n",
      "          vf_loss: 35.362003326416016\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.20458777248859406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008599617518484592\n",
      "          model: {}\n",
      "          policy_loss: -0.027774294838309288\n",
      "          total_loss: 4.549069881439209\n",
      "          vf_explained_var: 0.273370623588562\n",
      "          vf_loss: 4.570313930511475\n",
      "    num_agent_steps_sampled: 1136000\n",
      "    num_agent_steps_trained: 1136000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.9375\n",
      "    ram_util_percent: 70.91250000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 15.300000000000004\n",
      "  policy_reward_mean:\n",
      "    policy1: 37.15\n",
      "    policy2: 1.8139999999999992\n",
      "  policy_reward_min:\n",
      "    policy1: 8.0\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1664238881562613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09407757634066793\n",
      "    mean_inference_ms: 1.263366590942911\n",
      "    mean_raw_obs_processing_ms: 0.6331355598252393\n",
      "  time_since_restore: 654.5321345329285\n",
      "  time_this_iter_s: 6.033926963806152\n",
      "  time_total_s: 654.5321345329285\n",
      "  timers:\n",
      "    learn_throughput: 860.777\n",
      "    learn_time_ms: 4646.962\n",
      "    load_throughput: 1234499.312\n",
      "    load_time_ms: 3.24\n",
      "    sample_throughput: 10891.727\n",
      "    sample_time_ms: 367.251\n",
      "    update_time_ms: 2.636\n",
      "  timestamp: 1624525376\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         654.532</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\">  38.964</td><td style=\"text-align: right;\">                50.7</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-03-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.59999999999991\n",
      "  episode_reward_mean: 38.780999999999906\n",
      "  episode_reward_min: 8.999999999999956\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5700\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.22592586278915405\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011361761949956417\n",
      "          model: {}\n",
      "          policy_loss: -0.023921573534607887\n",
      "          total_loss: 44.94271469116211\n",
      "          vf_explained_var: 0.5940679311752319\n",
      "          vf_loss: 44.96088790893555\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.202354297041893\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007086870726197958\n",
      "          model: {}\n",
      "          policy_loss: -0.02121191844344139\n",
      "          total_loss: 11.456941604614258\n",
      "          vf_explained_var: 0.325809121131897\n",
      "          vf_loss: 11.472771644592285\n",
      "    num_agent_steps_sampled: 1144000\n",
      "    num_agent_steps_trained: 1144000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.2125\n",
      "    ram_util_percent: 70.69999999999999\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 42.79999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 36.285\n",
      "    policy2: 2.4959999999999996\n",
      "  policy_reward_min:\n",
      "    policy1: -8.0\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16674619072999608\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09416750311459499\n",
      "    mean_inference_ms: 1.262882094880298\n",
      "    mean_raw_obs_processing_ms: 0.6333106003182898\n",
      "  time_since_restore: 659.9292094707489\n",
      "  time_this_iter_s: 5.397074937820435\n",
      "  time_total_s: 659.9292094707489\n",
      "  timers:\n",
      "    learn_throughput: 845.654\n",
      "    learn_time_ms: 4730.067\n",
      "    load_throughput: 1220391.929\n",
      "    load_time_ms: 3.278\n",
      "    sample_throughput: 10577.472\n",
      "    sample_time_ms: 378.162\n",
      "    update_time_ms: 2.647\n",
      "  timestamp: 1624525381\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 143\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         659.929</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\">  38.781</td><td style=\"text-align: right;\">                51.6</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 52.79999999999992\n",
      "  episode_reward_mean: 36.83399999999992\n",
      "  episode_reward_min: 8.999999999999956\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 5750\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.19761675596237183\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010123346000909805\n",
      "          model: {}\n",
      "          policy_loss: -0.020393038168549538\n",
      "          total_loss: 56.9569091796875\n",
      "          vf_explained_var: 0.6359074115753174\n",
      "          vf_loss: 56.9721794128418\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.19509486854076385\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00623331731185317\n",
      "          model: {}\n",
      "          policy_loss: -0.015097827650606632\n",
      "          total_loss: 26.778621673583984\n",
      "          vf_explained_var: 0.28644058108329773\n",
      "          vf_loss: 26.788990020751953\n",
      "    num_agent_steps_sampled: 1152000\n",
      "    num_agent_steps_trained: 1152000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.82857142857143\n",
      "    ram_util_percent: 70.77142857142857\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 45.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.04\n",
      "    policy2: 3.7939999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -24.0\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1665028130485986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09411770633725623\n",
      "    mean_inference_ms: 1.2643209961636324\n",
      "    mean_raw_obs_processing_ms: 0.6336044589642214\n",
      "  time_since_restore: 664.9656484127045\n",
      "  time_this_iter_s: 5.036438941955566\n",
      "  time_total_s: 664.9656484127045\n",
      "  timers:\n",
      "    learn_throughput: 835.856\n",
      "    learn_time_ms: 4785.511\n",
      "    load_throughput: 1205857.501\n",
      "    load_time_ms: 3.317\n",
      "    sample_throughput: 10574.106\n",
      "    sample_time_ms: 378.283\n",
      "    update_time_ms: 2.667\n",
      "  timestamp: 1624525386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 144\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         664.966</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\">  36.834</td><td style=\"text-align: right;\">                52.8</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-03-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.2999999999999\n",
      "  episode_reward_mean: 37.94099999999992\n",
      "  episode_reward_min: 11.700000000000003\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5825\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.20331810414791107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013396869413554668\n",
      "          model: {}\n",
      "          policy_loss: -0.024723723530769348\n",
      "          total_loss: 52.11165237426758\n",
      "          vf_explained_var: 0.6604058742523193\n",
      "          vf_loss: 52.129600524902344\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.18580752611160278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007885206490755081\n",
      "          model: {}\n",
      "          policy_loss: -0.015923289582133293\n",
      "          total_loss: 25.498579025268555\n",
      "          vf_explained_var: 0.2031383514404297\n",
      "          vf_loss: 25.508516311645508\n",
      "    num_agent_steps_sampled: 1168000\n",
      "    num_agent_steps_trained: 1168000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.914285714285715\n",
      "    ram_util_percent: 70.3\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.5\n",
      "    policy2: 59.3\n",
      "  policy_reward_mean:\n",
      "    policy1: 29.505\n",
      "    policy2: 8.435999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -29.0\n",
      "    policy2: -7.799999999999981\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16653285377698615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0941350906257862\n",
      "    mean_inference_ms: 1.2647005179564241\n",
      "    mean_raw_obs_processing_ms: 0.6337583894533371\n",
      "  time_since_restore: 674.5527865886688\n",
      "  time_this_iter_s: 4.716874122619629\n",
      "  time_total_s: 674.5527865886688\n",
      "  timers:\n",
      "    learn_throughput: 830.622\n",
      "    learn_time_ms: 4815.668\n",
      "    load_throughput: 1186029.394\n",
      "    load_time_ms: 3.373\n",
      "    sample_throughput: 10499.199\n",
      "    sample_time_ms: 380.981\n",
      "    update_time_ms: 2.677\n",
      "  timestamp: 1624525396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 146\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         674.553</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\">  37.941</td><td style=\"text-align: right;\">                51.3</td><td style=\"text-align: right;\">                11.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-03-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 52.19999999999991\n",
      "  episode_reward_mean: 38.666999999999916\n",
      "  episode_reward_min: 21.299999999999933\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5900\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.21368350088596344\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011372498236596584\n",
      "          model: {}\n",
      "          policy_loss: -0.019039282575249672\n",
      "          total_loss: 60.702919006347656\n",
      "          vf_explained_var: 0.5923455357551575\n",
      "          vf_loss: 60.7161979675293\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.1883404403924942\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00579369580373168\n",
      "          model: {}\n",
      "          policy_loss: -0.009316980838775635\n",
      "          total_loss: 27.108476638793945\n",
      "          vf_explained_var: 0.2842789590358734\n",
      "          vf_loss: 27.1133975982666\n",
      "    num_agent_steps_sampled: 1184000\n",
      "    num_agent_steps_trained: 1184000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.0\n",
      "    ram_util_percent: 70.25714285714285\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 48.3\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.09\n",
      "    policy2: 6.576999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -15.0\n",
      "    policy2: -6.6999999999999895\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1667599541608395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09417481718806592\n",
      "    mean_inference_ms: 1.2632780472914509\n",
      "    mean_raw_obs_processing_ms: 0.6336828355038147\n",
      "  time_since_restore: 683.4815049171448\n",
      "  time_this_iter_s: 4.451847076416016\n",
      "  time_total_s: 683.4815049171448\n",
      "  timers:\n",
      "    learn_throughput: 859.369\n",
      "    learn_time_ms: 4654.578\n",
      "    load_throughput: 1174990.265\n",
      "    load_time_ms: 3.404\n",
      "    sample_throughput: 10518.673\n",
      "    sample_time_ms: 380.276\n",
      "    update_time_ms: 2.632\n",
      "  timestamp: 1624525405\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         683.482</td><td style=\"text-align: right;\">592000</td><td style=\"text-align: right;\">  38.667</td><td style=\"text-align: right;\">                52.2</td><td style=\"text-align: right;\">                21.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-03-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 53.99999999999991\n",
      "  episode_reward_mean: 38.91299999999992\n",
      "  episode_reward_min: 15.599999999999943\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6000\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2088169902563095\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013482782989740372\n",
      "          model: {}\n",
      "          policy_loss: -0.02733447402715683\n",
      "          total_loss: 46.111915588378906\n",
      "          vf_explained_var: 0.656093180179596\n",
      "          vf_loss: 46.132423400878906\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.19980944693088531\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005913416389375925\n",
      "          model: {}\n",
      "          policy_loss: -0.014601672068238258\n",
      "          total_loss: 8.434819221496582\n",
      "          vf_explained_var: 0.28599923849105835\n",
      "          vf_loss: 8.444929122924805\n",
      "    num_agent_steps_sampled: 1200000\n",
      "    num_agent_steps_trained: 1200000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.799999999999997\n",
      "    ram_util_percent: 69.88333333333334\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 54.0\n",
      "    policy2: 43.9\n",
      "  policy_reward_mean:\n",
      "    policy1: 34.58\n",
      "    policy2: 4.332999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -16.0\n",
      "    policy2: -5.599999999999984\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16651803327911288\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09404636908923124\n",
      "    mean_inference_ms: 1.2613873014260135\n",
      "    mean_raw_obs_processing_ms: 0.6330015225839984\n",
      "  time_since_restore: 692.3037040233612\n",
      "  time_this_iter_s: 4.360272169113159\n",
      "  time_total_s: 692.3037040233612\n",
      "  timers:\n",
      "    learn_throughput: 881.167\n",
      "    learn_time_ms: 4539.436\n",
      "    load_throughput: 1558771.729\n",
      "    load_time_ms: 2.566\n",
      "    sample_throughput: 11583.935\n",
      "    sample_time_ms: 345.306\n",
      "    update_time_ms: 2.39\n",
      "  timestamp: 1624525414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 150\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         692.304</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\">  38.913</td><td style=\"text-align: right;\">                  54</td><td style=\"text-align: right;\">                15.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 54.299999999999905\n",
      "  episode_reward_mean: 39.91199999999991\n",
      "  episode_reward_min: 19.499999999999908\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6075\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2036026567220688\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011194639839231968\n",
      "          model: {}\n",
      "          policy_loss: -0.021081706508994102\n",
      "          total_loss: 42.769195556640625\n",
      "          vf_explained_var: 0.7206318378448486\n",
      "          vf_loss: 42.78461837768555\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.21334820985794067\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007823157124221325\n",
      "          model: {}\n",
      "          policy_loss: -0.020087214186787605\n",
      "          total_loss: 4.281621932983398\n",
      "          vf_explained_var: 0.3642756938934326\n",
      "          vf_loss: 4.295769214630127\n",
      "    num_agent_steps_sampled: 1216000\n",
      "    num_agent_steps_trained: 1216000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.51666666666667\n",
      "    ram_util_percent: 68.75\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 37.3\n",
      "  policy_reward_mean:\n",
      "    policy1: 37.24\n",
      "    policy2: 2.671999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -16.0\n",
      "    policy2: -6.6999999999999895\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16600044668778524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09385819585905754\n",
      "    mean_inference_ms: 1.2605885402097374\n",
      "    mean_raw_obs_processing_ms: 0.6323115375757706\n",
      "  time_since_restore: 701.1891367435455\n",
      "  time_this_iter_s: 4.493305683135986\n",
      "  time_total_s: 701.1891367435455\n",
      "  timers:\n",
      "    learn_throughput: 927.251\n",
      "    learn_time_ms: 4313.828\n",
      "    load_throughput: 1546828.935\n",
      "    load_time_ms: 2.586\n",
      "    sample_throughput: 11705.267\n",
      "    sample_time_ms: 341.727\n",
      "    update_time_ms: 2.364\n",
      "  timestamp: 1624525423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 152\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         701.189</td><td style=\"text-align: right;\">608000</td><td style=\"text-align: right;\">  39.912</td><td style=\"text-align: right;\">                54.3</td><td style=\"text-align: right;\">                19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-03-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 52.79999999999992\n",
      "  episode_reward_mean: 40.076999999999906\n",
      "  episode_reward_min: 22.499999999999964\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6150\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.17195822298526764\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007302144076675177\n",
      "          model: {}\n",
      "          policy_loss: -0.017435776069760323\n",
      "          total_loss: 46.07633972167969\n",
      "          vf_explained_var: 0.6960427165031433\n",
      "          vf_loss: 46.0900764465332\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.21282841265201569\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008641368709504604\n",
      "          model: {}\n",
      "          policy_loss: -0.02211526222527027\n",
      "          total_loss: 6.6054487228393555\n",
      "          vf_explained_var: 0.27924564480781555\n",
      "          vf_loss: 6.621001720428467\n",
      "    num_agent_steps_sampled: 1232000\n",
      "    num_agent_steps_trained: 1232000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.283333333333335\n",
      "    ram_util_percent: 68.10000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 57.0\n",
      "    policy2: 56.0\n",
      "  policy_reward_mean:\n",
      "    policy1: 36.47\n",
      "    policy2: 3.606999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -23.0\n",
      "    policy2: -6.699999999999987\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1658750559012841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09378915914542127\n",
      "    mean_inference_ms: 1.2596523820082646\n",
      "    mean_raw_obs_processing_ms: 0.6320530970996162\n",
      "  time_since_restore: 710.1911435127258\n",
      "  time_this_iter_s: 4.460910797119141\n",
      "  time_total_s: 710.1911435127258\n",
      "  timers:\n",
      "    learn_throughput: 957.158\n",
      "    learn_time_ms: 4179.037\n",
      "    load_throughput: 1598439.024\n",
      "    load_time_ms: 2.502\n",
      "    sample_throughput: 11993.572\n",
      "    sample_time_ms: 333.512\n",
      "    update_time_ms: 2.324\n",
      "  timestamp: 1624525432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 154\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         710.191</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\">  40.077</td><td style=\"text-align: right;\">                52.8</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-04-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 56.69999999999991\n",
      "  episode_reward_mean: 39.93899999999991\n",
      "  episode_reward_min: 18.29999999999996\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6225\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.17422917485237122\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010955069214105606\n",
      "          model: {}\n",
      "          policy_loss: -0.02038237638771534\n",
      "          total_loss: 34.88921356201172\n",
      "          vf_explained_var: 0.7470054626464844\n",
      "          vf_loss: 34.904048919677734\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.19339627027511597\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00695293303579092\n",
      "          model: {}\n",
      "          policy_loss: -0.017842264845967293\n",
      "          total_loss: 2.5471816062927246\n",
      "          vf_explained_var: 0.3059156835079193\n",
      "          vf_loss: 2.559744358062744\n",
      "    num_agent_steps_sampled: 1248000\n",
      "    num_agent_steps_trained: 1248000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.41428571428572\n",
      "    ram_util_percent: 70.84285714285714\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 59.0\n",
      "    policy2: 15.30000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 38.565\n",
      "    policy2: 1.373999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: 19.0\n",
      "    policy2: -5.599999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16586651364912366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09378262385515469\n",
      "    mean_inference_ms: 1.259565101422111\n",
      "    mean_raw_obs_processing_ms: 0.632007783666891\n",
      "  time_since_restore: 720.1309864521027\n",
      "  time_this_iter_s: 5.21162486076355\n",
      "  time_total_s: 720.1309864521027\n",
      "  timers:\n",
      "    learn_throughput: 950.438\n",
      "    learn_time_ms: 4208.585\n",
      "    load_throughput: 1609634.078\n",
      "    load_time_ms: 2.485\n",
      "    sample_throughput: 11805.082\n",
      "    sample_time_ms: 338.837\n",
      "    update_time_ms: 2.466\n",
      "  timestamp: 1624525442\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 156\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         720.131</td><td style=\"text-align: right;\">624000</td><td style=\"text-align: right;\">  39.939</td><td style=\"text-align: right;\">                56.7</td><td style=\"text-align: right;\">                18.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-04-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 56.399999999999906\n",
      "  episode_reward_mean: 38.726999999999904\n",
      "  episode_reward_min: 12.899999999999942\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6275\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.1919061243534088\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011052010580897331\n",
      "          model: {}\n",
      "          policy_loss: -0.024201925843954086\n",
      "          total_loss: 37.64596176147461\n",
      "          vf_explained_var: 0.7756123542785645\n",
      "          vf_loss: 37.6645622253418\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.19753175973892212\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007563509978353977\n",
      "          model: {}\n",
      "          policy_loss: -0.015865206718444824\n",
      "          total_loss: 6.774978160858154\n",
      "          vf_explained_var: 0.2904708683490753\n",
      "          vf_loss: 6.785099506378174\n",
      "    num_agent_steps_sampled: 1256000\n",
      "    num_agent_steps_trained: 1256000\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.862500000000004\n",
      "    ram_util_percent: 72.7625\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 56.5\n",
      "    policy2: 24.1\n",
      "  policy_reward_mean:\n",
      "    policy1: 37.155\n",
      "    policy2: 1.5719999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: 2.0\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16598417604259655\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09384332605067826\n",
      "    mean_inference_ms: 1.2604391153763168\n",
      "    mean_raw_obs_processing_ms: 0.6324530491376283\n",
      "  time_since_restore: 725.3210623264313\n",
      "  time_this_iter_s: 5.190075874328613\n",
      "  time_total_s: 725.3210623264313\n",
      "  timers:\n",
      "    learn_throughput: 935.33\n",
      "    learn_time_ms: 4276.568\n",
      "    load_throughput: 1605767.173\n",
      "    load_time_ms: 2.491\n",
      "    sample_throughput: 11693.221\n",
      "    sample_time_ms: 342.079\n",
      "    update_time_ms: 2.483\n",
      "  timestamp: 1624525447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 157\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         725.321</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\">  38.727</td><td style=\"text-align: right;\">                56.4</td><td style=\"text-align: right;\">                12.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-04-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 55.49999999999991\n",
      "  episode_reward_mean: 37.46399999999991\n",
      "  episode_reward_min: 3.2999999999999394\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6300\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.1948051005601883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00936073437333107\n",
      "          model: {}\n",
      "          policy_loss: -0.019219348207116127\n",
      "          total_loss: 58.74837112426758\n",
      "          vf_explained_var: 0.5887207984924316\n",
      "          vf_loss: 58.76285171508789\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.20799939334392548\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0111903827637434\n",
      "          model: {}\n",
      "          policy_loss: -0.026632744818925858\n",
      "          total_loss: 2.9394705295562744\n",
      "          vf_explained_var: 0.309987336397171\n",
      "          vf_loss: 2.957606077194214\n",
      "    num_agent_steps_sampled: 1264000\n",
      "    num_agent_steps_trained: 1264000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.01428571428572\n",
      "    ram_util_percent: 74.22857142857143\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 56.0\n",
      "    policy2: 24.1\n",
      "  policy_reward_mean:\n",
      "    policy1: 36.035\n",
      "    policy2: 1.428999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: 2.0\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1663481274232592\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09395600692074461\n",
      "    mean_inference_ms: 1.2601070970180634\n",
      "    mean_raw_obs_processing_ms: 0.632762117917033\n",
      "  time_since_restore: 730.5640540122986\n",
      "  time_this_iter_s: 5.24299168586731\n",
      "  time_total_s: 730.5640540122986\n",
      "  timers:\n",
      "    learn_throughput: 918.877\n",
      "    learn_time_ms: 4353.14\n",
      "    load_throughput: 1603480.455\n",
      "    load_time_ms: 2.495\n",
      "    sample_throughput: 11610.557\n",
      "    sample_time_ms: 344.514\n",
      "    update_time_ms: 2.512\n",
      "  timestamp: 1624525452\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 158\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         730.564</td><td style=\"text-align: right;\">632000</td><td style=\"text-align: right;\">  37.464</td><td style=\"text-align: right;\">                55.5</td><td style=\"text-align: right;\">                 3.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-04-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 61.49999999999991\n",
      "  episode_reward_mean: 37.235999999999905\n",
      "  episode_reward_min: -0.30000000000003235\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6350\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.18926827609539032\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008791024796664715\n",
      "          model: {}\n",
      "          policy_loss: -0.01990867592394352\n",
      "          total_loss: 59.469017028808594\n",
      "          vf_explained_var: 0.6203258037567139\n",
      "          vf_loss: 59.48447799682617\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.19079284369945526\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006803471595048904\n",
      "          model: {}\n",
      "          policy_loss: -0.017935065552592278\n",
      "          total_loss: 4.698116779327393\n",
      "          vf_explained_var: 0.30311405658721924\n",
      "          vf_loss: 4.710885524749756\n",
      "    num_agent_steps_sampled: 1272000\n",
      "    num_agent_steps_trained: 1272000\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.285714285714285\n",
      "    ram_util_percent: 72.04285714285716\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 60.5\n",
      "    policy2: 16.4\n",
      "  policy_reward_mean:\n",
      "    policy1: 36.17\n",
      "    policy2: 1.0659999999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -14.5\n",
      "    policy2: -7.79999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1660497033362968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09388613540521923\n",
      "    mean_inference_ms: 1.2609738682038156\n",
      "    mean_raw_obs_processing_ms: 0.6329164991338329\n",
      "  time_since_restore: 735.5359699726105\n",
      "  time_this_iter_s: 4.97191596031189\n",
      "  time_total_s: 735.5359699726105\n",
      "  timers:\n",
      "    learn_throughput: 909.079\n",
      "    learn_time_ms: 4400.057\n",
      "    load_throughput: 1599993.897\n",
      "    load_time_ms: 2.5\n",
      "    sample_throughput: 11476.981\n",
      "    sample_time_ms: 348.524\n",
      "    update_time_ms: 2.521\n",
      "  timestamp: 1624525457\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 159\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         735.536</td><td style=\"text-align: right;\">636000</td><td style=\"text-align: right;\">  37.236</td><td style=\"text-align: right;\">                61.5</td><td style=\"text-align: right;\">                -0.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-04-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 61.49999999999991\n",
      "  episode_reward_mean: 38.48399999999991\n",
      "  episode_reward_min: -0.30000000000003235\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6400\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.21815864741802216\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015073290094733238\n",
      "          model: {}\n",
      "          policy_loss: -0.0368407778441906\n",
      "          total_loss: 58.6021842956543\n",
      "          vf_explained_var: 0.625806987285614\n",
      "          vf_loss: 58.63139724731445\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2028985321521759\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0031734902877360582\n",
      "          model: {}\n",
      "          policy_loss: -0.009466861374676228\n",
      "          total_loss: 26.063669204711914\n",
      "          vf_explained_var: 0.34896063804626465\n",
      "          vf_loss: 26.070730209350586\n",
      "    num_agent_steps_sampled: 1280000\n",
      "    num_agent_steps_trained: 1280000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.2125\n",
      "    ram_util_percent: 72.30000000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 60.5\n",
      "    policy2: 80.2\n",
      "  policy_reward_mean:\n",
      "    policy1: 35.79\n",
      "    policy2: 2.693999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -71.5\n",
      "    policy2: -5.599999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16636515829547352\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0939796102794579\n",
      "    mean_inference_ms: 1.2603267595915602\n",
      "    mean_raw_obs_processing_ms: 0.6331019113073761\n",
      "  time_since_restore: 741.0057210922241\n",
      "  time_this_iter_s: 5.4697511196136475\n",
      "  time_total_s: 741.0057210922241\n",
      "  timers:\n",
      "    learn_throughput: 887.451\n",
      "    learn_time_ms: 4507.292\n",
      "    load_throughput: 1608045.01\n",
      "    load_time_ms: 2.487\n",
      "    sample_throughput: 11359.044\n",
      "    sample_time_ms: 352.142\n",
      "    update_time_ms: 2.534\n",
      "  timestamp: 1624525463\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 160\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         741.006</td><td style=\"text-align: right;\">640000</td><td style=\"text-align: right;\">  38.484</td><td style=\"text-align: right;\">                61.5</td><td style=\"text-align: right;\">                -0.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-04-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 55.19999999999991\n",
      "  episode_reward_mean: 38.82299999999991\n",
      "  episode_reward_min: 8.700000000000003\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6425\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.15264013409614563\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010444672778248787\n",
      "          model: {}\n",
      "          policy_loss: -0.02501886896789074\n",
      "          total_loss: 36.776031494140625\n",
      "          vf_explained_var: 0.7408689856529236\n",
      "          vf_loss: 36.79575729370117\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.37968748807907104\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.22722600400447845\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023156289011240005\n",
      "          model: {}\n",
      "          policy_loss: -0.027359846979379654\n",
      "          total_loss: 4.960053443908691\n",
      "          vf_explained_var: 0.29050660133361816\n",
      "          vf_loss: 4.978621006011963\n",
      "    num_agent_steps_sampled: 1288000\n",
      "    num_agent_steps_trained: 1288000\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.81428571428571\n",
      "    ram_util_percent: 71.39999999999999\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.0\n",
      "    policy2: 80.2\n",
      "  policy_reward_mean:\n",
      "    policy1: 36.36\n",
      "    policy2: 2.4629999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: -71.5\n",
      "    policy2: -5.599999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1661767961330199\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09396108670979916\n",
      "    mean_inference_ms: 1.2623833023842388\n",
      "    mean_raw_obs_processing_ms: 0.6334228281134526\n",
      "  time_since_restore: 746.0497930049896\n",
      "  time_this_iter_s: 5.044071912765503\n",
      "  time_total_s: 746.0497930049896\n",
      "  timers:\n",
      "    learn_throughput: 879.236\n",
      "    learn_time_ms: 4549.405\n",
      "    load_throughput: 1585733.216\n",
      "    load_time_ms: 2.522\n",
      "    sample_throughput: 10663.401\n",
      "    sample_time_ms: 375.115\n",
      "    update_time_ms: 2.536\n",
      "  timestamp: 1624525468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 161\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">          746.05</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\">  38.823</td><td style=\"text-align: right;\">                55.2</td><td style=\"text-align: right;\">                 8.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-04-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 58.1999999999999\n",
      "  episode_reward_mean: 38.69999999999991\n",
      "  episode_reward_min: 8.399999999999922\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6500\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.16824431717395782\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009224750101566315\n",
      "          model: {}\n",
      "          policy_loss: -0.01887594349682331\n",
      "          total_loss: 54.80549621582031\n",
      "          vf_explained_var: 0.5833874344825745\n",
      "          vf_loss: 54.819698333740234\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.20431429147720337\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009255572222173214\n",
      "          model: {}\n",
      "          policy_loss: -0.02102326601743698\n",
      "          total_loss: 3.882382392883301\n",
      "          vf_explained_var: 0.22580230236053467\n",
      "          vf_loss: 3.8981335163116455\n",
      "    num_agent_steps_sampled: 1304000\n",
      "    num_agent_steps_trained: 1304000\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.08571428571429\n",
      "    ram_util_percent: 73.61428571428571\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 60.5\n",
      "    policy2: 12.00000000000001\n",
      "  policy_reward_mean:\n",
      "    policy1: 38.635\n",
      "    policy2: 0.06499999999999838\n",
      "  policy_reward_min:\n",
      "    policy1: 8.5\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1667423958431103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09419198615978547\n",
      "    mean_inference_ms: 1.2649177716158955\n",
      "    mean_raw_obs_processing_ms: 0.6344663223331074\n",
      "  time_since_restore: 755.9111559391022\n",
      "  time_this_iter_s: 4.988641023635864\n",
      "  time_total_s: 755.9111559391022\n",
      "  timers:\n",
      "    learn_throughput: 863.496\n",
      "    learn_time_ms: 4632.333\n",
      "    load_throughput: 1605505.943\n",
      "    load_time_ms: 2.491\n",
      "    sample_throughput: 10686.71\n",
      "    sample_time_ms: 374.297\n",
      "    update_time_ms: 2.926\n",
      "  timestamp: 1624525478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 163\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         755.911</td><td style=\"text-align: right;\">652000</td><td style=\"text-align: right;\">    38.7</td><td style=\"text-align: right;\">                58.2</td><td style=\"text-align: right;\">                 8.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-04-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 58.1999999999999\n",
      "  episode_reward_mean: 39.55199999999991\n",
      "  episode_reward_min: 8.399999999999922\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6550\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.1719139665365219\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009779537096619606\n",
      "          model: {}\n",
      "          policy_loss: -0.021653514355421066\n",
      "          total_loss: 50.661800384521484\n",
      "          vf_explained_var: 0.6548686027526855\n",
      "          vf_loss: 50.67850112915039\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.22068218886852264\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01208213996142149\n",
      "          model: {}\n",
      "          policy_loss: -0.028417346999049187\n",
      "          total_loss: 2.150475263595581\n",
      "          vf_explained_var: 0.3329959511756897\n",
      "          vf_loss: 2.172011613845825\n",
      "    num_agent_steps_sampled: 1312000\n",
      "    num_agent_steps_trained: 1312000\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.51111111111111\n",
      "    ram_util_percent: 73.08888888888889\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 60.5\n",
      "    policy2: 14.200000000000012\n",
      "  policy_reward_mean:\n",
      "    policy1: 39.245\n",
      "    policy2: 0.3069999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: 0.5\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16647746170412123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09412338778551232\n",
      "    mean_inference_ms: 1.266259135508364\n",
      "    mean_raw_obs_processing_ms: 0.6345685571416557\n",
      "  time_since_restore: 762.1071751117706\n",
      "  time_this_iter_s: 6.196019172668457\n",
      "  time_total_s: 762.1071751117706\n",
      "  timers:\n",
      "    learn_throughput: 833.909\n",
      "    learn_time_ms: 4796.689\n",
      "    load_throughput: 1586723.034\n",
      "    load_time_ms: 2.521\n",
      "    sample_throughput: 10436.978\n",
      "    sample_time_ms: 383.253\n",
      "    update_time_ms: 2.952\n",
      "  timestamp: 1624525484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 164\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         762.107</td><td style=\"text-align: right;\">656000</td><td style=\"text-align: right;\">  39.552</td><td style=\"text-align: right;\">                58.2</td><td style=\"text-align: right;\">                 8.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-04-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 55.799999999999905\n",
      "  episode_reward_mean: 40.292999999999914\n",
      "  episode_reward_min: 14.69999999999993\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6600\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.18796329200267792\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009349677711725235\n",
      "          model: {}\n",
      "          policy_loss: -0.017223592847585678\n",
      "          total_loss: 55.07964324951172\n",
      "          vf_explained_var: 0.5450179576873779\n",
      "          vf_loss: 55.09213638305664\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2118915617465973\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008497951552271843\n",
      "          model: {}\n",
      "          policy_loss: -0.022459879517555237\n",
      "          total_loss: 3.6663639545440674\n",
      "          vf_explained_var: 0.3187572956085205\n",
      "          vf_loss: 3.68398380279541\n",
      "    num_agent_steps_sampled: 1320000\n",
      "    num_agent_steps_trained: 1320000\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.6125\n",
      "    ram_util_percent: 72.11250000000001\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 57.0\n",
      "    policy2: 14.200000000000012\n",
      "  policy_reward_mean:\n",
      "    policy1: 39.425\n",
      "    policy2: 0.8679999999999987\n",
      "  policy_reward_min:\n",
      "    policy1: 0.5\n",
      "    policy2: -4.500000000000004\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1669707171921546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09430095485470204\n",
      "    mean_inference_ms: 1.2677093479436277\n",
      "    mean_raw_obs_processing_ms: 0.6356626635298084\n",
      "  time_since_restore: 767.5169451236725\n",
      "  time_this_iter_s: 5.4097700119018555\n",
      "  time_total_s: 767.5169451236725\n",
      "  timers:\n",
      "    learn_throughput: 824.611\n",
      "    learn_time_ms: 4850.772\n",
      "    load_throughput: 1428772.312\n",
      "    load_time_ms: 2.8\n",
      "    sample_throughput: 10066.836\n",
      "    sample_time_ms: 397.344\n",
      "    update_time_ms: 2.831\n",
      "  timestamp: 1624525489\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 165\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         767.517</td><td style=\"text-align: right;\">660000</td><td style=\"text-align: right;\">  40.293</td><td style=\"text-align: right;\">                55.8</td><td style=\"text-align: right;\">                14.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-04-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 55.799999999999905\n",
      "  episode_reward_mean: 40.055999999999905\n",
      "  episode_reward_min: 14.69999999999993\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6625\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.1798897534608841\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011657877825200558\n",
      "          model: {}\n",
      "          policy_loss: -0.023023085668683052\n",
      "          total_loss: 40.1066780090332\n",
      "          vf_explained_var: 0.6759636402130127\n",
      "          vf_loss: 40.12379837036133\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.22160594165325165\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011130287311971188\n",
      "          model: {}\n",
      "          policy_loss: -0.028074605390429497\n",
      "          total_loss: 6.121557235717773\n",
      "          vf_explained_var: 0.3112633526325226\n",
      "          vf_loss: 6.1432929039001465\n",
      "    num_agent_steps_sampled: 1328000\n",
      "    num_agent_steps_trained: 1328000\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.6625\n",
      "    ram_util_percent: 72.2\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 57.0\n",
      "    policy2: 15.300000000000013\n",
      "  policy_reward_mean:\n",
      "    policy1: 38.66\n",
      "    policy2: 1.395999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: 0.5\n",
      "    policy2: -4.500000000000004\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1667211868196775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09424309609086767\n",
      "    mean_inference_ms: 1.2690784619437052\n",
      "    mean_raw_obs_processing_ms: 0.6360190143827985\n",
      "  time_since_restore: 773.31347489357\n",
      "  time_this_iter_s: 5.796529769897461\n",
      "  time_total_s: 773.31347489357\n",
      "  timers:\n",
      "    learn_throughput: 813.721\n",
      "    learn_time_ms: 4915.691\n",
      "    load_throughput: 1426112.561\n",
      "    load_time_ms: 2.805\n",
      "    sample_throughput: 10242.469\n",
      "    sample_time_ms: 390.531\n",
      "    update_time_ms: 2.981\n",
      "  timestamp: 1624525495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 166\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         773.313</td><td style=\"text-align: right;\">664000</td><td style=\"text-align: right;\">  40.056</td><td style=\"text-align: right;\">                55.8</td><td style=\"text-align: right;\">                14.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-05-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 56.9999999999999\n",
      "  episode_reward_mean: 39.50999999999991\n",
      "  episode_reward_min: 17.69999999999991\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6675\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.18571794033050537\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011406851932406425\n",
      "          model: {}\n",
      "          policy_loss: -0.019185759127140045\n",
      "          total_loss: 54.76815414428711\n",
      "          vf_explained_var: 0.6597768664360046\n",
      "          vf_loss: 54.78157043457031\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.22416336834430695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011088020168244839\n",
      "          model: {}\n",
      "          policy_loss: -0.02061244286596775\n",
      "          total_loss: 4.873409271240234\n",
      "          vf_explained_var: 0.3954656422138214\n",
      "          vf_loss: 4.887706756591797\n",
      "    num_agent_steps_sampled: 1336000\n",
      "    num_agent_steps_trained: 1336000\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.03\n",
      "    ram_util_percent: 74.85999999999999\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 57.0\n",
      "    policy2: 24.099999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 37.19\n",
      "    policy2: 2.3199999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: 12.5\n",
      "    policy2: -4.500000000000004\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1674941253439558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09467066472189789\n",
      "    mean_inference_ms: 1.2763059846231783\n",
      "    mean_raw_obs_processing_ms: 0.6400338180997945\n",
      "  time_since_restore: 779.9452509880066\n",
      "  time_this_iter_s: 6.6317760944366455\n",
      "  time_total_s: 779.9452509880066\n",
      "  timers:\n",
      "    learn_throughput: 800.277\n",
      "    learn_time_ms: 4998.272\n",
      "    load_throughput: 1378005.421\n",
      "    load_time_ms: 2.903\n",
      "    sample_throughput: 8851.392\n",
      "    sample_time_ms: 451.906\n",
      "    update_time_ms: 3.009\n",
      "  timestamp: 1624525502\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 167\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         779.945</td><td style=\"text-align: right;\">668000</td><td style=\"text-align: right;\">   39.51</td><td style=\"text-align: right;\">                  57</td><td style=\"text-align: right;\">                17.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-05-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 56.9999999999999\n",
      "  episode_reward_mean: 39.704999999999906\n",
      "  episode_reward_min: 14.399999999999912\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6700\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.18895883858203888\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007572810631245375\n",
      "          model: {}\n",
      "          policy_loss: -0.016570691019296646\n",
      "          total_loss: 73.3616714477539\n",
      "          vf_explained_var: 0.4455322325229645\n",
      "          vf_loss: 73.37442779541016\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.21583281457424164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007134965620934963\n",
      "          model: {}\n",
      "          policy_loss: -0.015897007659077644\n",
      "          total_loss: 15.690657615661621\n",
      "          vf_explained_var: 0.3936210572719574\n",
      "          vf_loss: 15.702491760253906\n",
      "    num_agent_steps_sampled: 1344000\n",
      "    num_agent_steps_trained: 1344000\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.25555555555556\n",
      "    ram_util_percent: 74.34444444444445\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 56.0\n",
      "    policy2: 24.099999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 37.275\n",
      "    policy2: 2.4299999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: 14.5\n",
      "    policy2: -5.599999999999988\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16819462396075202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09497979735498013\n",
      "    mean_inference_ms: 1.279332392534181\n",
      "    mean_raw_obs_processing_ms: 0.642097471627043\n",
      "  time_since_restore: 786.0665299892426\n",
      "  time_this_iter_s: 6.121279001235962\n",
      "  time_total_s: 786.0665299892426\n",
      "  timers:\n",
      "    learn_throughput: 788.066\n",
      "    learn_time_ms: 5075.715\n",
      "    load_throughput: 1375508.605\n",
      "    load_time_ms: 2.908\n",
      "    sample_throughput: 8654.403\n",
      "    sample_time_ms: 462.192\n",
      "    update_time_ms: 3.03\n",
      "  timestamp: 1624525508\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 168\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         786.067</td><td style=\"text-align: right;\">672000</td><td style=\"text-align: right;\">  39.705</td><td style=\"text-align: right;\">                  57</td><td style=\"text-align: right;\">                14.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-05-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 55.79999999999992\n",
      "  episode_reward_mean: 40.163999999999916\n",
      "  episode_reward_min: 3.0000000000000098\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6750\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.17430074512958527\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007610743399709463\n",
      "          model: {}\n",
      "          policy_loss: -0.0182318314909935\n",
      "          total_loss: 43.638301849365234\n",
      "          vf_explained_var: 0.6343697309494019\n",
      "          vf_loss: 43.65267562866211\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.21927185356616974\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01355886273086071\n",
      "          model: {}\n",
      "          policy_loss: -0.02789551578462124\n",
      "          total_loss: 5.021977424621582\n",
      "          vf_explained_var: 0.36787232756614685\n",
      "          vf_loss: 5.042150497436523\n",
      "    num_agent_steps_sampled: 1352000\n",
      "    num_agent_steps_trained: 1352000\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 676000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.775\n",
      "    ram_util_percent: 73.2125\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 57.0\n",
      "    policy2: 65.9\n",
      "  policy_reward_mean:\n",
      "    policy1: 36.975\n",
      "    policy2: 3.1890000000000005\n",
      "  policy_reward_min:\n",
      "    policy1: -39.5\n",
      "    policy2: -5.599999999999988\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16928496267902182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09559541504971925\n",
      "    mean_inference_ms: 1.2929332594912444\n",
      "    mean_raw_obs_processing_ms: 0.6475760481478571\n",
      "  time_since_restore: 791.9860670566559\n",
      "  time_this_iter_s: 5.91953706741333\n",
      "  time_total_s: 791.9860670566559\n",
      "  timers:\n",
      "    learn_throughput: 784.63\n",
      "    learn_time_ms: 5097.946\n",
      "    load_throughput: 1365766.804\n",
      "    load_time_ms: 2.929\n",
      "    sample_throughput: 7482.439\n",
      "    sample_time_ms: 534.585\n",
      "    update_time_ms: 3.061\n",
      "  timestamp: 1624525514\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 169\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         791.986</td><td style=\"text-align: right;\">676000</td><td style=\"text-align: right;\">  40.164</td><td style=\"text-align: right;\">                55.8</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-05-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 58.4999999999999\n",
      "  episode_reward_mean: 38.34299999999992\n",
      "  episode_reward_min: 3.0000000000000098\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6800\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2069912999868393\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01624962128698826\n",
      "          model: {}\n",
      "          policy_loss: -0.029327120631933212\n",
      "          total_loss: 52.81320571899414\n",
      "          vf_explained_var: 0.4684823453426361\n",
      "          vf_loss: 52.83431625366211\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.21804861724376678\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01000300794839859\n",
      "          model: {}\n",
      "          policy_loss: -0.022653374820947647\n",
      "          total_loss: 9.304450988769531\n",
      "          vf_explained_var: 0.3965266942977905\n",
      "          vf_loss: 9.321406364440918\n",
      "    num_agent_steps_sampled: 1360000\n",
      "    num_agent_steps_trained: 1360000\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.528571428571425\n",
      "    ram_util_percent: 71.27142857142857\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 57.5\n",
      "    policy2: 65.9\n",
      "  policy_reward_mean:\n",
      "    policy1: 34.23\n",
      "    policy2: 4.112999999999999\n",
      "  policy_reward_min:\n",
      "    policy1: -39.5\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17056645130243178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09617533418191133\n",
      "    mean_inference_ms: 1.3015597787480928\n",
      "    mean_raw_obs_processing_ms: 0.6511769810973372\n",
      "  time_since_restore: 797.062155008316\n",
      "  time_this_iter_s: 5.076087951660156\n",
      "  time_total_s: 797.062155008316\n",
      "  timers:\n",
      "    learn_throughput: 791.292\n",
      "    learn_time_ms: 5055.024\n",
      "    load_throughput: 1353590.757\n",
      "    load_time_ms: 2.955\n",
      "    sample_throughput: 7435.156\n",
      "    sample_time_ms: 537.985\n",
      "    update_time_ms: 3.102\n",
      "  timestamp: 1624525519\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 170\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         797.062</td><td style=\"text-align: right;\">680000</td><td style=\"text-align: right;\">  38.343</td><td style=\"text-align: right;\">                58.5</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-05-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 58.4999999999999\n",
      "  episode_reward_mean: 38.70599999999991\n",
      "  episode_reward_min: -6.600000000000032\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6825\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.19648568332195282\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013811555691063404\n",
      "          model: {}\n",
      "          policy_loss: -0.030408276244997978\n",
      "          total_loss: 51.8018684387207\n",
      "          vf_explained_var: 0.6141733527183533\n",
      "          vf_loss: 51.82529067993164\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.22532397508621216\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009344946593046188\n",
      "          model: {}\n",
      "          policy_loss: -0.01743755303323269\n",
      "          total_loss: 6.263662815093994\n",
      "          vf_explained_var: 0.3368726372718811\n",
      "          vf_loss: 6.275778293609619\n",
      "    num_agent_steps_sampled: 1368000\n",
      "    num_agent_steps_trained: 1368000\n",
      "    num_steps_sampled: 684000\n",
      "    num_steps_trained: 684000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.214285714285715\n",
      "    ram_util_percent: 70.6\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 57.5\n",
      "    policy2: 49.39999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 35.715\n",
      "    policy2: 2.9909999999999997\n",
      "  policy_reward_min:\n",
      "    policy1: -15.5\n",
      "    policy2: -6.699999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17033953746263591\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09609341740593112\n",
      "    mean_inference_ms: 1.302570159469147\n",
      "    mean_raw_obs_processing_ms: 0.6514048271257543\n",
      "  time_since_restore: 802.0693662166595\n",
      "  time_this_iter_s: 5.007211208343506\n",
      "  time_total_s: 802.0693662166595\n",
      "  timers:\n",
      "    learn_throughput: 789.583\n",
      "    learn_time_ms: 5065.965\n",
      "    load_throughput: 1335637.996\n",
      "    load_time_ms: 2.995\n",
      "    sample_throughput: 7643.643\n",
      "    sample_time_ms: 523.311\n",
      "    update_time_ms: 3.114\n",
      "  timestamp: 1624525524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 684000\n",
      "  training_iteration: 171\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         802.069</td><td style=\"text-align: right;\">684000</td><td style=\"text-align: right;\">  38.706</td><td style=\"text-align: right;\">                58.5</td><td style=\"text-align: right;\">                -6.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-05-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 55.19999999999992\n",
      "  episode_reward_mean: 39.29699999999991\n",
      "  episode_reward_min: -6.600000000000032\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6900\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.20012882351875305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010228496044874191\n",
      "          model: {}\n",
      "          policy_loss: -0.020671674981713295\n",
      "          total_loss: 60.546146392822266\n",
      "          vf_explained_var: 0.4853813648223877\n",
      "          vf_loss: 60.56163787841797\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.22226913273334503\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012318629771471024\n",
      "          model: {}\n",
      "          policy_loss: -0.029818058013916016\n",
      "          total_loss: 4.893146991729736\n",
      "          vf_explained_var: 0.3317825496196747\n",
      "          vf_loss: 4.915949821472168\n",
      "    num_agent_steps_sampled: 1384000\n",
      "    num_agent_steps_trained: 1384000\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.599999999999998\n",
      "    ram_util_percent: 70.32857142857144\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 54.5\n",
      "    policy2: 62.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 37.285\n",
      "    policy2: 2.0119999999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -38.0\n",
      "    policy2: -5.599999999999999\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17067999686735014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09624909446685685\n",
      "    mean_inference_ms: 1.302459638135218\n",
      "    mean_raw_obs_processing_ms: 0.6518481083884657\n",
      "  time_since_restore: 811.5292582511902\n",
      "  time_this_iter_s: 4.657608985900879\n",
      "  time_total_s: 811.5292582511902\n",
      "  timers:\n",
      "    learn_throughput: 796.401\n",
      "    learn_time_ms: 5022.594\n",
      "    load_throughput: 1323051.251\n",
      "    load_time_ms: 3.023\n",
      "    sample_throughput: 7588.797\n",
      "    sample_time_ms: 527.093\n",
      "    update_time_ms: 2.731\n",
      "  timestamp: 1624525534\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 173\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         811.529</td><td style=\"text-align: right;\">692000</td><td style=\"text-align: right;\">  39.297</td><td style=\"text-align: right;\">                55.2</td><td style=\"text-align: right;\">                -6.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-05-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 55.199999999999896\n",
      "  episode_reward_mean: 36.27899999999991\n",
      "  episode_reward_min: 9.899999999999958\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 7000\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.20780831575393677\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012387819588184357\n",
      "          model: {}\n",
      "          policy_loss: -0.021960899233818054\n",
      "          total_loss: 51.60196304321289\n",
      "          vf_explained_var: 0.5551533102989197\n",
      "          vf_loss: 51.617645263671875\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.22125209867954254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007934236899018288\n",
      "          model: {}\n",
      "          policy_loss: -0.021471520885825157\n",
      "          total_loss: 11.128745079040527\n",
      "          vf_explained_var: 0.19364383816719055\n",
      "          vf_loss: 11.145700454711914\n",
      "    num_agent_steps_sampled: 1400000\n",
      "    num_agent_steps_trained: 1400000\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.52857142857143\n",
      "    ram_util_percent: 69.11428571428573\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 57.5\n",
      "    policy2: 20.799999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.805\n",
      "    policy2: 2.473999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: 4.0\n",
      "    policy2: -5.599999999999991\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17050703511451623\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0961658408786041\n",
      "    mean_inference_ms: 1.3010222318465388\n",
      "    mean_raw_obs_processing_ms: 0.6513816582092962\n",
      "  time_since_restore: 820.5488893985748\n",
      "  time_this_iter_s: 4.5179760456085205\n",
      "  time_total_s: 820.5488893985748\n",
      "  timers:\n",
      "    learn_throughput: 835.492\n",
      "    learn_time_ms: 4787.598\n",
      "    load_throughput: 1459611.808\n",
      "    load_time_ms: 2.74\n",
      "    sample_throughput: 7936.062\n",
      "    sample_time_ms: 504.028\n",
      "    update_time_ms: 2.688\n",
      "  timestamp: 1624525543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 175\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         820.549</td><td style=\"text-align: right;\">700000</td><td style=\"text-align: right;\">  36.279</td><td style=\"text-align: right;\">                55.2</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-05-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.59999999999991\n",
      "  episode_reward_mean: 35.21699999999991\n",
      "  episode_reward_min: -5.999999999999995\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 7075\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.19571714103221893\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010967262089252472\n",
      "          model: {}\n",
      "          policy_loss: -0.023247255012392998\n",
      "          total_loss: 43.2195930480957\n",
      "          vf_explained_var: 0.6368265748023987\n",
      "          vf_loss: 43.237281799316406\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2530260980129242\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012447717599570751\n",
      "          model: {}\n",
      "          policy_loss: -0.03225291520357132\n",
      "          total_loss: 3.1204562187194824\n",
      "          vf_explained_var: 0.30175504088401794\n",
      "          vf_loss: 3.1456196308135986\n",
      "    num_agent_steps_sampled: 1416000\n",
      "    num_agent_steps_trained: 1416000\n",
      "    num_steps_sampled: 708000\n",
      "    num_steps_trained: 708000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.6\n",
      "    ram_util_percent: 68.2\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 19.69999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.425\n",
      "    policy2: 1.7919999999999978\n",
      "  policy_reward_min:\n",
      "    policy1: 3.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17015010806368808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09602033844696567\n",
      "    mean_inference_ms: 1.300907817687041\n",
      "    mean_raw_obs_processing_ms: 0.6510919948807594\n",
      "  time_since_restore: 829.9288394451141\n",
      "  time_this_iter_s: 4.786085844039917\n",
      "  time_total_s: 829.9288394451141\n",
      "  timers:\n",
      "    learn_throughput: 879.749\n",
      "    learn_time_ms: 4546.753\n",
      "    load_throughput: 1526881.024\n",
      "    load_time_ms: 2.62\n",
      "    sample_throughput: 9077.511\n",
      "    sample_time_ms: 440.649\n",
      "    update_time_ms: 2.482\n",
      "  timestamp: 1624525552\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 708000\n",
      "  training_iteration: 177\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         829.929</td><td style=\"text-align: right;\">708000</td><td style=\"text-align: right;\">  35.217</td><td style=\"text-align: right;\">                51.6</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-06-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 51.89999999999992\n",
      "  episode_reward_mean: 37.03499999999991\n",
      "  episode_reward_min: -5.999999999999995\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 7150\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.1893988400697708\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009377741254866123\n",
      "          model: {}\n",
      "          policy_loss: -0.01907063089311123\n",
      "          total_loss: 38.072967529296875\n",
      "          vf_explained_var: 0.6940510869026184\n",
      "          vf_loss: 38.087284088134766\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.24640899896621704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011343508027493954\n",
      "          model: {}\n",
      "          policy_loss: -0.02985299751162529\n",
      "          total_loss: 4.880937576293945\n",
      "          vf_explained_var: 0.2524052858352661\n",
      "          vf_loss: 4.904330253601074\n",
      "    num_agent_steps_sampled: 1432000\n",
      "    num_agent_steps_trained: 1432000\n",
      "    num_steps_sampled: 716000\n",
      "    num_steps_trained: 716000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.88571428571428\n",
      "    ram_util_percent: 68.48571428571428\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 50.0\n",
      "    policy2: 18.59999999999995\n",
      "  policy_reward_mean:\n",
      "    policy1: 34.11\n",
      "    policy2: 2.9249999999999994\n",
      "  policy_reward_min:\n",
      "    policy1: 4.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17010354382570952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09599787069308785\n",
      "    mean_inference_ms: 1.3005255516331793\n",
      "    mean_raw_obs_processing_ms: 0.6510405909420265\n",
      "  time_since_restore: 839.7853345870972\n",
      "  time_this_iter_s: 4.928250074386597\n",
      "  time_total_s: 839.7853345870972\n",
      "  timers:\n",
      "    learn_throughput: 906.889\n",
      "    learn_time_ms: 4410.684\n",
      "    load_throughput: 1551434.807\n",
      "    load_time_ms: 2.578\n",
      "    sample_throughput: 11154.66\n",
      "    sample_time_ms: 358.595\n",
      "    update_time_ms: 2.426\n",
      "  timestamp: 1624525562\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 716000\n",
      "  training_iteration: 179\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         839.785</td><td style=\"text-align: right;\">716000</td><td style=\"text-align: right;\">  37.035</td><td style=\"text-align: right;\">                51.9</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-06-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 53.69999999999992\n",
      "  episode_reward_mean: 38.18399999999992\n",
      "  episode_reward_min: 14.099999999999962\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 7200\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.1839524358510971\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009239316917955875\n",
      "          model: {}\n",
      "          policy_loss: -0.018481917679309845\n",
      "          total_loss: 39.13581848144531\n",
      "          vf_explained_var: 0.6603797674179077\n",
      "          vf_loss: 39.14962387084961\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2164878100156784\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010713137686252594\n",
      "          model: {}\n",
      "          policy_loss: -0.02493382804095745\n",
      "          total_loss: 9.917719841003418\n",
      "          vf_explained_var: 0.21446338295936584\n",
      "          vf_loss: 9.936551094055176\n",
      "    num_agent_steps_sampled: 1440000\n",
      "    num_agent_steps_trained: 1440000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.375\n",
      "    ram_util_percent: 68.05\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 24.09999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.235\n",
      "    policy2: 4.949\n",
      "  policy_reward_min:\n",
      "    policy1: -10.0\n",
      "    policy2: -4.500000000000004\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17038827446185822\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09610914711436574\n",
      "    mean_inference_ms: 1.3000303024206439\n",
      "    mean_raw_obs_processing_ms: 0.6511061238935586\n",
      "  time_since_restore: 844.8702175617218\n",
      "  time_this_iter_s: 5.084882974624634\n",
      "  time_total_s: 844.8702175617218\n",
      "  timers:\n",
      "    learn_throughput: 906.659\n",
      "    learn_time_ms: 4411.803\n",
      "    load_throughput: 1533846.773\n",
      "    load_time_ms: 2.608\n",
      "    sample_throughput: 11159.124\n",
      "    sample_time_ms: 358.451\n",
      "    update_time_ms: 2.407\n",
      "  timestamp: 1624525567\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 180\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">          844.87</td><td style=\"text-align: right;\">720000</td><td style=\"text-align: right;\">  38.184</td><td style=\"text-align: right;\">                53.7</td><td style=\"text-align: right;\">                14.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-06-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 53.69999999999992\n",
      "  episode_reward_mean: 38.30999999999992\n",
      "  episode_reward_min: 14.099999999999962\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 7225\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.18384236097335815\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009791702032089233\n",
      "          model: {}\n",
      "          policy_loss: -0.022980432957410812\n",
      "          total_loss: 32.68582534790039\n",
      "          vf_explained_var: 0.7147133350372314\n",
      "          vf_loss: 32.70384979248047\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2229248583316803\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011093341745436192\n",
      "          model: {}\n",
      "          policy_loss: -0.026206865906715393\n",
      "          total_loss: 8.816132545471191\n",
      "          vf_explained_var: 0.2074778527021408\n",
      "          vf_loss: 8.836021423339844\n",
      "    num_agent_steps_sampled: 1448000\n",
      "    num_agent_steps_trained: 1448000\n",
      "    num_steps_sampled: 724000\n",
      "    num_steps_trained: 724000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.1\n",
      "    ram_util_percent: 68.04285714285713\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 49.0\n",
      "    policy2: 24.09999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.745\n",
      "    policy2: 5.565000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -10.0\n",
      "    policy2: -4.500000000000004\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17015865486925277\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09603079442949436\n",
      "    mean_inference_ms: 1.30095014929128\n",
      "    mean_raw_obs_processing_ms: 0.6512971204334259\n",
      "  time_since_restore: 849.9027533531189\n",
      "  time_this_iter_s: 5.032535791397095\n",
      "  time_total_s: 849.9027533531189\n",
      "  timers:\n",
      "    learn_throughput: 905.95\n",
      "    learn_time_ms: 4415.256\n",
      "    load_throughput: 1542718.32\n",
      "    load_time_ms: 2.593\n",
      "    sample_throughput: 11190.83\n",
      "    sample_time_ms: 357.435\n",
      "    update_time_ms: 2.44\n",
      "  timestamp: 1624525572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 724000\n",
      "  training_iteration: 181\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         849.903</td><td style=\"text-align: right;\">724000</td><td style=\"text-align: right;\">   38.31</td><td style=\"text-align: right;\">                53.7</td><td style=\"text-align: right;\">                14.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1464000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-06-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 54.89999999999992\n",
      "  episode_reward_mean: 39.85499999999992\n",
      "  episode_reward_min: 14.099999999999959\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 7300\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.18953587114810944\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008410096168518066\n",
      "          model: {}\n",
      "          policy_loss: -0.01896122470498085\n",
      "          total_loss: 67.42171478271484\n",
      "          vf_explained_var: 0.5771100521087646\n",
      "          vf_loss: 67.4364242553711\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.21100202202796936\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007773939054459333\n",
      "          model: {}\n",
      "          policy_loss: -0.018644414842128754\n",
      "          total_loss: 28.828433990478516\n",
      "          vf_explained_var: 0.33528849482536316\n",
      "          vf_loss: 28.842649459838867\n",
      "    num_agent_steps_sampled: 1464000\n",
      "    num_agent_steps_trained: 1464000\n",
      "    num_steps_sampled: 732000\n",
      "    num_steps_trained: 732000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.616666666666664\n",
      "    ram_util_percent: 67.89999999999999\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.0\n",
      "    policy2: 68.1\n",
      "  policy_reward_mean:\n",
      "    policy1: 31.155\n",
      "    policy2: 8.7\n",
      "  policy_reward_min:\n",
      "    policy1: -49.5\n",
      "    policy2: -2.300000000000004\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17050056658315918\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09617625109646293\n",
      "    mean_inference_ms: 1.300847129713569\n",
      "    mean_raw_obs_processing_ms: 0.6516381353673891\n",
      "  time_since_restore: 859.2361872196198\n",
      "  time_this_iter_s: 4.482649087905884\n",
      "  time_total_s: 859.2361872196198\n",
      "  timers:\n",
      "    learn_throughput: 908.64\n",
      "    learn_time_ms: 4402.185\n",
      "    load_throughput: 1549514.75\n",
      "    load_time_ms: 2.581\n",
      "    sample_throughput: 11177.001\n",
      "    sample_time_ms: 357.878\n",
      "    update_time_ms: 2.452\n",
      "  timestamp: 1624525582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 732000\n",
      "  training_iteration: 183\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         859.236</td><td style=\"text-align: right;\">732000</td><td style=\"text-align: right;\">  39.855</td><td style=\"text-align: right;\">                54.9</td><td style=\"text-align: right;\">                14.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1480000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-06-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 56.999999999999915\n",
      "  episode_reward_mean: 38.060999999999915\n",
      "  episode_reward_min: 17.099999999999998\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 7400\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.18408812582492828\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009026827290654182\n",
      "          model: {}\n",
      "          policy_loss: -0.0194469653069973\n",
      "          total_loss: 65.49800872802734\n",
      "          vf_explained_var: 0.5364126563072205\n",
      "          vf_loss: 65.51287841796875\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.22660298645496368\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009082945063710213\n",
      "          model: {}\n",
      "          policy_loss: -0.02203707955777645\n",
      "          total_loss: 21.448963165283203\n",
      "          vf_explained_var: 0.4168204665184021\n",
      "          vf_loss: 21.465824127197266\n",
      "    num_agent_steps_sampled: 1480000\n",
      "    num_agent_steps_trained: 1480000\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.99999999999999\n",
      "    ram_util_percent: 67.89999999999999\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 51.0\n",
      "    policy2: 68.1\n",
      "  policy_reward_mean:\n",
      "    policy1: 30.45\n",
      "    policy2: 7.611000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -51.0\n",
      "    policy2: -4.5000000000000036\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17036051106528077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09610323758064038\n",
      "    mean_inference_ms: 1.2997197116210406\n",
      "    mean_raw_obs_processing_ms: 0.6511930559199907\n",
      "  time_since_restore: 868.3299512863159\n",
      "  time_this_iter_s: 4.55812406539917\n",
      "  time_total_s: 868.3299512863159\n",
      "  timers:\n",
      "    learn_throughput: 907.519\n",
      "    learn_time_ms: 4407.621\n",
      "    load_throughput: 1533019.856\n",
      "    load_time_ms: 2.609\n",
      "    sample_throughput: 11117.096\n",
      "    sample_time_ms: 359.806\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1624525591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 185\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">          868.33</td><td style=\"text-align: right;\">740000</td><td style=\"text-align: right;\">  38.061</td><td style=\"text-align: right;\">                  57</td><td style=\"text-align: right;\">                17.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1496000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-06-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 53.99999999999991\n",
      "  episode_reward_mean: 39.023999999999916\n",
      "  episode_reward_min: 17.099999999999923\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 7475\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.19650813937187195\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011871038936078548\n",
      "          model: {}\n",
      "          policy_loss: -0.024063540622591972\n",
      "          total_loss: 38.556392669677734\n",
      "          vf_explained_var: 0.7021740674972534\n",
      "          vf_loss: 38.57444381713867\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.22809235751628876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008232084102928638\n",
      "          model: {}\n",
      "          policy_loss: -0.018718615174293518\n",
      "          total_loss: 9.289772987365723\n",
      "          vf_explained_var: 0.2491808533668518\n",
      "          vf_loss: 9.303802490234375\n",
      "    num_agent_steps_sampled: 1496000\n",
      "    num_agent_steps_trained: 1496000\n",
      "    num_steps_sampled: 748000\n",
      "    num_steps_trained: 748000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.53333333333333\n",
      "    ram_util_percent: 67.89999999999999\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 54.0\n",
      "    policy2: 61.5\n",
      "  policy_reward_mean:\n",
      "    policy1: 32.975\n",
      "    policy2: 6.0489999999999995\n",
      "  policy_reward_min:\n",
      "    policy1: -36.0\n",
      "    policy2: -4.500000000000001\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16999114872175014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09595451142902416\n",
      "    mean_inference_ms: 1.2995029686217399\n",
      "    mean_raw_obs_processing_ms: 0.6508898930512023\n",
      "  time_since_restore: 877.3189363479614\n",
      "  time_this_iter_s: 4.5206990242004395\n",
      "  time_total_s: 877.3189363479614\n",
      "  timers:\n",
      "    learn_throughput: 914.768\n",
      "    learn_time_ms: 4372.694\n",
      "    load_throughput: 1543839.812\n",
      "    load_time_ms: 2.591\n",
      "    sample_throughput: 11242.218\n",
      "    sample_time_ms: 355.802\n",
      "    update_time_ms: 2.451\n",
      "  timestamp: 1624525600\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 748000\n",
      "  training_iteration: 187\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         877.319</td><td style=\"text-align: right;\">748000</td><td style=\"text-align: right;\">  39.024</td><td style=\"text-align: right;\">                  54</td><td style=\"text-align: right;\">                17.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1512000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-06-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 57.599999999999916\n",
      "  episode_reward_mean: 40.17299999999992\n",
      "  episode_reward_min: -2.699999999999995\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 7550\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.20104047656059265\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011980127543210983\n",
      "          model: {}\n",
      "          policy_loss: -0.02441900037229061\n",
      "          total_loss: 49.33220672607422\n",
      "          vf_explained_var: 0.6595445275306702\n",
      "          vf_loss: 49.350563049316406\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.24310718476772308\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01222875714302063\n",
      "          model: {}\n",
      "          policy_loss: -0.027763407677412033\n",
      "          total_loss: 4.809042453765869\n",
      "          vf_explained_var: 0.25583869218826294\n",
      "          vf_loss: 4.829841136932373\n",
      "    num_agent_steps_sampled: 1512000\n",
      "    num_agent_steps_trained: 1512000\n",
      "    num_steps_sampled: 756000\n",
      "    num_steps_trained: 756000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.933333333333326\n",
      "    ram_util_percent: 68.31666666666668\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 54.5\n",
      "    policy2: 31.79999999999997\n",
      "  policy_reward_mean:\n",
      "    policy1: 35.18\n",
      "    policy2: 4.993000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -18.0\n",
      "    policy2: -4.500000000000004\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16989888669623632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09590280948543867\n",
      "    mean_inference_ms: 1.29871522693903\n",
      "    mean_raw_obs_processing_ms: 0.6506526986017992\n",
      "  time_since_restore: 886.4286968708038\n",
      "  time_this_iter_s: 4.5324318408966064\n",
      "  time_total_s: 886.4286968708038\n",
      "  timers:\n",
      "    learn_throughput: 930.584\n",
      "    learn_time_ms: 4298.377\n",
      "    load_throughput: 1542221.977\n",
      "    load_time_ms: 2.594\n",
      "    sample_throughput: 11252.851\n",
      "    sample_time_ms: 355.465\n",
      "    update_time_ms: 2.451\n",
      "  timestamp: 1624525609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 756000\n",
      "  training_iteration: 189\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         886.429</td><td style=\"text-align: right;\">756000</td><td style=\"text-align: right;\">  40.173</td><td style=\"text-align: right;\">                57.6</td><td style=\"text-align: right;\">                -2.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1528000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-06-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 55.4999999999999\n",
      "  episode_reward_mean: 39.14999999999992\n",
      "  episode_reward_min: -2.699999999999995\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 7625\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.20203912258148193\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012347275391221046\n",
      "          model: {}\n",
      "          policy_loss: -0.02589050866663456\n",
      "          total_loss: 39.561344146728516\n",
      "          vf_explained_var: 0.6760807633399963\n",
      "          vf_loss: 39.58098220825195\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.2411540150642395\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015485757030546665\n",
      "          model: {}\n",
      "          policy_loss: -0.03416997939348221\n",
      "          total_loss: 8.409971237182617\n",
      "          vf_explained_var: 0.3465215265750885\n",
      "          vf_loss: 8.435322761535645\n",
      "    num_agent_steps_sampled: 1528000\n",
      "    num_agent_steps_trained: 1528000\n",
      "    num_steps_sampled: 764000\n",
      "    num_steps_trained: 764000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.971428571428575\n",
      "    ram_util_percent: 68.48571428571428\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 54.5\n",
      "    policy2: 42.799999999999976\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.86\n",
      "    policy2: 5.29\n",
      "  policy_reward_min:\n",
      "    policy1: -18.0\n",
      "    policy2: -5.599999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16983556009263764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09587303656517658\n",
      "    mean_inference_ms: 1.2981928944300876\n",
      "    mean_raw_obs_processing_ms: 0.6504233754871417\n",
      "  time_since_restore: 895.8076198101044\n",
      "  time_this_iter_s: 4.737802982330322\n",
      "  time_total_s: 895.8076198101044\n",
      "  timers:\n",
      "    learn_throughput: 944.766\n",
      "    learn_time_ms: 4233.851\n",
      "    load_throughput: 1604124.374\n",
      "    load_time_ms: 2.494\n",
      "    sample_throughput: 11548.748\n",
      "    sample_time_ms: 346.358\n",
      "    update_time_ms: 2.413\n",
      "  timestamp: 1624525619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 764000\n",
      "  training_iteration: 191\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         895.808</td><td style=\"text-align: right;\">764000</td><td style=\"text-align: right;\">   39.15</td><td style=\"text-align: right;\">                55.5</td><td style=\"text-align: right;\">                -2.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">          900.74</td><td style=\"text-align: right;\">768000</td><td style=\"text-align: right;\">  39.096</td><td style=\"text-align: right;\">                51.9</td><td style=\"text-align: right;\">                 3.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1544000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-07-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 53.09999999999991\n",
      "  episode_reward_mean: 39.97799999999992\n",
      "  episode_reward_min: 3.5999999999999477\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 7700\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.18473400175571442\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011563507840037346\n",
      "          model: {}\n",
      "          policy_loss: -0.020871305838227272\n",
      "          total_loss: 89.98067474365234\n",
      "          vf_explained_var: 0.5401643514633179\n",
      "          vf_loss: 89.99569702148438\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.19520749151706696\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008265364915132523\n",
      "          model: {}\n",
      "          policy_loss: -0.018872855231165886\n",
      "          total_loss: 41.39750671386719\n",
      "          vf_explained_var: 0.44704779982566833\n",
      "          vf_loss: 41.41167449951172\n",
      "    num_agent_steps_sampled: 1544000\n",
      "    num_agent_steps_trained: 1544000\n",
      "    num_steps_sampled: 772000\n",
      "    num_steps_trained: 772000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.428571428571427\n",
      "    ram_util_percent: 68.55714285714285\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 75.8\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.885\n",
      "    policy2: 6.093000000000001\n",
      "  policy_reward_min:\n",
      "    policy1: -59.0\n",
      "    policy2: -5.599999999999995\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17027862476909228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09604323309033083\n",
      "    mean_inference_ms: 1.2985685299490926\n",
      "    mean_raw_obs_processing_ms: 0.6509597167099931\n",
      "  time_since_restore: 905.5105481147766\n",
      "  time_this_iter_s: 4.7705042362213135\n",
      "  time_total_s: 905.5105481147766\n",
      "  timers:\n",
      "    learn_throughput: 938.957\n",
      "    learn_time_ms: 4260.045\n",
      "    load_throughput: 1577443.516\n",
      "    load_time_ms: 2.536\n",
      "    sample_throughput: 11204.631\n",
      "    sample_time_ms: 356.995\n",
      "    update_time_ms: 2.423\n",
      "  timestamp: 1624525628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 772000\n",
      "  training_iteration: 193\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         910.114</td><td style=\"text-align: right;\">776000</td><td style=\"text-align: right;\">  40.551</td><td style=\"text-align: right;\">                53.1</td><td style=\"text-align: right;\">                 2.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1560000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-07-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 52.79999999999991\n",
      "  episode_reward_mean: 40.31399999999992\n",
      "  episode_reward_min: 2.399999999999977\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 7800\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.202640101313591\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011157268658280373\n",
      "          model: {}\n",
      "          policy_loss: -0.020765461027622223\n",
      "          total_loss: 31.57920265197754\n",
      "          vf_explained_var: 0.7093360424041748\n",
      "          vf_loss: 31.594318389892578\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.21966570615768433\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00893244706094265\n",
      "          model: {}\n",
      "          policy_loss: -0.024888603016734123\n",
      "          total_loss: 4.489319801330566\n",
      "          vf_explained_var: 0.30323436856269836\n",
      "          vf_loss: 4.506577014923096\n",
      "    num_agent_steps_sampled: 1560000\n",
      "    num_agent_steps_trained: 1560000\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.17142857142857\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 52.0\n",
      "    policy2: 86.8\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.495\n",
      "    policy2: 6.819\n",
      "  policy_reward_min:\n",
      "    policy1: -77.5\n",
      "    policy2: -6.6999999999999815\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17024280802688374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09602322262842286\n",
      "    mean_inference_ms: 1.2981237688569576\n",
      "    mean_raw_obs_processing_ms: 0.6509036818061099\n",
      "  time_since_restore: 914.5564157962799\n",
      "  time_this_iter_s: 4.442256689071655\n",
      "  time_total_s: 914.5564157962799\n",
      "  timers:\n",
      "    learn_throughput: 939.592\n",
      "    learn_time_ms: 4257.167\n",
      "    load_throughput: 1574157.761\n",
      "    load_time_ms: 2.541\n",
      "    sample_throughput: 11265.303\n",
      "    sample_time_ms: 355.073\n",
      "    update_time_ms: 2.423\n",
      "  timestamp: 1624525638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 195\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         919.025</td><td style=\"text-align: right;\">784000</td><td style=\"text-align: right;\">  39.357</td><td style=\"text-align: right;\">                52.8</td><td style=\"text-align: right;\">                 2.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1576000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-07-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 57.29999999999991\n",
      "  episode_reward_mean: 39.794999999999916\n",
      "  episode_reward_min: 0.3000000000000218\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 7875\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.19094966351985931\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012877286411821842\n",
      "          model: {}\n",
      "          policy_loss: -0.02434065379202366\n",
      "          total_loss: 39.71512222290039\n",
      "          vf_explained_var: 0.7025989890098572\n",
      "          vf_loss: 39.73294448852539\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.24979136884212494\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008634583093225956\n",
      "          model: {}\n",
      "          policy_loss: -0.02496694028377533\n",
      "          total_loss: 5.619181156158447\n",
      "          vf_explained_var: 0.252747118473053\n",
      "          vf_loss: 5.636770248413086\n",
      "    num_agent_steps_sampled: 1576000\n",
      "    num_agent_steps_trained: 1576000\n",
      "    num_steps_sampled: 788000\n",
      "    num_steps_trained: 788000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.5\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 53.0\n",
      "    policy2: 57.099999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 34.505\n",
      "    policy2: 5.290000000000003\n",
      "  policy_reward_min:\n",
      "    policy1: -29.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1698257257286295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09585403244084838\n",
      "    mean_inference_ms: 1.2976140427916139\n",
      "    mean_raw_obs_processing_ms: 0.6504720021711315\n",
      "  time_since_restore: 923.55779504776\n",
      "  time_this_iter_s: 4.533263206481934\n",
      "  time_total_s: 923.55779504776\n",
      "  timers:\n",
      "    learn_throughput: 939.369\n",
      "    learn_time_ms: 4258.18\n",
      "    load_throughput: 1581995.078\n",
      "    load_time_ms: 2.528\n",
      "    sample_throughput: 11257.665\n",
      "    sample_time_ms: 355.313\n",
      "    update_time_ms: 2.408\n",
      "  timestamp: 1624525647\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 788000\n",
      "  training_iteration: 197\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         928.169</td><td style=\"text-align: right;\">792000</td><td style=\"text-align: right;\">  38.325</td><td style=\"text-align: right;\">                57.3</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1592000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-07-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 57.5999999999999\n",
      "  episode_reward_mean: 38.35499999999991\n",
      "  episode_reward_min: 0.3000000000000218\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 7950\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.20364879071712494\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011059199459850788\n",
      "          model: {}\n",
      "          policy_loss: -0.022203342989087105\n",
      "          total_loss: 34.86799621582031\n",
      "          vf_explained_var: 0.7124600410461426\n",
      "          vf_loss: 34.88459777832031\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.23901718854904175\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007578826975077391\n",
      "          model: {}\n",
      "          policy_loss: -0.0234538484364748\n",
      "          total_loss: 4.9296770095825195\n",
      "          vf_explained_var: 0.30195239186286926\n",
      "          vf_loss: 4.946656227111816\n",
      "    num_agent_steps_sampled: 1592000\n",
      "    num_agent_steps_trained: 1592000\n",
      "    num_steps_sampled: 796000\n",
      "    num_steps_trained: 796000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.24285714285715\n",
      "    ram_util_percent: 68.47142857142856\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 18.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy1: 33.12\n",
      "    policy2: 5.235000000000003\n",
      "  policy_reward_min:\n",
      "    policy1: -4.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16974558976217474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0958105042063076\n",
      "    mean_inference_ms: 1.2970186514942212\n",
      "    mean_raw_obs_processing_ms: 0.6502697077015586\n",
      "  time_since_restore: 932.9047169685364\n",
      "  time_this_iter_s: 4.735816955566406\n",
      "  time_total_s: 932.9047169685364\n",
      "  timers:\n",
      "    learn_throughput: 934.281\n",
      "    learn_time_ms: 4281.366\n",
      "    load_throughput: 1560322.905\n",
      "    load_time_ms: 2.564\n",
      "    sample_throughput: 11242.247\n",
      "    sample_time_ms: 355.801\n",
      "    update_time_ms: 2.404\n",
      "  timestamp: 1624525656\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 796000\n",
      "  training_iteration: 199\n",
      "  trial_id: 62cb1_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_62cb1_00000:\n",
      "  agent_timesteps_total: 1600000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-24_11-07-41\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 57.5999999999999\n",
      "  episode_reward_mean: 39.21899999999991\n",
      "  episode_reward_min: 15.59999999999991\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 8000\n",
      "  experiment_id: 3d6fd9cc179a437092430c6672b67fdc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.20134295523166656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009897777810692787\n",
      "          model: {}\n",
      "          policy_loss: -0.020787587389349937\n",
      "          total_loss: 40.71434020996094\n",
      "          vf_explained_var: 0.6131030917167664\n",
      "          vf_loss: 40.73011779785156\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 0.20474961400032043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005795901641249657\n",
      "          model: {}\n",
      "          policy_loss: -0.02007148414850235\n",
      "          total_loss: 6.106076240539551\n",
      "          vf_explained_var: 0.27850353717803955\n",
      "          vf_loss: 6.1211957931518555\n",
      "    num_agent_steps_sampled: 1600000\n",
      "    num_agent_steps_trained: 1600000\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.67142857142857\n",
      "    ram_util_percent: 68.39999999999999\n",
      "  pid: 2801\n",
      "  policy_reward_max:\n",
      "    policy1: 55.5\n",
      "    policy2: 16.39999999999999\n",
      "  policy_reward_mean:\n",
      "    policy1: 34.05\n",
      "    policy2: 5.169000000000002\n",
      "  policy_reward_min:\n",
      "    policy1: 4.0\n",
      "    policy2: -3.4000000000000057\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17002226799727588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09590577620408046\n",
      "    mean_inference_ms: 1.2964176359228663\n",
      "    mean_raw_obs_processing_ms: 0.6502087971090785\n",
      "  time_since_restore: 937.6105468273163\n",
      "  time_this_iter_s: 4.705829858779907\n",
      "  time_total_s: 937.6105468273163\n",
      "  timers:\n",
      "    learn_throughput: 933.656\n",
      "    learn_time_ms: 4284.232\n",
      "    load_throughput: 1540635.824\n",
      "    load_time_ms: 2.596\n",
      "    sample_throughput: 11133.386\n",
      "    sample_time_ms: 359.28\n",
      "    update_time_ms: 2.408\n",
      "  timestamp: 1624525661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 200\n",
      "  trial_id: 62cb1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>RUNNING </td><td>192.168.0.179:2801</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         937.611</td><td style=\"text-align: right;\">800000</td><td style=\"text-align: right;\">  39.219</td><td style=\"text-align: right;\">                57.6</td><td style=\"text-align: right;\">                15.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_62cb1_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         937.611</td><td style=\"text-align: right;\">800000</td><td style=\"text-align: right;\">  39.219</td><td style=\"text-align: right;\">                57.6</td><td style=\"text-align: right;\">                15.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m 2021-06-24 11:07:41,997\tERROR worker.py:409 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1015, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1091, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"python/ray/_raylet.pyx\", line 595, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"python/ray/_raylet.pyx\", line 453, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"python/ray/_raylet.pyx\", line 490, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 120, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m     return list(TracebackException(\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 366, in extract\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 288, in line\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 16, in getline\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 47, in getlines\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m     return updatecache(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 136, in updatecache\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m     with tokenize.open(fullname) as fp:\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/tokenize.py\", line 396, in open\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m     text = TextIOWrapper(buffer, encoding, line_buffering=True)\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/codecs.py\", line 309, in __init__\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m     def __init__(self, errors='strict'):\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 406, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=2836)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m 2021-06-24 11:07:41,996\tERROR worker.py:409 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1015, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1091, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"python/ray/_raylet.pyx\", line 595, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"python/ray/_raylet.pyx\", line 453, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"python/ray/_raylet.pyx\", line 490, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 120, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m     return list(TracebackException(\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 366, in extract\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 288, in line\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 16, in getline\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 47, in getlines\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m     return updatecache(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 136, in updatecache\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m     with tokenize.open(fullname) as fp:\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 406, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=2837)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m 2021-06-24 11:07:41,997\tERROR worker.py:409 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1015, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1091, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"python/ray/_raylet.pyx\", line 595, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"python/ray/_raylet.pyx\", line 453, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"python/ray/_raylet.pyx\", line 490, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 120, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m     return list(TracebackException(\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 366, in extract\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 288, in line\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 16, in getline\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 47, in getlines\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m     return updatecache(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 137, in updatecache\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m     lines = fp.readlines()\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/codecs.py\", line 319, in decode\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m     def decode(self, input, final=False):\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 406, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=2833)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m 2021-06-24 11:07:41,997\tERROR worker.py:409 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1015, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1091, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"python/ray/_raylet.pyx\", line 595, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"python/ray/_raylet.pyx\", line 453, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"python/ray/_raylet.pyx\", line 490, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 120, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     return list(TracebackException(\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 366, in extract\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 288, in line\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 16, in getline\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 47, in getlines\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     return updatecache(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 136, in updatecache\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     with tokenize.open(fullname) as fp:\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/tokenize.py\", line 394, in open\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     encoding, lines = detect_encoding(buffer.readline)\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/tokenize.py\", line 371, in detect_encoding\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     encoding = find_cookie(first)\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/tokenize.py\", line 337, in find_cookie\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     match = cookie_re.match(line_string)\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 406, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=2834)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m 2021-06-24 11:07:41,997\tERROR worker.py:409 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1015, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1091, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"python/ray/_raylet.pyx\", line 595, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"python/ray/_raylet.pyx\", line 453, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"python/ray/_raylet.pyx\", line 490, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 120, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m     return list(TracebackException(\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 366, in extract\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/traceback.py\", line 288, in line\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 16, in getline\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 47, in getlines\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m     return updatecache(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/linecache.py\", line 137, in updatecache\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m     lines = fp.readlines()\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/codecs.py\", line 319, in decode\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m     def decode(self, input, final=False):\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 406, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=2835)\u001b[0m SystemExit: 1\n",
      "2021-06-24 11:07:42,103\tINFO tune.py:549 -- Total run time: 964.51 seconds (963.75 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "\n",
    "# Solution to Exercise #2\n",
    "\n",
    "# Run for longer this time (100 iterations) and try to reach 40.0 reward (sum of both agents).\n",
    "stop = {\n",
    "    \"training_iteration\": 180,  # we have the 15min break now to run this many iterations\n",
    "    \"episode_reward_mean\": 60.0,  # sum of both agents' rewards. Probably won't reach it, but we should try nevertheless :)\n",
    "}\n",
    "\n",
    "# tune_config.update({\n",
    "# ???\n",
    "# })\n",
    "\n",
    "# analysis = tune.run(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d282a-4ad1-4d5f-9dec-00afb8154048",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 15 min break :)\n",
    "------------------\n",
    "\n",
    "\n",
    "(while the above experiment is running (and hopefully learning))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82057-6b4c-4075-bd32-93c3426a1700",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How do we extract any checkpoint from a trial of a tune.run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5160e4d0-8feb-411d-a457-dfc10d50e909",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 11:14:28,753\tWARNING ppo.py:135 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=5 num_envs_per_worker=5 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 160.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis object at 0x7fbc2e7e3f70>\n",
      "Found best checkpoint for trial #2: /Users/sven/ray_results/PPO/PPO_MultiAgentArena_62cb1_00000_0_2021-06-24_10-51-37/checkpoint_000200/checkpoint-200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "2021-06-24 11:14:41,972\tINFO trainable.py:101 -- Trainable.setup took 13.221 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-06-24 11:14:41,974\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "2021-06-24 11:14:42,252\tINFO trainable.py:377 -- Restored on 192.168.0.179 from checkpoint: /Users/sven/ray_results/PPO/PPO_MultiAgentArena_62cb1_00000_0_2021-06-24_10-51-37/checkpoint_000200/checkpoint-200\n",
      "2021-06-24 11:14:42,253\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 200, '_timesteps_total': None, '_time_total': 937.6105468273163, '_episodes_total': 8000}\n"
     ]
    }
   ],
   "source": [
    "# The previous tune.run (the one we did before the exercise) returned an Analysis object, from which we can access any checkpoint\n",
    "# (given we set checkpoint_freq or checkpoint_at_end to reasonable values) like so:\n",
    "print(analysis)\n",
    "# Get all trials (we only have one).\n",
    "trials = analysis.trials\n",
    "# Assuming, the first trial was the best, we'd like to extract this trial's best checkpoint \"\":\n",
    "best_checkpoint = analysis.get_best_checkpoint(trial=trials[0], metric=\"episode_reward_mean\", mode=\"max\")\n",
    "print(f\"Found best checkpoint for trial #2: {best_checkpoint}\")\n",
    "\n",
    "# Undo the grid-search config, which RLlib doesn't understand.\n",
    "rllib_config = tune_config.copy()\n",
    "rllib_config[\"lr\"] = 0.00005\n",
    "rllib_config[\"train_batch_size\"] = 4000\n",
    "\n",
    "# Restore a RLlib Trainer from the checkpoint.\n",
    "new_trainer = PPOTrainer(config=rllib_config)\n",
    "new_trainer.restore(best_checkpoint)\n",
    "new_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a2e104d-72f8-4b80-bf9a-2f8cbd25d9cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97eb3a1a4e9544f2b4a54655e6866598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "with out:\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        a1 = new_trainer.compute_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "        a2 = new_trainer.compute_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "        actions = {\"agent1\": a1, \"agent2\": a2}\n",
    "        obs, rewards, dones, _ = env.step(actions)\n",
    "\n",
    "        out.clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.07)\n",
    "\n",
    "        if dones[\"agent1\"] is True:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3f56e-c4e1-4503-9ce5-589e826d1e5a",
   "metadata": {},
   "source": [
    "## Let's talk about customization options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f940b-697c-4d1d-af28-1d174331dc3c",
   "metadata": {},
   "source": [
    "### Deep Dive: How do we customize RLlib's RL loop?\n",
    "\n",
    "RLlib offers a callbacks API that allows you to add custom behavior to\n",
    "all major events during the environment sampling- and learning process.\n",
    "\n",
    "**Our problem:** So far, we can only see standard stats, such as rewards, episode lengths, etc..\n",
    "This does not give us enough insights sometimes into important questions, such as: How many times\n",
    "have both agents collided? or How many times has agent1 discovered a new field?\n",
    "\n",
    "In the following cell, we will create custom callback \"hooks\" that will allow us to\n",
    "add these stats to the returned metrics dict, and which will therefore be displayed in tensorboard!\n",
    "\n",
    "For that we will override RLlib's DefaultCallbacks class and implement the\n",
    "`on_episode_start`, `on_episode_step`, and `on_episode_end` methods therein:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "adcb9733-01bc-426b-ad57-7983fc7db8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the DefaultCallbacks with your own and implement any methods (hooks)\n",
    "# that you need.\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(self,\n",
    "                         *,\n",
    "                         worker,\n",
    "                         base_env,\n",
    "                         policies,\n",
    "                         episode: MultiAgentEpisode,\n",
    "                         env_index,\n",
    "                         **kwargs):\n",
    "        # We will use the `MultiAgentEpisode` object being passed into\n",
    "        # all episode-related callbacks. It comes with a user_data property (dict),\n",
    "        # which we can write arbitrary data into.\n",
    "\n",
    "        # At the end of an episode, we'll transfer that data into the `hist_data`, and `custom_metrics`\n",
    "        # properties to make sure our custom data is displayed in TensorBoard.\n",
    "\n",
    "        # The episode is starting:\n",
    "        # Set per-episode object to capture, which states (observations)\n",
    "        # have been visited by agent1.\n",
    "        episode.user_data[\"new_fields_discovered\"] = 0\n",
    "        # Set per-episode agent2-blocks counter (how many times has agent2 blocked agent1?).\n",
    "        episode.user_data[\"num_collisions\"] = 0\n",
    "\n",
    "    def on_episode_step(self,\n",
    "                        *,\n",
    "                        worker,\n",
    "                        base_env,\n",
    "                        episode: MultiAgentEpisode,\n",
    "                        env_index,\n",
    "                        **kwargs):\n",
    "        # Get both rewards.\n",
    "        ag1_r = episode.prev_reward_for(\"agent1\")\n",
    "        ag2_r = episode.prev_reward_for(\"agent2\")\n",
    "\n",
    "        # Agent1 discovered a new field.\n",
    "        if ag1_r == 1.0:\n",
    "            episode.user_data[\"new_fields_discovered\"] += 1\n",
    "        # Collision.\n",
    "        elif ag2_r == 1.0:\n",
    "            episode.user_data[\"num_collisions\"] += 1\n",
    "\n",
    "    def on_episode_end(self,\n",
    "                       *,\n",
    "                       worker,\n",
    "                       base_env,\n",
    "                       policies,\n",
    "                       episode: MultiAgentEpisode,\n",
    "                       env_index,\n",
    "                       **kwargs):\n",
    "        # Episode is done:\n",
    "        # Write scalar values (sum over rewards) to `custom_metrics` and\n",
    "        # time-series data (rewards per time step) to `hist_data`.\n",
    "        # Both will be visible then in TensorBoard.\n",
    "        episode.custom_metrics[\"new_fields_discovered\"] = episode.user_data[\"new_fields_discovered\"]\n",
    "        episode.custom_metrics[\"num_collisions\"] = episode.user_data[\"num_collisions\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd2fe8eb-c52f-4a26-9067-96ad9fe160a4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_bdcf5_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3056)\u001b[0m 2021-06-24 11:44:23,017\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=3056)\u001b[0m 2021-06-24 11:44:23,017\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=3056)\u001b[0m 2021-06-24 11:44:32,054\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_bdcf5_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 42\n",
      "    new_fields_discovered_mean: 33.85\n",
      "    new_fields_discovered_min: 22\n",
      "    num_collisions_max: 6\n",
      "    num_collisions_mean: 0.9\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-44-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.800000000000013\n",
      "  episode_reward_mean: -8.309999999999997\n",
      "  episode_reward_min: -27.000000000000043\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: ceeef6f183654ea083d83a3b87cf5a84\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3663787841796875\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02001993916928768\n",
      "          model: {}\n",
      "          policy_loss: -0.05442614480853081\n",
      "          total_loss: 16.660539627075195\n",
      "          vf_explained_var: 0.1777394711971283\n",
      "          vf_loss: 16.71096420288086\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.86\n",
      "    ram_util_percent: 67.52000000000001\n",
      "  pid: 3056\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051706820934802514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026571643459689726\n",
      "    mean_inference_ms: 0.5553355821958191\n",
      "    mean_raw_obs_processing_ms: 0.1294213932353657\n",
      "  time_since_restore: 3.0595901012420654\n",
      "  time_this_iter_s: 3.0595901012420654\n",
      "  time_total_s: 3.0595901012420654\n",
      "  timers:\n",
      "    learn_throughput: 1807.826\n",
      "    learn_time_ms: 2212.602\n",
      "    load_throughput: 119936.633\n",
      "    load_time_ms: 33.351\n",
      "    sample_throughput: 5071.783\n",
      "    sample_time_ms: 788.677\n",
      "    update_time_ms: 1.515\n",
      "  timestamp: 1624527875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: bdcf5_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_bdcf5_00000</td><td>RUNNING </td><td>192.168.0.179:3056</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.05959</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">   -8.31</td><td style=\"text-align: right;\">                 4.8</td><td style=\"text-align: right;\">                 -27</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_bdcf5_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 51\n",
      "    new_fields_discovered_mean: 37.5\n",
      "    new_fields_discovered_min: 20\n",
      "    num_collisions_max: 6\n",
      "    num_collisions_mean: 1.0\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-44-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.199999999999932\n",
      "  episode_reward_mean: -2.6449999999999956\n",
      "  episode_reward_min: -28.500000000000064\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: ceeef6f183654ea083d83a3b87cf5a84\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3080261945724487\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01789451576769352\n",
      "          model: {}\n",
      "          policy_loss: -0.05154493451118469\n",
      "          total_loss: 14.574112892150879\n",
      "          vf_explained_var: 0.37112146615982056\n",
      "          vf_loss: 14.62028980255127\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.900000000000002\n",
      "    ram_util_percent: 67.4\n",
      "  pid: 3056\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051352002494182626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026677893807661206\n",
      "    mean_inference_ms: 0.5396981521413505\n",
      "    mean_raw_obs_processing_ms: 0.12917813648925053\n",
      "  time_since_restore: 8.572405815124512\n",
      "  time_this_iter_s: 2.722538948059082\n",
      "  time_total_s: 8.572405815124512\n",
      "  timers:\n",
      "    learn_throughput: 1927.418\n",
      "    learn_time_ms: 2075.315\n",
      "    load_throughput: 311622.128\n",
      "    load_time_ms: 12.836\n",
      "    sample_throughput: 5281.454\n",
      "    sample_time_ms: 757.367\n",
      "    update_time_ms: 1.603\n",
      "  timestamp: 1624527880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: bdcf5_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_bdcf5_00000</td><td>RUNNING </td><td>192.168.0.179:3056</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         8.57241</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -2.645</td><td style=\"text-align: right;\">                19.2</td><td style=\"text-align: right;\">               -28.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_bdcf5_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 55\n",
      "    new_fields_discovered_mean: 38.9\n",
      "    new_fields_discovered_min: 20\n",
      "    num_collisions_max: 6\n",
      "    num_collisions_mean: 1.11\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-44-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999943\n",
      "  episode_reward_mean: -0.515999999999994\n",
      "  episode_reward_min: -28.500000000000064\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: ceeef6f183654ea083d83a3b87cf5a84\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2383875846862793\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02025177888572216\n",
      "          model: {}\n",
      "          policy_loss: -0.060310278087854385\n",
      "          total_loss: 13.677404403686523\n",
      "          vf_explained_var: 0.43174514174461365\n",
      "          vf_loss: 13.73163890838623\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.700000000000003\n",
      "    ram_util_percent: 67.35000000000001\n",
      "  pid: 3056\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05105487838279888\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0266053474948197\n",
      "    mean_inference_ms: 0.5324943125497549\n",
      "    mean_raw_obs_processing_ms: 0.12858417413376397\n",
      "  time_since_restore: 14.109519004821777\n",
      "  time_this_iter_s: 2.726815938949585\n",
      "  time_total_s: 14.109519004821777\n",
      "  timers:\n",
      "    learn_throughput: 1942.88\n",
      "    learn_time_ms: 2058.8\n",
      "    load_throughput: 464874.175\n",
      "    load_time_ms: 8.604\n",
      "    sample_throughput: 5369.309\n",
      "    sample_time_ms: 744.975\n",
      "    update_time_ms: 1.595\n",
      "  timestamp: 1624527886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: bdcf5_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_bdcf5_00000</td><td>RUNNING </td><td>192.168.0.179:3056</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         14.1095</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -0.516</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">               -28.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_bdcf5_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 55\n",
      "    new_fields_discovered_mean: 41.02\n",
      "    new_fields_discovered_min: 28\n",
      "    num_collisions_max: 10\n",
      "    num_collisions_mean: 1.53\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-44-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999943\n",
      "  episode_reward_mean: 2.826000000000006\n",
      "  episode_reward_min: -17.999999999999993\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 140\n",
      "  experiment_id: ceeef6f183654ea083d83a3b87cf5a84\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1907373666763306\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017137594521045685\n",
      "          model: {}\n",
      "          policy_loss: -0.053841304033994675\n",
      "          total_loss: 14.601898193359375\n",
      "          vf_explained_var: 0.4164199233055115\n",
      "          vf_loss: 14.648026466369629\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.575000000000003\n",
      "    ram_util_percent: 68.025\n",
      "  pid: 3056\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0507041061756604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02658429544413976\n",
      "    mean_inference_ms: 0.5224579663657494\n",
      "    mean_raw_obs_processing_ms: 0.1282608848680318\n",
      "  time_since_restore: 19.757194995880127\n",
      "  time_this_iter_s: 2.823655128479004\n",
      "  time_total_s: 19.757194995880127\n",
      "  timers:\n",
      "    learn_throughput: 1939.967\n",
      "    learn_time_ms: 2061.89\n",
      "    load_throughput: 584063.22\n",
      "    load_time_ms: 6.849\n",
      "    sample_throughput: 5367.949\n",
      "    sample_time_ms: 745.164\n",
      "    update_time_ms: 1.614\n",
      "  timestamp: 1624527891\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: bdcf5_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_bdcf5_00000</td><td>RUNNING </td><td>192.168.0.179:3056</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         19.7572</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   2.826</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">                 -18</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_bdcf5_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 54\n",
      "    new_fields_discovered_mean: 40.71\n",
      "    new_fields_discovered_min: 21\n",
      "    num_collisions_max: 10\n",
      "    num_collisions_mean: 1.72\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-44-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.599999999999973\n",
      "  episode_reward_mean: 2.385000000000004\n",
      "  episode_reward_min: -24.900000000000013\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 180\n",
      "  experiment_id: ceeef6f183654ea083d83a3b87cf5a84\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1365190744400024\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017470214515924454\n",
      "          model: {}\n",
      "          policy_loss: -0.0472416952252388\n",
      "          total_loss: 17.196090698242188\n",
      "          vf_explained_var: 0.3536999523639679\n",
      "          vf_loss: 17.235471725463867\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.950000000000003\n",
      "    ram_util_percent: 68.1\n",
      "  pid: 3056\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05058529267954035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026635925473790963\n",
      "    mean_inference_ms: 0.5191143448970729\n",
      "    mean_raw_obs_processing_ms: 0.12865662593291513\n",
      "  time_since_restore: 25.415394067764282\n",
      "  time_this_iter_s: 2.8130290508270264\n",
      "  time_total_s: 25.415394067764282\n",
      "  timers:\n",
      "    learn_throughput: 1936.004\n",
      "    learn_time_ms: 2066.111\n",
      "    load_throughput: 684151.37\n",
      "    load_time_ms: 5.847\n",
      "    sample_throughput: 5376.774\n",
      "    sample_time_ms: 743.941\n",
      "    update_time_ms: 1.618\n",
      "  timestamp: 1624527897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: bdcf5_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_bdcf5_00000</td><td>RUNNING </td><td>192.168.0.179:3056</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         25.4154</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   2.385</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -24.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_bdcf5_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 54\n",
      "    new_fields_discovered_mean: 42.43\n",
      "    new_fields_discovered_min: 21\n",
      "    num_collisions_max: 9\n",
      "    num_collisions_mean: 1.72\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-45-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.599999999999973\n",
      "  episode_reward_mean: 5.039999999999999\n",
      "  episode_reward_min: -24.900000000000013\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 220\n",
      "  experiment_id: ceeef6f183654ea083d83a3b87cf5a84\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0675801038742065\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018168041482567787\n",
      "          model: {}\n",
      "          policy_loss: -0.05329393595457077\n",
      "          total_loss: 14.962299346923828\n",
      "          vf_explained_var: 0.4676744043827057\n",
      "          vf_loss: 15.007417678833008\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.549999999999997\n",
      "    ram_util_percent: 69.275\n",
      "  pid: 3056\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05066573411394511\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026747252507122187\n",
      "    mean_inference_ms: 0.5186621493686637\n",
      "    mean_raw_obs_processing_ms: 0.12912562143993866\n",
      "  time_since_restore: 31.067670822143555\n",
      "  time_this_iter_s: 2.8114078044891357\n",
      "  time_total_s: 31.067670822143555\n",
      "  timers:\n",
      "    learn_throughput: 1949.271\n",
      "    learn_time_ms: 2052.05\n",
      "    load_throughput: 1575680.529\n",
      "    load_time_ms: 2.539\n",
      "    sample_throughput: 5404.235\n",
      "    sample_time_ms: 740.16\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1624527903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: bdcf5_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_bdcf5_00000</td><td>RUNNING </td><td>192.168.0.179:3056</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         31.0677</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">    5.04</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -24.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_bdcf5_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 55\n",
      "    new_fields_discovered_mean: 43.61\n",
      "    new_fields_discovered_min: 29\n",
      "    num_collisions_max: 9\n",
      "    num_collisions_mean: 1.54\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-45-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.999999999999908\n",
      "  episode_reward_mean: 6.680999999999995\n",
      "  episode_reward_min: -14.099999999999973\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 260\n",
      "  experiment_id: ceeef6f183654ea083d83a3b87cf5a84\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0019896030426025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017336105927824974\n",
      "          model: {}\n",
      "          policy_loss: -0.04900958761572838\n",
      "          total_loss: 19.24382781982422\n",
      "          vf_explained_var: 0.4008074402809143\n",
      "          vf_loss: 19.2850341796875\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.4\n",
      "    ram_util_percent: 69.3\n",
      "  pid: 3056\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05064369332387795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026812461142205236\n",
      "    mean_inference_ms: 0.5182995937423401\n",
      "    mean_raw_obs_processing_ms: 0.1294271451826657\n",
      "  time_since_restore: 36.754611015319824\n",
      "  time_this_iter_s: 2.8641269207000732\n",
      "  time_total_s: 36.754611015319824\n",
      "  timers:\n",
      "    learn_throughput: 1930.927\n",
      "    learn_time_ms: 2071.544\n",
      "    load_throughput: 1603158.689\n",
      "    load_time_ms: 2.495\n",
      "    sample_throughput: 5419.815\n",
      "    sample_time_ms: 738.033\n",
      "    update_time_ms: 1.625\n",
      "  timestamp: 1624527909\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: bdcf5_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_bdcf5_00000</td><td>RUNNING </td><td>192.168.0.179:3056</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         36.7546</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   6.681</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">               -14.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_bdcf5_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 55\n",
      "    new_fields_discovered_mean: 44.87\n",
      "    new_fields_discovered_min: 30\n",
      "    num_collisions_max: 11\n",
      "    num_collisions_mean: 1.16\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-45-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.999999999999908\n",
      "  episode_reward_mean: 8.35199999999999\n",
      "  episode_reward_min: -13.199999999999973\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 300\n",
      "  experiment_id: ceeef6f183654ea083d83a3b87cf5a84\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9213407635688782\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018423136323690414\n",
      "          model: {}\n",
      "          policy_loss: -0.05448216572403908\n",
      "          total_loss: 19.917644500732422\n",
      "          vf_explained_var: 0.44381415843963623\n",
      "          vf_loss: 19.963834762573242\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.0\n",
      "    ram_util_percent: 68.975\n",
      "  pid: 3056\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05050336959766158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026840589003414923\n",
      "    mean_inference_ms: 0.5167274684662714\n",
      "    mean_raw_obs_processing_ms: 0.12980626341700743\n",
      "  time_since_restore: 42.412195682525635\n",
      "  time_this_iter_s: 2.7925608158111572\n",
      "  time_total_s: 42.412195682525635\n",
      "  timers:\n",
      "    learn_throughput: 1918.9\n",
      "    learn_time_ms: 2084.528\n",
      "    load_throughput: 1586137.992\n",
      "    load_time_ms: 2.522\n",
      "    sample_throughput: 5426.932\n",
      "    sample_time_ms: 737.065\n",
      "    update_time_ms: 1.581\n",
      "  timestamp: 1624527914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: bdcf5_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_bdcf5_00000</td><td>RUNNING </td><td>192.168.0.179:3056</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         42.4122</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">   8.352</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">               -13.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_bdcf5_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 59\n",
      "    new_fields_discovered_mean: 46.98\n",
      "    new_fields_discovered_min: 30\n",
      "    num_collisions_max: 11\n",
      "    num_collisions_mean: 1.24\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-45-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.69999999999989\n",
      "  episode_reward_mean: 11.528999999999973\n",
      "  episode_reward_min: -13.199999999999973\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 340\n",
      "  experiment_id: ceeef6f183654ea083d83a3b87cf5a84\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8655449748039246\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017543723806738853\n",
      "          model: {}\n",
      "          policy_loss: -0.04964448884129524\n",
      "          total_loss: 22.470949172973633\n",
      "          vf_explained_var: 0.38823872804641724\n",
      "          vf_loss: 22.512699127197266\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.25\n",
      "    ram_util_percent: 68.35\n",
      "  pid: 3056\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.050379403528712265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026845929810382533\n",
      "    mean_inference_ms: 0.5146744480914031\n",
      "    mean_raw_obs_processing_ms: 0.13011793026160584\n",
      "  time_since_restore: 47.996766567230225\n",
      "  time_this_iter_s: 2.8018507957458496\n",
      "  time_total_s: 47.996766567230225\n",
      "  timers:\n",
      "    learn_throughput: 1924.229\n",
      "    learn_time_ms: 2078.754\n",
      "    load_throughput: 1592990.439\n",
      "    load_time_ms: 2.511\n",
      "    sample_throughput: 5431.012\n",
      "    sample_time_ms: 736.511\n",
      "    update_time_ms: 1.537\n",
      "  timestamp: 1624527920\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: bdcf5_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_bdcf5_00000</td><td>RUNNING </td><td>192.168.0.179:3056</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         47.9968</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  11.529</td><td style=\"text-align: right;\">                29.7</td><td style=\"text-align: right;\">               -13.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_bdcf5_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 59\n",
      "    new_fields_discovered_mean: 47.79\n",
      "    new_fields_discovered_min: 32\n",
      "    num_collisions_max: 34\n",
      "    num_collisions_mean: 1.5\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-45-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.69999999999989\n",
      "  episode_reward_mean: 12.809999999999963\n",
      "  episode_reward_min: -5.699999999999989\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 380\n",
      "  experiment_id: ceeef6f183654ea083d83a3b87cf5a84\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8129876255989075\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017267907038331032\n",
      "          model: {}\n",
      "          policy_loss: -0.042705416679382324\n",
      "          total_loss: 37.50997543334961\n",
      "          vf_explained_var: 0.3289172053337097\n",
      "          vf_loss: 37.54490280151367\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.525\n",
      "    ram_util_percent: 67.77499999999999\n",
      "  pid: 3056\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05038772686441932\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026848984310152767\n",
      "    mean_inference_ms: 0.5143160075609502\n",
      "    mean_raw_obs_processing_ms: 0.13031566112355425\n",
      "  time_since_restore: 53.64990830421448\n",
      "  time_this_iter_s: 2.8178467750549316\n",
      "  time_total_s: 53.64990830421448\n",
      "  timers:\n",
      "    learn_throughput: 1927.456\n",
      "    learn_time_ms: 2075.274\n",
      "    load_throughput: 1613162.824\n",
      "    load_time_ms: 2.48\n",
      "    sample_throughput: 5408.598\n",
      "    sample_time_ms: 739.563\n",
      "    update_time_ms: 1.522\n",
      "  timestamp: 1624527926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: bdcf5_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_bdcf5_00000</td><td>RUNNING </td><td>192.168.0.179:3056</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         53.6499</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">   12.81</td><td style=\"text-align: right;\">                29.7</td><td style=\"text-align: right;\">                -5.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_bdcf5_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 59\n",
      "    new_fields_discovered_mean: 47.89\n",
      "    new_fields_discovered_min: 32\n",
      "    num_collisions_max: 34\n",
      "    num_collisions_mean: 1.93\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-45-29\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.69999999999989\n",
      "  episode_reward_mean: 13.232999999999954\n",
      "  episode_reward_min: -5.699999999999989\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 400\n",
      "  experiment_id: ceeef6f183654ea083d83a3b87cf5a84\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8082447052001953\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018926914781332016\n",
      "          model: {}\n",
      "          policy_loss: -0.045885637402534485\n",
      "          total_loss: 28.56772232055664\n",
      "          vf_explained_var: 0.3264906108379364\n",
      "          vf_loss: 28.605087280273438\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.95\n",
      "    ram_util_percent: 67.65\n",
      "  pid: 3056\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0504375598490869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026855939830461587\n",
      "    mean_inference_ms: 0.5147394538590221\n",
      "    mean_raw_obs_processing_ms: 0.13038493135042686\n",
      "  time_since_restore: 56.56677532196045\n",
      "  time_this_iter_s: 2.9168670177459717\n",
      "  time_total_s: 56.56677532196045\n",
      "  timers:\n",
      "    learn_throughput: 1921.336\n",
      "    learn_time_ms: 2081.884\n",
      "    load_throughput: 1666953.083\n",
      "    load_time_ms: 2.4\n",
      "    sample_throughput: 5400.545\n",
      "    sample_time_ms: 740.666\n",
      "    update_time_ms: 1.499\n",
      "  timestamp: 1624527929\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: bdcf5_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_bdcf5_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         56.5668</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  13.233</td><td style=\"text-align: right;\">                29.7</td><td style=\"text-align: right;\">                -5.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 11:45:29,870\tINFO tune.py:549 -- Total run time: 73.10 seconds (72.30 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fbc155c7760>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up our config to point to our new custom callbacks class:\n",
    "config = {\n",
    "    \"env\": MultiAgentArena,\n",
    "    \"callbacks\": MyCallbacks,  # by default, this would point to `rllib.agents.callbacks.DefaultCallbacks`, which does nothing.\n",
    "    \"num_workers\": 5,  # we know now: this speeds up things!\n",
    "}\n",
    "\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,\n",
    "    stop={\"training_iteration\": 20},\n",
    "    checkpoint_at_end=True,\n",
    "    # If you'd like to restore the tune run from an existing checkpoint file, you can do the following:\n",
    "    #restore=\"/Users/sven/ray_results/PPO/PPO_MultiAgentArena_fd451_00000_0_2021-05-25_15-13-26/checkpoint_000010/checkpoint-10\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa6a24",
   "metadata": {},
   "source": [
    "### Let's check tensorboard for the new custom metrics!\n",
    "\n",
    "1. Head over to the Anyscale project view and click on the \"TensorBoard\" butten:\n",
    "\n",
    "<img src=\"images/tensorboard_button.png\" width=1000>\n",
    "\n",
    "Alternatively - if you ran this locally on your own machine:\n",
    "\n",
    "1. Head over to ~/ray_results/PPO/PPO_MultiAgentArena_[some key]_00000_0_[date]_[time]/\n",
    "1. In that directory, you should see a `event.out....` file.\n",
    "1. Run `tensorboard --logdir .` and head to https://localhost:6006\n",
    "\n",
    "<img src=\"images/tensorboard.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac90dc-097d-4f10-b5ea-c4c1167f1f3a",
   "metadata": {},
   "source": [
    "### Deep Dive: Writing custom Models in tf or torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5516d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "\n",
    "tf1, tf, tf_version = try_import_tf()\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "# Custom Neural Network Models.\n",
    "class MyKerasModel(TFModelV2):\n",
    "    \"\"\"Custom model for policy gradient algorithms.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        super(MyKerasModel, self).__init__(obs_space, action_space,\n",
    "                                           num_outputs, model_config, name)\n",
    "        \n",
    "        # Keras Input layer.\n",
    "        self.inputs = tf.keras.layers.Input(\n",
    "            shape=obs_space.shape, name=\"observations\")\n",
    "\n",
    "        # Hidden layer (shared by action logits outputs and value output).\n",
    "        layer_1 = tf.keras.layers.Dense(\n",
    "            16,\n",
    "            name=\"layer1\",\n",
    "            activation=tf.nn.relu)(self.inputs)\n",
    "        \n",
    "        # Action logits output.\n",
    "        logits = tf.keras.layers.Dense(\n",
    "            num_outputs,\n",
    "            name=\"out\",\n",
    "            activation=None)(layer_1)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        value_out = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            name=\"value\",\n",
    "            activation=None)(layer_1)\n",
    "\n",
    "        # The actual Keras model:\n",
    "        self.base_model = tf.keras.Model(self.inputs,\n",
    "                                         [logits, value_out])\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers and calculate the \"value\"\n",
    "        # of the observation and store it for when `value_function` is called.\n",
    "        logits, self.cur_value = self.base_model(input_dict[\"obs\"])\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return tf.reshape(self.cur_value, [-1])\n",
    "\n",
    "\n",
    "class MyTorchModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.device = torch.device(\"cuda\"\n",
    "                                   if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Hidden layer (shared by action logits outputs and value output).\n",
    "        self.layer_1 = nn.Linear(obs_space.shape[0], 16).to(self.device)\n",
    "\n",
    "        # Action logits output.\n",
    "        self.layer_out = nn.Linear(16, num_outputs).to(self.device)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        self.value_branch = nn.Linear(16, 1).to(self.device)\n",
    "        self.cur_value = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers.\n",
    "        layer_1_out = self.layer_1(input_dict[\"obs\"])\n",
    "        logits = self.layer_out(layer_1_out)\n",
    "\n",
    "        # Calculate the \"value\" of the observation and store it for\n",
    "        # when `value_function` is called.\n",
    "        self.cur_value = self.value_branch(layer_1_out).squeeze(1)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return self.cur_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "controversial-repair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-output=(<tf.Tensor 'model_8/out/BiasAdd:0' shape=(1, 2) dtype=float32>, [])\n"
     ]
    }
   ],
   "source": [
    "# Do a quick test on the custom model classes.\n",
    "test_model_tf = MyKerasModel(\n",
    "    obs_space=gym.spaces.Box(-1.0, 1.0, (2, )),\n",
    "    action_space=None,\n",
    "    num_outputs=2,\n",
    "    model_config={},\n",
    "    name=\"MyModel\",\n",
    ")\n",
    "\n",
    "print(\"TF-output={}\".format(test_model_tf({\"obs\": np.array([[0.5, 0.5]])})))\n",
    "\n",
    "# For PyTorch, you can do:\n",
    "#test_model_torch = MyTorchModel(\n",
    "#    obs_space=gym.spaces.Box(-1.0, 1.0, (2, )),\n",
    "#    action_space=None,\n",
    "#    num_outputs=2,\n",
    "#    model_config={},\n",
    "#    name=\"MyModel\",\n",
    "#)\n",
    "#print(\"Torch-output={}\".format(test_model_torch({\"obs\": torch.from_numpy(np.array([[0.5, 0.5]], dtype=np.float32))})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2237526a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m 2021-06-24 11:45:35,723\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m 2021-06-24 11:45:35,723\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m 2021-06-24 11:45:44,605\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 2400\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 40\n",
      "    new_fields_discovered_mean: 29.583333333333332\n",
      "    new_fields_discovered_min: 15\n",
      "    num_collisions_max: 7\n",
      "    num_collisions_mean: 0.75\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-45-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5000000000000093\n",
      "  episode_reward_mean: -14.800000000000002\n",
      "  episode_reward_min: -37.50000000000004\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 12\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.3625845909118652\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02165747620165348\n",
      "          model: {}\n",
      "          policy_loss: -0.021877726539969444\n",
      "          total_loss: 22.76353645324707\n",
      "          vf_explained_var: 0.07934900373220444\n",
      "          vf_loss: 22.78108024597168\n",
      "    num_agent_steps_sampled: 2400\n",
      "    num_agent_steps_trained: 2400\n",
      "    num_steps_sampled: 2400\n",
      "    num_steps_trained: 2400\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.5\n",
      "    ram_util_percent: 67.1\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05113384291256923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02760835574589633\n",
      "    mean_inference_ms: 0.5194069740181161\n",
      "    mean_raw_obs_processing_ms: 0.13269600574664786\n",
      "  time_since_restore: 1.4699571132659912\n",
      "  time_this_iter_s: 1.4699571132659912\n",
      "  time_total_s: 1.4699571132659912\n",
      "  timers:\n",
      "    learn_throughput: 2483.959\n",
      "    learn_time_ms: 966.2\n",
      "    load_throughput: 81621.756\n",
      "    load_time_ms: 29.404\n",
      "    sample_throughput: 5290.672\n",
      "    sample_time_ms: 453.629\n",
      "    update_time_ms: 1.097\n",
      "  timestamp: 1624527946\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2400\n",
      "  training_iteration: 1\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.46996</td><td style=\"text-align: right;\">2400</td><td style=\"text-align: right;\">   -14.8</td><td style=\"text-align: right;\">                 1.5</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 14400\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 51\n",
      "    new_fields_discovered_mean: 35.833333333333336\n",
      "    new_fields_discovered_min: 15\n",
      "    num_collisions_max: 15\n",
      "    num_collisions_mean: 2.0277777777777777\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-45-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.099999999999977\n",
      "  episode_reward_mean: -4.604166666666655\n",
      "  episode_reward_min: -37.50000000000004\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 72\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.3478621244430542\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009249470196664333\n",
      "          model: {}\n",
      "          policy_loss: -0.014932680875062943\n",
      "          total_loss: 16.798166275024414\n",
      "          vf_explained_var: 0.3807923495769501\n",
      "          vf_loss: 16.81171226501465\n",
      "    num_agent_steps_sampled: 14400\n",
      "    num_agent_steps_trained: 14400\n",
      "    num_steps_sampled: 14400\n",
      "    num_steps_trained: 14400\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.2\n",
      "    ram_util_percent: 67.0\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.049926483206241774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.027182213246939375\n",
      "    mean_inference_ms: 0.4856084163681683\n",
      "    mean_raw_obs_processing_ms: 0.13025806011886593\n",
      "  time_since_restore: 7.609846115112305\n",
      "  time_this_iter_s: 1.26031494140625\n",
      "  time_total_s: 7.609846115112305\n",
      "  timers:\n",
      "    learn_throughput: 2871.999\n",
      "    learn_time_ms: 835.655\n",
      "    load_throughput: 417996.44\n",
      "    load_time_ms: 5.742\n",
      "    sample_throughput: 5717.209\n",
      "    sample_time_ms: 419.785\n",
      "    update_time_ms: 1.087\n",
      "  timestamp: 1624527952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14400\n",
      "  training_iteration: 6\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         7.60985</td><td style=\"text-align: right;\">14400</td><td style=\"text-align: right;\">-4.60417</td><td style=\"text-align: right;\">                17.1</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 26400\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 51\n",
      "    new_fields_discovered_mean: 37.31\n",
      "    new_fields_discovered_min: 23\n",
      "    num_collisions_max: 16\n",
      "    num_collisions_mean: 2.82\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-45-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.099999999999977\n",
      "  episode_reward_mean: -1.9769999999999852\n",
      "  episode_reward_min: -24.900000000000006\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 132\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.3668407201766968\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006161996629089117\n",
      "          model: {}\n",
      "          policy_loss: 0.0035586217418313026\n",
      "          total_loss: 15.40011978149414\n",
      "          vf_explained_var: 0.33664563298225403\n",
      "          vf_loss: 15.395174026489258\n",
      "    num_agent_steps_sampled: 26400\n",
      "    num_agent_steps_trained: 26400\n",
      "    num_steps_sampled: 26400\n",
      "    num_steps_trained: 26400\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.049999999999997\n",
      "    ram_util_percent: 67.0\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04946239877550086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026979600072368007\n",
      "    mean_inference_ms: 0.47100852911889374\n",
      "    mean_raw_obs_processing_ms: 0.1284400753523119\n",
      "  time_since_restore: 13.61452841758728\n",
      "  time_this_iter_s: 1.1809237003326416\n",
      "  time_total_s: 13.61452841758728\n",
      "  timers:\n",
      "    learn_throughput: 2995.102\n",
      "    learn_time_ms: 801.308\n",
      "    load_throughput: 2445361.253\n",
      "    load_time_ms: 0.981\n",
      "    sample_throughput: 5890.259\n",
      "    sample_time_ms: 407.452\n",
      "    update_time_ms: 1.071\n",
      "  timestamp: 1624527958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26400\n",
      "  training_iteration: 11\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         13.6145</td><td style=\"text-align: right;\">26400</td><td style=\"text-align: right;\">  -1.977</td><td style=\"text-align: right;\">                17.1</td><td style=\"text-align: right;\">               -24.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 38400\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 54\n",
      "    new_fields_discovered_mean: 38.03\n",
      "    new_fields_discovered_min: 22\n",
      "    num_collisions_max: 16\n",
      "    num_collisions_mean: 2.12\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999968\n",
      "  episode_reward_mean: -1.3709999999999871\n",
      "  episode_reward_min: -27.000000000000014\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 192\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.3432700634002686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005619957577437162\n",
      "          model: {}\n",
      "          policy_loss: -0.004460564814507961\n",
      "          total_loss: 16.029203414916992\n",
      "          vf_explained_var: 0.26832371950149536\n",
      "          vf_loss: 16.032400131225586\n",
      "    num_agent_steps_sampled: 38400\n",
      "    num_agent_steps_trained: 38400\n",
      "    num_steps_sampled: 38400\n",
      "    num_steps_trained: 38400\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.65\n",
      "    ram_util_percent: 67.6\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04934219201006683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02691176351519448\n",
      "    mean_inference_ms: 0.4663318638518624\n",
      "    mean_raw_obs_processing_ms: 0.12784280628665695\n",
      "  time_since_restore: 19.717113971710205\n",
      "  time_this_iter_s: 1.2441401481628418\n",
      "  time_total_s: 19.717113971710205\n",
      "  timers:\n",
      "    learn_throughput: 3012.666\n",
      "    learn_time_ms: 796.637\n",
      "    load_throughput: 2568203.286\n",
      "    load_time_ms: 0.935\n",
      "    sample_throughput: 5880.408\n",
      "    sample_time_ms: 408.135\n",
      "    update_time_ms: 1.044\n",
      "  timestamp: 1624527964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38400\n",
      "  training_iteration: 16\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         19.7171</td><td style=\"text-align: right;\">38400</td><td style=\"text-align: right;\">  -1.371</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">                 -27</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 54\n",
      "    new_fields_discovered_mean: 38.5\n",
      "    new_fields_discovered_min: 22\n",
      "    num_collisions_max: 12\n",
      "    num_collisions_mean: 1.82\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-46-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999968\n",
      "  episode_reward_mean: -0.7919999999999873\n",
      "  episode_reward_min: -27.000000000000014\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 240\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.356558084487915\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011327591724693775\n",
      "          model: {}\n",
      "          policy_loss: -0.012248469516634941\n",
      "          total_loss: 13.447689056396484\n",
      "          vf_explained_var: 0.3305691182613373\n",
      "          vf_loss: 13.457388877868652\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.7\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04932959078814979\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026912466026390823\n",
      "    mean_inference_ms: 0.46550055505046584\n",
      "    mean_raw_obs_processing_ms: 0.1280086182678864\n",
      "  time_since_restore: 24.651865005493164\n",
      "  time_this_iter_s: 1.2271959781646729\n",
      "  time_total_s: 24.651865005493164\n",
      "  timers:\n",
      "    learn_throughput: 2979.524\n",
      "    learn_time_ms: 805.498\n",
      "    load_throughput: 2626912.735\n",
      "    load_time_ms: 0.914\n",
      "    sample_throughput: 5847.39\n",
      "    sample_time_ms: 410.44\n",
      "    update_time_ms: 1.023\n",
      "  timestamp: 1624527969\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 20\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         24.6519</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  -0.792</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">                 -27</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 57600\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 51\n",
      "    new_fields_discovered_mean: 38.58\n",
      "    new_fields_discovered_min: 21\n",
      "    num_collisions_max: 12\n",
      "    num_collisions_mean: 2.0\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-46-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.999999999999964\n",
      "  episode_reward_mean: -0.5729999999999895\n",
      "  episode_reward_min: -23.100000000000005\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 288\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.357210636138916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00745764933526516\n",
      "          model: {}\n",
      "          policy_loss: -0.0030743994284421206\n",
      "          total_loss: 16.26702117919922\n",
      "          vf_explained_var: 0.2826105058193207\n",
      "          vf_loss: 16.268415451049805\n",
      "    num_agent_steps_sampled: 57600\n",
      "    num_agent_steps_trained: 57600\n",
      "    num_steps_sampled: 57600\n",
      "    num_steps_trained: 57600\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.85\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04931182898752441\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02687644011019662\n",
      "    mean_inference_ms: 0.46552227945593444\n",
      "    mean_raw_obs_processing_ms: 0.12777921499460596\n",
      "  time_since_restore: 29.585914611816406\n",
      "  time_this_iter_s: 1.2432711124420166\n",
      "  time_total_s: 29.585914611816406\n",
      "  timers:\n",
      "    learn_throughput: 2941.944\n",
      "    learn_time_ms: 815.787\n",
      "    load_throughput: 2530754.626\n",
      "    load_time_ms: 0.948\n",
      "    sample_throughput: 5820.326\n",
      "    sample_time_ms: 412.348\n",
      "    update_time_ms: 1.035\n",
      "  timestamp: 1624527974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57600\n",
      "  training_iteration: 24\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         29.5859</td><td style=\"text-align: right;\">57600</td><td style=\"text-align: right;\">  -0.573</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">               -23.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 69600\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 57\n",
      "    new_fields_discovered_mean: 39.21\n",
      "    new_fields_discovered_min: 20\n",
      "    num_collisions_max: 9\n",
      "    num_collisions_mean: 1.68\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-46-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.69999999999993\n",
      "  episode_reward_mean: 0.1320000000000069\n",
      "  episode_reward_min: -30.000000000000018\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 348\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.3456530570983887\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015158603899180889\n",
      "          model: {}\n",
      "          policy_loss: -0.013682883232831955\n",
      "          total_loss: 19.523094177246094\n",
      "          vf_explained_var: 0.24900995194911957\n",
      "          vf_loss: 19.533367156982422\n",
      "    num_agent_steps_sampled: 69600\n",
      "    num_agent_steps_trained: 69600\n",
      "    num_steps_sampled: 69600\n",
      "    num_steps_trained: 69600\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.35\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.049183387962650024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02677146208866569\n",
      "    mean_inference_ms: 0.46487158404778567\n",
      "    mean_raw_obs_processing_ms: 0.12732121381721556\n",
      "  time_since_restore: 35.73079490661621\n",
      "  time_this_iter_s: 1.2354159355163574\n",
      "  time_total_s: 35.73079490661621\n",
      "  timers:\n",
      "    learn_throughput: 2939.552\n",
      "    learn_time_ms: 816.451\n",
      "    load_throughput: 2515073.356\n",
      "    load_time_ms: 0.954\n",
      "    sample_throughput: 5880.137\n",
      "    sample_time_ms: 408.154\n",
      "    update_time_ms: 1.047\n",
      "  timestamp: 1624527980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69600\n",
      "  training_iteration: 29\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         35.7308</td><td style=\"text-align: right;\">69600</td><td style=\"text-align: right;\">   0.132</td><td style=\"text-align: right;\">                26.7</td><td style=\"text-align: right;\">                 -30</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 79200\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 57\n",
      "    new_fields_discovered_mean: 39.02\n",
      "    new_fields_discovered_min: 20\n",
      "    num_collisions_max: 8\n",
      "    num_collisions_mean: 1.54\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-46-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.69999999999993\n",
      "  episode_reward_mean: -0.22199999999999143\n",
      "  episode_reward_min: -30.000000000000018\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 396\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.3362138271331787\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009175430983304977\n",
      "          model: {}\n",
      "          policy_loss: -0.011759666725993156\n",
      "          total_loss: 18.12287139892578\n",
      "          vf_explained_var: 0.3377915322780609\n",
      "          vf_loss: 18.1325626373291\n",
      "    num_agent_steps_sampled: 79200\n",
      "    num_agent_steps_trained: 79200\n",
      "    num_steps_sampled: 79200\n",
      "    num_steps_trained: 79200\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.799999999999997\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04909582632462369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02667590101822913\n",
      "    mean_inference_ms: 0.46430789924284693\n",
      "    mean_raw_obs_processing_ms: 0.1269751987728456\n",
      "  time_since_restore: 40.70330262184143\n",
      "  time_this_iter_s: 1.3394660949707031\n",
      "  time_total_s: 40.70330262184143\n",
      "  timers:\n",
      "    learn_throughput: 2923.563\n",
      "    learn_time_ms: 820.916\n",
      "    load_throughput: 2503065.844\n",
      "    load_time_ms: 0.959\n",
      "    sample_throughput: 5867.105\n",
      "    sample_time_ms: 409.06\n",
      "    update_time_ms: 1.08\n",
      "  timestamp: 1624527986\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79200\n",
      "  training_iteration: 33\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         40.7033</td><td style=\"text-align: right;\">79200</td><td style=\"text-align: right;\">  -0.222</td><td style=\"text-align: right;\">                26.7</td><td style=\"text-align: right;\">                 -30</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 88800\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 56\n",
      "    new_fields_discovered_mean: 38.72\n",
      "    new_fields_discovered_min: 26\n",
      "    num_collisions_max: 8\n",
      "    num_collisions_mean: 2.07\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-46-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 25.79999999999994\n",
      "  episode_reward_mean: -0.3479999999999864\n",
      "  episode_reward_min: -20.400000000000013\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 444\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.3210833072662354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014266552403569221\n",
      "          model: {}\n",
      "          policy_loss: -0.004544407594949007\n",
      "          total_loss: 15.715394973754883\n",
      "          vf_explained_var: 0.3750072419643402\n",
      "          vf_loss: 15.716727256774902\n",
      "    num_agent_steps_sampled: 88800\n",
      "    num_agent_steps_trained: 88800\n",
      "    num_steps_sampled: 88800\n",
      "    num_steps_trained: 88800\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.4\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04919152324815647\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026704762357609255\n",
      "    mean_inference_ms: 0.4653007527330056\n",
      "    mean_raw_obs_processing_ms: 0.12712400696095505\n",
      "  time_since_restore: 45.69986319541931\n",
      "  time_this_iter_s: 1.2499761581420898\n",
      "  time_total_s: 45.69986319541931\n",
      "  timers:\n",
      "    learn_throughput: 2924.655\n",
      "    learn_time_ms: 820.609\n",
      "    load_throughput: 2503750.678\n",
      "    load_time_ms: 0.959\n",
      "    sample_throughput: 5762.744\n",
      "    sample_time_ms: 416.468\n",
      "    update_time_ms: 1.071\n",
      "  timestamp: 1624527991\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88800\n",
      "  training_iteration: 37\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         45.6999</td><td style=\"text-align: right;\">88800</td><td style=\"text-align: right;\">  -0.348</td><td style=\"text-align: right;\">                25.8</td><td style=\"text-align: right;\">               -20.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 100800\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 63\n",
      "    new_fields_discovered_mean: 38.68\n",
      "    new_fields_discovered_min: 25\n",
      "    num_collisions_max: 11\n",
      "    num_collisions_mean: 2.38\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-46-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.49999999999992\n",
      "  episode_reward_mean: -0.2249999999999929\n",
      "  episode_reward_min: -21.000000000000007\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 504\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.305944561958313\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01196583267301321\n",
      "          model: {}\n",
      "          policy_loss: -0.005668370984494686\n",
      "          total_loss: 15.36308479309082\n",
      "          vf_explained_var: 0.3620873689651489\n",
      "          vf_loss: 15.366060256958008\n",
      "    num_agent_steps_sampled: 100800\n",
      "    num_agent_steps_trained: 100800\n",
      "    num_steps_sampled: 100800\n",
      "    num_steps_trained: 100800\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.0\n",
      "    ram_util_percent: 67.4\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04932167348230512\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02680226667248757\n",
      "    mean_inference_ms: 0.46640106101918105\n",
      "    mean_raw_obs_processing_ms: 0.1276705587622394\n",
      "  time_since_restore: 51.78306174278259\n",
      "  time_this_iter_s: 1.2335338592529297\n",
      "  time_total_s: 51.78306174278259\n",
      "  timers:\n",
      "    learn_throughput: 2942.197\n",
      "    learn_time_ms: 815.717\n",
      "    load_throughput: 2463614.684\n",
      "    load_time_ms: 0.974\n",
      "    sample_throughput: 5714.378\n",
      "    sample_time_ms: 419.993\n",
      "    update_time_ms: 1.069\n",
      "  timestamp: 1624527997\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100800\n",
      "  training_iteration: 42\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         51.7831</td><td style=\"text-align: right;\">100800</td><td style=\"text-align: right;\">  -0.225</td><td style=\"text-align: right;\">                34.5</td><td style=\"text-align: right;\">                 -21</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 110400\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 51\n",
      "    new_fields_discovered_mean: 38.15\n",
      "    new_fields_discovered_min: 24\n",
      "    num_collisions_max: 11\n",
      "    num_collisions_mean: 2.03\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-46-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.099999999999973\n",
      "  episode_reward_mean: -1.1459999999999904\n",
      "  episode_reward_min: -24.000000000000007\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 552\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.3314334154129028\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010029635392129421\n",
      "          model: {}\n",
      "          policy_loss: -0.008674049749970436\n",
      "          total_loss: 23.89590835571289\n",
      "          vf_explained_var: 0.24472162127494812\n",
      "          vf_loss: 23.90232276916504\n",
      "    num_agent_steps_sampled: 110400\n",
      "    num_agent_steps_trained: 110400\n",
      "    num_steps_sampled: 110400\n",
      "    num_steps_trained: 110400\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.35\n",
      "    ram_util_percent: 67.4\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.049319182363330064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026821025216831832\n",
      "    mean_inference_ms: 0.4662940494810549\n",
      "    mean_raw_obs_processing_ms: 0.12782390381831038\n",
      "  time_since_restore: 56.71735239028931\n",
      "  time_this_iter_s: 1.2406988143920898\n",
      "  time_total_s: 56.71735239028931\n",
      "  timers:\n",
      "    learn_throughput: 2967.581\n",
      "    learn_time_ms: 808.739\n",
      "    load_throughput: 2359885.971\n",
      "    load_time_ms: 1.017\n",
      "    sample_throughput: 5826.854\n",
      "    sample_time_ms: 411.886\n",
      "    update_time_ms: 1.046\n",
      "  timestamp: 1624528002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110400\n",
      "  training_iteration: 46\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         56.7174</td><td style=\"text-align: right;\">110400</td><td style=\"text-align: right;\">  -1.146</td><td style=\"text-align: right;\">                20.1</td><td style=\"text-align: right;\">                 -24</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 55\n",
      "    new_fields_discovered_mean: 38.61\n",
      "    new_fields_discovered_min: 24\n",
      "    num_collisions_max: 9\n",
      "    num_collisions_mean: 1.96\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-46-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.49999999999994\n",
      "  episode_reward_mean: -0.6029999999999884\n",
      "  episode_reward_min: -24.000000000000007\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 600\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.2924516201019287\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020449697971343994\n",
      "          model: {}\n",
      "          policy_loss: -0.006957357749342918\n",
      "          total_loss: 21.50468635559082\n",
      "          vf_explained_var: 0.3620283901691437\n",
      "          vf_loss: 21.50704002380371\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.55\n",
      "    ram_util_percent: 67.4\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.049314857600393724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026811927373703522\n",
      "    mean_inference_ms: 0.466265494124564\n",
      "    mean_raw_obs_processing_ms: 0.12779181754784788\n",
      "  time_since_restore: 61.7071316242218\n",
      "  time_this_iter_s: 1.2442691326141357\n",
      "  time_total_s: 61.7071316242218\n",
      "  timers:\n",
      "    learn_throughput: 2928.292\n",
      "    learn_time_ms: 819.59\n",
      "    load_throughput: 2373798.425\n",
      "    load_time_ms: 1.011\n",
      "    sample_throughput: 5801.458\n",
      "    sample_time_ms: 413.689\n",
      "    update_time_ms: 1.085\n",
      "  timestamp: 1624528007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 50\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         61.7071</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">  -0.603</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">                 -24</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 57\n",
      "    new_fields_discovered_mean: 40.41\n",
      "    new_fields_discovered_min: 17\n",
      "    num_collisions_max: 11\n",
      "    num_collisions_mean: 2.24\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-46-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.099999999999923\n",
      "  episode_reward_mean: 2.3160000000000043\n",
      "  episode_reward_min: -33.90000000000003\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 660\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.297267198562622\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013116950169205666\n",
      "          model: {}\n",
      "          policy_loss: -0.0033358586952090263\n",
      "          total_loss: 20.894317626953125\n",
      "          vf_explained_var: 0.4117658734321594\n",
      "          vf_loss: 20.89322853088379\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.45\n",
      "    ram_util_percent: 67.4\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04932923099185077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02679610028989399\n",
      "    mean_inference_ms: 0.46645723684519425\n",
      "    mean_raw_obs_processing_ms: 0.12763887970861798\n",
      "  time_since_restore: 67.86194586753845\n",
      "  time_this_iter_s: 1.239915132522583\n",
      "  time_total_s: 67.86194586753845\n",
      "  timers:\n",
      "    learn_throughput: 2920.642\n",
      "    learn_time_ms: 821.737\n",
      "    load_throughput: 2502194.78\n",
      "    load_time_ms: 0.959\n",
      "    sample_throughput: 5841.508\n",
      "    sample_time_ms: 410.853\n",
      "    update_time_ms: 1.098\n",
      "  timestamp: 1624528013\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 55\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         67.8619</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">   2.316</td><td style=\"text-align: right;\">                26.1</td><td style=\"text-align: right;\">               -33.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 57\n",
      "    new_fields_discovered_mean: 41.78\n",
      "    new_fields_discovered_min: 24\n",
      "    num_collisions_max: 15\n",
      "    num_collisions_mean: 2.15\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-46-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 25.499999999999936\n",
      "  episode_reward_mean: 4.316999999999998\n",
      "  episode_reward_min: -22.500000000000036\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 720\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.2994266748428345\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013928941451013088\n",
      "          model: {}\n",
      "          policy_loss: 0.0023119570687413216\n",
      "          total_loss: 13.807937622070312\n",
      "          vf_explained_var: 0.47254931926727295\n",
      "          vf_loss: 13.800922393798828\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 0.0\n",
      "    ram_util_percent: 67.4\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.049304727701184256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02677553797765793\n",
      "    mean_inference_ms: 0.4663623591875905\n",
      "    mean_raw_obs_processing_ms: 0.12745594011021186\n",
      "  time_since_restore: 73.91998076438904\n",
      "  time_this_iter_s: 1.1742160320281982\n",
      "  time_total_s: 73.91998076438904\n",
      "  timers:\n",
      "    learn_throughput: 2969.694\n",
      "    learn_time_ms: 808.164\n",
      "    load_throughput: 2640764.343\n",
      "    load_time_ms: 0.909\n",
      "    sample_throughput: 5894.2\n",
      "    sample_time_ms: 407.18\n",
      "    update_time_ms: 1.048\n",
      "  timestamp: 1624528019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 60\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">           73.92</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">   4.317</td><td style=\"text-align: right;\">                25.5</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 54\n",
      "    new_fields_discovered_mean: 42.09\n",
      "    new_fields_discovered_min: 24\n",
      "    num_collisions_max: 15\n",
      "    num_collisions_mean: 1.8\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-47-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.5\n",
      "  episode_reward_mean: 4.736999999999999\n",
      "  episode_reward_min: -22.500000000000036\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 780\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.2918246984481812\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017548246309161186\n",
      "          model: {}\n",
      "          policy_loss: 0.004775038920342922\n",
      "          total_loss: 21.260982513427734\n",
      "          vf_explained_var: 0.3736603856086731\n",
      "          vf_loss: 21.24732208251953\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.799999999999997\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04922783864867965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.026765191908494548\n",
      "    mean_inference_ms: 0.4656607430464008\n",
      "    mean_raw_obs_processing_ms: 0.12748456179199702\n",
      "  time_since_restore: 79.8377537727356\n",
      "  time_this_iter_s: 1.2182488441467285\n",
      "  time_total_s: 79.8377537727356\n",
      "  timers:\n",
      "    learn_throughput: 3051.003\n",
      "    learn_time_ms: 786.626\n",
      "    load_throughput: 2510306.633\n",
      "    load_time_ms: 0.956\n",
      "    sample_throughput: 5927.467\n",
      "    sample_time_ms: 404.895\n",
      "    update_time_ms: 1.008\n",
      "  timestamp: 1624528025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 65\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         79.8378</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">   4.737</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_e96ef_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 58\n",
      "    new_fields_discovered_mean: 42.41\n",
      "    new_fields_discovered_min: 31\n",
      "    num_collisions_max: 7\n",
      "    num_collisions_mean: 1.88\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-06-24_11-47-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.49999999999994\n",
      "  episode_reward_mean: 5.346000000000003\n",
      "  episode_reward_min: -11.999999999999975\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 840\n",
      "  experiment_id: f92dc745139c4b73a2c52cbb5ede0c35\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.0005000000237487257\n",
      "          entropy: 1.2910246849060059\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013177911750972271\n",
      "          model: {}\n",
      "          policy_loss: -0.011083748191595078\n",
      "          total_loss: 9.791471481323242\n",
      "          vf_explained_var: 0.4842108488082886\n",
      "          vf_loss: 9.795886039733887\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.5\n",
      "    ram_util_percent: 67.4\n",
      "  pid: 3058\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04914958361491295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02675709289462365\n",
      "    mean_inference_ms: 0.4648034770474672\n",
      "    mean_raw_obs_processing_ms: 0.1275411830208537\n",
      "  time_since_restore: 85.917062997818\n",
      "  time_this_iter_s: 1.2147109508514404\n",
      "  time_total_s: 85.917062997818\n",
      "  timers:\n",
      "    learn_throughput: 3037.406\n",
      "    learn_time_ms: 790.148\n",
      "    load_throughput: 2522004.71\n",
      "    load_time_ms: 0.952\n",
      "    sample_throughput: 5948.347\n",
      "    sample_time_ms: 403.473\n",
      "    update_time_ms: 1.025\n",
      "  timestamp: 1624528032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 70\n",
      "  trial_id: e96ef_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         85.9171</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">   5.346</td><td style=\"text-align: right;\">                28.5</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 11:47:14,561\tWARNING tune.py:506 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.56 GiB heap, 0.0/2.28 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_e96ef_00000</td><td>RUNNING </td><td>192.168.0.179:3058</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         88.3456</td><td style=\"text-align: right;\">172800</td><td style=\"text-align: right;\">   6.012</td><td style=\"text-align: right;\">                28.5</td><td style=\"text-align: right;\">               -14.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m 2021-06-24 11:47:14,610\tERROR worker.py:409 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"python/ray/_raylet.pyx\", line 595, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"python/ray/_raylet.pyx\", line 453, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"python/ray/_raylet.pyx\", line 490, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 594, in train\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     result = Trainable.train(self)\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\", line 173, in step\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     res = next(self.train_exec_impl)\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     return next(self.built_iterator)\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   [Previous line repeated 1 more time]\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 876, in apply_flatten\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 828, in add_wait_hooks\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     item = next(it)\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   [Previous line repeated 1 more time]\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 471, in base_iterator\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     yield ray.get(futures, timeout=timeout)\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 62, in wrapper\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 1487, in get\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     values, debugger_breakpoint = worker.get_objects(\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 328, in get_objects\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     data_metadata_pairs = self.core_worker.get_objects(\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 406, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=3058)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m 2021-06-24 11:47:14,604\tERROR worker.py:409 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"python/ray/_raylet.pyx\", line 595, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"python/ray/_raylet.pyx\", line 453, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"python/ray/_raylet.pyx\", line 490, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 333, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 726, in sample\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 99, in next\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 226, in get_data\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 625, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     eval_results = _do_policy_eval(\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 1011, in _do_policy_eval\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     policy.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py\", line 388, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     fetched = builder.get(to_fetch)\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/utils/tf_run_builder.py\", line 42, in get\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     self._executed = run_timeline(\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/utils/tf_run_builder.py\", line 89, in run_timeline\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 967, in run\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1190, in _run\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     results = self._do_run(handle, final_targets, final_fetches,\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1368, in _do_run\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     return self._do_call(_run_fn, feeds, fetches, targets, options,\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     return fn(*args)\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1359, in _run_fn\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1451, in _call_tf_sessionrun\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 406, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=3191)\u001b[0m SystemExit: 1\n",
      "2021-06-24 11:47:14,807\tERROR tune.py:545 -- Trials did not complete: [PPO_MultiAgentArena_e96ef_00000]\n",
      "2021-06-24 11:47:14,807\tINFO tune.py:549 -- Total run time: 104.85 seconds (104.62 seconds for the tuning loop).\n",
      "2021-06-24 11:47:14,808\tWARNING tune.py:553 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fbc2e7ca520>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up our custom model and re-run the experiment.\n",
    "config.update({\n",
    "    \"model\": {\n",
    "        \"custom_model\": MyKerasModel,\n",
    "        \"custom_model_config\": {\n",
    "            #\"layers\": [128, 128],\n",
    "        },\n",
    "    },\n",
    "})\n",
    "\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,\n",
    "    stop={\n",
    "        \"training_iteration\": 3,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85ad95",
   "metadata": {},
   "source": [
    "### Deep Dive: A closer look at RLlib's APIs and components\n",
    "#### (Depending on time left and amount of questions having been accumulated :)\n",
    "\n",
    "We already took a quick look inside an RLlib Trainer object and extracted its Policy(ies) and the Policy's model (neural network). Here is a much more detailed overview of what's inside a Trainer object.\n",
    "\n",
    "At the core is the so-called `WorkerSet` sitting under `Trainer.workers`. A WorkerSet is a group of `RolloutWorker` (`rllib.evaluation.rollout_worker.py`) objects that always consists of a \"local worker\" (`Trainer.workers.local_worker()`) and n \"remote workers\" (`Trainer.workers.remote_workers()`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f37549",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_structure.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d72883",
   "metadata": {},
   "source": [
    "### Scaling RLlib\n",
    "\n",
    "Scaling RLlib works by parallelizing the \"jobs\" that the remote `RolloutWorkers` do. In a vanilla RL algorithm, like PPO, DQN, and many others, the `@ray.remote` labeled RolloutWorkers in the figure above are responsible for interacting with one or more environments and thereby collecting experiences. Observations are produced by the environment, actions are then computed by the Policy(ies) copy located on the remote worker and sent to the environment in order to produce yet another observation. This cycle is repeated endlessly and only sometimes interrupted to send experience batches (\"train batches\") of a certain size to the \"local worker\". There these batches are used to call `Policy.learn_on_batch()`, which performs a loss calculation, followed by a model weights update, and a subsequent weights broadcast back to all the remote workers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {},
   "source": [
    "## Time for Q&A\n",
    "\n",
    "...\n",
    "\n",
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful.\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials.git\">github repo of this tutorial</a>.\n",
    "- <a href=\"https://docs.ray.io/en/master/rllib.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on RLlib.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f116b3-0962-44ff-9c08-558c6890abd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
