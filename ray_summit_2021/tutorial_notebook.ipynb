{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "# Hands-on RL with Ray’s RLlib\n",
    "## A beginner’s tutorial for working with multi-agent environments, models, and algorithms\n",
    "\n",
    "<img src=\"images/pitfall.jpg\" width=250> <img src=\"images/tesla.jpg\" width=254> <img src=\"images/forklifts.jpg\" width=169> <img src=\"images/robots.jpg\" width=252> <img src=\"images/dota2.jpg\" width=213>\n",
    "\n",
    "### Overview\n",
    "“Hands-on RL with Ray’s RLlib” is a beginners tutorial for working with reinforcement learning (RL) environments, models, and algorithms using Ray’s RLlib library. RLlib offers high scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL) before proceeding to RLlib (multi- and single-agent) environments, neural network models, hyperparameter tuning, debugging, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo.\n",
    "\n",
    "### Intended Audience\n",
    "* Python programmers who want to get started with reinforcement learning and RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Linux):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install ray[rllib]\n",
    "$ pip install tensorflow  # <- either one works!\n",
    "$ pip install torch  # <- either one works!\n",
    "$ pip install jupyterlab\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install cmake \"ray[rllib]\"\n",
    "$ pip install tensorflow  # <- either one works!\n",
    "$ pip install torch  # <- either one works!\n",
    "$ pip install jupyterlab\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install ray[rllib]\n",
    "$ pip install [tensorflow|torch]  # <- either one works!\n",
    "$ pip install jupyterlab\n",
    "$ conda install pywin32\n",
    "```\n",
    "\n",
    "Also, for Win10 Atari support, we have to install atari_py from a different source (gym does not support Atari envs on Windows).\n",
    "\n",
    "```\n",
    "$ pip install git+https://github.com/Kojoley/atari-py.git\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and why RLlib?\n",
    "* Core concepts of RLlib: Environments, Trainers, Policies, and Models.\n",
    "* How to configure, hyperparameter-tune, and parallelize RLlib.\n",
    "* RLlib debugging best practices.\n",
    "\n",
    "### Tutorial Outline\n",
    "1. RL and RLlib in a nutshell.\n",
    "1. Defining an RL-solvable problem: Our first environment.\n",
    "1. **Exercise No.1**: Environment loop.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. Picking an algorithm and training our first RLlib Trainer.\n",
    "1. Configurations and hyperparameters - Easy tuning with Ray Tune.\n",
    "1. Fixing our experiment's config - Going multi-agent.\n",
    "1. The \"infinite laptop\": Quick intro into how to use RLlib with Anyscale's product.\n",
    "1. **Exercise No.2**: Run your own Ray RLlib+Tune experiment)\n",
    "1. Neural network models - Provide your custom models using tf.keras or torch.nn.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. Deeper dive into RLlib's parallelization architecture.\n",
    "1. Specifying different compute resources and parallelization options through our config.\n",
    "1. \"Hacking in\": Using callbacks to customize the RL loop and generate our own metrics.\n",
    "1. **Exercise No.3**: Write your own custom callback.\n",
    "1. \"Hacking in (part II)\" - Debugging with RLlib and PyCharm.\n",
    "1. Checking on the \"infinite laptop\" - Did RLlib learn to solve the problem?\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-yorkshire",
   "metadata": {},
   "source": [
    "<img src=\"images/rl-cycle.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62744730",
   "metadata": {},
   "source": [
    "### Coding/defining our \"problem\" via an RL environment.\n",
    "\n",
    "We will use the following (adversarial) multi-agent environment\n",
    "throughout this tutorial to demonstrate a large fraction of RLlib's\n",
    "APIs, features, and customization options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb35116-efda-4799-8bae-e96d7775a0d1",
   "metadata": {},
   "source": [
    "<img src=\"images/environment.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe753-d7e0-4de1-b937-160507f75ed8",
   "metadata": {},
   "source": [
    "### A word or two on Spaces:\n",
    "\n",
    "Spaces are used in ML to describe what possible/valid values inputs and outputs of a neural network can have.\n",
    "\n",
    "RL environments also use them to describe what their valid observations and actions are.\n",
    "\n",
    "Spaces are usually defined by their shape (e.g. 84x84x3 RGB images) and datatype (e.g. uint8 for RGB values between 0 and 255).\n",
    "However, spaces could also be composed of other spaces (see Tuple or Dict spaces) or could be simply discrete with n fixed possible values\n",
    "(represented by integers). For example, in our game, where each agent can only go up/down/left/right, the action space would be \"Discrete(4)\"\n",
    "(no datatype, no shape needs to be defined here)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e4135-98ed-4e65-9e26-66f340747529",
   "metadata": {},
   "source": [
    "<img src=\"images/spaces.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6925507-0210-49d2-9e68-5d1e1157ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________\n",
      "|.         |\n",
      "|1         |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|         2|\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "\n",
      "R1=1.0\n",
      "R2=-0.1\n",
      "\n",
      "Agent1's x/y position=[1, 0]\n",
      "Agent2's x/y position=[8, 9]\n",
      "Env timesteps=1\n"
     ]
    }
   ],
   "source": [
    "# Let's code (parts of) our multi-agent environment.\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "class MultiAgentArena(MultiAgentEnv):\n",
    "    def __init__(self, config=None):\n",
    "        config = config or {}\n",
    "        # Dimensions of the grid.\n",
    "        self.width = config.get(\"width\", 10)\n",
    "        self.height = config.get(\"height\", 10)\n",
    "\n",
    "        # End an episode after this many timesteps.\n",
    "        self.timestep_limit = config.get(\"ts\", 100)\n",
    "\n",
    "        self.observation_space = MultiDiscrete([self.width * self.height,\n",
    "                                                self.width * self.height])\n",
    "        # 0=up, 1=right, 2=down, 3=left.\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        # Reset env.\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Returns initial observation of next(!) episode.\"\"\"\n",
    "        # Row-major coords!\n",
    "        self.agent1_pos = [0, 0]\n",
    "        self.agent2_pos = [self.height - 1, self.width - 1]\n",
    "\n",
    "        # Accumulated rewards in this episode.\n",
    "        self.agent1_R = 0.0\n",
    "        self.agent2_R = 0.0\n",
    "\n",
    "        # Reset agent1's visited fields.\n",
    "        self.agent1_visited_fields = set([tuple(self.agent1_pos)])\n",
    "\n",
    "        # How many timesteps have we done in this episode.\n",
    "        self.timesteps = 0\n",
    "\n",
    "        # Return the initial observation in the new episode.\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: dict):\n",
    "        \"\"\"Returns (next observation, rewards, dones, infos) after having taken the given action.\"\"\"\n",
    "        # increase our time steps counter by 1.\n",
    "        self.timesteps += 1\n",
    "        # An episode is \"done\" when we reach the time step limit.\n",
    "        is_done = self.timesteps >= self.timestep_limit\n",
    "\n",
    "        # Determine, who is allowed to move first (50:50).\n",
    "        if random.random() > 0.5:\n",
    "            # events = [collision|new_field]\n",
    "            events = self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "            events |= self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        else:\n",
    "            events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "            events |= self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "\n",
    "        # Useful for rendering.\n",
    "        self.collision = \"collision\" in events\n",
    "            \n",
    "        # Get observations (based on new agent positions).\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Determine rewards based on the collected events:\n",
    "        r1 = -1.0 if \"collision\" in events else 1.0 if \"new_field\" in events else -0.5\n",
    "        r2 = 1.0 if \"collision\" in events else -0.1\n",
    "\n",
    "        self.agent1_R += r1\n",
    "        self.agent2_R += r2\n",
    "        \n",
    "        rewards = {\n",
    "            \"agent1\": r1,\n",
    "            \"agent2\": r2,\n",
    "        }\n",
    "\n",
    "        # Generate a `done` dict (per-agent and total).\n",
    "        dones = {\n",
    "            \"agent1\": is_done,\n",
    "            \"agent2\": is_done,\n",
    "            # special `__all__` key indicates that the episode is done for all agents.\n",
    "            \"__all__\": is_done,\n",
    "        }\n",
    "\n",
    "        return obs, rewards, dones, {}  # <- info dict (not needed here).\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns obs dict (agent name to discrete-pos tuple) using each\n",
    "        agent's current x/y-positions.\n",
    "        \"\"\"\n",
    "        ag1_discrete_pos = self.agent1_pos[0] * self.width + \\\n",
    "            (self.agent1_pos[1] % self.width)\n",
    "        ag2_discrete_pos = self.agent2_pos[0] * self.width + \\\n",
    "            (self.agent2_pos[1] % self.width)\n",
    "        return {\n",
    "            \"agent1\": np.array([ag1_discrete_pos, ag2_discrete_pos]),\n",
    "            \"agent2\": np.array([ag2_discrete_pos, ag1_discrete_pos]),\n",
    "        }\n",
    "\n",
    "    def _move(self, coords, action, is_agent1):\n",
    "        \"\"\"\n",
    "        Moves an agent (agent1 iff is_agent1=True, else agent2) from `coords` (x/y) using the\n",
    "        given action (0=up, 1=right, etc..) and returns a resulting events dict:\n",
    "        Agent1: \"new\" when entering a new field. \"bumped\" when having been bumped into by agent2.\n",
    "        Agent2: \"bumped\" when bumping into agent1 (agent1 then gets -1.0).\n",
    "        \"\"\"\n",
    "        orig_coords = coords[:]\n",
    "        # Change the row: 0=up (-1), 2=down (+1)\n",
    "        coords[0] += -1 if action == 0 else 1 if action == 2 else 0\n",
    "        # Change the column: 1=right (+1), 3=left (-1)\n",
    "        coords[1] += 1 if action == 1 else -1 if action == 3 else 0\n",
    "\n",
    "        # Solve collisions.\n",
    "        # Make sure, we don't end up on the other agent's position.\n",
    "        # If yes, don't move (we are blocked).\n",
    "        if (is_agent1 and coords == self.agent2_pos) or (not is_agent1 and coords == self.agent1_pos):\n",
    "            coords[0], coords[1] = orig_coords\n",
    "            # Agent2 blocked agent1 (agent1 tried to run into agent2)\n",
    "            # OR Agent2 bumped into agent1 (agent2 tried to run into agent1)\n",
    "            return {\"collision\"}\n",
    "\n",
    "        # No agent blocking -> check walls.\n",
    "        if coords[0] < 0:\n",
    "            coords[0] = 0\n",
    "        elif coords[0] >= self.height:\n",
    "            coords[0] = self.height - 1\n",
    "        if coords[1] < 0:\n",
    "            coords[1] = 0\n",
    "        elif coords[1] >= self.width:\n",
    "            coords[1] = self.width - 1\n",
    "\n",
    "        # If agent1 -> \"new\" if new tile covered.\n",
    "        if is_agent1 and not tuple(coords) in self.agent1_visited_fields:\n",
    "            self.agent1_visited_fields.add(tuple(coords))\n",
    "            return {\"new_field\"}\n",
    "        # No new tile for agent1.\n",
    "        return set()\n",
    "\n",
    "    def render(self, mode=None):\n",
    "        print(\"_\" * (self.width + 2))\n",
    "        for r in range(self.height):\n",
    "            print(\"|\", end=\"\")\n",
    "            for c in range(self.width):\n",
    "                field = r * self.width + c % self.width\n",
    "                if self.agent1_pos == [r, c]:\n",
    "                    print(\"1\", end=\"\")\n",
    "                elif self.agent2_pos == [r, c]:\n",
    "                    print(\"2\", end=\"\")\n",
    "                elif (r, c) in self.agent1_visited_fields:\n",
    "                    print(\".\", end=\"\")\n",
    "                else:\n",
    "                    print(\" \", end=\"\")\n",
    "            print(\"|\")\n",
    "        print(\"‾\" * (self.width + 2))\n",
    "        print(f\"{'!!Collision!!' if self.collision else ''}\")\n",
    "        print(f\"R1={self.agent1_R}\")\n",
    "        print(f\"R2={self.agent2_R}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "dummy_env = MultiAgentArena()\n",
    "\n",
    "obs = dummy_env.reset()\n",
    "\n",
    "# Agent1 will move down, Agent2 moves up.\n",
    "obs, rewards, dones, infos = dummy_env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "\n",
    "dummy_env.render()\n",
    "\n",
    "print(\"Agent1's x/y position={}\".format(dummy_env.agent1_pos))\n",
    "print(\"Agent2's x/y position={}\".format(dummy_env.agent2_pos))\n",
    "print(\"Env timesteps={}\".format(dummy_env.timesteps))\n",
    "\n",
    "#TODO: merge exercise 2 and long tune learnign run somehow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-sussex",
   "metadata": {},
   "source": [
    "## Exercise No 1\n",
    "\n",
    "<hr />\n",
    "\n",
    "<img src=\"images/exercise1.png\" width=400>\n",
    "\n",
    "In the cell above, we performed a `reset()` and a single `step()` call. To walk through an entire episode, one would normally call `step()` repeatedly (with different actions) until the returned `done` dict has the \"agent1\" or \"agent2\" (or \"__all__\") key set to True. Your task is to write an \"environment loop\" that runs for exactly one episode using our `MultiAgentArena` class.\n",
    "\n",
    "Follow these instructions here to get this done.\n",
    "\n",
    "1. Create an env object.\n",
    "1. `reset` your environment to get the first (initial) observation.\n",
    "1. Compute the actions for \"agent1\" and \"agent2\" calling `DummyTrainer.compute_action([obs])` twice and putting the results into an action dict to be passed into `step()`, just like it's done in the above cell (where we do a single `step()`).\n",
    "1. Repeat this, `step`ing through an entire episode.\n",
    "1. When an episode is done, `step()` will return a done dict with key `__all__` set to True.\n",
    "1. If you feel, this is way too easy for you ;) , try to extract each agent's reward, sum it up over the episode and - at the end of the episode - print out each agent's accumulated reward (also called the \"return\" of an episode).\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_agent1=2\n",
      "action_agent2=3\n",
      "\n",
      "action_agent1=2\n",
      "action_agent2=0\n",
      "\n",
      "action_agent1=1\n",
      "action_agent2=3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DummyTrainer:\n",
    "    \"\"\"Dummy Trainer class used in Exercise #1.\n",
    "\n",
    "    Use its `compute_action` method to get a new action for one of the agents,\n",
    "    given the agent's observation (a single discrete value encoding the field\n",
    "    the agent is currently in).\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_action(self, single_agent_obs=None):\n",
    "        # Returns a random action for a single agent.\n",
    "        return np.random.randint(4)  # Discrete(4) -> return rand int between 0 and 3 (incl. 3).\n",
    "\n",
    "dummy_trainer = DummyTrainer()\n",
    "# Check, whether it's working.\n",
    "for _ in range(3):\n",
    "    # Get action for agent1 (providing agent1's and agent2's positions).\n",
    "    print(\"action_agent1={}\".format(dummy_trainer.compute_action(np.array([0, 99]))))\n",
    "\n",
    "    # Get action for agent2 (providing agent2's and agent1's positions).\n",
    "    print(\"action_agent2={}\".format(dummy_trainer.compute_action(np.array([99, 0]))))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa8a1d-7d82-4b79-b2d4-3ce7ffa6fae1",
   "metadata": {},
   "source": [
    "Write your solution code into this cell here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da37ee03-e4ec-4ad7-9a1e-d7f56805ba5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________\n",
      "|..        |\n",
      "|...       |\n",
      "|.....  2  |\n",
      "|.....     |\n",
      "|.....     |\n",
      "|....      |\n",
      "| ....     |\n",
      "| ..1.     |\n",
      "|   .      |\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "\n",
      "R1=-2.5\n",
      "R2=-7.6999999999999815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "\n",
    "# Leave the following as-is. It'll help us with rendering the env in this very cell's output.\n",
    "import time\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "out = Output()\n",
    "\n",
    "with out:\n",
    "\n",
    "    # Solution to Exercise #1:\n",
    "    # Start coding here inside this `with`-block:\n",
    "    # ...\n",
    "    env = MultiAgentArena()\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # Compute actions separately for each agent.\n",
    "        a1 = dummy_trainer.compute_action(obs[\"agent1\"])\n",
    "        a2 = dummy_trainer.compute_action(obs[\"agent2\"])\n",
    "\n",
    "        # Send the action-dict to the env.\n",
    "        obs, rewards, dones, _ = env.step({\"agent1\": a1, \"agent2\": a2})\n",
    "\n",
    "        if dones[\"agent1\"]:\n",
    "            break\n",
    "\n",
    "        # Get a rendered image from the env.\n",
    "        time.sleep(0.1)\n",
    "        display.clear_output(wait=True)\n",
    "        env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4196a5-7e7a-442a-8100-96bc7393c59d",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 15 min break :)\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
   "metadata": {},
   "source": [
    "### And now for something completely different:\n",
    "#### Plugging in RLlib!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd830b90-5762-4d22-8fa9-0abf0777a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-22 11:04:09,211\tINFO services.py:1272 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.179',\n",
       " 'raylet_ip_address': '192.168.0.179',\n",
       " 'redis_address': '192.168.0.179:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-06-22_11-04-07_432956_57764/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-06-22_11-04-07_432956_57764/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-06-22_11-04-07_432956_57764',\n",
       " 'metrics_export_port': 63393,\n",
       " 'node_id': '5c9016c977d25fe796393275228fddf56a0d2ccd63ce1aad66e683ea'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import ray\n",
    "\n",
    "# Start a new instance of Ray (when running this tutorial locally) or\n",
    "# connect to an already running one (when running this tutorial through Anyscale).\n",
    "\n",
    "ray.init()  # Hear the engine humming? ;)\n",
    "\n",
    "# In case you encounter the following error during our tutorial: `RuntimeError: Maybe you called ray.init twice by accident?`\n",
    "# Try: `ray.shutdown() + ray.init()` or `ray.init(ignore_reinit_error=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
   "metadata": {},
   "source": [
    "### Picking an RLlib algorithm - We'll use PPO throughout this tutorial (one-size-fits-all-kind-of-algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_algos.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bcc1116-a14c-4479-87c0-6ece58ab0464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-22 11:04:11,526\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-06-22 11:04:11,529\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-06-22 11:04:20,682\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPO"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "# We use the PPO algorithm here b/c its very flexible wrt its supported\n",
    "# action spaces and model types and b/c it learns well almost any problem.\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "# Specify a very simple config, defining our environment and some environment\n",
    "# options (see environment.py).\n",
    "config = {\n",
    "    \"env\": MultiAgentArena,  # \"my_env\" <- if we previously have registered the env with `tune.register_env(\"[name]\", lambda config: [returns env object])`.\n",
    "    \"env_config\": {\n",
    "        \"config\": {\n",
    "            \"width\": 10,\n",
    "            \"height\": 10,\n",
    "            \"ts\": 100,\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # !PyTorch users!\n",
    "    #\"framework\": \"torch\",  # If users have chosen to install torch instead of tf.\n",
    "\n",
    "    \"create_env_on_driver\": True,\n",
    "}\n",
    "# Instantiate the Trainer object using above config.\n",
    "rllib_trainer = PPOTrainer(config=config)\n",
    "rllib_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ae150-c0a3-477f-8d78-d0a34f147958",
   "metadata": {},
   "source": [
    "### Ready to train with RLlib's PPO algorithm\n",
    "\n",
    "That's it, we are ready to train.\n",
    "Calling `Trainer.train()` will execute a single \"training iteration\".\n",
    "\n",
    "One iteration for most algos involves:\n",
    "\n",
    "1) sampling from the environment(s)\n",
    "2) using the sampled data (observations, actions taken, rewards) to update the policy model (neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f6c94d4-6871-4d20-81af-3d4081f05f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_timesteps_total': 4000,\n",
      " 'custom_metrics': {},\n",
      " 'date': '2021-06-22_11-05-14',\n",
      " 'done': False,\n",
      " 'episode_len_mean': 100.0,\n",
      " 'episode_media': {},\n",
      " 'episode_reward_max': 11.100000000000016,\n",
      " 'episode_reward_mean': -8.4,\n",
      " 'episode_reward_min': -28.800000000000054,\n",
      " 'episodes_this_iter': 20,\n",
      " 'episodes_total': 20,\n",
      " 'experiment_id': '349d430c5b2d411c9baec1a1d03ead03',\n",
      " 'hist_stats': {'episode_lengths': [100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100],\n",
      "                'episode_reward': [-4.4999999999999964,\n",
      "                                   -21.00000000000002,\n",
      "                                   -21.000000000000064,\n",
      "                                   -11.99999999999998,\n",
      "                                   -0.599999999999994,\n",
      "                                   -11.999999999999973,\n",
      "                                   -3.89999999999999,\n",
      "                                   10.499999999999998,\n",
      "                                   -20.700000000000074,\n",
      "                                   -1.915134717478395e-15,\n",
      "                                   -10.499999999999975,\n",
      "                                   -10.499999999999991,\n",
      "                                   -10.499999999999973,\n",
      "                                   6.000000000000009,\n",
      "                                   2.700000000000015,\n",
      "                                   -28.800000000000054,\n",
      "                                   -11.999999999999995,\n",
      "                                   -14.999999999999979,\n",
      "                                   11.100000000000016,\n",
      "                                   -15.299999999999985]},\n",
      " 'hostname': 'Svens-MacBook-Pro.local',\n",
      " 'info': {'learner': {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224,\n",
      "                                                           'cur_lr': 4.999999873689376e-05,\n",
      "                                                           'entropy': 1.3683686,\n",
      "                                                           'entropy_coeff': 0.0,\n",
      "                                                           'kl': 0.018267294,\n",
      "                                                           'model': {},\n",
      "                                                           'policy_loss': -0.05145205,\n",
      "                                                           'total_loss': 22.289083,\n",
      "                                                           'vf_explained_var': 0.14045309,\n",
      "                                                           'vf_loss': 22.336882}}},\n",
      "          'num_agent_steps_sampled': 4000,\n",
      "          'num_agent_steps_trained': 4000,\n",
      "          'num_steps_sampled': 4000,\n",
      "          'num_steps_trained': 4000},\n",
      " 'iterations_since_restore': 1,\n",
      " 'node_ip': '192.168.0.179',\n",
      " 'num_healthy_workers': 2,\n",
      " 'off_policy_estimator': {},\n",
      " 'perf': {'cpu_util_percent': 6.673076923076922,\n",
      "          'ram_util_percent': 63.199999999999996},\n",
      " 'pid': 57764,\n",
      " 'policy_reward_max': {},\n",
      " 'policy_reward_mean': {},\n",
      " 'policy_reward_min': {},\n",
      " 'sampler_perf': {'mean_action_processing_ms': 0.05173849892782999,\n",
      "                  'mean_env_render_ms': 0.0,\n",
      "                  'mean_env_wait_ms': 0.02736442691677219,\n",
      "                  'mean_inference_ms': 0.5831973297850832,\n",
      "                  'mean_raw_obs_processing_ms': 0.12795408288915672},\n",
      " 'time_since_restore': 3.148030996322632,\n",
      " 'time_this_iter_s': 3.148030996322632,\n",
      " 'time_total_s': 3.148030996322632,\n",
      " 'timers': {'learn_throughput': 1744.911,\n",
      "            'learn_time_ms': 2292.38,\n",
      "            'load_throughput': 123278.488,\n",
      "            'load_time_ms': 32.447,\n",
      "            'sample_throughput': 4889.157,\n",
      "            'sample_time_ms': 818.137,\n",
      "            'update_time_ms': 1.64},\n",
      " 'timestamp': 1624352714,\n",
      " 'timesteps_since_restore': 0,\n",
      " 'timesteps_total': 4000,\n",
      " 'training_iteration': 1}\n"
     ]
    }
   ],
   "source": [
    "results = rllib_trainer.train()\n",
    "\n",
    "# Delete the config from the results for clarity.\n",
    "# Only the stats will remain, then.\n",
    "del results[\"config\"]\n",
    "# Pretty print the stats.\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96f682-fc7d-46a0-b136-f5d62cd7ad67",
   "metadata": {},
   "source": [
    "### Going from single policy (RLlib's default) to multi-policy:\n",
    "\n",
    "So far, our experiment has been ill-configured, because both\n",
    "agents, which should behave differently due to their different\n",
    "tasks and reward functions, learn the same policy: the \"default_policy\",\n",
    "which RLlib always provides if you don't configure anything else.\n",
    "Remember that RLlib does not know at Trainer setup time, how many and which agents\n",
    "the environment will \"produce\". Agent control (adding agents, removing them, terminating\n",
    "episodes for agents) is entirely in the Env's hands.\n",
    "Let's fix our single policy problem and introduce the \"multiagent\" API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13900163-f520-40f1-87be-d759760bd3a5",
   "metadata": {},
   "source": [
    "<img src=\"images/from_single_agent_to_multi_agent.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a813b988-e40f-4890-8c9c-f5f7d0f49cc9",
   "metadata": {},
   "source": [
    "In order to turn on RLlib's multi-agent functionality, we need two things:\n",
    "\n",
    "1. A policy mapping function, mapping agent IDs (e.g. a string like \"agent1\", produced by the environment in the returned observation/rewards/dones-dicts) to a policy ID (another string, e.g. \"policy1\", which is under our control).\n",
    "1. A policies definition dict, mapping policy IDs (e.g. \"policy1\") to 4-tuples consisting of 1) policy class (None for using the default class), 2) observation space, 3) action space, and 4) config overrides (empty dict for no overrides and using the Trainer's main config dict).\n",
    "\n",
    "Let's take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dff7017-f1b9-41e8-94fd-266bbe56cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an agent->policy mapping function.\n",
    "# Which agents (defined by the environment) use which policies (defined by us)?\n",
    "# The mapping here is M (agents) -> N (policies), where M >= N.\n",
    "def policy_mapping_fn(agent: str):\n",
    "    # Make sure agent ID is valid.\n",
    "    assert agent in [\"agent1\", \"agent2\"], f\"ERROR: invalid agent {agent}!\"\n",
    "    # Map agent1 to policy1, and agent2 to policy2.\n",
    "    return \"policy1\" if agent == \"agent1\" else \"policy2\"\n",
    "\n",
    "# Get the spaces for our two policies from our already existing Trainer object:\n",
    "observation_space = dummy_env.observation_space\n",
    "action_space = dummy_env.action_space\n",
    "\n",
    "# Define the policies definition dict:\n",
    "# Each policy in there is defined by its ID (key) mapping to a 4-tuple (value):\n",
    "# - Policy class (None for using the \"default\" class, e.g. PPOTFPolicy for PPO+tf or PPOTorchPolicy for PPO+torch).\n",
    "# - obs-space (we get this directly from our already created env object).\n",
    "# - act-space (we get this directly from our already created env object).\n",
    "# - config-overrides dict (leave empty for using the Trainer's config as-is)\n",
    "policies = {\n",
    "    \"policy1\": (None, observation_space, action_space, {\"lr\": 0.0002}),\n",
    "    \"policy2\": (None, observation_space, action_space, {}),\n",
    "}\n",
    "# Note that now we won't have a \"default_policy\" anymore, just \"policy1\" and \"policy2\".\n",
    "\n",
    "# We could - if we wanted - specify, which policies should be learnt (by default, RLlib learns all).\n",
    "# Non-learnt policies will be frozen and not updated:\n",
    "# policies_to_train = [\"policy1\", \"policy2\"]\n",
    "\n",
    "# Adding the above to our config.\n",
    "config.update({\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        # We'll leave this empty: Means, we train both policy1 and policy2.\n",
    "        # \"policies_to_train\": policies_to_train,\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "646f8800-941b-43cb-a924-622af6788aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-22 11:08:03,805\tINFO trainable.py:101 -- Trainable.setup took 12.909 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-06-22 11:08:03,807\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# Recreate our Trainer (we cannot just change the config on-the-fly).\n",
    "rllib_trainer.stop()\n",
    "\n",
    "# Using our updated (now multiagent!) config dict.\n",
    "rllib_trainer = PPOTrainer(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {},
   "source": [
    "Now that we are setup correctly with two policies as per our \"multiagent\" config, let's call `train()` on the new Trainer several times (what about 10 times?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17ae724d-71cc-422b-96cb-3dc9faa2d111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=1: R(\"return\")=-10.627500000000001\n",
      "Iteration=2: R(\"return\")=-7.829999999999996\n",
      "Iteration=3: R(\"return\")=-4.484999999999991\n",
      "Iteration=4: R(\"return\")=-1.3409999999999898\n",
      "Iteration=5: R(\"return\")=-0.47099999999998965\n",
      "Iteration=6: R(\"return\")=1.2780000000000067\n",
      "Iteration=7: R(\"return\")=1.914000000000008\n",
      "Iteration=8: R(\"return\")=2.1180000000000097\n",
      "Iteration=9: R(\"return\")=1.515000000000009\n",
      "Iteration=10: R(\"return\")=2.3670000000000093\n"
     ]
    }
   ],
   "source": [
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "# Move on once you see (agent1 + agent2) episode rewards of 10.0 or more.\n",
    "for _ in range(10):\n",
    "    results = rllib_trainer.train()\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R(\\\"return\\\")={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "365ef0d7-9977-4d9d-9fa5-ffaa7c111b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=11: R1(\"return\")=9.105 R2(\"return\")=-7.766999999999984\n",
      "Iteration=12: R1(\"return\")=11.48 R2(\"return\")=-7.6129999999999844\n",
      "Iteration=13: R1(\"return\")=12.625 R2(\"return\")=-7.557999999999988\n",
      "Iteration=14: R1(\"return\")=13.985 R2(\"return\")=-7.3159999999999865\n",
      "Iteration=15: R1(\"return\")=14.585 R2(\"return\")=-7.249999999999987\n",
      "Iteration=16: R1(\"return\")=15.135 R2(\"return\")=-7.106999999999987\n",
      "Iteration=17: R1(\"return\")=14.49 R2(\"return\")=-6.479999999999987\n",
      "Iteration=18: R1(\"return\")=16.03 R2(\"return\")=-6.468999999999989\n",
      "Iteration=19: R1(\"return\")=16.77 R2(\"return\")=-6.182999999999988\n",
      "Iteration=20: R1(\"return\")=18.64 R2(\"return\")=-6.435999999999989\n"
     ]
    }
   ],
   "source": [
    "# Do another loop, but this time, we will print out each policies' individual rewards.\n",
    "for _ in range(10):\n",
    "    results = rllib_trainer.train()\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R1={results['policy_reward_mean']['policy1']} R2={results['policy_reward_mean']['policy2']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80ad33-a55b-4e18-857b-b884eedda0a4",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK! (<-- we will not do these during the tutorial, but feel free to try these cells by yourself)\n",
    "\n",
    "Use the above solution of Exercise #1 and replace our `dummy_trainer` in that solution\n",
    "with the now trained `rllib_trainer`. You should see a better performance of the two agents.\n",
    "\n",
    "However, keep in mind that we are mostly training agent1 as we only trian a single policy and agent1\n",
    "is the \"easier\" one to collect high rewards with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK!\n",
    "\n",
    "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using Policies and their model(s) inside our Trainer object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aff679e8-74b4-4603-9d5c-4cc0c6ebe45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our (only!) Policy right now is: <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f81ec1eda00>\n",
      "Our Policy's observation space is: Box(-1.0, 1.0, (200,), float32)\n",
      "Our Policy's action space is: Discrete(4)\n",
      "sampled action=3\n"
     ]
    }
   ],
   "source": [
    "# Let's actually \"look inside\" our Trainer to see what's in there.\n",
    "from ray.rllib.utils.numpy import softmax\n",
    "\n",
    "# To get to one of the policies inside the Trainer, use `Trainer.get_policy([policy ID])`:\n",
    "policy = rllib_trainer.get_policy(\"policy1\")\n",
    "print(f\"Our (only!) Policy right now is: {policy}\")\n",
    "\n",
    "# To get to the model inside any policy, do:\n",
    "model = policy.model\n",
    "#print(f\"Our Policy's model is: {model}\")\n",
    "\n",
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\")\n",
    "\n",
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = np.array([policy.observation_space.sample()])\n",
    "# Alternatively for PyTorch:\n",
    "#import torch\n",
    "#obs = torch.from_numpy(obs)\n",
    "\n",
    "# Get the action logits (as tf tensor).\n",
    "# If you are using torch, you would get a torch tensor here.\n",
    "logits, _ = model({\"obs\": obs})\n",
    "logits\n",
    "\n",
    "# Numpyize the tensor by running `logits` through the Policy's own tf.Session.\n",
    "logits_np = policy.get_session().run(logits)\n",
    "# For torch, you can simply do: `logits_np = logits.detach().cpu().numpy()`.\n",
    "\n",
    "# Convert logits into action probabilities and remove the B=1.\n",
    "action_probs = np.squeeze(softmax(logits_np))\n",
    "\n",
    "# Sample an action, using the probabilities.\n",
    "action = np.random.choice([0, 1, 2, 3], p=action_probs)\n",
    "\n",
    "# Print out the action.\n",
    "print(f\"sampled action={action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd66c3-f07a-4795-84ea-6b232ba6a047",
   "metadata": {},
   "source": [
    "### Saving and restoring a trained Trainer.\n",
    "Currently, `rllib_trainer` is in an already trained state.\n",
    "It holds optimized weights in its Policy's model that allow it to act\n",
    "already somewhat smart in our environment when given an observation.\n",
    "\n",
    "However, if we closed this notebook right now, all the effort would have been for nothing.\n",
    "Let's therefore save the state of our trainer to disk for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57eae1e4-3cc4-4282-9a83-bc374bdad978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer (at iteration 20 was saved in '/Users/sven/ray_results/PPO_MultiAgentArena_2021-06-22_11-07-50va2w_ji5/checkpoint_000020/checkpoint-20'!\n",
      "The checkpoint directory contains the following files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['checkpoint-20', 'checkpoint-20.tune_metadata', '.is_checkpoint']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the `Trainer.save()` method to create a checkpoint.\n",
    "checkpoint_file = rllib_trainer.save()\n",
    "print(f\"Trainer (at iteration {rllib_trainer.iteration} was saved in '{checkpoint_file}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "import os\n",
    "os.listdir(os.path.dirname(checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e0ab-2c10-469a-97b1-4aadf1a1ec97",
   "metadata": {},
   "source": [
    "### Restoring and evaluating a Trainer\n",
    "In the following cell, we'll learn how to restore a saved Trainer from a checkpoint file.\n",
    "\n",
    "We'll also evaluate a completely new Trainer (should act more or less randomly) vs an already trained one (the one we just restored from the created checkpoint file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74ceedb9-c225-46f2-ad1d-f902c81d3256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-22 11:10:29,827\tINFO trainable.py:101 -- Trainable.setup took 12.382 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-06-22 11:10:29,829\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating new trainer: R=-10.605000000000011\n",
      "Before restoring: Trainer is at iteration=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-22 11:10:32,847\tINFO trainable.py:377 -- Restored on 192.168.0.179 from checkpoint: /Users/sven/ray_results/PPO_MultiAgentArena_2021-06-22_11-07-50va2w_ji5/checkpoint_000020/checkpoint-20\n",
      "2021-06-22 11:10:32,847\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': None, '_time_total': 132.84255385398865, '_episodes_total': 800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After restoring: Trainer is at iteration=20\n",
      "Evaluating restored trainer: R=11.759999999999962\n"
     ]
    }
   ],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_trainer = PPOTrainer(config=config)\n",
    "# Evaluate the new trainer (this should yield random results).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "print(f\"Before restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "new_trainer.restore(checkpoint_file)\n",
    "print(f\"After restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "\n",
    "# Evaluate again (this should yield results we saw after having trained our saved agent).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating restored trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de603d14-f0cb-4363-a72b-8f147c094071",
   "metadata": {},
   "source": [
    "In order to release all resources from a Trainer, you can use a Trainer's `stop()` method.\n",
    "You should definitley run this cell as it frees resources that we'll need later in this tutorial, when we'll do parallel hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "737dca4f-942f-4fda-abcc-0052263a103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rllib_trainer.stop()\n",
    "new_trainer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c1e4c-cb02-4719-ac5a-0106172a6c6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Moving stuff to the professional level: RLlib in connection w/ Ray Tune\n",
    "\n",
    "Running any experiments through Ray Tune is the recommended way of doing things with RLlib. If you look at our\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/rllib/examples\">examples scripts folder</a>, you will see that almost all of the scripts use Ray Tune to run the particular RLlib workload demonstrated in each script.\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdacebb-d27f-4174-9002-35c5657f146c",
   "metadata": {
    "tags": []
   },
   "source": [
    "When setting up hyperparameter sweeps for Tune, we'll do this in our already familiar config dict.\n",
    "\n",
    "So let's take a quick look at our PPO algo's default config to understand, which hyperparameters we may want to play around with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1b32582-52bd-4585-9009-2f877a0723a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO's default config is:\n",
      "{'_fake_gpus': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': True,\n",
      " 'clip_param': 0.3,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': 180,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'eager_tracing': False,\n",
      " 'entropy_coeff': 0.0,\n",
      " 'entropy_coeff_schedule': None,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'env_task_fn': None,\n",
      " 'evaluation_config': {},\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': 10,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'evaluation_parallel_to_training': False,\n",
      " 'exploration_config': {'type': 'StochasticSampling'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'tf',\n",
      " 'gamma': 0.99,\n",
      " 'grad_clip': None,\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'kl_coeff': 0.2,\n",
      " 'kl_target': 0.01,\n",
      " 'lambda': 1.0,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'lr': 5e-05,\n",
      " 'lr_schedule': None,\n",
      " 'metrics_smoothing_episodes': 100,\n",
      " 'min_iter_time_s': 0,\n",
      " 'model': {'_time_major': False,\n",
      "           '_use_default_native_models': False,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'num_framestacks': 'auto',\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': False,\n",
      "           'zero_mean': True},\n",
      " 'monitor': -1,\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_mapping_fn': None,\n",
      "                'replay_mode': 'independent'},\n",
      " 'no_done_at_end': False,\n",
      " 'normalize_actions': False,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_sgd_iter': 30,\n",
      " 'num_workers': 2,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'record_env': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'rollout_fragment_length': 200,\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'seed': None,\n",
      " 'sgd_minibatch_size': 128,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'shuffle_sequences': True,\n",
      " 'simple_optimizer': -1,\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 0,\n",
      " 'train_batch_size': 4000,\n",
      " 'use_critic': True,\n",
      " 'use_gae': True,\n",
      " 'vf_clip_param': 10.0,\n",
      " 'vf_loss_coeff': 1.0,\n",
      " 'vf_share_layers': -1}\n"
     ]
    }
   ],
   "source": [
    "# Configuration dicts and Ray Tune.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# PPO algorithm:\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as PPO_DEFAULT_CONFIG\n",
    "print(f\"PPO's default config is:\")\n",
    "pprint.pprint(PPO_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint.pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint.pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded886cc-436e-46cd-8fea-d68af8b41236",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's do a very simple grid-search over two learning rates with tune.run().\n",
    "\n",
    "In particular, we will try the learning rates 0.00005 and 0.5 using `tune.grid_search([...])`\n",
    "inside our config dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5063991e-173b-49be-a4e7-467e2e18321a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/3.78 GiB heap, 0.0/1.89 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">   lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_38c9e_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">5e-05</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_38c9e_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.5  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=58208)\u001b[0m 2021-06-22 11:43:02,337\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=58208)\u001b[0m 2021-06-22 11:43:02,337\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=58216)\u001b[0m 2021-06-22 11:43:02,337\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=58216)\u001b[0m 2021-06-22 11:43:02,337\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=58216)\u001b[0m 2021-06-22 11:43:14,173\tINFO trainable.py:101 -- Trainable.setup took 11.837 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=58216)\u001b[0m 2021-06-22 11:43:14,174\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=58208)\u001b[0m 2021-06-22 11:43:14,891\tINFO trainable.py:101 -- Trainable.setup took 12.554 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=58208)\u001b[0m 2021-06-22 11:43:14,891\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_38c9e_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-22_11-43-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.500000000000027\n",
      "  episode_reward_mean: -7.229999999999997\n",
      "  episode_reward_min: -37.50000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: e01e80a4879747e58fbf19a5912d5bf2\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3465654850006104\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04035121574997902\n",
      "          model: {}\n",
      "          policy_loss: -0.07320234924554825\n",
      "          total_loss: 38.04973602294922\n",
      "          vf_explained_var: 0.13369648158550262\n",
      "          vf_loss: 38.1148681640625\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.364458441734314\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021879037842154503\n",
      "          model: {}\n",
      "          policy_loss: -0.054815199226140976\n",
      "          total_loss: 2.0010361671447754\n",
      "          vf_explained_var: 0.3664141595363617\n",
      "          vf_loss: 2.051475763320923\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.8125\n",
      "    ram_util_percent: 66.33125000000001\n",
      "  pid: 58216\n",
      "  policy_reward_max:\n",
      "    policy1: 23.5\n",
      "    policy2: -3.4000000000000044\n",
      "  policy_reward_mean:\n",
      "    policy1: 1.5875\n",
      "    policy2: -8.817499999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -27.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08754757629043755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.041336670093450587\n",
      "    mean_inference_ms: 1.483284551343103\n",
      "    mean_raw_obs_processing_ms: 0.18845481434087644\n",
      "  time_since_restore: 10.82135009765625\n",
      "  time_this_iter_s: 10.82135009765625\n",
      "  time_total_s: 10.82135009765625\n",
      "  timers:\n",
      "    learn_throughput: 582.226\n",
      "    learn_time_ms: 6870.181\n",
      "    load_throughput: 25933.788\n",
      "    load_time_ms: 154.239\n",
      "    sample_throughput: 1087.387\n",
      "    sample_time_ms: 3678.542\n",
      "    update_time_ms: 5.017\n",
      "  timestamp: 1624355004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 38c9e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/3.78 GiB heap, 0.0/1.89 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_38c9e_00000</td><td>RUNNING </td><td>192.168.0.179:58216</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.8214</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">   -7.23</td><td style=\"text-align: right;\">                13.5</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_38c9e_00001</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_38c9e_00001:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-22_11-43-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.500000000000023\n",
      "  episode_reward_mean: -12.442500000000004\n",
      "  episode_reward_min: -33.900000000000034\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 35145892a4bf4cc4a4af79467da0fcb1\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3476173877716064\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.039893217384815216\n",
      "          model: {}\n",
      "          policy_loss: -0.07687855511903763\n",
      "          total_loss: 35.35573196411133\n",
      "          vf_explained_var: 0.13819590210914612\n",
      "          vf_loss: 35.42463302612305\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.1299269050359726\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 15.365917205810547\n",
      "          model: {}\n",
      "          policy_loss: 0.45972010493278503\n",
      "          total_loss: 8.158370018005371\n",
      "          vf_explained_var: 0.03607312589883804\n",
      "          vf_loss: 4.625467300415039\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.131249999999994\n",
      "    ram_util_percent: 66.775\n",
      "  pid: 58208\n",
      "  policy_reward_max:\n",
      "    policy1: 17.5\n",
      "    policy2: -5.599999999999982\n",
      "  policy_reward_mean:\n",
      "    policy1: -3.2125\n",
      "    policy2: -9.22999999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -25.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0900202903194704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042773853952082795\n",
      "    mean_inference_ms: 1.5129424642765896\n",
      "    mean_raw_obs_processing_ms: 0.19333792709815747\n",
      "  time_since_restore: 10.973721981048584\n",
      "  time_this_iter_s: 10.973721981048584\n",
      "  time_total_s: 10.973721981048584\n",
      "  timers:\n",
      "    learn_throughput: 572.88\n",
      "    learn_time_ms: 6982.266\n",
      "    load_throughput: 27601.641\n",
      "    load_time_ms: 144.919\n",
      "    sample_throughput: 1067.194\n",
      "    sample_time_ms: 3748.147\n",
      "    update_time_ms: 2.926\n",
      "  timestamp: 1624355005\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 38c9e_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_38c9e_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-22_11-43-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.99999999999995\n",
      "  episode_reward_mean: -4.331249999999995\n",
      "  episode_reward_min: -37.50000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: e01e80a4879747e58fbf19a5912d5bf2\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.3005321025848389\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.038319576531648636\n",
      "          model: {}\n",
      "          policy_loss: -0.08007244765758514\n",
      "          total_loss: 35.873905181884766\n",
      "          vf_explained_var: 0.17271865904331207\n",
      "          vf_loss: 35.94248580932617\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3314083814620972\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01932242140173912\n",
      "          model: {}\n",
      "          policy_loss: -0.05493776127696037\n",
      "          total_loss: 2.0015101432800293\n",
      "          vf_explained_var: 0.36646607518196106\n",
      "          vf_loss: 2.0506513118743896\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.78666666666667\n",
      "    ram_util_percent: 68.75333333333333\n",
      "  pid: 58216\n",
      "  policy_reward_max:\n",
      "    policy1: 28.0\n",
      "    policy2: -3.4000000000000044\n",
      "  policy_reward_mean:\n",
      "    policy1: 4.3625\n",
      "    policy2: -8.693749999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -27.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09125269400392864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04353820969060552\n",
      "    mean_inference_ms: 1.546249308417874\n",
      "    mean_raw_obs_processing_ms: 0.19783720327831125\n",
      "  time_since_restore: 21.46284508705139\n",
      "  time_this_iter_s: 10.641494989395142\n",
      "  time_total_s: 21.46284508705139\n",
      "  timers:\n",
      "    learn_throughput: 606.295\n",
      "    learn_time_ms: 6597.453\n",
      "    load_throughput: 50316.075\n",
      "    load_time_ms: 79.497\n",
      "    sample_throughput: 1002.532\n",
      "    sample_time_ms: 3989.898\n",
      "    update_time_ms: 4.045\n",
      "  timestamp: 1624355015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 38c9e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/3.78 GiB heap, 0.0/1.89 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_38c9e_00000</td><td>RUNNING </td><td>192.168.0.179:58216</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         21.4628</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> -4.33125</td><td style=\"text-align: right;\">                18  </td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_38c9e_00001</td><td>RUNNING </td><td>192.168.0.179:58208</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.9737</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-12.4425 </td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               -33.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_38c9e_00001:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-22_11-43-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.500000000000023\n",
      "  episode_reward_mean: -14.977500000000006\n",
      "  episode_reward_min: -45.000000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 35145892a4bf4cc4a4af79467da0fcb1\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.345901608467102\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029133351519703865\n",
      "          model: {}\n",
      "          policy_loss: -0.04670372232794762\n",
      "          total_loss: 51.22841262817383\n",
      "          vf_explained_var: 0.08547636866569519\n",
      "          vf_loss: 51.26637268066406\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.030852800235152245\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 11.159064292907715\n",
      "          model: {}\n",
      "          policy_loss: 0.2337779551744461\n",
      "          total_loss: 12.517848014831543\n",
      "          vf_explained_var: -0.0002740390773396939\n",
      "          vf_loss: 8.936349868774414\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.919999999999995\n",
      "    ram_util_percent: 68.60666666666665\n",
      "  pid: 58208\n",
      "  policy_reward_max:\n",
      "    policy1: 17.5\n",
      "    policy2: -5.599999999999982\n",
      "  policy_reward_mean:\n",
      "    policy1: -5.3625\n",
      "    policy2: -9.614999999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -35.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09443969160269905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04527404329930128\n",
      "    mean_inference_ms: 1.580522920851477\n",
      "    mean_raw_obs_processing_ms: 0.2037635978761842\n",
      "  time_since_restore: 21.546197175979614\n",
      "  time_this_iter_s: 10.57247519493103\n",
      "  time_total_s: 21.546197175979614\n",
      "  timers:\n",
      "    learn_throughput: 610.048\n",
      "    learn_time_ms: 6556.864\n",
      "    load_throughput: 53403.497\n",
      "    load_time_ms: 74.901\n",
      "    sample_throughput: 978.612\n",
      "    sample_time_ms: 4087.421\n",
      "    update_time_ms: 3.101\n",
      "  timestamp: 1624355016\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 38c9e_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_38c9e_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-22_11-43-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.99999999999995\n",
      "  episode_reward_mean: -2.7809999999999917\n",
      "  episode_reward_min: -31.50000000000002\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: e01e80a4879747e58fbf19a5912d5bf2\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2656893730163574\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0305814016610384\n",
      "          model: {}\n",
      "          policy_loss: -0.07033588737249374\n",
      "          total_loss: 24.11922264099121\n",
      "          vf_explained_var: 0.30102092027664185\n",
      "          vf_loss: 24.175796508789062\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2985825538635254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020230518653988838\n",
      "          model: {}\n",
      "          policy_loss: -0.054081276059150696\n",
      "          total_loss: 2.5788140296936035\n",
      "          vf_explained_var: 0.4049268662929535\n",
      "          vf_loss: 2.626826286315918\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.45714285714286\n",
      "    ram_util_percent: 68.36428571428573\n",
      "  pid: 58216\n",
      "  policy_reward_max:\n",
      "    policy1: 28.0\n",
      "    policy2: 2.099999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 5.8\n",
      "    policy2: -8.580999999999985\n",
      "  policy_reward_min:\n",
      "    policy1: -22.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09367555276756803\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.045087706491794124\n",
      "    mean_inference_ms: 1.5929867618947633\n",
      "    mean_raw_obs_processing_ms: 0.20424539629334768\n",
      "  time_since_restore: 30.92416000366211\n",
      "  time_this_iter_s: 9.461314916610718\n",
      "  time_total_s: 30.92416000366211\n",
      "  timers:\n",
      "    learn_throughput: 648.099\n",
      "    learn_time_ms: 6171.893\n",
      "    load_throughput: 73328.061\n",
      "    load_time_ms: 54.549\n",
      "    sample_throughput: 991.522\n",
      "    sample_time_ms: 4034.2\n",
      "    update_time_ms: 4.469\n",
      "  timestamp: 1624355025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 38c9e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/3.78 GiB heap, 0.0/1.89 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_38c9e_00000</td><td>RUNNING </td><td>192.168.0.179:58216</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         30.9242</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -2.781 </td><td style=\"text-align: right;\">                18  </td><td style=\"text-align: right;\">               -31.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_38c9e_00001</td><td>RUNNING </td><td>192.168.0.179:58208</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         21.5462</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">-14.9775</td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               -45  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_38c9e_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-22_11-43-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.000000000000014\n",
      "  episode_reward_mean: -12.723000000000003\n",
      "  episode_reward_min: -45.000000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 35145892a4bf4cc4a4af79467da0fcb1\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.311800479888916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028345393016934395\n",
      "          model: {}\n",
      "          policy_loss: -0.052656546235084534\n",
      "          total_loss: 38.49173355102539\n",
      "          vf_explained_var: 0.04838701710104942\n",
      "          vf_loss: 38.53163528442383\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.5\n",
      "          entropy: 1.0367326441773628e-10\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.287576765207632e-07\n",
      "          model: {}\n",
      "          policy_loss: 0.001953327329829335\n",
      "          total_loss: 5.331360816955566\n",
      "          vf_explained_var: 0.023790504783391953\n",
      "          vf_loss: 5.329407691955566\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.41428571428572\n",
      "    ram_util_percent: 68.38571428571429\n",
      "  pid: 58208\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -5.599999999999982\n",
      "  policy_reward_mean:\n",
      "    policy1: -2.8\n",
      "    policy2: -9.922999999999984\n",
      "  policy_reward_min:\n",
      "    policy1: -35.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0972129872386625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.046791935344333895\n",
      "    mean_inference_ms: 1.6171898408036471\n",
      "    mean_raw_obs_processing_ms: 0.20909981274776201\n",
      "  time_since_restore: 30.903090000152588\n",
      "  time_this_iter_s: 9.356892824172974\n",
      "  time_total_s: 30.903090000152588\n",
      "  timers:\n",
      "    learn_throughput: 650.044\n",
      "    learn_time_ms: 6153.433\n",
      "    load_throughput: 77646.294\n",
      "    load_time_ms: 51.516\n",
      "    sample_throughput: 985.756\n",
      "    sample_time_ms: 4057.799\n",
      "    update_time_ms: 2.813\n",
      "  timestamp: 1624355025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 38c9e_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_38c9e_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-22_11-43-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.699999999999925\n",
      "  episode_reward_mean: -0.10799999999998913\n",
      "  episode_reward_min: -19.5\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: e01e80a4879747e58fbf19a5912d5bf2\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2326761484146118\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02380692958831787\n",
      "          model: {}\n",
      "          policy_loss: -0.06353594362735748\n",
      "          total_loss: 24.988330841064453\n",
      "          vf_explained_var: 0.35419580340385437\n",
      "          vf_loss: 25.035791397094727\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2664393186569214\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016825878992676735\n",
      "          model: {}\n",
      "          policy_loss: -0.05461708456277847\n",
      "          total_loss: 2.854233503341675\n",
      "          vf_explained_var: 0.2750512659549713\n",
      "          vf_loss: 2.9012796878814697\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.766666666666666\n",
      "    ram_util_percent: 68.21333333333332\n",
      "  pid: 58216\n",
      "  policy_reward_max:\n",
      "    policy1: 31.5\n",
      "    policy2: 2.099999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 7.725\n",
      "    policy2: -7.832999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -9.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09488989282122944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.045894987520496165\n",
      "    mean_inference_ms: 1.6191976466139744\n",
      "    mean_raw_obs_processing_ms: 0.2075265127731744\n",
      "  time_since_restore: 41.32073903083801\n",
      "  time_this_iter_s: 10.396579027175903\n",
      "  time_total_s: 41.32073903083801\n",
      "  timers:\n",
      "    learn_throughput: 638.61\n",
      "    learn_time_ms: 6263.603\n",
      "    load_throughput: 94685.827\n",
      "    load_time_ms: 42.245\n",
      "    sample_throughput: 1004.088\n",
      "    sample_time_ms: 3983.713\n",
      "    update_time_ms: 5.372\n",
      "  timestamp: 1624355035\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 38c9e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/3.78 GiB heap, 0.0/1.89 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_38c9e_00000</td><td>RUNNING </td><td>192.168.0.179:58216</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         41.3207</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  -0.108</td><td style=\"text-align: right;\">                23.7</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_38c9e_00001</td><td>RUNNING </td><td>192.168.0.179:58208</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         30.9031</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -12.723</td><td style=\"text-align: right;\">                15  </td><td style=\"text-align: right;\">               -45  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_38c9e_00001:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-22_11-43-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.000000000000023\n",
      "  episode_reward_mean: -6.482999999999993\n",
      "  episode_reward_min: -45.000000000000064\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 35145892a4bf4cc4a4af79467da0fcb1\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.283635139465332\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021360205486416817\n",
      "          model: {}\n",
      "          policy_loss: -0.04965157434344292\n",
      "          total_loss: 32.91261291503906\n",
      "          vf_explained_var: 0.16926634311676025\n",
      "          vf_loss: 32.947845458984375\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 0.5\n",
      "          entropy: 6.081121023404634e-11\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -8.76650645395129e-17\n",
      "          model: {}\n",
      "          policy_loss: 0.0026725218631327152\n",
      "          total_loss: 8.312509536743164\n",
      "          vf_explained_var: 0.00670907786116004\n",
      "          vf_loss: 8.309837341308594\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.87999999999999\n",
      "    ram_util_percent: 68.00666666666665\n",
      "  pid: 58208\n",
      "  policy_reward_max:\n",
      "    policy1: 25.0\n",
      "    policy2: -8.89999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 3.495\n",
      "    policy2: -9.977999999999982\n",
      "  policy_reward_min:\n",
      "    policy1: -35.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09886043672937386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04767659455754077\n",
      "    mean_inference_ms: 1.646158471006011\n",
      "    mean_raw_obs_processing_ms: 0.21285628302483547\n",
      "  time_since_restore: 41.68005394935608\n",
      "  time_this_iter_s: 10.776963949203491\n",
      "  time_total_s: 41.68005394935608\n",
      "  timers:\n",
      "    learn_throughput: 638.836\n",
      "    learn_time_ms: 6261.383\n",
      "    load_throughput: 99879.69\n",
      "    load_time_ms: 40.048\n",
      "    sample_throughput: 978.91\n",
      "    sample_time_ms: 4086.177\n",
      "    update_time_ms: 3.588\n",
      "  timestamp: 1624355036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 38c9e_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_38c9e_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-22_11-44-06\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.699999999999925\n",
      "  episode_reward_mean: 0.9510000000000115\n",
      "  episode_reward_min: -21.30000000000002\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: e01e80a4879747e58fbf19a5912d5bf2\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2068381309509277\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01780601590871811\n",
      "          model: {}\n",
      "          policy_loss: -0.05921831727027893\n",
      "          total_loss: 23.56062126159668\n",
      "          vf_explained_var: 0.35213178396224976\n",
      "          vf_loss: 23.601810455322266\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2288659811019897\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018553538247942924\n",
      "          model: {}\n",
      "          policy_loss: -0.059628624469041824\n",
      "          total_loss: 2.7461516857147217\n",
      "          vf_explained_var: 0.31079331040382385\n",
      "          vf_loss: 2.797431230545044\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.186666666666675\n",
      "    ram_util_percent: 64.92\n",
      "  pid: 58216\n",
      "  policy_reward_max:\n",
      "    policy1: 31.5\n",
      "    policy2: 2.0999999999999974\n",
      "  policy_reward_mean:\n",
      "    policy1: 8.52\n",
      "    policy2: -7.568999999999986\n",
      "  policy_reward_min:\n",
      "    policy1: -13.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09582451006114417\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04628839020183136\n",
      "    mean_inference_ms: 1.6392542244780899\n",
      "    mean_raw_obs_processing_ms: 0.20953371147199715\n",
      "  time_since_restore: 52.00999093055725\n",
      "  time_this_iter_s: 10.689251899719238\n",
      "  time_total_s: 52.00999093055725\n",
      "  timers:\n",
      "    learn_throughput: 644.302\n",
      "    learn_time_ms: 6208.269\n",
      "    load_throughput: 114554.808\n",
      "    load_time_ms: 34.918\n",
      "    sample_throughput: 969.772\n",
      "    sample_time_ms: 4124.683\n",
      "    update_time_ms: 4.805\n",
      "  timestamp: 1624355046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 38c9e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/3.78 GiB heap, 0.0/1.89 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_38c9e_00000</td><td>RUNNING </td><td>192.168.0.179:58216</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         52.01  </td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   0.951</td><td style=\"text-align: right;\">                23.7</td><td style=\"text-align: right;\">               -21.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_38c9e_00001</td><td>RUNNING </td><td>192.168.0.179:58208</td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         41.6801</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  -6.483</td><td style=\"text-align: right;\">                15  </td><td style=\"text-align: right;\">               -45  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_38c9e_00001:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-22_11-44-06\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.499999999999936\n",
      "  episode_reward_mean: -0.16199999999999096\n",
      "  episode_reward_min: -28.500000000000014\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 35145892a4bf4cc4a4af79467da0fcb1\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.2669665813446045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016159681603312492\n",
      "          model: {}\n",
      "          policy_loss: -0.04417208582162857\n",
      "          total_loss: 32.857418060302734\n",
      "          vf_explained_var: 0.28577640652656555\n",
      "          vf_loss: 32.88522720336914\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11249999701976776\n",
      "          cur_lr: 0.5\n",
      "          entropy: 8.225354425350773e-11\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.909639801913996e-17\n",
      "          model: {}\n",
      "          policy_loss: -0.0001582033437443897\n",
      "          total_loss: 26.02437973022461\n",
      "          vf_explained_var: 0.025698725134134293\n",
      "          vf_loss: 26.024539947509766\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.286666666666655\n",
      "    ram_util_percent: 64.82000000000001\n",
      "  pid: 58208\n",
      "  policy_reward_max:\n",
      "    policy1: 32.5\n",
      "    policy2: -8.89999999999998\n",
      "  policy_reward_mean:\n",
      "    policy1: 9.805\n",
      "    policy2: -9.966999999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -18.5\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0991790862054205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0477898771116767\n",
      "    mean_inference_ms: 1.6566125275282773\n",
      "    mean_raw_obs_processing_ms: 0.2135365143079847\n",
      "  time_since_restore: 51.895389795303345\n",
      "  time_this_iter_s: 10.215335845947266\n",
      "  time_total_s: 51.895389795303345\n",
      "  timers:\n",
      "    learn_throughput: 646.131\n",
      "    learn_time_ms: 6190.691\n",
      "    load_throughput: 121324.172\n",
      "    load_time_ms: 32.97\n",
      "    sample_throughput: 968.999\n",
      "    sample_time_ms: 4127.973\n",
      "    update_time_ms: 3.394\n",
      "  timestamp: 1624355046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 38c9e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/3.78 GiB heap, 0.0/1.89 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_38c9e_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         52.01  </td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   0.951</td><td style=\"text-align: right;\">                23.7</td><td style=\"text-align: right;\">               -21.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_38c9e_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.5  </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         51.8954</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -0.162</td><td style=\"text-align: right;\">                22.5</td><td style=\"text-align: right;\">               -28.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-22 11:44:07,702\tINFO tune.py:549 -- Total run time: 71.79 seconds (71.22 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f81ec46fa60>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plugging in Ray Tune.\n",
    "# Note that this is the recommended way to run any experiments with RLlib.\n",
    "# Reasons:\n",
    "# - Tune allows you to do hyperparameter tuning in a user-friendly way\n",
    "#   and at large scale!\n",
    "# - Tune automatically allocates needed resources for the different\n",
    "#   hyperparam trials and experiment runs on a cluster.\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Running stuff with tune, we can re-use the exact\n",
    "# same config that we used when working with RLlib directly!\n",
    "tune_config = config.copy()\n",
    "\n",
    "# Let's add our first hyperparameter search via our config.\n",
    "# How about we try two different learning rates? Let's say 0.00005 and 0.5 (ouch!).\n",
    "tune_config[\"lr\"] = tune.grid_search([0.00005, 0.5])  # <- 0.5? again: ouch!\n",
    "\n",
    "# Now that we will run things \"automatically\" through tune, we have to\n",
    "# define one or more stopping criteria.\n",
    "# Tune will stop the run, once any single one of the criteria is matched (not all of them!).\n",
    "stop = {\n",
    "    # Note that the keys used here can be anything present in the above `rllib_trainer.train()` output dict.\n",
    "    \"training_iteration\": 5,\n",
    "    \"episode_reward_mean\": 20.0,\n",
    "}\n",
    "\n",
    "# \"PPO\" is a registered name that points to RLlib's PPOTrainer.\n",
    "# See `ray/rllib/agents/registry.py`\n",
    "\n",
    "# Run a simple experiment until one of the stopping criteria is met.\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=tune_config,\n",
    "    stop=stop,\n",
    "\n",
    "    # Note that no trainers will be returned from this call here.\n",
    "    # Tune will create n Trainers internally, run them in parallel and destroy them at the end.\n",
    "    # However, you can ...\n",
    "    checkpoint_at_end=True,  # ... create a checkpoint when done.\n",
    "    checkpoint_freq=10,  # ... create a checkpoint every 10 training iterations.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b886fb8-6ccd-4be2-80bb-fc0936808d11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Why did we use 6 CPUs in the tune run above (3 CPUs per trial)?\n",
    "\n",
    "PPO - by default - uses 2 (\"rollout\") workers (`num_workers=2`). These are Ray Actors that have their own environment copy(ies) and step through those in parallel. On top of these two \"rollout\" workers, every Trainer in RLlib always also has a \"local\" worker, which - in case of PPO - handles the learning updates. This gives us 3 workers (2 rollout + 1 local learner), which require 3 CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a74ec7-a6c1-431d-83aa-35df56d93185",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise No 2\n",
    "\n",
    "<hr />\n",
    "\n",
    "Using the `tune_config` that we have built so far to run another `tune.run()`, but apply the following changes to our setup this time:\n",
    "- Setup only 1 learning rates under the \"lr\" config key: Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Setup 2 train batch sizes using `tune.grid_search([batch size 1, batch size 2])` under the \"train_batch_size\" config key. Use the values 3000 and 4000.\n",
    "- Set the `num_envs_per_worker` config parameter to 5. This will \"sequentialize\" our env, but parallelize action computing forward passes through our neural network.\n",
    "\n",
    "Other than that, use the exact same args as in our `tune.run()` call in the previous cell.\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff184330-4229-4476-a9e0-1fdbaed948d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/3.78 GiB heap, 0.0/1.89 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_b1a2b_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_b1a2b_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">              4000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=58399)\u001b[0m 2021-06-22 12:00:43,998\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=58399)\u001b[0m 2021-06-22 12:00:43,998\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=58399)\u001b[0m 2021-06-22 12:00:43,999\tWARNING ppo.py:135 -- `train_batch_size` (3000) cannot be achieved with your other settings (num_workers=2 num_envs_per_worker=10 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 150.\n",
      "\u001b[2m\u001b[36m(pid=58400)\u001b[0m 2021-06-22 12:00:43,998\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=58400)\u001b[0m 2021-06-22 12:00:43,998\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=58399)\u001b[0m 2021-06-22 12:00:57,405\tINFO trainable.py:101 -- Trainable.setup took 13.408 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=58399)\u001b[0m 2021-06-22 12:00:57,405\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=58400)\u001b[0m 2021-06-22 12:00:57,465\tINFO trainable.py:101 -- Trainable.setup took 13.468 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=58400)\u001b[0m 2021-06-22 12:00:57,465\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_b1a2b_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-22_12-01-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.500000000000028\n",
      "  episode_reward_mean: -9.194999999999995\n",
      "  episode_reward_min: -30.000000000000018\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: b67292c4160f4be099f3182821b66779\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.346530556678772\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.041253022849559784\n",
      "          model: {}\n",
      "          policy_loss: -0.08638487756252289\n",
      "          total_loss: 36.72517013549805\n",
      "          vf_explained_var: 0.1265425831079483\n",
      "          vf_loss: 36.80330276489258\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.369309425354004\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017152342945337296\n",
      "          model: {}\n",
      "          policy_loss: -0.05576443672180176\n",
      "          total_loss: 2.1652491092681885\n",
      "          vf_explained_var: 0.2670084238052368\n",
      "          vf_loss: 2.217583417892456\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.59090909090909\n",
      "    ram_util_percent: 63.09090909090908\n",
      "  pid: 58399\n",
      "  policy_reward_max:\n",
      "    policy1: 20.5\n",
      "    policy2: -6.6999999999999815\n",
      "  policy_reward_mean:\n",
      "    policy1: 0.2\n",
      "    policy2: -9.39499999999998\n",
      "  policy_reward_min:\n",
      "    policy1: -20.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3373362370674184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19932819518032457\n",
      "    mean_inference_ms: 3.347127642852582\n",
      "    mean_raw_obs_processing_ms: 1.5080867224181722\n",
      "  time_since_restore: 7.522234916687012\n",
      "  time_this_iter_s: 7.522234916687012\n",
      "  time_total_s: 7.522234916687012\n",
      "  timers:\n",
      "    learn_throughput: 464.748\n",
      "    learn_time_ms: 6455.11\n",
      "    load_throughput: 19636.714\n",
      "    load_time_ms: 152.775\n",
      "    sample_throughput: 3574.058\n",
      "    sample_time_ms: 839.382\n",
      "    update_time_ms: 9.224\n",
      "  timestamp: 1624356064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 1\n",
      "  trial_id: b1a2b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/3.78 GiB heap, 0.0/1.89 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_b1a2b_00000</td><td>RUNNING </td><td>192.168.0.179:58399</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.52223</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">  -9.195</td><td style=\"text-align: right;\">                10.5</td><td style=\"text-align: right;\">                 -30</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_b1a2b_00001</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_b1a2b_00001:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-22_12-01-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.20000000000001\n",
      "  episode_reward_mean: -7.394999999999999\n",
      "  episode_reward_min: -36.90000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 9d3a10e163f347328c1aa983ffea5cb5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.349806547164917\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03762923553586006\n",
      "          model: {}\n",
      "          policy_loss: -0.07011692225933075\n",
      "          total_loss: 42.256065368652344\n",
      "          vf_explained_var: 0.11579272150993347\n",
      "          vf_loss: 42.31865310668945\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3644301891326904\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022044247016310692\n",
      "          model: {}\n",
      "          policy_loss: -0.052763018757104874\n",
      "          total_loss: 2.17820405960083\n",
      "          vf_explained_var: 0.4103226959705353\n",
      "          vf_loss: 2.226557731628418\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.95333333333333\n",
      "    ram_util_percent: 63.126666666666665\n",
      "  pid: 58400\n",
      "  policy_reward_max:\n",
      "    policy1: 24.0\n",
      "    policy2: -2.2999999999999914\n",
      "  policy_reward_mean:\n",
      "    policy1: 1.175\n",
      "    policy2: -8.569999999999983\n",
      "  policy_reward_min:\n",
      "    policy1: -28.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.35205468609558405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21055622480401945\n",
      "    mean_inference_ms: 3.1642302944885556\n",
      "    mean_raw_obs_processing_ms: 1.4333250510751903\n",
      "  time_since_restore: 10.02516508102417\n",
      "  time_this_iter_s: 10.02516508102417\n",
      "  time_total_s: 10.02516508102417\n",
      "  timers:\n",
      "    learn_throughput: 460.64\n",
      "    learn_time_ms: 8683.569\n",
      "    load_throughput: 30154.782\n",
      "    load_time_ms: 132.649\n",
      "    sample_throughput: 3762.773\n",
      "    sample_time_ms: 1063.046\n",
      "    update_time_ms: 9.294\n",
      "  timestamp: 1624356067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: b1a2b_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_b1a2b_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-22_12-01-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.500000000000028\n",
      "  episode_reward_mean: -7.689999999999994\n",
      "  episode_reward_min: -30.000000000000018\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 60\n",
      "  experiment_id: b67292c4160f4be099f3182821b66779\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      policy1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00019999999494757503\n",
      "          entropy: 1.299340844154358\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03937702998518944\n",
      "          model: {}\n",
      "          policy_loss: -0.0832560658454895\n",
      "          total_loss: 25.453144073486328\n",
      "          vf_explained_var: 0.24721239507198334\n",
      "          vf_loss: 25.524585723876953\n",
      "      policy2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3381983041763306\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020829828456044197\n",
      "          model: {}\n",
      "          policy_loss: -0.06347181648015976\n",
      "          total_loss: 2.2024729251861572\n",
      "          vf_explained_var: 0.35691967606544495\n",
      "          vf_loss: 2.2617790699005127\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.179\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.34\n",
      "    ram_util_percent: 63.33\n",
      "  pid: 58399\n",
      "  policy_reward_max:\n",
      "    policy1: 20.5\n",
      "    policy2: -1.2000000000000046\n",
      "  policy_reward_mean:\n",
      "    policy1: 1.1916666666666667\n",
      "    policy2: -8.88166666666665\n",
      "  policy_reward_min:\n",
      "    policy1: -20.0\n",
      "    policy2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3879041527253643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2266766507373808\n",
      "    mean_inference_ms: 3.2538628734330586\n",
      "    mean_raw_obs_processing_ms: 1.6323531653075298\n",
      "  time_since_restore: 14.44275164604187\n",
      "  time_this_iter_s: 6.920516729354858\n",
      "  time_total_s: 14.44275164604187\n",
      "  timers:\n",
      "    learn_throughput: 483.076\n",
      "    learn_time_ms: 6210.203\n",
      "    load_throughput: 38152.581\n",
      "    load_time_ms: 78.632\n",
      "    sample_throughput: 3502.015\n",
      "    sample_time_ms: 856.65\n",
      "    update_time_ms: 8.461\n",
      "  timestamp: 1624356071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 2\n",
      "  trial_id: b1a2b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/3.78 GiB heap, 0.0/1.89 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_b1a2b_00000</td><td>RUNNING </td><td>192.168.0.179:58399</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         14.4428</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">  -7.69 </td><td style=\"text-align: right;\">                10.5</td><td style=\"text-align: right;\">               -30  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_b1a2b_00001</td><td>RUNNING </td><td>192.168.0.179:58400</td><td style=\"text-align: right;\">              4000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.0252</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  -7.395</td><td style=\"text-align: right;\">                16.2</td><td style=\"text-align: right;\">               -36.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "\n",
    "# Solution to Exercise #2\n",
    "\n",
    "# Run for longer this time (not just 2 iterations) and try to reach 40.0 reward (sum of both agents).\n",
    "stop = {\n",
    "    \"training_iteration\": 100,\n",
    "    \"episode_reward_mean\": 40.0,\n",
    "}\n",
    "\n",
    "# tune_config.update({\n",
    "# ???\n",
    "# })\n",
    "\n",
    "# analysis = tune.run(...)\n",
    "\n",
    "tune_config[\"lr\"] = 0.00005\n",
    "tune_config[\"train_batch_size\"] = tune.grid_search([3000, 4000])\n",
    "tune_config[\"num_envs_per_worker\"] = 10\n",
    "tune_config[\"num_sgd_iter\"] = 20\n",
    "tune_config[\"model\"] = {\"fcnet_hiddens\": [512, 512]}\n",
    "\n",
    "analysis = tune.run(\"PPO\", config=tune_config, stop=stop, checkpoint_at_end=True, checkpoint_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d282a-4ad1-4d5f-9dec-00afb8154048",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 15 min break :)\n",
    "------------------\n",
    "\n",
    "\n",
    "(while the above experiment is running (and hopefully learning))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82057-6b4c-4075-bd32-93c3426a1700",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How do we extract any checkpoint from some trial of a tune.run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a9fbdc-fc83-4fd6-80ff-11b987dac16d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The previous tune.run (the one we did before the exercise) returned an Analysis object, from which we can access any checkpoint\n",
    "# (given we set checkpoint_freq or checkpoint_at_end to reasonable values) like so:\n",
    "print(analysis_multi_agent_run)\n",
    "# Get all trials.\n",
    "trials = analysis_multi_agent_run.trials\n",
    "# Assuming, the first trial was the best, we'd like to extract this trial's best checkpoint \"\":\n",
    "best_checkpoint = analysis_multi_agent_run.get_best_checkpoint(trial=trials[1], mode=\"max\")\n",
    "print(f\"Found best checkpoint for trial #2: {best_checkpoint}\")\n",
    "\n",
    "# Undo the grid-search config, which RLlib doesn't understand.\n",
    "rllib_config = tune_config.copy()\n",
    "rllib_config[\"lr\"] = 0.00005\n",
    "rllib_config[\"train_batch_size\"] = 3000\n",
    "\n",
    "# Restore a RLlib Trainer from the checkpoint.\n",
    "new_trainer = PPOTrainer(config=rllib_config)\n",
    "new_trainer.restore(best_checkpoint)\n",
    "# Evaluate to see, how it's doing.\n",
    "print(new_trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794fd95b-06b2-4bec-866b-52b06a98db61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<video width=\"400\" height=\"300\" controls>\n",
    "  <source src=\"videos/learnt_2_policies_to_30_reward.mov\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3f56e-c4e1-4503-9ce5-589e826d1e5a",
   "metadata": {},
   "source": [
    "## Let's talk about customization options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f940b-697c-4d1d-af28-1d174331dc3c",
   "metadata": {},
   "source": [
    "### Deep Dive: How do we customize RLlib's RL loop?\n",
    "\n",
    "RLlib offers a callbacks API that allows you to add custom behavior to\n",
    "all major events during the environment sampling- and learning process.\n",
    "\n",
    "**Our problem:** So far, we can only see the total reward (sum for both agents).\n",
    "This does not give us enough insights into the question of which agent\n",
    "learns what (maybe agent2 doesn't learn anything and the reward we are observing\n",
    "is mostly due to agent1's progress in covering the map!).\n",
    "\n",
    "In the following cell, we will create some custom callback \"hooks\" that will allow us to\n",
    "add each agents single reward to the returned metrics (which will then be displayed in tensorboard!).\n",
    "\n",
    "For that we will override RLlib's DefaultCallbacks class and implement the\n",
    "`on_episode_start`, `on_episode_step`, and `on_episode_end` methods therein:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb9733-01bc-426b-ad57-7983fc7db8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the DefaultCallbacks with your own and implement any methods (hooks)\n",
    "# that you need.\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(self,\n",
    "                         *,\n",
    "                         worker,\n",
    "                         base_env,\n",
    "                         policies,\n",
    "                         episode: MultiAgentEpisode,\n",
    "                         env_index,\n",
    "                         **kwargs):\n",
    "        # We will use the `MultiAgentEpisode` object being passed into\n",
    "        # all episode-related callbacks. It comes with a user_data property (dict),\n",
    "        # which we can write arbitrary data into.\n",
    "\n",
    "        # At the end of an episode, we'll transfer that data into the `hist_data`, and `custom_metrics`\n",
    "        # properties to make sure our custom data is displayed in TensorBoard.\n",
    "\n",
    "        # The episode is starting:\n",
    "        # Set per-episode object to capture, which states (observations)\n",
    "        # have been visited by agent1.\n",
    "        episode.user_data[\"new_fields_discovered\"] = 0\n",
    "        # Set per-episode agent2-blocks counter (how many times has agent2 blocked agent1?).\n",
    "        episode.user_data[\"num_collisions\"] = 0\n",
    "\n",
    "    def on_episode_step(self,\n",
    "                        *,\n",
    "                        worker,\n",
    "                        base_env,\n",
    "                        episode: MultiAgentEpisode,\n",
    "                        env_index,\n",
    "                        **kwargs):\n",
    "        # Get both rewards.\n",
    "        ag1_r = episode.prev_reward_for(\"agent1\")\n",
    "        ag2_r = episode.prev_reward_for(\"agent2\")\n",
    "\n",
    "        # Agent1 discovered a new field.\n",
    "        if ag1_r == 1.0:\n",
    "            episode.user_data[\"new_fields_discovered\"] += 1\n",
    "        # Collision.\n",
    "        elif ag2_r == 1.0:\n",
    "            episode.user_data[\"num_collisions\"] += 1\n",
    "\n",
    "    def on_episode_end(self,\n",
    "                       *,\n",
    "                       worker,\n",
    "                       base_env,\n",
    "                       policies,\n",
    "                       episode: MultiAgentEpisode,\n",
    "                       env_index,\n",
    "                       **kwargs):\n",
    "        # Episode is done:\n",
    "        # Write scalar values (sum over rewards) to `custom_metrics` and\n",
    "        # time-series data (rewards per time step) to `hist_data`.\n",
    "        # Both will be visible then in TensorBoard.\n",
    "        episode.custom_metrics[\"new_fields_discovered\"] = episode.user_data[\"new_fields_discovered\"]\n",
    "        episode.custom_metrics[\"num_collisions\"] = episode.user_data[\"num_collisions\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2fe8eb-c52f-4a26-9067-96ad9fe160a4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up our config to point to our new custom callbacks class:\n",
    "config.update({\n",
    "    \"callbacks\": MyCallbacks,  # by default, this would point to `rllib.agents.callbacks.DefaultCallbacks`, which does nothing.\n",
    "})\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,\n",
    "    stop={\"training_iteration\": 20},\n",
    "    checkpoint_at_end=True,\n",
    "    # If you'd like to restore the tune run from an existing checkpoint file, you can do the following:\n",
    "    #restore=\"/Users/sven/ray_results/PPO/PPO_MultiAgentArena_fd451_00000_0_2021-05-25_15-13-26/checkpoint_000010/checkpoint-10\",\n",
    ")\n",
    "print(analysis.get_last_checkpoint())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa6a24",
   "metadata": {},
   "source": [
    "### Let's check tensorboard for the new custom metrics!\n",
    "\n",
    "1. Head over to ~/ray_results/PPO/PPO_MultiAgentArena_[some key]_00000_0_[date]_[time]/\n",
    "1. In that directory, you should see a `event.out....` file.\n",
    "1. Run `tensorboard --logdir .` and head to https://localhost:6006\n",
    "\n",
    "<img src=\"images/tensorboard.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac90dc-097d-4f10-b5ea-c4c1167f1f3a",
   "metadata": {},
   "source": [
    "### Deep Dive: Providing your custom Models in tf or torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Neural Network Models.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                input_space,\n",
    "                action_space,\n",
    "                num_outputs,\n",
    "                name=\"\",\n",
    "                *,\n",
    "                layers = (256, 256)):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense_layers = []\n",
    "        for i, layer_size in enumerate(layers):\n",
    "            self.dense_layers.append(tf.keras.layers.Dense(\n",
    "                layer_size, activation=tf.nn.relu, name=f\"dense_{i}\"))\n",
    "\n",
    "        self.logits = tf.keras.layers.Dense(\n",
    "            num_outputs,\n",
    "            activation=tf.keras.activations.linear,\n",
    "            name=\"logits\")\n",
    "        self.values = tf.keras.layers.Dense(\n",
    "            1, activation=None, name=\"values\")\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # Standardized input args:\n",
    "        # - input_dict (RLlib `SampleBatch` object, which is basically a dict with numpy arrays\n",
    "        # in it)\n",
    "        out = inputs[\"obs\"]\n",
    "        for l in self.dense_layers:\n",
    "            out = l(out)\n",
    "        logits = self.logits(out)\n",
    "        values = self.values(out)\n",
    "\n",
    "        # Standardized output:\n",
    "        # - \"normal\" model output tensor (e.g. action logits).\n",
    "        # - list of internal state outputs (only needed for RNN-/memory enhanced models).\n",
    "        # - \"extra outs\", such as model's side branches, e.g. value function outputs.\n",
    "        return logits, [], {\"vf_preds\": tf.reshape(values, [-1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a quick test on the custom model class.\n",
    "from gym.spaces import Box\n",
    "test_model = MyModel(\n",
    "    input_space=Box(-1.0, 1.0, (2, )),\n",
    "    action_space=None,\n",
    "    num_outputs=2,\n",
    ")\n",
    "test_model({\"obs\": np.array([[0.5, 0.5]])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our custom model and re-run the experiment.\n",
    "config.update({\n",
    "    \"model\": {\n",
    "        \"custom_model\": MyModel,\n",
    "        \"custom_model_config\": {\n",
    "            \"layers\": [128, 128],\n",
    "        },\n",
    "    },\n",
    "    # Revert these to single trials (and use those hyperparams that performed well in our Exercise #2).\n",
    "    \"lr\": 0.0005,\n",
    "    \"train_batch_size\": 2000,\n",
    "})\n",
    "\n",
    "tune.run(\"PPO\", config=config, stop=stop)\n",
    "\n",
    "TODO: Introduce custom Model earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85ad95",
   "metadata": {},
   "source": [
    "## A closer look at RLlib's APIs and Components\n",
    "### (Depending on time left and amount of questions having been accumulated :)\n",
    "\n",
    "We already took a quick look inside an RLlib Trainer object and extracted its Policy(ies) and the Policy's model (neural network). Here is a much more detailed overview of what's inside a Trainer object.\n",
    "\n",
    "At the core is the so-called `WorkerSet` sitting under `Trainer.workers`. A WorkerSet is a group of `RolloutWorker` (`rllib.evaluation.rollout_worker.py`) objects that always consists of a \"local worker\" (`Trainer.workers.local_worker()`) and n \"remote workers\" (`Trainer.workers.remote_workers()`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f37549",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_structure.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d72883",
   "metadata": {},
   "source": [
    "### Scaling RLlib\n",
    "\n",
    "Scaling RLlib works by parallelizing the \"jobs\" that the remote `RolloutWorkers` do. In a vanilla RL algorithm, like PPO, DQN, and many others, the `@ray.remote` labeled RolloutWorkers in the figure above are responsible for interacting with one or more environments and thereby collecting experiences. Observations are produced by the environment, actions are then computed by the Policy(ies) copy located on the remote worker and sent to the environment in order to produce yet another observation. This cycle is repeated endlessly and only sometimes interrupted to send experience batches (\"train batches\") of a certain size to the \"local worker\". There these batches are used to call `Policy.learn_on_batch()`, which performs a loss calculation, followed by a model weights update, and a subsequent weights broadcast back to all the remote workers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {},
   "source": [
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials.git\">github repo of this tutorial</a>.\n",
    "- <a href=\"https://docs.ray.io/en/master/rllib.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on RLlib.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n",
    "\n",
    "## Time for Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13abd3-3ac0-490d-8040-63110e26677a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
