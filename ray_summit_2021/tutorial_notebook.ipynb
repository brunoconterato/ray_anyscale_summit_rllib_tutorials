{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "# Hands-on RL with Ray’s RLlib\n",
    "## A beginner’s tutorial for working with multi-agent environments, models, and algorithms\n",
    "\n",
    "### Overview\n",
    "“Hands-on RL with Ray’s RLlib” is a beginners tutorial for working with reinforcement learning (RL) environments, models, and algorithms using Ray’s RLlib library. RLlib offers high scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL) before proceeding to RLlib (multi- and single-agent) environments, neural network models, hyperparameter tuning, debugging, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo.\n",
    "\n",
    "### Intended Audience\n",
    "* Python programmers who want to get started with reinforcement learning and RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac and Linux):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install ray[rllib]\n",
    "$ pip install [tensorflow|torch]  # <- either one works!\n",
    "$ pip install jupyter-labs\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install ray[rllib]\n",
    "$ pip install [tensorflow|torch]  # <- either one works!\n",
    "$ pip install jupyter-labs\n",
    "$ conda install pywin32\n",
    "```\n",
    "\n",
    "Also, for Win10 Atari support, we have to install atari_py from a different source (gym does not support Atari envs on Windows).\n",
    "\n",
    "```\n",
    "$ pip install git+https://github.com/Kojoley/atari-py.git\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and why RLlib?\n",
    "* Core concepts of RLlib: Environments, Trainers, Policies, and Models.\n",
    "* How to configure, hyperparameter-tune, and parallelize RLlib.\n",
    "* RLlib debugging best practices.\n",
    "\n",
    "### Tutorial Outline\n",
    "1. RL and RLlib in a nutshell.\n",
    "1. Defining an RL-solvable problem: Our first environment.\n",
    "1. Exercise No.1 (env loop)\n",
    "1. Picking an algorithm and training our first RLlib Trainer.\n",
    "1. Configurations and hyperparameters - Easy tuning with Ray Tune.\n",
    "1. Fixing our experiment's config - Going multi-agent.\n",
    "1. The \"infinite laptop\": Quick intro into how to use RLlib with Anyscale's product.\n",
    "1. Exercise No.2 (run your own Ray RLlib+Tune experiment)\n",
    "1. Neural network models - Provide your custom models using tf.keras or torch.nn.\n",
    "1. Deeper dive into RLlib's parallelization architecture.\n",
    "1. Specifying different compute resources and parallelization options through our config.\n",
    "1. \"Hacking in\": Using callbacks to customize the RL loop and generate our own metrics.\n",
    "1. Exercise No.3 (write your own custom callback)\n",
    "1. \"Hacking in (part II)\" - Debugging with RLlib and PyCharm.\n",
    "1. Checking on the \"infinite laptop\" - Did RLlib learn to solve the problem?\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fuzzy-career",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 15:30:53,640\tINFO services.py:1267 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.100',\n",
       " 'raylet_ip_address': '192.168.0.100',\n",
       " 'redis_address': '192.168.0.100:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-04-29_15-30-52_145625_27624/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-04-29_15-30-52_145625_27624/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-04-29_15-30-52_145625_27624',\n",
       " 'metrics_export_port': 63009,\n",
       " 'node_id': 'f9c56d30f3030d5011db17b9f583f5477ac374d682c8820c0477a48e'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "\n",
    "# Start a new instance of Ray or connect to an already running one.\n",
    "ray.init()\n",
    "# In case you encounter this error during our tutorial:\n",
    "# RuntimeError: Maybe you called ray.init twice by accident?\n",
    "# Try: ray.shutdown() or ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "proud-yorkshire",
   "metadata": {},
   "source": [
    "<img src=\"images/rl-cycle.png\" width=1200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d65cdf6-16b2-4041-b65d-42c941cbc06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2) Coding/defining our \"problem\" via an RL environment.\n",
    "# We will use the following (adversarial) multi-agent environment\n",
    "# throughout this tutorial to demonstrate a large fraction of RLlib's\n",
    "# APIs, features, and customization options."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1eb35116-efda-4799-8bae-e96d7775a0d1",
   "metadata": {},
   "source": [
    "<img src=\"images/environment.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe753-d7e0-4de1-b937-160507f75ed8",
   "metadata": {},
   "source": [
    "### A word or two on Spaces:\n",
    "\n",
    "Spaces are used in ML to describe what possible/valid values inputs and outputs of a neural network can have.\n",
    "\n",
    "RL environments also use them to describe what their valid observations and actions are.\n",
    "\n",
    "Spaces are usually defined by their shape (e.g. 84x84x3 RGB images) and datatype (e.g. uint8 for RGB values between 0 and 255).\n",
    "However, spaces could also be composed of other spaces (see Tuple or Dict spaces) or could be simply discrete with n fixed possible values\n",
    "(represented by integers). For example, in our game, where each agent can only go up/down/left/right, the action space would be \"Discrete(4)\"\n",
    "(no datatype, no shape needs to be defined here)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "023e4135-98ed-4e65-9e26-66f340747529",
   "metadata": {},
   "source": [
    "<img src=\"images/spaces.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "871a3661-2d74-4a50-b4ef-a89c27d978f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "import random\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "class MultiAgentArena(MultiAgentEnv):\n",
    "    def __init__(self, config=None):\n",
    "        # !LIVE CODING!\n",
    "        config = config or {}\n",
    "        self.width = config.get(\"width\", 10)\n",
    "        self.height = config.get(\"height\", 10)\n",
    "\n",
    "        # 0=up, 1=right, 2=down, 3=left.\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = MultiDiscrete([self.width * self.height,\n",
    "                                                self.width * self.height])\n",
    "        # End an episode after this many timesteps.\n",
    "        self.timestep_limit = config.get(\"ts\", 100)\n",
    "        # Reset env.\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # !LIVE CODING!\n",
    "        # Row-major coords.\n",
    "        self.agent1_pos = [0, 0]\n",
    "        self.agent2_pos = [self.height - 1, self.width - 1]\n",
    "        # Reset agent1's visited states.\n",
    "        self.agent1_visited_states = set()\n",
    "        # How many timesteps have we done in this episode.\n",
    "        self.timesteps = 0\n",
    "\n",
    "        return self.get_obs()\n",
    "\n",
    "    def step(self, action: dict):\n",
    "        # !LIVE CODING!\n",
    "        self.timesteps += 1\n",
    "        # Determine, who is allowed to move first.\n",
    "        agent1_first = random.random() > 0.5\n",
    "        # Move first agent (could be agent 1 or 2).\n",
    "        if agent1_first:\n",
    "            r1, r2 = self.move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "            add = self.move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        else:\n",
    "            r1, r2 = self.move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "            add = self.move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "        r1 += add[0]\n",
    "        r2 += add[1]\n",
    "\n",
    "        obs = self.get_obs()\n",
    "\n",
    "        reward = {\"agent1\": r1, \"agent2\": r2}\n",
    "\n",
    "        done = self.timesteps >= self.timestep_limit\n",
    "        done = {\"agent1\": done, \"agent2\": done, \"__all__\": done}\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def get_obs(self):\n",
    "        ag1_discrete_pos = self.agent1_pos[0] * self.width + \\\n",
    "            (self.agent1_pos[1] % self.width)\n",
    "        ag2_discrete_pos = self.agent2_pos[0] * self.width + \\\n",
    "            (self.agent2_pos[1] % self.width)\n",
    "        return {\n",
    "            \"agent1\": np.array([ag1_discrete_pos, ag2_discrete_pos]),\n",
    "            \"agent2\": np.array([ag2_discrete_pos, ag1_discrete_pos]),\n",
    "        }\n",
    "\n",
    "    def move(self, coords, action, is_agent1):\n",
    "        orig_coords = coords[:]\n",
    "        # Change the row: 0=up (-1), 2=down (+1)\n",
    "        coords[0] += -1 if action == 0 else 1 if action == 2 else 0\n",
    "        # Change the column: 1=right (+1), 3=left (-1)\n",
    "        coords[1] += 1 if action == 1 else -1 if action == 3 else 0\n",
    "\n",
    "        # Solve collisions.\n",
    "        # Make sure, we don't end up on the other agent's position.\n",
    "        # If yes, don't move (we are blocked).\n",
    "        if (is_agent1 and coords == self.agent2_pos) or (not is_agent1 and coords == self.agent1_pos):\n",
    "            coords[0], coords[1] = orig_coords\n",
    "            # Agent2 blocked agent1 (agent1 tried to run into agent2)\n",
    "            # OR Agent2 bumped into agent1 (agent2 tried to run into agent1)\n",
    "            # -> +1 for agent2; -1 for agent1\n",
    "            return -1.0, 1.0\n",
    "\n",
    "        # No agent blocking -> check walls.\n",
    "        if coords[0] < 0:\n",
    "            coords[0] = 0\n",
    "        elif coords[0] >= self.height:\n",
    "            coords[0] = self.height - 1\n",
    "        if coords[1] < 0:\n",
    "            coords[1] = 0\n",
    "        elif coords[1] >= self.width:\n",
    "            coords[1] = self.width - 1\n",
    "\n",
    "        # If agent1 -> +1.0 if new tile covered.\n",
    "        if is_agent1 and not tuple(coords) in self.agent1_visited_states:\n",
    "            self.agent1_visited_states.add(tuple(coords))\n",
    "            return 1.0, -0.1\n",
    "        # No new tile for agent1 -> Negative reward.\n",
    "        return -0.5, -0.1\n",
    "\n",
    "    # Optionally: Add `render` method returning some img.\n",
    "    def render(self, mode=None):\n",
    "        return np.random.randint(0, 256, (20, 20, 3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-sussex",
   "metadata": {},
   "source": [
    "## Exercise No 1\n",
    "\n",
    "Write an \"environment loop\" using our `MultiAgentArena` class.\n",
    "\n",
    "1. Create an env object.\n",
    "1. `reset` your environment to get the first (initial) observation.\n",
    "1. `step` through the environment using a provided\n",
    "   \"DummyTrainer.compute_action([obs])\" method to compute action dicts (see cell below, in which you can create a DummyTrainer object and query it for random actions).\n",
    "1. When an episode is done, remember to `reset()` your environment before the next call to `step()`.\n",
    "1. If you feel, this is way too easy for you ;) , try to extract each agent's reward, sum it up over one episode and - at the end of an episode (when done=True) - print out each agent's accumulated reward (also called \"return\").\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent1': 3, 'agent2': 1}\n",
      "{'agent1': 1, 'agent2': 2}\n",
      "{'agent1': 1, 'agent2': 0}\n"
     ]
    }
   ],
   "source": [
    "class DummyTrainer:\n",
    "    \"\"\"Dummy Trainer class used in Exercise #1.\n",
    "\n",
    "    Use its `compute_action` method to get a new action, given some environment\n",
    "    observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_action(self, obs):\n",
    "        # Returns a random action.\n",
    "        return {\n",
    "            \"agent1\": np.random.randint(4),\n",
    "            \"agent2\": np.random.randint(4)\n",
    "        }\n",
    "\n",
    "dummy_trainer = DummyTrainer()\n",
    "# Check, whether it's working.\n",
    "for _ in range(3):\n",
    "    print(dummy_trainer.compute_action({\"agent1\": np.array([0, 10]), \"agent2\": np.array([10, 0])}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "liable-district",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done. R1=-60.0 R2=-18.899999999999963\n",
      "Episode done. R1=-48.5 R2=-17.79999999999996\n",
      "Episode done. R1=-50.5 R2=-16.699999999999964\n",
      "Episode done. R1=-68.5 R2=-19.99999999999996\n",
      "Episode done. R1=-80.0 R2=-11.19999999999997\n",
      "Episode done. R1=-45.5 R2=-14.499999999999972\n",
      "Episode done. R1=-53.5 R2=-19.99999999999996\n",
      "Episode done. R1=-59.5 R2=-19.99999999999996\n",
      "Episode done. R1=-51.0 R2=-15.599999999999962\n",
      "Episode done. R1=-46.0 R2=-19.99999999999996\n"
     ]
    }
   ],
   "source": [
    "# Solution to Exercise #1:\n",
    "#from gym.envs.classic_control.rendering import SimpleImageViewer\n",
    "#simple_image_viewer = SimpleImageViewer()\n",
    "\n",
    "# Solution:\n",
    "env = MultiAgentArena(config={\"width\": 10, \"height\": 10})\n",
    "obs = env.reset()\n",
    "# Play through a single episode.\n",
    "done = {\"__all__\": False}\n",
    "return_ag1 = return_ag2 = 0.0\n",
    "num_episodes = 0\n",
    "while num_episodes < 10:\n",
    "    action = dummy_trainer.compute_action(obs)\n",
    "    obs, rewards, done, _ = env.step(action)\n",
    "    return_ag1 += rewards[\"agent1\"]\n",
    "    return_ag2 += rewards[\"agent2\"]    \n",
    "    if done[\"__all__\"]:\n",
    "        print(f\"Episode done. R1={return_ag1} R2={return_ag2}\")\n",
    "        num_episodes += 1\n",
    "        return_ag1 = return_ag2 = 0.0\n",
    "        obs = env.reset()\n",
    "    # Optional:\n",
    "    #img = env.render()\n",
    "    #simple_image_viewer.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gentle-reliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 15:31:12,852\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-04-29 15:31:12,853\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27764)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27764)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27764)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27764)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27764)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27764)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27765)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27765)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27765)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27765)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27765)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27765)\u001b[0m non-resource variables are not supported in the long term\n",
      "2021-04-29 15:31:18,138\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# 4) Plugging in RLlib.\n",
    "\n",
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "# We use the PPO algorithm here b/c its very flexible wrt its supported\n",
    "# action spaces and model types and b/c it learns well almost any problem.\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "# Specify a very simple config, defining our environment and some environment\n",
    "# options (see environment.py).\n",
    "config = {\n",
    "    \"env\": MultiAgentArena,\n",
    "    \"env_config\": {\n",
    "        \"config\": {\n",
    "            \"width\": 10,\n",
    "            \"height\": 10,\n",
    "        },\n",
    "    },\n",
    "    # \"framework\": \"torch\",\n",
    "    \"create_env_on_driver\": True,\n",
    "}\n",
    "# Instantiate the Trainer object using above config.\n",
    "rllib_trainer = PPOTrainer(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spectacular-guard",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 15:31:19,175\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'episode_reward_max': -35.999999999999986, 'episode_reward_min': -88.50000000000016, 'episode_reward_mean': -65.5800000000001, 'episode_len_mean': 100.0, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-52.50000000000007, -60.00000000000004, -76.80000000000015, -78.60000000000014, -70.50000000000016, -52.50000000000004, -84.00000000000011, -58.50000000000006, -76.50000000000014, -35.999999999999986, -63.00000000000006, -72.00000000000011, -64.50000000000014, -70.50000000000011, -70.20000000000007, -78.00000000000011, -47.70000000000003, -88.50000000000016, -55.8000000000001, -55.50000000000007], 'episode_lengths': [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.1742773122720785, 'mean_inference_ms': 0.6968317689238253, 'mean_action_processing_ms': 0.06828584394731245, 'mean_env_wait_ms': 0.0373147703431822, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_healthy_workers': 2, 'timesteps_total': 4000, 'agent_timesteps_total': 4000, 'timers': {'sample_time_ms': 1031.841, 'sample_throughput': 3876.566, 'learn_time_ms': 1890.792, 'learn_throughput': 2115.515, 'update_time_ms': 1.593}, 'info': {'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': 115.52576, 'policy_loss': -0.06079081, 'vf_loss': 115.58223, 'vf_explained_var': 0.051240265, 'kl': 0.021561788, 'entropy': 1.364907, 'entropy_coeff': 0.0, 'model': {}}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000}, 'done': False, 'episodes_total': 20, 'training_iteration': 1, 'experiment_id': '5fae94dd07be48ebbb7e098a744eb115', 'date': '2021-04-29_15-31-21', 'timestamp': 1619703081, 'time_this_iter_s': 2.9621880054473877, 'time_total_s': 2.9621880054473877, 'pid': 27624, 'hostname': 'Svens-MacBook-Pro.local', 'node_ip': '192.168.0.100', 'config': {'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': True, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'train_batch_size': 4000, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'env_config': {'config': {'width': 10, 'height': 10}}, 'render_env': False, 'record_env': False, 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 5e-05, 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'simple_optimizer': True, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'time_since_restore': 2.9621880054473877, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': 25.7, 'ram_util_percent': 62.239999999999995}}\n"
     ]
    }
   ],
   "source": [
    "# That's it, we are ready to train.\n",
    "# Calling `train` once runs a single \"training iteration\". One iteration\n",
    "# for most algos contains a) sampling from the environment(s) + b) using the\n",
    "# sampled data (observations, actions taken, rewards) to update the policy\n",
    "# model (neural network), such that it would pick better actions in the future,\n",
    "# leading to higher rewards.\n",
    "print(rllib_trainer.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ed7b0ca-6981-44ae-9656-18566ed14a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: R=-56.406000000000056\n",
      "iteration 1: R=-56.043000000000056\n",
      "iteration 2: R=-54.92700000000004\n",
      "iteration 3: R=-53.10300000000006\n",
      "iteration 4: R=-52.950000000000045\n",
      "iteration 5: R=-51.99300000000006\n",
      "iteration 6: R=-51.91500000000005\n",
      "iteration 7: R=-52.30500000000006\n",
      "iteration 8: R=-53.940000000000055\n",
      "iteration 9: R=-53.89200000000005\n"
     ]
    }
   ],
   "source": [
    "# Run `train()` n times. Try to repeatedly call this to see rewards increase.\n",
    "# Move on once you see episode rewards > -55.0.\n",
    "for i in range(10):\n",
    "    results = rllib_trainer.train()\n",
    "    print(f\"iteration {i}: R={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa7e0d09-e4fa-4657-b602-aa9d6750a33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7ffc05b10670>; Observation-space: Box(-1.0, 1.0, (200,), float32); Action-space: Discrete(4)\n",
      "Model: <ray.rllib.models.tf.fcnet.FullyConnectedNetwork object at 0x7ffc05b106d0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "# Let's actually \"look inside\" our Trainer to see what's in there.\n",
    "pol = rllib_trainer.get_policy()\n",
    "print(f\"Policy: {pol}; Observation-space: {pol.observation_space}; Action-space: {pol.action_space}\")\n",
    "\n",
    "print(f\"Model: {pol.model}\")\n",
    "\n",
    "# Create a fake numpy B=1 (single) observation consisting of both agents positions (\"one-hot'd\" and \"concat'd\").\n",
    "from ray.rllib.utils.numpy import one_hot\n",
    "single_obs = np.concatenate([one_hot(0, depth=100), one_hot(99, depth=100)])\n",
    "single_obs = np.array([single_obs])\n",
    "#single_obs.shape\n",
    "\n",
    "# Generate the Model's output.\n",
    "out, state_out = pol.model({\"obs\": single_obs})\n",
    "\n",
    "# tf1.x (static graph) -> Need to run this through a tf session.\n",
    "numpy_out = pol._sess.run(out)\n",
    "\n",
    "# RLlib then passes the model's output to the policy's \"action distribution\" to sample an action.\n",
    "action_dist = pol.dist_class(out)\n",
    "action = action_dist.sample()\n",
    "\n",
    "# Show us the actual action.\n",
    "pol._sess.run(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "saved-equilibrium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer was saved in '/Users/sven/ray_results/PPO_MultiAgentArena_2021-04-29_15-31-12gg_j3w3g/checkpoint_000041/checkpoint-41'!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['checkpoint-41.tune_metadata', 'checkpoint-41', '.is_checkpoint']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save our trainer.\n",
    "checkpoint_path = rllib_trainer.save()\n",
    "print(f\"Trainer was saved in '{checkpoint_path}'!\")\n",
    "\n",
    "import os\n",
    "os.listdir(os.path.dirname(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "global-canon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27766)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27766)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27766)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27766)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27766)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27766)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27760)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27760)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27760)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27760)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27760)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27760)\u001b[0m non-resource variables are not supported in the long term\n",
      "2021-04-29 15:35:35,001\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PPO' object has no attribute 'evaluation_workers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2b5d6e6a7dc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnew_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPOTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Evaluate the new trainer (this should yield random results).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;31m# Sync weights to the evaluation WorkerSet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_weights_to_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_filters_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PPO' object has no attribute 'evaluation_workers'"
     ]
    }
   ],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_trainer = PPOTrainer(config=config)\n",
    "# Evaluate the new trainer (this should yield random results).\n",
    "results = new_trainer._evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "new_trainer.restore(checkpoint_path)\n",
    "\n",
    "# Evaluate again (this should yield results we saw after having trained our saved agent).\n",
    "results = new_trainer._evaluate()\n",
    "print(f\"Evaluating restored trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "continent-architecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO's default config is:\n",
      "{'_fake_gpus': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': True,\n",
      " 'clip_param': 0.3,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': 180,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'eager_tracing': False,\n",
      " 'entropy_coeff': 0.0,\n",
      " 'entropy_coeff_schedule': None,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'evaluation_config': {},\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': 10,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'exploration_config': {'type': 'StochasticSampling'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'tf',\n",
      " 'gamma': 0.99,\n",
      " 'grad_clip': None,\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'kl_coeff': 0.2,\n",
      " 'kl_target': 0.01,\n",
      " 'lambda': 1.0,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'lr': 5e-05,\n",
      " 'lr_schedule': None,\n",
      " 'metrics_smoothing_episodes': 100,\n",
      " 'min_iter_time_s': 0,\n",
      " 'model': {'_time_major': False,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'num_framestacks': 'auto',\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': False,\n",
      "           'zero_mean': True},\n",
      " 'monitor': -1,\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_mapping_fn': None,\n",
      "                'replay_mode': 'independent'},\n",
      " 'no_done_at_end': False,\n",
      " 'normalize_actions': False,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_sgd_iter': 30,\n",
      " 'num_workers': 2,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'record_env': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'rollout_fragment_length': 200,\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'seed': None,\n",
      " 'sgd_minibatch_size': 128,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'shuffle_sequences': True,\n",
      " 'simple_optimizer': -1,\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 0,\n",
      " 'train_batch_size': 4000,\n",
      " 'use_critic': True,\n",
      " 'use_gae': True,\n",
      " 'vf_clip_param': 10.0,\n",
      " 'vf_loss_coeff': 1.0,\n",
      " 'vf_share_layers': -1}\n"
     ]
    }
   ],
   "source": [
    "# 5) Configuration dicts and Ray Tune.\n",
    "# Where are the default configuration dicts stored?\n",
    "import pprint\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as PPO_DEFAULT_CONFIG\n",
    "print(f\"PPO's default config is:\")\n",
    "pprint.pprint(PPO_DEFAULT_CONFIG)\n",
    "\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint.pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint.pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "latest-feature",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_4525b_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27758)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27758)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27758)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27758)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27758)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27758)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27758)\u001b[0m 2021-04-29 15:39:12,833\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27758)\u001b[0m 2021-04-29 15:39:12,833\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27758)\u001b[0m 2021-04-29 15:39:12,833\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27758)\u001b[0m 2021-04-29 15:39:12,833\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27759)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27759)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27759)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27759)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27759)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27759)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27763)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27763)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27763)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27763)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27763)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27763)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27758)\u001b[0m 2021-04-29 15:39:18,171\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=27758)\u001b[0m 2021-04-29 15:39:18,171\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=27758)\u001b[0m 2021-04-29 15:39:18,902\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=27758)\u001b[0m 2021-04-29 15:39:18,902\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_4525b_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-39-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -43.800000000000026\n",
      "  episode_reward_mean: -70.08000000000011\n",
      "  episode_reward_min: -86.40000000000015\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 8553c35b91814519ad2ca12c7fa11668\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.362125039100647\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02420918457210064\n",
      "          model: {}\n",
      "          policy_loss: -0.059532247483730316\n",
      "          total_loss: 133.79750061035156\n",
      "          vf_explained_var: 0.0809241235256195\n",
      "          vf_loss: 133.8521728515625\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.3\n",
      "    ram_util_percent: 66.68333333333332\n",
      "  pid: 27758\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04839599430263341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024588196189491658\n",
      "    mean_inference_ms: 0.49195684991278255\n",
      "    mean_raw_obs_processing_ms: 0.12189882261293394\n",
      "  time_since_restore: 3.918405055999756\n",
      "  time_this_iter_s: 3.918405055999756\n",
      "  time_total_s: 3.918405055999756\n",
      "  timers:\n",
      "    learn_throughput: 1285.604\n",
      "    learn_time_ms: 3111.378\n",
      "    sample_throughput: 5486.759\n",
      "    sample_time_ms: 729.028\n",
      "    update_time_ms: 4.881\n",
      "  timestamp: 1619703562\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 4525b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_4525b_00000</td><td>RUNNING </td><td>192.168.0.100:27758</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.91841</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  -70.08</td><td style=\"text-align: right;\">               -43.8</td><td style=\"text-align: right;\">               -86.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_4525b_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-39-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.99999999999999\n",
      "  episode_reward_mean: -62.04000000000009\n",
      "  episode_reward_min: -88.50000000000014\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: 8553c35b91814519ad2ca12c7fa11668\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3045539855957031\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01824115589261055\n",
      "          model: {}\n",
      "          policy_loss: -0.05359369516372681\n",
      "          total_loss: 66.01841735839844\n",
      "          vf_explained_var: 0.2051105946302414\n",
      "          vf_loss: 66.06654357910156\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.85\n",
      "    ram_util_percent: 66.7\n",
      "  pid: 27758\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05424758761163229\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.028421326556242663\n",
      "    mean_inference_ms: 0.542134077307555\n",
      "    mean_raw_obs_processing_ms: 0.13808162493726267\n",
      "  time_since_restore: 9.2779541015625\n",
      "  time_this_iter_s: 2.5401298999786377\n",
      "  time_total_s: 9.2779541015625\n",
      "  timers:\n",
      "    learn_throughput: 1785.889\n",
      "    learn_time_ms: 2239.781\n",
      "    sample_throughput: 4976.346\n",
      "    sample_time_ms: 803.803\n",
      "    update_time_ms: 2.613\n",
      "  timestamp: 1619703567\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 4525b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_4525b_00000</td><td>RUNNING </td><td>192.168.0.100:27758</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         9.27795</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -62.04</td><td style=\"text-align: right;\">                 -36</td><td style=\"text-align: right;\">               -88.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_4525b_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-39-32\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.99999999999999\n",
      "  episode_reward_mean: -61.71000000000009\n",
      "  episode_reward_min: -88.50000000000016\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: 8553c35b91814519ad2ca12c7fa11668\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2382826805114746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01850685104727745\n",
      "          model: {}\n",
      "          policy_loss: -0.04642891883850098\n",
      "          total_loss: 71.22037506103516\n",
      "          vf_explained_var: 0.23586240410804749\n",
      "          vf_loss: 71.2612533569336\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.200000000000003\n",
      "    ram_util_percent: 66.4\n",
      "  pid: 27758\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05376863610128311\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02828116729021798\n",
      "    mean_inference_ms: 0.5308928286923138\n",
      "    mean_raw_obs_processing_ms: 0.1372858764834551\n",
      "  time_since_restore: 14.249615907669067\n",
      "  time_this_iter_s: 2.629438877105713\n",
      "  time_total_s: 14.249615907669067\n",
      "  timers:\n",
      "    learn_throughput: 1943.786\n",
      "    learn_time_ms: 2057.84\n",
      "    sample_throughput: 5339.621\n",
      "    sample_time_ms: 749.117\n",
      "    update_time_ms: 2.112\n",
      "  timestamp: 1619703572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 4525b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_4525b_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         14.2496</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -61.71</td><td style=\"text-align: right;\">                 -36</td><td style=\"text-align: right;\">               -88.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_4525b_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         14.2496</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -61.71</td><td style=\"text-align: right;\">                 -36</td><td style=\"text-align: right;\">               -88.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 15:39:33,025\tINFO tune.py:549 -- Total run time: 25.97 seconds (25.42 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7ffc08b79370>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plugging in Ray Tune.\n",
    "# Note that this is the recommended way to run any experiments with RLlib.\n",
    "# Reasons:\n",
    "# - Tune allows you to do hyperparameter tuning in a user-friendly way\n",
    "#   and at large scale!\n",
    "# - Tune automatically allocates needed resources for the different\n",
    "#   hyperparam trials and experiment runs.\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Now that we will run things \"automatically\" through tune, we have to\n",
    "# define one or more stopping criteria.\n",
    "stop = {\n",
    "    # explain that keys here can be anything present in the above print(trainer.train())\n",
    "    \"training_iteration\": 5,\n",
    "    \"episode_reward_mean\": 9999.9,\n",
    "}\n",
    "\n",
    "# \"PPO\" is a registered name that points to RLlib's PPOTrainer.\n",
    "# See `ray/rllib/agents/registry.py`\n",
    "# Run our simple experiment until one of the stop criteria is met.\n",
    "tune.run(\"PPO\", config=config, stop=stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "discrete-quilt",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_54a34_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_54a34_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.5   </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27753)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27753)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27753)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27753)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27753)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27753)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27755)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27755)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27755)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27755)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27755)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27755)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27753)\u001b[0m 2021-04-29 15:39:37,463\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27753)\u001b[0m 2021-04-29 15:39:37,463\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27753)\u001b[0m 2021-04-29 15:39:37,463\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27753)\u001b[0m 2021-04-29 15:39:37,463\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27755)\u001b[0m 2021-04-29 15:39:37,463\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27755)\u001b[0m 2021-04-29 15:39:37,463\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27755)\u001b[0m 2021-04-29 15:39:37,463\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27755)\u001b[0m 2021-04-29 15:39:37,463\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27757)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27757)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27757)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27757)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27757)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27757)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27756)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27756)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27756)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27756)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27756)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27756)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27762)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27762)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27762)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27762)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27762)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27762)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27753)\u001b[0m 2021-04-29 15:39:43,331\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=27753)\u001b[0m 2021-04-29 15:39:43,331\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=27755)\u001b[0m 2021-04-29 15:39:43,326\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=27755)\u001b[0m 2021-04-29 15:39:43,326\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=27753)\u001b[0m 2021-04-29 15:39:44,397\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=27753)\u001b[0m 2021-04-29 15:39:44,397\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=27755)\u001b[0m 2021-04-29 15:39:44,397\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=27755)\u001b[0m 2021-04-29 15:39:44,397\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_54a34_00001:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-39-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -48.30000000000009\n",
      "  episode_reward_mean: -66.69000000000011\n",
      "  episode_reward_min: -93.00000000000013\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 3898debe393b45bba3ba5505420443c5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.03141152858734131\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 36.357173919677734\n",
      "          model: {}\n",
      "          policy_loss: 0.4721342921257019\n",
      "          total_loss: 158.54925537109375\n",
      "          vf_explained_var: -0.0006072241812944412\n",
      "          vf_loss: 150.80569458007812\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.75714285714286\n",
      "    ram_util_percent: 66.75714285714285\n",
      "  pid: 27755\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06795882225989343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03502180764486978\n",
      "    mean_inference_ms: 0.7388587002749449\n",
      "    mean_raw_obs_processing_ms: 0.17083953548740077\n",
      "  time_since_restore: 4.337473154067993\n",
      "  time_this_iter_s: 4.337473154067993\n",
      "  time_total_s: 4.337473154067993\n",
      "  timers:\n",
      "    learn_throughput: 1242.517\n",
      "    learn_time_ms: 3219.273\n",
      "    sample_throughput: 3743.957\n",
      "    sample_time_ms: 1068.388\n",
      "    update_time_ms: 2.708\n",
      "  timestamp: 1619703587\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 54a34_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_54a34_00000</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_54a34_00001</td><td>RUNNING </td><td>192.168.0.100:27755</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.33747</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  -66.69</td><td style=\"text-align: right;\">               -48.3</td><td style=\"text-align: right;\">                 -93</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_54a34_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-39-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -56.4000000000001\n",
      "  episode_reward_mean: -71.61000000000011\n",
      "  episode_reward_min: -88.50000000000017\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: e48d71458a3945f4bcc1000b5dec669e\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.345606803894043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.041982509195804596\n",
      "          model: {}\n",
      "          policy_loss: -0.07474327087402344\n",
      "          total_loss: 138.66407775878906\n",
      "          vf_explained_var: 0.1261296421289444\n",
      "          vf_loss: 138.73043823242188\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.8\n",
      "    ram_util_percent: 66.75714285714285\n",
      "  pid: 27753\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06739159564038258\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03505944014786482\n",
      "    mean_inference_ms: 0.7387129338709387\n",
      "    mean_raw_obs_processing_ms: 0.16821931292127063\n",
      "  time_since_restore: 4.336025238037109\n",
      "  time_this_iter_s: 4.336025238037109\n",
      "  time_total_s: 4.336025238037109\n",
      "  timers:\n",
      "    learn_throughput: 1241.499\n",
      "    learn_time_ms: 3221.912\n",
      "    sample_throughput: 3758.515\n",
      "    sample_time_ms: 1064.25\n",
      "    update_time_ms: 2.778\n",
      "  timestamp: 1619703587\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 54a34_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_54a34_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-39-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -48.30000000000009\n",
      "  episode_reward_mean: -93.23000000000015\n",
      "  episode_reward_min: -106.50000000000017\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: 3898debe393b45bba3ba5505420443c5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          model: {}\n",
      "          policy_loss: -0.00342764426022768\n",
      "          total_loss: 567.5249633789062\n",
      "          vf_explained_var: 1.862645149230957e-09\n",
      "          vf_loss: 567.5283813476562\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.25\n",
      "    ram_util_percent: 66.61666666666667\n",
      "  pid: 27755\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0676359648909997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0349074466943395\n",
      "    mean_inference_ms: 0.7174128618281794\n",
      "    mean_raw_obs_processing_ms: 0.1683478400576864\n",
      "  time_since_restore: 12.55409026145935\n",
      "  time_this_iter_s: 4.18924617767334\n",
      "  time_total_s: 12.55409026145935\n",
      "  timers:\n",
      "    learn_throughput: 1280.786\n",
      "    learn_time_ms: 3123.083\n",
      "    sample_throughput: 3969.761\n",
      "    sample_time_ms: 1007.617\n",
      "    update_time_ms: 2.738\n",
      "  timestamp: 1619703595\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 54a34_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_54a34_00000</td><td>RUNNING </td><td>192.168.0.100:27753</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         8.36267</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">-70.2975</td><td style=\"text-align: right;\">               -47.7</td><td style=\"text-align: right;\">               -90.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_54a34_00001</td><td>RUNNING </td><td>192.168.0.100:27755</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">        12.5541 </td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-93.23  </td><td style=\"text-align: right;\">               -48.3</td><td style=\"text-align: right;\">              -106.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_54a34_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-39-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -47.70000000000005\n",
      "  episode_reward_mean: -68.0600000000001\n",
      "  episode_reward_min: -90.90000000000015\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: e48d71458a3945f4bcc1000b5dec669e\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2796211242675781\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02720813825726509\n",
      "          model: {}\n",
      "          policy_loss: -0.06847807765007019\n",
      "          total_loss: 64.20436096191406\n",
      "          vf_explained_var: 0.3745890259742737\n",
      "          vf_loss: 64.26058959960938\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.300000000000004\n",
      "    ram_util_percent: 66.61666666666667\n",
      "  pid: 27753\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06791640382438438\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03504255734001482\n",
      "    mean_inference_ms: 0.7232466610375178\n",
      "    mean_raw_obs_processing_ms: 0.16700388635135474\n",
      "  time_since_restore: 12.53986120223999\n",
      "  time_this_iter_s: 4.177187919616699\n",
      "  time_total_s: 12.53986120223999\n",
      "  timers:\n",
      "    learn_throughput: 1282.25\n",
      "    learn_time_ms: 3119.517\n",
      "    sample_throughput: 3934.49\n",
      "    sample_time_ms: 1016.65\n",
      "    update_time_ms: 2.145\n",
      "  timestamp: 1619703596\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 54a34_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_54a34_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-04\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -44.70000000000003\n",
      "  episode_reward_mean: -66.0600000000001\n",
      "  episode_reward_min: -90.90000000000015\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: e48d71458a3945f4bcc1000b5dec669e\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2488763332366943\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017017770558595657\n",
      "          model: {}\n",
      "          policy_loss: -0.0573711097240448\n",
      "          total_loss: 73.14822387695312\n",
      "          vf_explained_var: 0.281551718711853\n",
      "          vf_loss: 73.18836975097656\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.31666666666667\n",
      "    ram_util_percent: 66.68333333333332\n",
      "  pid: 27753\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0684550624486117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03538114402208399\n",
      "    mean_inference_ms: 0.719853010004424\n",
      "    mean_raw_obs_processing_ms: 0.16719772888197995\n",
      "  time_since_restore: 20.572372913360596\n",
      "  time_this_iter_s: 4.067825794219971\n",
      "  time_total_s: 20.572372913360596\n",
      "  timers:\n",
      "    learn_throughput: 1311.106\n",
      "    learn_time_ms: 3050.859\n",
      "    sample_throughput: 3918.436\n",
      "    sample_time_ms: 1020.816\n",
      "    update_time_ms: 2.046\n",
      "  timestamp: 1619703604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 54a34_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_54a34_00000</td><td>RUNNING </td><td>192.168.0.100:27753</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         20.5724</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-66.06  </td><td style=\"text-align: right;\">               -44.7</td><td style=\"text-align: right;\">               -90.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_54a34_00001</td><td>RUNNING </td><td>192.168.0.100:27755</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         16.5365</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-96.5475</td><td style=\"text-align: right;\">               -48.3</td><td style=\"text-align: right;\">              -106.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_54a34_00001:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-04\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -48.30000000000009\n",
      "  episode_reward_mean: -98.53800000000018\n",
      "  episode_reward_min: -106.50000000000017\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: 3898debe393b45bba3ba5505420443c5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          model: {}\n",
      "          policy_loss: -0.004698052071034908\n",
      "          total_loss: 424.4887390136719\n",
      "          vf_explained_var: -1.6763806343078613e-08\n",
      "          vf_loss: 424.4934387207031\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.26666666666667\n",
      "    ram_util_percent: 66.68333333333332\n",
      "  pid: 27755\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06781834943456368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03486061833316395\n",
      "    mean_inference_ms: 0.7136966191083459\n",
      "    mean_raw_obs_processing_ms: 0.1679541499613468\n",
      "  time_since_restore: 20.617169618606567\n",
      "  time_this_iter_s: 4.080684185028076\n",
      "  time_total_s: 20.617169618606567\n",
      "  timers:\n",
      "    learn_throughput: 1307.581\n",
      "    learn_time_ms: 3059.083\n",
      "    sample_throughput: 3943.733\n",
      "    sample_time_ms: 1014.267\n",
      "    update_time_ms: 2.412\n",
      "  timestamp: 1619703604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 54a34_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_54a34_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         20.5724</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> -66.06 </td><td style=\"text-align: right;\">               -44.7</td><td style=\"text-align: right;\">               -90.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_54a34_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         20.6172</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> -98.538</td><td style=\"text-align: right;\">               -48.3</td><td style=\"text-align: right;\">              -106.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m 2021-04-29 15:40:04,397\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1077, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 42, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/json/__init__.py\", line 183, in dumps\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m     def dumps(obj, *, skipkeys=False, ensure_ascii=True, check_circular=True,\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m 2021-04-29 15:40:04,397\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1077, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 42, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/json/__init__.py\", line 183, in dumps\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m     def dumps(obj, *, skipkeys=False, ensure_ascii=True, check_circular=True,\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=27754)\u001b[0m SystemExit: 1\n",
      "2021-04-29 15:40:04,503\tINFO tune.py:549 -- Total run time: 31.45 seconds (31.15 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis object at 0x7ffc09734130>\n"
     ]
    }
   ],
   "source": [
    "# Updating an algo's default config dict and adding hyperparameter tuning\n",
    "# options to it.\n",
    "# Note: Hyperparameter tuning options (e.g. grid_search) will only work,\n",
    "# if we run these configs via `tune.run`.\n",
    "config.update(\n",
    "    {\n",
    "        # Try 2 different learning rates.\n",
    "        \"lr\": tune.grid_search([0.0001, 0.5]),\n",
    "        # NN model config to tweak the default model\n",
    "        # that'll be created by RLlib for the policy.\n",
    "        \"model\": {\n",
    "            # e.g. change the dense layer stack.\n",
    "            \"fcnet_hiddens\": [256, 256, 256],\n",
    "            # Alternatively, you can specify a custom model here\n",
    "            # (we'll cover that later).\n",
    "            # \"custom_model\": ...\n",
    "            # Pass kwargs to your custom model.\n",
    "            # \"custom_model_config\": {}\n",
    "        },\n",
    "    }\n",
    ")\n",
    "# Repeat our experiment using tune's grid-search feature.\n",
    "results = tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,\n",
    "    stop=stop,\n",
    "    checkpoint_at_end=True,  # create a checkpoint when done.\n",
    "    checkpoint_freq=1,  # create a checkpoint on every iteration.\n",
    ")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "experimental-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Going multi-policy: Our experiment is ill-configured b/c both\n",
    "# agents, which should behave differently due to their different\n",
    "# tasks and reward functions, learn the same policy (the \"default_policy\",\n",
    "# which RLlib always provides if you don't configure anything else; Remember\n",
    "# that RLlib does not know at Trainer setup time, how many and which agents\n",
    "# the environment will \"produce\").\n",
    "# Let's fix this and introduce the \"multiagent\" API.\n",
    "\n",
    "# 6.1.) Define an agent->policy mapping function.\n",
    "# Which agents (defined by the environment) use which policies\n",
    "# (defined by us)? Mapping is M (agents) -> N (policies), where M >= N.\n",
    "def policy_mapping_fn(agent: str):\n",
    "    assert agent in [\"agent1\", \"agent2\"], f\"ERROR: invalid agent {agent}!\"\n",
    "    return \"pol1\" if agent == \"agent1\" else \"pol2\"\n",
    "    \n",
    "# 6.2.) Define details for our two policies.\n",
    "#TODO: coding Sven: Make it possible to not need obs/action spaces\n",
    "#  if they are the default anyways.\n",
    "observation_space = rllib_trainer.workers.local_worker().env.observation_space\n",
    "action_space = rllib_trainer.workers.local_worker().env.action_space\n",
    "# Btw, the above is equivalent to saying:\n",
    "# >>> rllib_trainer.get_policy(\"default_policy\").obs/action_space\n",
    "policies = {\n",
    "    \"pol1\": (None, observation_space, action_space, {\"lr\": 0.0003}),\n",
    "    \"pol2\": (None, observation_space, action_space, {\"lr\": 0.0004}),\n",
    "}\n",
    "\n",
    "#policies_to_train = [\"pol1\", \"pol2\"]\n",
    "\n",
    "# 6.3) Adding the above to our config.\n",
    "config.update({\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        #\"policies_to_train\": policies_to_train,\n",
    "    },\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a74ec7-a6c1-431d-83aa-35df56d93185",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise No 2\n",
    "\n",
    "Try learning our environment using Ray tune.run and a simple hyperparameter grid_search over:\n",
    "- 2 different learning rates (pick your own values).\n",
    "- AND 2 different `train_batch_size` settings (use 2000 and 3000).\n",
    "\n",
    "Also, make RLlib use a [128, 128] dense layer stack as the NN model.\n",
    "\n",
    "Also, use the config setting of `num_envs_per_worker=10` to increase the sampling throughput.\n",
    "\n",
    "In case your local machine has less than 12 CPUs, try setting `num_workers=1` to make all tune trials run at the same time.\n",
    "Background: PPO by default uses 2 workers, which makes 1 trial use 3 CPUs (2 workers + \"driver\" (\"local-worker\")),\n",
    "which makes the entire experiment use 12 CPUs. Tune will run trials in sequence in case it cannot allocate enough CPUs at once\n",
    "(which is also fine, but then takes longer).\n",
    "\n",
    "Try to reach a total reward (sum of agent1 and agent2) of -25.0.\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c0077e6-16b1-428b-80b7-2516e1302030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=27751)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27751)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27751)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27751)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27751)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27751)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27752)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27752)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27752)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27752)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27752)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27752)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27761)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27761)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27761)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27761)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27761)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27761)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27751)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27751)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27751)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27751)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27752)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27752)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27752)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27752)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27761)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27761)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27761)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27761)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m 2021-04-29 15:40:09,528\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=27830)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27830)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27830)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27830)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27830)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27830)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27836)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27836)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27836)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27836)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27836)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27836)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27837)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27837)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27837)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27837)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27837)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27837)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27839)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27839)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27839)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27839)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27839)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27839)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27841)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27841)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27841)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27841)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27841)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27841)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27842)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27842)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27842)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=27842)\u001b[0m WARNING:tensorflow:From /Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=27842)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=27842)\u001b[0m non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27751)\u001b[0m 2021-04-29 15:40:18,899\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=27751)\u001b[0m 2021-04-29 15:40:18,899\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=27761)\u001b[0m 2021-04-29 15:40:19,289\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=27761)\u001b[0m 2021-04-29 15:40:19,289\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m 2021-04-29 15:40:19,282\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m 2021-04-29 15:40:19,282\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=27752)\u001b[0m 2021-04-29 15:40:19,307\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=27752)\u001b[0m 2021-04-29 15:40:19,307\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=27751)\u001b[0m 2021-04-29 15:40:19,579\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=27751)\u001b[0m 2021-04-29 15:40:19,579\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m 2021-04-29 15:40:20,071\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m 2021-04-29 15:40:20,071\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=27752)\u001b[0m 2021-04-29 15:40:20,107\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=27752)\u001b[0m 2021-04-29 15:40:20,107\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=27761)\u001b[0m 2021-04-29 15:40:20,091\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=27761)\u001b[0m 2021-04-29 15:40:20,091\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.90000000000006\n",
      "  episode_reward_mean: -69.2925000000001\n",
      "  episode_reward_min: -91.20000000000014\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.3394379615783691\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04784049838781357\n",
      "          model: {}\n",
      "          policy_loss: -0.07369829714298248\n",
      "          total_loss: 226.8167724609375\n",
      "          vf_explained_var: 0.05389287695288658\n",
      "          vf_loss: 226.88088989257812\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.3424718379974365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04574945569038391\n",
      "          model: {}\n",
      "          policy_loss: -0.07607969641685486\n",
      "          total_loss: 6.756410598754883\n",
      "          vf_explained_var: 0.4919566214084625\n",
      "          vf_loss: 6.82334041595459\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.711111111111116\n",
      "    ram_util_percent: 68.06666666666666\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -27.0\n",
      "    pol2: -8.999999999999964\n",
      "  policy_reward_mean:\n",
      "    pol1: -50.2\n",
      "    pol2: -19.092499999999962\n",
      "  policy_reward_min:\n",
      "    pol1: -74.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2572880455510533\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14823408269170502\n",
      "    mean_inference_ms: 1.51105425251064\n",
      "    mean_raw_obs_processing_ms: 1.0834914534839233\n",
      "  time_since_restore: 5.617585897445679\n",
      "  time_this_iter_s: 5.617585897445679\n",
      "  time_total_s: 5.617585897445679\n",
      "  timers:\n",
      "    learn_throughput: 854.298\n",
      "    learn_time_ms: 4682.206\n",
      "    sample_throughput: 5912.363\n",
      "    sample_time_ms: 676.548\n",
      "    update_time_ms: 3.272\n",
      "  timestamp: 1619703624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.61759</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-69.2925</td><td style=\"text-align: right;\">               -45.9</td><td style=\"text-align: right;\">               -91.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -46.5\n",
      "  episode_reward_mean: -67.2675000000001\n",
      "  episode_reward_min: -90.00000000000017\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.3452237844467163\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0427456833422184\n",
      "          model: {}\n",
      "          policy_loss: -0.0751623585820198\n",
      "          total_loss: 249.20498657226562\n",
      "          vf_explained_var: 0.015425518155097961\n",
      "          vf_loss: 249.2716064453125\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.346801996231079\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0414825975894928\n",
      "          model: {}\n",
      "          policy_loss: -0.07796703279018402\n",
      "          total_loss: 7.978614807128906\n",
      "          vf_explained_var: 0.42117130756378174\n",
      "          vf_loss: 8.048285484313965\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.06666666666667\n",
      "    ram_util_percent: 68.07777777777777\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -26.5\n",
      "    pol2: -11.199999999999967\n",
      "  policy_reward_mean:\n",
      "    pol1: -49.275\n",
      "    pol2: -17.99249999999996\n",
      "  policy_reward_min:\n",
      "    pol1: -70.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.32016827692439903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17797353848889103\n",
      "    mean_inference_ms: 1.7779270807902017\n",
      "    mean_raw_obs_processing_ms: 1.2374410581825976\n",
      "  time_since_restore: 6.1030871868133545\n",
      "  time_this_iter_s: 6.1030871868133545\n",
      "  time_total_s: 6.1030871868133545\n",
      "  timers:\n",
      "    learn_throughput: 786.066\n",
      "    learn_time_ms: 5088.629\n",
      "    sample_throughput: 5094.594\n",
      "    sample_time_ms: 785.146\n",
      "    update_time_ms: 3.314\n",
      "  timestamp: 1619703625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -46.20000000000001\n",
      "  episode_reward_mean: -66.9750000000001\n",
      "  episode_reward_min: -97.80000000000015\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.344066858291626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.043867602944374084\n",
      "          model: {}\n",
      "          policy_loss: -0.08330787718296051\n",
      "          total_loss: 228.63287353515625\n",
      "          vf_explained_var: 0.022387288510799408\n",
      "          vf_loss: 228.7073974609375\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.348684310913086\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03948436677455902\n",
      "          model: {}\n",
      "          policy_loss: -0.07739048451185226\n",
      "          total_loss: 7.998737812042236\n",
      "          vf_explained_var: 0.3869269788265228\n",
      "          vf_loss: 8.068231582641602\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.62222222222223\n",
      "    ram_util_percent: 68.07777777777777\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -28.0\n",
      "    pol2: -6.80000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: -48.7625\n",
      "    pol2: -18.212499999999967\n",
      "  policy_reward_min:\n",
      "    pol1: -84.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.32278969513243105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17810698172346276\n",
      "    mean_inference_ms: 1.7962277825198956\n",
      "    mean_raw_obs_processing_ms: 1.2453308152915235\n",
      "  time_since_restore: 6.081349849700928\n",
      "  time_this_iter_s: 6.081349849700928\n",
      "  time_total_s: 6.081349849700928\n",
      "  timers:\n",
      "    learn_throughput: 791.252\n",
      "    learn_time_ms: 5055.28\n",
      "    sample_throughput: 5021.649\n",
      "    sample_time_ms: 796.551\n",
      "    update_time_ms: 3.695\n",
      "  timestamp: 1619703625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -43.200000000000045\n",
      "  episode_reward_mean: -69.64500000000012\n",
      "  episode_reward_min: -99.00000000000016\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.3491137027740479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03835487365722656\n",
      "          model: {}\n",
      "          policy_loss: -0.06311054527759552\n",
      "          total_loss: 273.507568359375\n",
      "          vf_explained_var: 0.017716674134135246\n",
      "          vf_loss: 273.56298828125\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.3441987037658691\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04376373440027237\n",
      "          model: {}\n",
      "          policy_loss: -0.0808994397521019\n",
      "          total_loss: 7.642914772033691\n",
      "          vf_explained_var: 0.4347640872001648\n",
      "          vf_loss: 7.715061187744141\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.56666666666667\n",
      "    ram_util_percent: 68.07777777777777\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -26.5\n",
      "    pol2: -11.199999999999964\n",
      "  policy_reward_mean:\n",
      "    pol1: -51.075\n",
      "    pol2: -18.569999999999965\n",
      "  policy_reward_min:\n",
      "    pol1: -79.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31882613452512826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1812070163328256\n",
      "    mean_inference_ms: 1.7941235309809591\n",
      "    mean_raw_obs_processing_ms: 1.268201206453997\n",
      "  time_since_restore: 6.106901168823242\n",
      "  time_this_iter_s: 6.106901168823242\n",
      "  time_total_s: 6.106901168823242\n",
      "  timers:\n",
      "    learn_throughput: 786.289\n",
      "    learn_time_ms: 5087.189\n",
      "    sample_throughput: 5002.134\n",
      "    sample_time_ms: 799.659\n",
      "    update_time_ms: 3.523\n",
      "  timestamp: 1619703625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -32.39999999999997\n",
      "  episode_reward_mean: -65.9775000000001\n",
      "  episode_reward_min: -91.20000000000014\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2962405681610107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03551680967211723\n",
      "          model: {}\n",
      "          policy_loss: -0.07215029001235962\n",
      "          total_loss: 81.86832427978516\n",
      "          vf_explained_var: 0.19913028180599213\n",
      "          vf_loss: 81.92981719970703\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2996799945831299\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03850480169057846\n",
      "          model: {}\n",
      "          policy_loss: -0.07547569274902344\n",
      "          total_loss: 6.519376754760742\n",
      "          vf_explained_var: 0.45898088812828064\n",
      "          vf_loss: 6.583301067352295\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.6875\n",
      "    ram_util_percent: 68.51249999999999\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -13.5\n",
      "    pol2: -8.999999999999964\n",
      "  policy_reward_mean:\n",
      "    pol1: -47.1875\n",
      "    pol2: -18.789999999999964\n",
      "  policy_reward_min:\n",
      "    pol1: -74.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2953629706926753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1704623756248249\n",
      "    mean_inference_ms: 1.5812812538528913\n",
      "    mean_raw_obs_processing_ms: 1.2350451046875546\n",
      "  time_since_restore: 11.446921110153198\n",
      "  time_this_iter_s: 5.8293352127075195\n",
      "  time_total_s: 11.446921110153198\n",
      "  timers:\n",
      "    learn_throughput: 846.814\n",
      "    learn_time_ms: 4723.585\n",
      "    sample_throughput: 5252.857\n",
      "    sample_time_ms: 761.49\n",
      "    update_time_ms: 3.997\n",
      "  timestamp: 1619703630\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        11.4469 </td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-65.9775</td><td style=\"text-align: right;\">               -32.4</td><td style=\"text-align: right;\">               -91.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.1069 </td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-69.645 </td><td style=\"text-align: right;\">               -43.2</td><td style=\"text-align: right;\">               -99  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.08135</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-66.975 </td><td style=\"text-align: right;\">               -46.2</td><td style=\"text-align: right;\">               -97.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.10309</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-67.2675</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               -90  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -37.5\n",
      "  episode_reward_mean: -63.42750000000008\n",
      "  episode_reward_min: -97.80000000000015\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.3139889240264893\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03743421286344528\n",
      "          model: {}\n",
      "          policy_loss: -0.07891766726970673\n",
      "          total_loss: 73.26046752929688\n",
      "          vf_explained_var: 0.2710918188095093\n",
      "          vf_loss: 73.3281478881836\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.30776047706604\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.039015453308820724\n",
      "          model: {}\n",
      "          policy_loss: -0.07669613510370255\n",
      "          total_loss: 7.215541839599609\n",
      "          vf_explained_var: 0.4085002541542053\n",
      "          vf_loss: 7.280533313751221\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.412499999999994\n",
      "    ram_util_percent: 68.525\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -17.5\n",
      "    pol2: -6.80000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: -45.05\n",
      "    pol2: -18.377499999999962\n",
      "  policy_reward_min:\n",
      "    pol1: -84.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3390810501220361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1861875090438913\n",
      "    mean_inference_ms: 1.7399730155078152\n",
      "    mean_raw_obs_processing_ms: 1.317249035294246\n",
      "  time_since_restore: 11.688183784484863\n",
      "  time_this_iter_s: 5.6068339347839355\n",
      "  time_total_s: 11.688183784484863\n",
      "  timers:\n",
      "    learn_throughput: 824.588\n",
      "    learn_time_ms: 4850.909\n",
      "    sample_throughput: 5133.049\n",
      "    sample_time_ms: 779.264\n",
      "    update_time_ms: 3.986\n",
      "  timestamp: 1619703631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -43.200000000000045\n",
      "  episode_reward_mean: -66.1875000000001\n",
      "  episode_reward_min: -99.00000000000016\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.3089298009872437\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03398147225379944\n",
      "          model: {}\n",
      "          policy_loss: -0.07374817132949829\n",
      "          total_loss: 84.64259338378906\n",
      "          vf_explained_var: 0.16995956003665924\n",
      "          vf_loss: 84.7061538696289\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.306799054145813\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.035717327147722244\n",
      "          model: {}\n",
      "          policy_loss: -0.07395875453948975\n",
      "          total_loss: 6.609086513519287\n",
      "          vf_explained_var: 0.45820197463035583\n",
      "          vf_loss: 6.672329902648926\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.2625\n",
      "    ram_util_percent: 68.525\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -26.5\n",
      "    pol2: -11.199999999999964\n",
      "  policy_reward_mean:\n",
      "    pol1: -47.49375\n",
      "    pol2: -18.693749999999962\n",
      "  policy_reward_min:\n",
      "    pol1: -79.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3351638282233214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18860811089832202\n",
      "    mean_inference_ms: 1.7530562575908408\n",
      "    mean_raw_obs_processing_ms: 1.3653491139729914\n",
      "  time_since_restore: 11.680433988571167\n",
      "  time_this_iter_s: 5.573532819747925\n",
      "  time_total_s: 11.680433988571167\n",
      "  timers:\n",
      "    learn_throughput: 828.5\n",
      "    learn_time_ms: 4828.005\n",
      "    sample_throughput: 4972.943\n",
      "    sample_time_ms: 804.353\n",
      "    update_time_ms: 3.847\n",
      "  timestamp: 1619703631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -46.5\n",
      "  episode_reward_mean: -67.29375000000009\n",
      "  episode_reward_min: -93.00000000000016\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2987558841705322\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03738018870353699\n",
      "          model: {}\n",
      "          policy_loss: -0.07396349310874939\n",
      "          total_loss: 106.31910705566406\n",
      "          vf_explained_var: 0.15458989143371582\n",
      "          vf_loss: 106.3818588256836\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.313311219215393\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03538857400417328\n",
      "          model: {}\n",
      "          policy_loss: -0.06854873150587082\n",
      "          total_loss: 8.869882583618164\n",
      "          vf_explained_var: 0.3280048072338104\n",
      "          vf_loss: 8.927814483642578\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.6\n",
      "    ram_util_percent: 68.5375\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -26.5\n",
      "    pol2: -11.199999999999967\n",
      "  policy_reward_mean:\n",
      "    pol1: -48.80625\n",
      "    pol2: -18.487499999999965\n",
      "  policy_reward_min:\n",
      "    pol1: -73.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.33383286710255844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1840543939810506\n",
      "    mean_inference_ms: 1.7253324971631083\n",
      "    mean_raw_obs_processing_ms: 1.2940450190367845\n",
      "  time_since_restore: 11.760315179824829\n",
      "  time_this_iter_s: 5.657227993011475\n",
      "  time_total_s: 11.760315179824829\n",
      "  timers:\n",
      "    learn_throughput: 814.694\n",
      "    learn_time_ms: 4909.82\n",
      "    sample_throughput: 5239.264\n",
      "    sample_time_ms: 763.466\n",
      "    update_time_ms: 3.421\n",
      "  timestamp: 1619703631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -32.39999999999997\n",
      "  episode_reward_mean: -61.20600000000009\n",
      "  episode_reward_min: -87.00000000000016\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2518072128295898\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03171969950199127\n",
      "          model: {}\n",
      "          policy_loss: -0.07902167737483978\n",
      "          total_loss: 68.90869140625\n",
      "          vf_explained_var: 0.21231824159622192\n",
      "          vf_loss: 68.97343444824219\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2672456502914429\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03226224333047867\n",
      "          model: {}\n",
      "          policy_loss: -0.07257924973964691\n",
      "          total_loss: 8.111026763916016\n",
      "          vf_explained_var: 0.3652001917362213\n",
      "          vf_loss: 8.169087409973145\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.66\n",
      "    ram_util_percent: 68.79\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -13.5\n",
      "    pol2: -5.700000000000009\n",
      "  policy_reward_mean:\n",
      "    pol1: -43.065\n",
      "    pol2: -18.140999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -67.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.33472334561865574\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19230254982149744\n",
      "    mean_inference_ms: 1.643509264279972\n",
      "    mean_raw_obs_processing_ms: 1.3816159635011862\n",
      "  time_since_restore: 18.522559881210327\n",
      "  time_this_iter_s: 7.075638771057129\n",
      "  time_total_s: 18.522559881210327\n",
      "  timers:\n",
      "    learn_throughput: 791.984\n",
      "    learn_time_ms: 5050.606\n",
      "    sample_throughput: 4962.18\n",
      "    sample_time_ms: 806.097\n",
      "    update_time_ms: 5.478\n",
      "  timestamp: 1619703637\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         18.5226</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-61.206 </td><td style=\"text-align: right;\">               -32.4</td><td style=\"text-align: right;\">               -87  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         11.6804</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">-66.1875</td><td style=\"text-align: right;\">               -43.2</td><td style=\"text-align: right;\">               -99  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         11.6882</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">-63.4275</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               -97.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         11.7603</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">-67.2938</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               -93  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -37.5\n",
      "  episode_reward_mean: -62.36700000000009\n",
      "  episode_reward_min: -97.80000000000015\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2709401845932007\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.030720677226781845\n",
      "          model: {}\n",
      "          policy_loss: -0.07197196781635284\n",
      "          total_loss: 67.42460632324219\n",
      "          vf_explained_var: 0.2721448838710785\n",
      "          vf_loss: 67.48274993896484\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.278892159461975\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03226149454712868\n",
      "          model: {}\n",
      "          policy_loss: -0.07273483276367188\n",
      "          total_loss: 7.490102767944336\n",
      "          vf_explained_var: 0.4056863486766815\n",
      "          vf_loss: 7.5483198165893555\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.56\n",
      "    ram_util_percent: 68.83000000000001\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -17.5\n",
      "    pol2: -7.899999999999994\n",
      "  policy_reward_mean:\n",
      "    pol1: -43.83\n",
      "    pol2: -18.536999999999964\n",
      "  policy_reward_min:\n",
      "    pol1: -80.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3536689822262943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19379291065027274\n",
      "    mean_inference_ms: 1.6908592926537895\n",
      "    mean_raw_obs_processing_ms: 1.3827320806876033\n",
      "  time_since_restore: 18.772042751312256\n",
      "  time_this_iter_s: 7.083858966827393\n",
      "  time_total_s: 18.772042751312256\n",
      "  timers:\n",
      "    learn_throughput: 764.409\n",
      "    learn_time_ms: 5232.797\n",
      "    sample_throughput: 5143.053\n",
      "    sample_time_ms: 777.748\n",
      "    update_time_ms: 4.008\n",
      "  timestamp: 1619703638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -40.80000000000004\n",
      "  episode_reward_mean: -63.633000000000095\n",
      "  episode_reward_min: -99.00000000000016\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2684849500656128\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03198305517435074\n",
      "          model: {}\n",
      "          policy_loss: -0.07899376004934311\n",
      "          total_loss: 71.31511688232422\n",
      "          vf_explained_var: 0.2886843681335449\n",
      "          vf_loss: 71.37971496582031\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.275442361831665\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02949884906411171\n",
      "          model: {}\n",
      "          policy_loss: -0.06455319374799728\n",
      "          total_loss: 6.639023780822754\n",
      "          vf_explained_var: 0.46154695749282837\n",
      "          vf_loss: 6.690302848815918\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.57000000000002\n",
      "    ram_util_percent: 68.83000000000001\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -23.0\n",
      "    pol2: -11.199999999999966\n",
      "  policy_reward_mean:\n",
      "    pol1: -45.195\n",
      "    pol2: -18.437999999999967\n",
      "  policy_reward_min:\n",
      "    pol1: -79.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.35030766803050445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19498800767336455\n",
      "    mean_inference_ms: 1.7169938665658686\n",
      "    mean_raw_obs_processing_ms: 1.461452345794537\n",
      "  time_since_restore: 18.794994831085205\n",
      "  time_this_iter_s: 7.114560842514038\n",
      "  time_total_s: 18.794994831085205\n",
      "  timers:\n",
      "    learn_throughput: 766.919\n",
      "    learn_time_ms: 5215.673\n",
      "    sample_throughput: 4923.429\n",
      "    sample_time_ms: 812.442\n",
      "    update_time_ms: 4.275\n",
      "  timestamp: 1619703638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -46.5\n",
      "  episode_reward_mean: -64.93800000000009\n",
      "  episode_reward_min: -93.00000000000016\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2702410221099854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.030056701973080635\n",
      "          model: {}\n",
      "          policy_loss: -0.07369863241910934\n",
      "          total_loss: 59.97228240966797\n",
      "          vf_explained_var: 0.26302412152290344\n",
      "          vf_loss: 60.0324592590332\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2839224338531494\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0318729430437088\n",
      "          model: {}\n",
      "          policy_loss: -0.07663877308368683\n",
      "          total_loss: 6.66535758972168\n",
      "          vf_explained_var: 0.4851074814796448\n",
      "          vf_loss: 6.727653503417969\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.31818181818181\n",
      "    ram_util_percent: 68.98181818181818\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -26.5\n",
      "    pol2: -11.199999999999967\n",
      "  policy_reward_mean:\n",
      "    pol1: -46.225\n",
      "    pol2: -18.712999999999965\n",
      "  policy_reward_min:\n",
      "    pol1: -73.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3555511712027365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19330551336026416\n",
      "    mean_inference_ms: 1.698480588531142\n",
      "    mean_raw_obs_processing_ms: 1.4064112048682313\n",
      "  time_since_restore: 19.235661506652832\n",
      "  time_this_iter_s: 7.475346326828003\n",
      "  time_total_s: 19.235661506652832\n",
      "  timers:\n",
      "    learn_throughput: 752.845\n",
      "    learn_time_ms: 5313.177\n",
      "    sample_throughput: 4955.006\n",
      "    sample_time_ms: 807.264\n",
      "    update_time_ms: 3.178\n",
      "  timestamp: 1619703638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.29999999999999\n",
      "  episode_reward_mean: -59.50800000000006\n",
      "  episode_reward_min: -78.00000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2339704036712646\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023932427167892456\n",
      "          model: {}\n",
      "          policy_loss: -0.06924664229154587\n",
      "          total_loss: 64.23357391357422\n",
      "          vf_explained_var: 0.2349700629711151\n",
      "          vf_loss: 64.28666687011719\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.246011734008789\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02453068271279335\n",
      "          model: {}\n",
      "          policy_loss: -0.06184965372085571\n",
      "          total_loss: 6.481815338134766\n",
      "          vf_explained_var: 0.4670517146587372\n",
      "          vf_loss: 6.527106285095215\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.25833333333334\n",
      "    ram_util_percent: 66.72500000000001\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -15.5\n",
      "    pol2: -5.700000000000009\n",
      "  policy_reward_mean:\n",
      "    pol1: -41.565\n",
      "    pol2: -17.942999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -64.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40197516266401406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22741736962779058\n",
      "    mean_inference_ms: 1.840494960972008\n",
      "    mean_raw_obs_processing_ms: 1.6749382188619435\n",
      "  time_since_restore: 26.770012140274048\n",
      "  time_this_iter_s: 8.24745225906372\n",
      "  time_total_s: 26.770012140274048\n",
      "  timers:\n",
      "    learn_throughput: 743.099\n",
      "    learn_time_ms: 5382.859\n",
      "    sample_throughput: 4042.466\n",
      "    sample_time_ms: 989.495\n",
      "    update_time_ms: 5.204\n",
      "  timestamp: 1619703645\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         26.77  </td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -59.508</td><td style=\"text-align: right;\">               -33.3</td><td style=\"text-align: right;\">               -78  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         18.795 </td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -63.633</td><td style=\"text-align: right;\">               -40.8</td><td style=\"text-align: right;\">               -99  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         18.772 </td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -62.367</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               -97.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         19.2357</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -64.938</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               -93  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.99999999999998\n",
      "  episode_reward_mean: -59.92200000000008\n",
      "  episode_reward_min: -81.60000000000015\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2504839897155762\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02280413918197155\n",
      "          model: {}\n",
      "          policy_loss: -0.06321410834789276\n",
      "          total_loss: 67.6986083984375\n",
      "          vf_explained_var: 0.22486630082130432\n",
      "          vf_loss: 67.74642944335938\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.245950698852539\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025716762989759445\n",
      "          model: {}\n",
      "          policy_loss: -0.06710878014564514\n",
      "          total_loss: 6.752125263214111\n",
      "          vf_explained_var: 0.45099860429763794\n",
      "          vf_loss: 6.801875114440918\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.47500000000001\n",
      "    ram_util_percent: 66.40833333333333\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -16.0\n",
      "    pol2: -7.899999999999994\n",
      "  policy_reward_mean:\n",
      "    pol1: -41.495\n",
      "    pol2: -18.426999999999968\n",
      "  policy_reward_min:\n",
      "    pol1: -66.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39538440067389385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21634895925275988\n",
      "    mean_inference_ms: 1.8103948392218956\n",
      "    mean_raw_obs_processing_ms: 1.5995517522334155\n",
      "  time_since_restore: 26.749440908432007\n",
      "  time_this_iter_s: 7.977398157119751\n",
      "  time_total_s: 26.749440908432007\n",
      "  timers:\n",
      "    learn_throughput: 729.304\n",
      "    learn_time_ms: 5484.68\n",
      "    sample_throughput: 4181.223\n",
      "    sample_time_ms: 956.658\n",
      "    update_time_ms: 4.332\n",
      "  timestamp: 1619703646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.39999999999998\n",
      "  episode_reward_mean: -59.75400000000007\n",
      "  episode_reward_min: -80.40000000000012\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2448859214782715\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02478659525513649\n",
      "          model: {}\n",
      "          policy_loss: -0.06837988644838333\n",
      "          total_loss: 60.31132507324219\n",
      "          vf_explained_var: 0.27534520626068115\n",
      "          vf_loss: 60.36297607421875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.24430513381958\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024360578507184982\n",
      "          model: {}\n",
      "          policy_loss: -0.06889995187520981\n",
      "          total_loss: 6.462377548217773\n",
      "          vf_explained_var: 0.46286576986312866\n",
      "          vf_loss: 6.514833450317383\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.47500000000001\n",
      "    ram_util_percent: 66.40833333333333\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -16.5\n",
      "    pol2: -13.399999999999963\n",
      "  policy_reward_mean:\n",
      "    pol1: -41.47\n",
      "    pol2: -18.28399999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -67.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3985582302035151\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2175277368272934\n",
      "    mean_inference_ms: 1.8302957048251505\n",
      "    mean_raw_obs_processing_ms: 1.6748278099887535\n",
      "  time_since_restore: 26.675606727600098\n",
      "  time_this_iter_s: 7.880611896514893\n",
      "  time_total_s: 26.675606727600098\n",
      "  timers:\n",
      "    learn_throughput: 732.746\n",
      "    learn_time_ms: 5458.915\n",
      "    sample_throughput: 4114.102\n",
      "    sample_time_ms: 972.266\n",
      "    update_time_ms: 4.193\n",
      "  timestamp: 1619703646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -37.499999999999986\n",
      "  episode_reward_mean: -61.22400000000009\n",
      "  episode_reward_min: -90.90000000000015\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2459850311279297\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02468830905854702\n",
      "          model: {}\n",
      "          policy_loss: -0.0642673522233963\n",
      "          total_loss: 76.68006896972656\n",
      "          vf_explained_var: 0.1961810290813446\n",
      "          vf_loss: 76.72766876220703\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2603431940078735\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02416001819074154\n",
      "          model: {}\n",
      "          policy_loss: -0.06834355741739273\n",
      "          total_loss: 8.67955207824707\n",
      "          vf_explained_var: 0.33397144079208374\n",
      "          vf_loss: 8.731587409973145\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.4\n",
      "    ram_util_percent: 66.02727272727273\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -17.5\n",
      "    pol2: -5.700000000000008\n",
      "  policy_reward_mean:\n",
      "    pol1: -42.775\n",
      "    pol2: -18.448999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -73.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.411380675116966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22539599381742007\n",
      "    mean_inference_ms: 1.8431753158098556\n",
      "    mean_raw_obs_processing_ms: 1.7010457295316008\n",
      "  time_since_restore: 26.96674942970276\n",
      "  time_this_iter_s: 7.731087923049927\n",
      "  time_total_s: 26.96674942970276\n",
      "  timers:\n",
      "    learn_throughput: 731.059\n",
      "    learn_time_ms: 5471.512\n",
      "    sample_throughput: 3954.268\n",
      "    sample_time_ms: 1011.565\n",
      "    update_time_ms: 3.129\n",
      "  timestamp: 1619703646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.29999999999999\n",
      "  episode_reward_mean: -57.885000000000076\n",
      "  episode_reward_min: -78.00000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2156085968017578\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018810773268342018\n",
      "          model: {}\n",
      "          policy_loss: -0.06540484726428986\n",
      "          total_loss: 61.25014114379883\n",
      "          vf_explained_var: 0.19794073700904846\n",
      "          vf_loss: 61.29650115966797\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2288047075271606\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018567349761724472\n",
      "          model: {}\n",
      "          policy_loss: -0.05837967246770859\n",
      "          total_loss: 9.131372451782227\n",
      "          vf_explained_var: 0.32790452241897583\n",
      "          vf_loss: 9.170952796936035\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.15\n",
      "    ram_util_percent: 64.4875\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -15.5\n",
      "    pol2: -2.3999999999999995\n",
      "  policy_reward_mean:\n",
      "    pol1: -40.195\n",
      "    pol2: -17.689999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -58.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4373408834670121\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.24482651503911562\n",
      "    mean_inference_ms: 1.9516021641751973\n",
      "    mean_raw_obs_processing_ms: 1.832655853973725\n",
      "  time_since_restore: 32.668559074401855\n",
      "  time_this_iter_s: 5.898546934127808\n",
      "  time_total_s: 32.668559074401855\n",
      "  timers:\n",
      "    learn_throughput: 754.471\n",
      "    learn_time_ms: 5301.728\n",
      "    sample_throughput: 4282.54\n",
      "    sample_time_ms: 934.025\n",
      "    update_time_ms: 5.454\n",
      "  timestamp: 1619703651\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         32.6686</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> -57.885</td><td style=\"text-align: right;\">               -33.3</td><td style=\"text-align: right;\">               -78  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         26.6756</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -59.754</td><td style=\"text-align: right;\">               -35.4</td><td style=\"text-align: right;\">               -80.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         26.7494</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -59.922</td><td style=\"text-align: right;\">               -36  </td><td style=\"text-align: right;\">               -81.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         26.9667</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -61.224</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               -90.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.99999999999998\n",
      "  episode_reward_mean: -58.89900000000008\n",
      "  episode_reward_min: -81.60000000000015\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2248653173446655\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01879333332180977\n",
      "          model: {}\n",
      "          policy_loss: -0.06278271228075027\n",
      "          total_loss: 83.66142272949219\n",
      "          vf_explained_var: 0.16791298985481262\n",
      "          vf_loss: 83.7051773071289\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2243239879608154\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01793172024190426\n",
      "          model: {}\n",
      "          policy_loss: -0.05355294048786163\n",
      "          total_loss: 8.739631652832031\n",
      "          vf_explained_var: 0.3586257994174957\n",
      "          vf_loss: 8.775028228759766\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.8625\n",
      "    ram_util_percent: 64.5625\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -16.0\n",
      "    pol2: -6.800000000000009\n",
      "  policy_reward_mean:\n",
      "    pol1: -40.67\n",
      "    pol2: -18.228999999999967\n",
      "  policy_reward_min:\n",
      "    pol1: -67.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4238542641590098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2316418822107785\n",
      "    mean_inference_ms: 1.9204681814724598\n",
      "    mean_raw_obs_processing_ms: 1.746485691849169\n",
      "  time_since_restore: 32.78783106803894\n",
      "  time_this_iter_s: 6.038390159606934\n",
      "  time_total_s: 32.78783106803894\n",
      "  timers:\n",
      "    learn_throughput: 740.861\n",
      "    learn_time_ms: 5399.121\n",
      "    sample_throughput: 4360.852\n",
      "    sample_time_ms: 917.252\n",
      "    update_time_ms: 4.392\n",
      "  timestamp: 1619703652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.39999999999998\n",
      "  episode_reward_mean: -57.70800000000006\n",
      "  episode_reward_min: -84.00000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2280123233795166\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01673324964940548\n",
      "          model: {}\n",
      "          policy_loss: -0.05899575352668762\n",
      "          total_loss: 69.50953674316406\n",
      "          vf_explained_var: 0.15029862523078918\n",
      "          vf_loss: 69.55159759521484\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2148380279541016\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01970771700143814\n",
      "          model: {}\n",
      "          policy_loss: -0.061516232788562775\n",
      "          total_loss: 7.514415264129639\n",
      "          vf_explained_var: 0.3999420404434204\n",
      "          vf_loss: 7.5559773445129395\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.53333333333333\n",
      "    ram_util_percent: 64.64444444444445\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -16.5\n",
      "    pol2: -7.900000000000008\n",
      "  policy_reward_mean:\n",
      "    pol1: -39.875\n",
      "    pol2: -17.832999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -67.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4305333378167778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23348275397012544\n",
      "    mean_inference_ms: 1.933264952819199\n",
      "    mean_raw_obs_processing_ms: 1.8151098100936758\n",
      "  time_since_restore: 32.71064472198486\n",
      "  time_this_iter_s: 6.035037994384766\n",
      "  time_total_s: 32.71064472198486\n",
      "  timers:\n",
      "    learn_throughput: 744.62\n",
      "    learn_time_ms: 5371.87\n",
      "    sample_throughput: 4269.869\n",
      "    sample_time_ms: 936.797\n",
      "    update_time_ms: 4.308\n",
      "  timestamp: 1619703652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -37.499999999999986\n",
      "  episode_reward_mean: -58.16700000000006\n",
      "  episode_reward_min: -90.90000000000015\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2239415645599365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01979697123169899\n",
      "          model: {}\n",
      "          policy_loss: -0.05772041901946068\n",
      "          total_loss: 58.82630920410156\n",
      "          vf_explained_var: 0.2534714937210083\n",
      "          vf_loss: 58.863983154296875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2495124340057373\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017683710902929306\n",
      "          model: {}\n",
      "          policy_loss: -0.049880288541316986\n",
      "          total_loss: 6.026740074157715\n",
      "          vf_explained_var: 0.4944237470626831\n",
      "          vf_loss: 6.0587158203125\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.81111111111112\n",
      "    ram_util_percent: 64.64444444444445\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -17.5\n",
      "    pol2: -5.700000000000008\n",
      "  policy_reward_mean:\n",
      "    pol1: -40.015\n",
      "    pol2: -18.151999999999965\n",
      "  policy_reward_min:\n",
      "    pol1: -72.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.46446589567383567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2546811167732524\n",
      "    mean_inference_ms: 2.0108870601294826\n",
      "    mean_raw_obs_processing_ms: 1.945578231036874\n",
      "  time_since_restore: 33.16432428359985\n",
      "  time_this_iter_s: 6.197574853897095\n",
      "  time_total_s: 33.16432428359985\n",
      "  timers:\n",
      "    learn_throughput: 744.35\n",
      "    learn_time_ms: 5373.816\n",
      "    sample_throughput: 3920.917\n",
      "    sample_time_ms: 1020.17\n",
      "    update_time_ms: 3.038\n",
      "  timestamp: 1619703652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-40-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -40.199999999999996\n",
      "  episode_reward_mean: -56.87400000000008\n",
      "  episode_reward_min: -81.0000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1985224485397339\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018230445683002472\n",
      "          model: {}\n",
      "          policy_loss: -0.06084809452295303\n",
      "          total_loss: 70.87631225585938\n",
      "          vf_explained_var: 0.2045556902885437\n",
      "          vf_loss: 70.918701171875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2116998434066772\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01913291960954666\n",
      "          model: {}\n",
      "          policy_loss: -0.06388917565345764\n",
      "          total_loss: 9.231274604797363\n",
      "          vf_explained_var: 0.30277374386787415\n",
      "          vf_loss: 9.27579116821289\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.58333333333333\n",
      "    ram_util_percent: 62.21666666666667\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -20.5\n",
      "    pol2: -2.3999999999999995\n",
      "  policy_reward_mean:\n",
      "    pol1: -39.58\n",
      "    pol2: -17.29399999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -61.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4470277020377024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.24862811318490985\n",
      "    mean_inference_ms: 1.9808665488307837\n",
      "    mean_raw_obs_processing_ms: 1.8780144545457993\n",
      "  time_since_restore: 40.66658329963684\n",
      "  time_this_iter_s: 7.998024225234985\n",
      "  time_total_s: 40.66658329963684\n",
      "  timers:\n",
      "    learn_throughput: 720.403\n",
      "    learn_time_ms: 5552.451\n",
      "    sample_throughput: 4291.98\n",
      "    sample_time_ms: 931.971\n",
      "    update_time_ms: 5.618\n",
      "  timestamp: 1619703659\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         40.6666</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> -56.874</td><td style=\"text-align: right;\">               -40.2</td><td style=\"text-align: right;\">               -81  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         32.7106</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> -57.708</td><td style=\"text-align: right;\">               -35.4</td><td style=\"text-align: right;\">               -84  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         32.7878</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> -58.899</td><td style=\"text-align: right;\">               -36  </td><td style=\"text-align: right;\">               -81.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         33.1643</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> -58.167</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               -90.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.29999999999998\n",
      "  episode_reward_mean: -58.45200000000007\n",
      "  episode_reward_min: -81.00000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.205172061920166\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017608027905225754\n",
      "          model: {}\n",
      "          policy_loss: -0.061123598366975784\n",
      "          total_loss: 58.42267608642578\n",
      "          vf_explained_var: 0.23633694648742676\n",
      "          vf_loss: 58.465972900390625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2154531478881836\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01982831582427025\n",
      "          model: {}\n",
      "          policy_loss: -0.06494495272636414\n",
      "          total_loss: 8.223055839538574\n",
      "          vf_explained_var: 0.41037750244140625\n",
      "          vf_loss: 8.267925262451172\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.79166666666666\n",
      "    ram_util_percent: 61.775000000000006\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -16.0\n",
      "    pol2: -6.799999999999982\n",
      "  policy_reward_mean:\n",
      "    pol1: -40.685\n",
      "    pol2: -17.766999999999967\n",
      "  policy_reward_min:\n",
      "    pol1: -67.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4311565921429171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2362980583167319\n",
      "    mean_inference_ms: 1.9364538091746095\n",
      "    mean_raw_obs_processing_ms: 1.783433708867222\n",
      "  time_since_restore: 40.89197778701782\n",
      "  time_this_iter_s: 8.104146718978882\n",
      "  time_total_s: 40.89197778701782\n",
      "  timers:\n",
      "    learn_throughput: 705.13\n",
      "    learn_time_ms: 5672.714\n",
      "    sample_throughput: 4473.432\n",
      "    sample_time_ms: 894.168\n",
      "    update_time_ms: 4.437\n",
      "  timestamp: 1619703660\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.39999999999998\n",
      "  episode_reward_mean: -56.430000000000064\n",
      "  episode_reward_min: -84.00000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2129489183425903\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01609439216554165\n",
      "          model: {}\n",
      "          policy_loss: -0.05043585225939751\n",
      "          total_loss: 65.89459991455078\n",
      "          vf_explained_var: 0.184599369764328\n",
      "          vf_loss: 65.92874145507812\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2128357887268066\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018469229340553284\n",
      "          model: {}\n",
      "          policy_loss: -0.05414968729019165\n",
      "          total_loss: 8.841499328613281\n",
      "          vf_explained_var: 0.410642147064209\n",
      "          vf_loss: 8.876949310302734\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.48181818181819\n",
      "    ram_util_percent: 61.46363636363637\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -16.5\n",
      "    pol2: -2.4000000000000066\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.96\n",
      "    pol2: -17.469999999999967\n",
      "  policy_reward_min:\n",
      "    pol1: -64.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.44129491932061604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23897827692971116\n",
      "    mean_inference_ms: 1.9547132171662964\n",
      "    mean_raw_obs_processing_ms: 1.8544027975627357\n",
      "  time_since_restore: 40.85824704170227\n",
      "  time_this_iter_s: 8.147602319717407\n",
      "  time_total_s: 40.85824704170227\n",
      "  timers:\n",
      "    learn_throughput: 707.645\n",
      "    learn_time_ms: 5652.553\n",
      "    sample_throughput: 4341.489\n",
      "    sample_time_ms: 921.343\n",
      "    update_time_ms: 4.311\n",
      "  timestamp: 1619703660\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.29999999999998\n",
      "  episode_reward_mean: -58.03200000000007\n",
      "  episode_reward_min: -90.90000000000015\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.2118556499481201\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017191685736179352\n",
      "          model: {}\n",
      "          policy_loss: -0.05881150811910629\n",
      "          total_loss: 69.64667510986328\n",
      "          vf_explained_var: 0.19078561663627625\n",
      "          vf_loss: 69.68807983398438\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.23216712474823\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01791689172387123\n",
      "          model: {}\n",
      "          policy_loss: -0.054553549736738205\n",
      "          total_loss: 7.634612083435059\n",
      "          vf_explained_var: 0.3562149405479431\n",
      "          vf_loss: 7.671024322509766\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.5181818181818\n",
      "    ram_util_percent: 61.45454545454545\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -18.5\n",
      "    pol2: -5.700000000000008\n",
      "  policy_reward_mean:\n",
      "    pol1: -39.935\n",
      "    pol2: -18.096999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -72.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.48490566776263977\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2645646493990283\n",
      "    mean_inference_ms: 2.071910599990478\n",
      "    mean_raw_obs_processing_ms: 2.030115996253005\n",
      "  time_since_restore: 41.170894384384155\n",
      "  time_this_iter_s: 8.006570100784302\n",
      "  time_total_s: 41.170894384384155\n",
      "  timers:\n",
      "    learn_throughput: 709.127\n",
      "    learn_time_ms: 5640.737\n",
      "    sample_throughput: 4048.514\n",
      "    sample_time_ms: 988.017\n",
      "    update_time_ms: 2.998\n",
      "  timestamp: 1619703660\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.299999999999976\n",
      "  episode_reward_mean: -55.67100000000005\n",
      "  episode_reward_min: -81.0000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1794216632843018\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017249654978513718\n",
      "          model: {}\n",
      "          policy_loss: -0.05875872075557709\n",
      "          total_loss: 70.01824951171875\n",
      "          vf_explained_var: 0.13863515853881836\n",
      "          vf_loss: 70.05953979492188\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1910345554351807\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0180655550211668\n",
      "          model: {}\n",
      "          policy_loss: -0.06699153780937195\n",
      "          total_loss: 6.362544059753418\n",
      "          vf_explained_var: 0.47859498858451843\n",
      "          vf_loss: 6.4112443923950195\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.13999999999999\n",
      "    ram_util_percent: 60.0\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -15.5\n",
      "    pol2: -2.3999999999999995\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.52\n",
      "    pol2: -17.150999999999968\n",
      "  policy_reward_min:\n",
      "    pol1: -64.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4524355050442025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2511777585930047\n",
      "    mean_inference_ms: 1.9828818845193266\n",
      "    mean_raw_obs_processing_ms: 1.888259966322665\n",
      "  time_since_restore: 48.22524046897888\n",
      "  time_this_iter_s: 7.558657169342041\n",
      "  time_total_s: 48.22524046897888\n",
      "  timers:\n",
      "    learn_throughput: 710.943\n",
      "    learn_time_ms: 5626.33\n",
      "    sample_throughput: 4145.131\n",
      "    sample_time_ms: 964.988\n",
      "    update_time_ms: 5.59\n",
      "  timestamp: 1619703667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         48.2252</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> -55.671</td><td style=\"text-align: right;\">               -33.3</td><td style=\"text-align: right;\">               -81  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         40.8582</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> -56.43 </td><td style=\"text-align: right;\">               -35.4</td><td style=\"text-align: right;\">               -84  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         40.892 </td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> -58.452</td><td style=\"text-align: right;\">               -33.3</td><td style=\"text-align: right;\">               -81  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         41.1709</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> -58.032</td><td style=\"text-align: right;\">               -36.3</td><td style=\"text-align: right;\">               -90.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.29999999999998\n",
      "  episode_reward_mean: -58.53900000000006\n",
      "  episode_reward_min: -81.00000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.186113953590393\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018820296972990036\n",
      "          model: {}\n",
      "          policy_loss: -0.054979607462882996\n",
      "          total_loss: 66.61347198486328\n",
      "          vf_explained_var: 0.1913606822490692\n",
      "          vf_loss: 66.64939880371094\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1971992254257202\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01979505643248558\n",
      "          model: {}\n",
      "          policy_loss: -0.07705191522836685\n",
      "          total_loss: 7.1265411376953125\n",
      "          vf_explained_var: 0.41881421208381653\n",
      "          vf_loss: 7.183550834655762\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.87272727272727\n",
      "    ram_util_percent: 59.9\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -19.5\n",
      "    pol2: -6.799999999999982\n",
      "  policy_reward_mean:\n",
      "    pol1: -40.53\n",
      "    pol2: -18.00899999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -63.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4309085534391294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23746142294286293\n",
      "    mean_inference_ms: 1.9149911627709861\n",
      "    mean_raw_obs_processing_ms: 1.7797922105122816\n",
      "  time_since_restore: 48.41545486450195\n",
      "  time_this_iter_s: 7.523477077484131\n",
      "  time_total_s: 48.41545486450195\n",
      "  timers:\n",
      "    learn_throughput: 696.351\n",
      "    learn_time_ms: 5744.233\n",
      "    sample_throughput: 4355.114\n",
      "    sample_time_ms: 918.46\n",
      "    update_time_ms: 4.668\n",
      "  timestamp: 1619703668\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -37.49999999999998\n",
      "  episode_reward_mean: -56.358000000000054\n",
      "  episode_reward_min: -87.90000000000012\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1942009925842285\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01691635698080063\n",
      "          model: {}\n",
      "          policy_loss: -0.052114494144916534\n",
      "          total_loss: 69.33134460449219\n",
      "          vf_explained_var: 0.15026789903640747\n",
      "          vf_loss: 69.36634063720703\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1967432498931885\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018842345103621483\n",
      "          model: {}\n",
      "          policy_loss: -0.054386984556913376\n",
      "          total_loss: 7.015664100646973\n",
      "          vf_explained_var: 0.44378992915153503\n",
      "          vf_loss: 7.050973415374756\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.9090909090909\n",
      "    ram_util_percent: 59.9\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -17.5\n",
      "    pol2: -2.4000000000000066\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.745\n",
      "    pol2: -17.612999999999964\n",
      "  policy_reward_min:\n",
      "    pol1: -69.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.43952281234846047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23912571914387015\n",
      "    mean_inference_ms: 1.9359420305166506\n",
      "    mean_raw_obs_processing_ms: 1.8491375837429727\n",
      "  time_since_restore: 48.34171915054321\n",
      "  time_this_iter_s: 7.483472108840942\n",
      "  time_total_s: 48.34171915054321\n",
      "  timers:\n",
      "    learn_throughput: 697.508\n",
      "    learn_time_ms: 5734.697\n",
      "    sample_throughput: 4280.131\n",
      "    sample_time_ms: 934.551\n",
      "    update_time_ms: 4.591\n",
      "  timestamp: 1619703668\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.70000000000004\n",
      "  episode_reward_mean: -56.70300000000007\n",
      "  episode_reward_min: -81.00000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.192169189453125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01818525791168213\n",
      "          model: {}\n",
      "          policy_loss: -0.05528649687767029\n",
      "          total_loss: 70.37608337402344\n",
      "          vf_explained_var: 0.1930633932352066\n",
      "          vf_loss: 70.41295623779297\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.206467628479004\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017931632697582245\n",
      "          model: {}\n",
      "          policy_loss: -0.062037382274866104\n",
      "          total_loss: 9.419702529907227\n",
      "          vf_explained_var: 0.3835175633430481\n",
      "          vf_loss: 9.463583946228027\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.89090909090909\n",
      "    ram_util_percent: 59.9\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -18.0\n",
      "    pol2: 2.0000000000000115\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.87\n",
      "    pol2: -17.83299999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -61.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4808997784670923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2610087649409326\n",
      "    mean_inference_ms: 2.0505369002852665\n",
      "    mean_raw_obs_processing_ms: 2.004311856096456\n",
      "  time_since_restore: 48.4941143989563\n",
      "  time_this_iter_s: 7.3232200145721436\n",
      "  time_total_s: 48.4941143989563\n",
      "  timers:\n",
      "    learn_throughput: 699.737\n",
      "    learn_time_ms: 5716.43\n",
      "    sample_throughput: 4074.312\n",
      "    sample_time_ms: 981.761\n",
      "    update_time_ms: 3.024\n",
      "  timestamp: 1619703668\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.299999999999976\n",
      "  episode_reward_mean: -56.43600000000006\n",
      "  episode_reward_min: -81.00000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 320\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1719460487365723\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017236964777112007\n",
      "          model: {}\n",
      "          policy_loss: -0.05449264869093895\n",
      "          total_loss: 67.51435852050781\n",
      "          vf_explained_var: 0.1987788826227188\n",
      "          vf_loss: 67.55140686035156\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1700439453125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015901528298854828\n",
      "          model: {}\n",
      "          policy_loss: -0.04987417161464691\n",
      "          total_loss: 6.414371490478516\n",
      "          vf_explained_var: 0.44614750146865845\n",
      "          vf_loss: 6.448145866394043\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.03999999999999\n",
      "    ram_util_percent: 59.63000000000001\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -15.5\n",
      "    pol2: -6.800000000000006\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.845\n",
      "    pol2: -17.59099999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -64.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.45791003934173374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2537496294685423\n",
      "    mean_inference_ms: 1.9951486042952973\n",
      "    mean_raw_obs_processing_ms: 1.9057187199112986\n",
      "  time_since_restore: 54.599279165267944\n",
      "  time_this_iter_s: 6.3740386962890625\n",
      "  time_total_s: 54.599279165267944\n",
      "  timers:\n",
      "    learn_throughput: 715.411\n",
      "    learn_time_ms: 5591.195\n",
      "    sample_throughput: 4223.442\n",
      "    sample_time_ms: 947.095\n",
      "    update_time_ms: 5.384\n",
      "  timestamp: 1619703673\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         54.5993</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> -56.436</td><td style=\"text-align: right;\">               -33.3</td><td style=\"text-align: right;\">               -81  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         48.3417</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> -56.358</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               -87.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         48.4155</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> -58.539</td><td style=\"text-align: right;\">               -33.3</td><td style=\"text-align: right;\">               -81  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         48.4941</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> -56.703</td><td style=\"text-align: right;\">               -35.7</td><td style=\"text-align: right;\">               -81  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.29999999999998\n",
      "  episode_reward_mean: -57.990000000000066\n",
      "  episode_reward_min: -78.90000000000012\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 320\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1739661693572998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017135027796030045\n",
      "          model: {}\n",
      "          policy_loss: -0.059963442385196686\n",
      "          total_loss: 63.47212600708008\n",
      "          vf_explained_var: 0.1945434808731079\n",
      "          vf_loss: 63.514739990234375\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1844605207443237\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01814110577106476\n",
      "          model: {}\n",
      "          policy_loss: -0.05952300503849983\n",
      "          total_loss: 8.039064407348633\n",
      "          vf_explained_var: 0.39459437131881714\n",
      "          vf_loss: 8.080219268798828\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.83333333333333\n",
      "    ram_util_percent: 59.75555555555555\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -19.5\n",
      "    pol2: -3.500000000000009\n",
      "  policy_reward_mean:\n",
      "    pol1: -39.915\n",
      "    pol2: -18.074999999999967\n",
      "  policy_reward_min:\n",
      "    pol1: -60.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.43225658320491817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23925576950066918\n",
      "    mean_inference_ms: 1.9129727399960346\n",
      "    mean_raw_obs_processing_ms: 1.7873248917364675\n",
      "  time_since_restore: 54.7616286277771\n",
      "  time_this_iter_s: 6.3461737632751465\n",
      "  time_total_s: 54.7616286277771\n",
      "  timers:\n",
      "    learn_throughput: 702.63\n",
      "    learn_time_ms: 5692.894\n",
      "    sample_throughput: 4412.242\n",
      "    sample_time_ms: 906.569\n",
      "    update_time_ms: 4.6\n",
      "  timestamp: 1619703674\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -38.39999999999999\n",
      "  episode_reward_mean: -56.63700000000007\n",
      "  episode_reward_min: -87.90000000000012\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 320\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1769171953201294\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017039068043231964\n",
      "          model: {}\n",
      "          policy_loss: -0.05089005082845688\n",
      "          total_loss: 70.28028869628906\n",
      "          vf_explained_var: 0.18767544627189636\n",
      "          vf_loss: 70.31392669677734\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.175365924835205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018090063706040382\n",
      "          model: {}\n",
      "          policy_loss: -0.06719927489757538\n",
      "          total_loss: 6.63745641708374\n",
      "          vf_explained_var: 0.4735753834247589\n",
      "          vf_loss: 6.686339378356934\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.82222222222224\n",
      "    ram_util_percent: 59.766666666666666\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -19.5\n",
      "    pol2: -2.4000000000000066\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.76\n",
      "    pol2: -17.876999999999967\n",
      "  policy_reward_min:\n",
      "    pol1: -69.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.440833730683449\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.24012071104247767\n",
      "    mean_inference_ms: 1.9344543475512317\n",
      "    mean_raw_obs_processing_ms: 1.8483124410417455\n",
      "  time_since_restore: 54.66123127937317\n",
      "  time_this_iter_s: 6.319512128829956\n",
      "  time_total_s: 54.66123127937317\n",
      "  timers:\n",
      "    learn_throughput: 704.217\n",
      "    learn_time_ms: 5680.07\n",
      "    sample_throughput: 4327.255\n",
      "    sample_time_ms: 924.374\n",
      "    update_time_ms: 4.353\n",
      "  timestamp: 1619703674\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.70000000000004\n",
      "  episode_reward_mean: -55.575000000000074\n",
      "  episode_reward_min: -76.5000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 320\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1664645671844482\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016069285571575165\n",
      "          model: {}\n",
      "          policy_loss: -0.05634508281946182\n",
      "          total_loss: 67.1173095703125\n",
      "          vf_explained_var: 0.16554805636405945\n",
      "          vf_loss: 67.15739440917969\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.2008142471313477\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01760798878967762\n",
      "          model: {}\n",
      "          policy_loss: -0.05331683158874512\n",
      "          total_loss: 6.841431140899658\n",
      "          vf_explained_var: 0.42367061972618103\n",
      "          vf_loss: 6.876919746398926\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.91111111111111\n",
      "    ram_util_percent: 59.766666666666666\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -18.0\n",
      "    pol2: 2.0000000000000115\n",
      "  policy_reward_mean:\n",
      "    pol1: -37.94\n",
      "    pol2: -17.634999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -56.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4737681108088876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.25677788107649313\n",
      "    mean_inference_ms: 2.0301909167764567\n",
      "    mean_raw_obs_processing_ms: 1.9725917535934014\n",
      "  time_since_restore: 54.84333920478821\n",
      "  time_this_iter_s: 6.349224805831909\n",
      "  time_total_s: 54.84333920478821\n",
      "  timers:\n",
      "    learn_throughput: 705.876\n",
      "    learn_time_ms: 5666.714\n",
      "    sample_throughput: 4126.242\n",
      "    sample_time_ms: 969.405\n",
      "    update_time_ms: 2.954\n",
      "  timestamp: 1619703674\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -34.79999999999998\n",
      "  episode_reward_mean: -56.28900000000006\n",
      "  episode_reward_min: -81.00000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 360\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1402552127838135\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01827695593237877\n",
      "          model: {}\n",
      "          policy_loss: -0.06143146753311157\n",
      "          total_loss: 74.89872741699219\n",
      "          vf_explained_var: 0.2264055609703064\n",
      "          vf_loss: 74.94165802001953\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1329857110977173\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018550919368863106\n",
      "          model: {}\n",
      "          policy_loss: -0.06071136146783829\n",
      "          total_loss: 7.45644474029541\n",
      "          vf_explained_var: 0.419964075088501\n",
      "          vf_loss: 7.498373508453369\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.8\n",
      "    ram_util_percent: 62.300000000000004\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -16.0\n",
      "    pol2: -4.600000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.885\n",
      "    pol2: -17.403999999999964\n",
      "  policy_reward_min:\n",
      "    pol1: -61.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4596744422745108\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2542491541513452\n",
      "    mean_inference_ms: 1.991325539780122\n",
      "    mean_raw_obs_processing_ms: 1.9111085368522072\n",
      "  time_since_restore: 60.88524293899536\n",
      "  time_this_iter_s: 6.285963773727417\n",
      "  time_total_s: 60.88524293899536\n",
      "  timers:\n",
      "    learn_throughput: 722.97\n",
      "    learn_time_ms: 5532.735\n",
      "    sample_throughput: 4223.228\n",
      "    sample_time_ms: 947.143\n",
      "    update_time_ms: 5.157\n",
      "  timestamp: 1619703680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         60.8852</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> -56.289</td><td style=\"text-align: right;\">               -34.8</td><td style=\"text-align: right;\">               -81  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         54.6612</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> -56.637</td><td style=\"text-align: right;\">               -38.4</td><td style=\"text-align: right;\">               -87.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         54.7616</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> -57.99 </td><td style=\"text-align: right;\">               -33.3</td><td style=\"text-align: right;\">               -78.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         54.8433</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> -55.575</td><td style=\"text-align: right;\">               -35.7</td><td style=\"text-align: right;\">               -76.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -37.500000000000014\n",
      "  episode_reward_mean: -56.679000000000066\n",
      "  episode_reward_min: -76.5000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 360\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1497597694396973\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017849018797278404\n",
      "          model: {}\n",
      "          policy_loss: -0.06419435888528824\n",
      "          total_loss: 75.64063262939453\n",
      "          vf_explained_var: 0.17273123562335968\n",
      "          vf_loss: 75.68675231933594\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1798253059387207\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01783401519060135\n",
      "          model: {}\n",
      "          policy_loss: -0.06273237615823746\n",
      "          total_loss: 9.551460266113281\n",
      "          vf_explained_var: 0.3036649227142334\n",
      "          vf_loss: 9.596135139465332\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.52222222222223\n",
      "    ram_util_percent: 62.400000000000006\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -20.5\n",
      "    pol2: -5.699999999999972\n",
      "  policy_reward_mean:\n",
      "    pol1: -39.22\n",
      "    pol2: -17.45899999999996\n",
      "  policy_reward_min:\n",
      "    pol1: -61.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.465288664426769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.25193645625502303\n",
      "    mean_inference_ms: 1.999755653690854\n",
      "    mean_raw_obs_processing_ms: 1.9314731463698198\n",
      "  time_since_restore: 60.88018035888672\n",
      "  time_this_iter_s: 6.036841154098511\n",
      "  time_total_s: 60.88018035888672\n",
      "  timers:\n",
      "    learn_throughput: 713.668\n",
      "    learn_time_ms: 5604.85\n",
      "    sample_throughput: 4256.837\n",
      "    sample_time_ms: 939.665\n",
      "    update_time_ms: 2.937\n",
      "  timestamp: 1619703680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.999999999999986\n",
      "  episode_reward_mean: -57.855000000000075\n",
      "  episode_reward_min: -90.00000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 360\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1724495887756348\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01764662377536297\n",
      "          model: {}\n",
      "          policy_loss: -0.059893347322940826\n",
      "          total_loss: 70.37356567382812\n",
      "          vf_explained_var: 0.17505621910095215\n",
      "          vf_loss: 70.41558837890625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.170584797859192\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018299825489521027\n",
      "          model: {}\n",
      "          policy_loss: -0.052722059190273285\n",
      "          total_loss: 6.7446746826171875\n",
      "          vf_explained_var: 0.4320705533027649\n",
      "          vf_loss: 6.778867721557617\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.56666666666666\n",
      "    ram_util_percent: 62.400000000000006\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -16.0\n",
      "    pol2: -3.500000000000009\n",
      "  policy_reward_mean:\n",
      "    pol1: -39.67\n",
      "    pol2: -18.184999999999967\n",
      "  policy_reward_min:\n",
      "    pol1: -70.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.43092432556582294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23846811777126284\n",
      "    mean_inference_ms: 1.8993231389472283\n",
      "    mean_raw_obs_processing_ms: 1.781623672737428\n",
      "  time_since_restore: 60.886088371276855\n",
      "  time_this_iter_s: 6.124459743499756\n",
      "  time_total_s: 60.886088371276855\n",
      "  timers:\n",
      "    learn_throughput: 710.227\n",
      "    learn_time_ms: 5632.001\n",
      "    sample_throughput: 4482.879\n",
      "    sample_time_ms: 892.284\n",
      "    update_time_ms: 4.381\n",
      "  timestamp: 1619703680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.999999999999986\n",
      "  episode_reward_mean: -56.04900000000007\n",
      "  episode_reward_min: -76.80000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 360\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.169323444366455\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01775728352367878\n",
      "          model: {}\n",
      "          policy_loss: -0.05368635803461075\n",
      "          total_loss: 77.3713150024414\n",
      "          vf_explained_var: 0.13910771906375885\n",
      "          vf_loss: 77.40702819824219\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.152648687362671\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017677024006843567\n",
      "          model: {}\n",
      "          policy_loss: -0.055067360401153564\n",
      "          total_loss: 11.505033493041992\n",
      "          vf_explained_var: 0.3489765524864197\n",
      "          vf_loss: 11.542203903198242\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.56666666666666\n",
      "    ram_util_percent: 62.400000000000006\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -19.5\n",
      "    pol2: 5.300000000000024\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.425\n",
      "    pol2: -17.623999999999967\n",
      "  policy_reward_min:\n",
      "    pol1: -59.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4384469811051176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23889636240233147\n",
      "    mean_inference_ms: 1.9208841672032952\n",
      "    mean_raw_obs_processing_ms: 1.8344563928693036\n",
      "  time_since_restore: 60.76967930793762\n",
      "  time_this_iter_s: 6.108448028564453\n",
      "  time_total_s: 60.76967930793762\n",
      "  timers:\n",
      "    learn_throughput: 711.614\n",
      "    learn_time_ms: 5621.024\n",
      "    sample_throughput: 4412.587\n",
      "    sample_time_ms: 906.498\n",
      "    update_time_ms: 4.29\n",
      "  timestamp: 1619703680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -34.79999999999998\n",
      "  episode_reward_mean: -56.50200000000007\n",
      "  episode_reward_min: -81.0000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 400\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1151700019836426\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017267737537622452\n",
      "          model: {}\n",
      "          policy_loss: -0.05193793773651123\n",
      "          total_loss: 61.492671966552734\n",
      "          vf_explained_var: 0.20684102177619934\n",
      "          vf_loss: 61.527130126953125\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1226301193237305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01738419570028782\n",
      "          model: {}\n",
      "          policy_loss: -0.05602315068244934\n",
      "          total_loss: 6.78670597076416\n",
      "          vf_explained_var: 0.46697819232940674\n",
      "          vf_loss: 6.825127601623535\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.7125\n",
      "    ram_util_percent: 63.275\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -17.5\n",
      "    pol2: -3.5000000000000053\n",
      "  policy_reward_mean:\n",
      "    pol1: -39.065\n",
      "    pol2: -17.436999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -61.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.45745287063336443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2526061130072826\n",
      "    mean_inference_ms: 1.9700490366721743\n",
      "    mean_raw_obs_processing_ms: 1.9016075451541792\n",
      "  time_since_restore: 66.90926790237427\n",
      "  time_this_iter_s: 6.024024963378906\n",
      "  time_total_s: 66.90926790237427\n",
      "  timers:\n",
      "    learn_throughput: 730.517\n",
      "    learn_time_ms: 5475.577\n",
      "    sample_throughput: 4273.872\n",
      "    sample_time_ms: 935.92\n",
      "    update_time_ms: 5.139\n",
      "  timestamp: 1619703686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         66.9093</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> -56.502</td><td style=\"text-align: right;\">               -34.8</td><td style=\"text-align: right;\">               -81  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         60.7697</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> -56.049</td><td style=\"text-align: right;\">               -36  </td><td style=\"text-align: right;\">               -76.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         60.8861</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> -57.855</td><td style=\"text-align: right;\">               -36  </td><td style=\"text-align: right;\">               -90  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         60.8802</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> -56.679</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               -76.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -37.500000000000014\n",
      "  episode_reward_mean: -56.679000000000066\n",
      "  episode_reward_min: -76.5000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 400\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.137955665588379\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017595399171113968\n",
      "          model: {}\n",
      "          policy_loss: -0.05801450461149216\n",
      "          total_loss: 81.1055908203125\n",
      "          vf_explained_var: 0.11618998646736145\n",
      "          vf_loss: 81.14579010009766\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1557631492614746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019097749143838882\n",
      "          model: {}\n",
      "          policy_loss: -0.05935549736022949\n",
      "          total_loss: 10.385112762451172\n",
      "          vf_explained_var: 0.3816404640674591\n",
      "          vf_loss: 10.425131797790527\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.85000000000001\n",
      "    ram_util_percent: 63.2\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -20.5\n",
      "    pol2: -0.19999999999998352\n",
      "  policy_reward_mean:\n",
      "    pol1: -39.715\n",
      "    pol2: -16.963999999999967\n",
      "  policy_reward_min:\n",
      "    pol1: -61.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4558801208365641\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2466925945212082\n",
      "    mean_inference_ms: 1.9566521337552576\n",
      "    mean_raw_obs_processing_ms: 1.8894885331854028\n",
      "  time_since_restore: 66.8704743385315\n",
      "  time_this_iter_s: 5.990293979644775\n",
      "  time_total_s: 66.8704743385315\n",
      "  timers:\n",
      "    learn_throughput: 721.316\n",
      "    learn_time_ms: 5545.421\n",
      "    sample_throughput: 4337.288\n",
      "    sample_time_ms: 922.235\n",
      "    update_time_ms: 3.197\n",
      "  timestamp: 1619703686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.999999999999986\n",
      "  episode_reward_mean: -56.16000000000006\n",
      "  episode_reward_min: -90.00000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 400\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1330084800720215\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0190021600574255\n",
      "          model: {}\n",
      "          policy_loss: -0.058174096047878265\n",
      "          total_loss: 57.471656799316406\n",
      "          vf_explained_var: 0.2252703458070755\n",
      "          vf_loss: 57.510589599609375\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.148437261581421\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01809656247496605\n",
      "          model: {}\n",
      "          policy_loss: -0.0566730871796608\n",
      "          total_loss: 6.804785251617432\n",
      "          vf_explained_var: 0.4097687602043152\n",
      "          vf_loss: 6.843135833740234\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.762499999999996\n",
      "    ram_util_percent: 63.2125\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -16.0\n",
      "    pol2: -7.899999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.14\n",
      "    pol2: -18.019999999999968\n",
      "  policy_reward_min:\n",
      "    pol1: -70.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42539023188533237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23516152861215908\n",
      "    mean_inference_ms: 1.863878921069953\n",
      "    mean_raw_obs_processing_ms: 1.7556775317944417\n",
      "  time_since_restore: 66.86901664733887\n",
      "  time_this_iter_s: 5.982928276062012\n",
      "  time_total_s: 66.86901664733887\n",
      "  timers:\n",
      "    learn_throughput: 717.348\n",
      "    learn_time_ms: 5576.09\n",
      "    sample_throughput: 4575.808\n",
      "    sample_time_ms: 874.163\n",
      "    update_time_ms: 4.284\n",
      "  timestamp: 1619703686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -34.199999999999974\n",
      "  episode_reward_mean: -55.66200000000007\n",
      "  episode_reward_min: -77.1000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 400\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1456950902938843\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016825228929519653\n",
      "          model: {}\n",
      "          policy_loss: -0.053483881056308746\n",
      "          total_loss: 74.53015899658203\n",
      "          vf_explained_var: 0.17134372889995575\n",
      "          vf_loss: 74.56660461425781\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.120850682258606\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017448583617806435\n",
      "          model: {}\n",
      "          policy_loss: -0.05170867592096329\n",
      "          total_loss: 6.985192775726318\n",
      "          vf_explained_var: 0.4391814172267914\n",
      "          vf_loss: 7.019234657287598\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.8\n",
      "    ram_util_percent: 63.2125\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -17.0\n",
      "    pol2: 5.300000000000024\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.39\n",
      "    pol2: -17.271999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -61.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4312484190025775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23523205494847624\n",
      "    mean_inference_ms: 1.8839016987775006\n",
      "    mean_raw_obs_processing_ms: 1.8004762298377046\n",
      "  time_since_restore: 66.70895624160767\n",
      "  time_this_iter_s: 5.939276933670044\n",
      "  time_total_s: 66.70895624160767\n",
      "  timers:\n",
      "    learn_throughput: 718.805\n",
      "    learn_time_ms: 5564.794\n",
      "    sample_throughput: 4529.094\n",
      "    sample_time_ms: 883.179\n",
      "    update_time_ms: 4.224\n",
      "  timestamp: 1619703686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -37.499999999999986\n",
      "  episode_reward_mean: -56.11200000000006\n",
      "  episode_reward_min: -75.0000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 440\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1035401821136475\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01817264035344124\n",
      "          model: {}\n",
      "          policy_loss: -0.06331721693277359\n",
      "          total_loss: 53.947364807128906\n",
      "          vf_explained_var: 0.19582703709602356\n",
      "          vf_loss: 53.992286682128906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0979588031768799\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01779167726635933\n",
      "          model: {}\n",
      "          policy_loss: -0.0547010563313961\n",
      "          total_loss: 6.320467948913574\n",
      "          vf_explained_var: 0.5273592472076416\n",
      "          vf_loss: 6.357154846191406\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.74444444444445\n",
      "    ram_util_percent: 62.5\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -19.5\n",
      "    pol2: -3.5000000000000053\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.73\n",
      "    pol2: -17.381999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -55.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.45548637088070704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2513366784956271\n",
      "    mean_inference_ms: 1.9485440857172072\n",
      "    mean_raw_obs_processing_ms: 1.8930784776668959\n",
      "  time_since_restore: 72.82167482376099\n",
      "  time_this_iter_s: 5.912406921386719\n",
      "  time_total_s: 72.82167482376099\n",
      "  timers:\n",
      "    learn_throughput: 727.948\n",
      "    learn_time_ms: 5494.897\n",
      "    sample_throughput: 4195.997\n",
      "    sample_time_ms: 953.29\n",
      "    update_time_ms: 5.255\n",
      "  timestamp: 1619703692\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         72.8217</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\"> -56.112</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               -75  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         66.709 </td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> -55.662</td><td style=\"text-align: right;\">               -34.2</td><td style=\"text-align: right;\">               -77.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         66.869 </td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> -56.16 </td><td style=\"text-align: right;\">               -36  </td><td style=\"text-align: right;\">               -90  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         66.8705</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> -56.679</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               -76.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -34.199999999999974\n",
      "  episode_reward_mean: -53.79900000000005\n",
      "  episode_reward_min: -77.1000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 440\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1288137435913086\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017578192055225372\n",
      "          model: {}\n",
      "          policy_loss: -0.05381487309932709\n",
      "          total_loss: 55.07358932495117\n",
      "          vf_explained_var: 0.17492404580116272\n",
      "          vf_loss: 55.10960388183594\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.116856575012207\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016753138974308968\n",
      "          model: {}\n",
      "          policy_loss: -0.048221979290246964\n",
      "          total_loss: 8.335115432739258\n",
      "          vf_explained_var: 0.4171212911605835\n",
      "          vf_loss: 8.366374969482422\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.0375\n",
      "    ram_util_percent: 62.45\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -17.0\n",
      "    pol2: -4.599999999999993\n",
      "  policy_reward_mean:\n",
      "    pol1: -36.725\n",
      "    pol2: -17.073999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -61.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4232455268363185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23079260245684233\n",
      "    mean_inference_ms: 1.842356770683281\n",
      "    mean_raw_obs_processing_ms: 1.7658261113823313\n",
      "  time_since_restore: 72.3747673034668\n",
      "  time_this_iter_s: 5.665811061859131\n",
      "  time_total_s: 72.3747673034668\n",
      "  timers:\n",
      "    learn_throughput: 723.246\n",
      "    learn_time_ms: 5530.62\n",
      "    sample_throughput: 4572.03\n",
      "    sample_time_ms: 874.885\n",
      "    update_time_ms: 4.244\n",
      "  timestamp: 1619703692\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -38.70000000000005\n",
      "  episode_reward_mean: -56.28300000000006\n",
      "  episode_reward_min: -75.3000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 440\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1253412961959839\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0167127326130867\n",
      "          model: {}\n",
      "          policy_loss: -0.05345502495765686\n",
      "          total_loss: 74.9169921875\n",
      "          vf_explained_var: 0.10543835163116455\n",
      "          vf_loss: 74.95352172851562\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1457974910736084\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017060501500964165\n",
      "          model: {}\n",
      "          policy_loss: -0.05593540146946907\n",
      "          total_loss: 8.618722915649414\n",
      "          vf_explained_var: 0.43928787112236023\n",
      "          vf_loss: 8.657384872436523\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.75555555555555\n",
      "    ram_util_percent: 62.35555555555556\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -22.0\n",
      "    pol2: 2.0000000000000044\n",
      "  policy_reward_mean:\n",
      "    pol1: -39.77\n",
      "    pol2: -16.51299999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -61.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4464788576993906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2415293768695799\n",
      "    mean_inference_ms: 1.9111100156114378\n",
      "    mean_raw_obs_processing_ms: 1.8483815391880947\n",
      "  time_since_restore: 72.68251729011536\n",
      "  time_this_iter_s: 5.812042951583862\n",
      "  time_total_s: 72.68251729011536\n",
      "  timers:\n",
      "    learn_throughput: 723.528\n",
      "    learn_time_ms: 5528.466\n",
      "    sample_throughput: 4384.475\n",
      "    sample_time_ms: 912.31\n",
      "    update_time_ms: 3.116\n",
      "  timestamp: 1619703692\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -40.80000000000004\n",
      "  episode_reward_mean: -56.79000000000005\n",
      "  episode_reward_min: -90.00000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 440\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1235499382019043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01647169515490532\n",
      "          model: {}\n",
      "          policy_loss: -0.05505962297320366\n",
      "          total_loss: 60.727821350097656\n",
      "          vf_explained_var: 0.191221684217453\n",
      "          vf_loss: 60.766204833984375\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1409165859222412\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01784278266131878\n",
      "          model: {}\n",
      "          policy_loss: -0.0490930937230587\n",
      "          total_loss: 6.48598575592041\n",
      "          vf_explained_var: 0.493736207485199\n",
      "          vf_loss: 6.5170135498046875\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.666666666666664\n",
      "    ram_util_percent: 62.35555555555556\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -22.0\n",
      "    pol2: -7.899999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: -39.155\n",
      "    pol2: -17.63499999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -70.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4183329320514872\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23121943191598285\n",
      "    mean_inference_ms: 1.823539309816259\n",
      "    mean_raw_obs_processing_ms: 1.7234571036885646\n",
      "  time_since_restore: 72.67139554023743\n",
      "  time_this_iter_s: 5.80237889289856\n",
      "  time_total_s: 72.67139554023743\n",
      "  timers:\n",
      "    learn_throughput: 718.733\n",
      "    learn_time_ms: 5565.347\n",
      "    sample_throughput: 4648.43\n",
      "    sample_time_ms: 860.506\n",
      "    update_time_ms: 4.198\n",
      "  timestamp: 1619703692\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -38.39999999999999\n",
      "  episode_reward_mean: -52.16100000000005\n",
      "  episode_reward_min: -72.30000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 480\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.1091678142547607\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01576945185661316\n",
      "          model: {}\n",
      "          policy_loss: -0.05128895491361618\n",
      "          total_loss: 58.3338508605957\n",
      "          vf_explained_var: 0.17385871708393097\n",
      "          vf_loss: 58.369171142578125\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1174399852752686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01677548699080944\n",
      "          model: {}\n",
      "          policy_loss: -0.04407854378223419\n",
      "          total_loss: 6.136851787567139\n",
      "          vf_explained_var: 0.5045318603515625\n",
      "          vf_loss: 6.163945198059082\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.044444444444444\n",
      "    ram_util_percent: 61.044444444444444\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -19.5\n",
      "    pol2: -4.599999999999993\n",
      "  policy_reward_mean:\n",
      "    pol1: -35.505\n",
      "    pol2: -16.65599999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -54.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4158815014700562\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22674040797735295\n",
      "    mean_inference_ms: 1.805978772106389\n",
      "    mean_raw_obs_processing_ms: 1.7342229147688357\n",
      "  time_since_restore: 78.00675225257874\n",
      "  time_this_iter_s: 5.6319849491119385\n",
      "  time_total_s: 78.00675225257874\n",
      "  timers:\n",
      "    learn_throughput: 721.87\n",
      "    learn_time_ms: 5541.165\n",
      "    sample_throughput: 4638.332\n",
      "    sample_time_ms: 862.379\n",
      "    update_time_ms: 4.462\n",
      "  timestamp: 1619703698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         72.8217</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\"> -56.112</td><td style=\"text-align: right;\">               -37.5</td><td style=\"text-align: right;\">               -75  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         78.0068</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -52.161</td><td style=\"text-align: right;\">               -38.4</td><td style=\"text-align: right;\">               -72.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         72.6714</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\"> -56.79 </td><td style=\"text-align: right;\">               -40.8</td><td style=\"text-align: right;\">               -90  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         72.6825</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\"> -56.283</td><td style=\"text-align: right;\">               -38.7</td><td style=\"text-align: right;\">               -75.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -34.199999999999974\n",
      "  episode_reward_mean: -55.39500000000005\n",
      "  episode_reward_min: -81.0000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 480\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0768318176269531\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01763078197836876\n",
      "          model: {}\n",
      "          policy_loss: -0.05409485101699829\n",
      "          total_loss: 68.98703002929688\n",
      "          vf_explained_var: 0.18389922380447388\n",
      "          vf_loss: 69.02326965332031\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.073373794555664\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016299687325954437\n",
      "          model: {}\n",
      "          policy_loss: -0.054439477622509\n",
      "          total_loss: 6.525823593139648\n",
      "          vf_explained_var: 0.4956984519958496\n",
      "          vf_loss: 6.5637593269348145\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.30000000000001\n",
      "    ram_util_percent: 61.075\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -22.5\n",
      "    pol2: -4.600000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.75\n",
      "    pol2: -16.64499999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -61.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.45275188134112326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.24954903179190716\n",
      "    mean_inference_ms: 1.9263824461517356\n",
      "    mean_raw_obs_processing_ms: 1.880886354980791\n",
      "  time_since_restore: 78.65839266777039\n",
      "  time_this_iter_s: 5.836717844009399\n",
      "  time_total_s: 78.65839266777039\n",
      "  timers:\n",
      "    learn_throughput: 727.933\n",
      "    learn_time_ms: 5495.01\n",
      "    sample_throughput: 4204.584\n",
      "    sample_time_ms: 951.343\n",
      "    update_time_ms: 5.198\n",
      "  timestamp: 1619703698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.39999999999998\n",
      "  episode_reward_mean: -56.430000000000064\n",
      "  episode_reward_min: -78.00000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 480\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.100325345993042\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016595203429460526\n",
      "          model: {}\n",
      "          policy_loss: -0.05288269370794296\n",
      "          total_loss: 101.96937561035156\n",
      "          vf_explained_var: 0.11419353634119034\n",
      "          vf_loss: 102.00545501708984\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1383557319641113\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01577388308942318\n",
      "          model: {}\n",
      "          policy_loss: -0.047373369336128235\n",
      "          total_loss: 15.778626441955566\n",
      "          vf_explained_var: 0.33961135149002075\n",
      "          vf_loss: 15.810028076171875\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.025\n",
      "    ram_util_percent: 60.974999999999994\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -16.5\n",
      "    pol2: 10.800000000000013\n",
      "  policy_reward_mean:\n",
      "    pol1: -40.555\n",
      "    pol2: -15.87499999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -70.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4378061728470611\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2369350376191411\n",
      "    mean_inference_ms: 1.8705020060774167\n",
      "    mean_raw_obs_processing_ms: 1.8106600058479279\n",
      "  time_since_restore: 78.4572582244873\n",
      "  time_this_iter_s: 5.774740934371948\n",
      "  time_total_s: 78.4572582244873\n",
      "  timers:\n",
      "    learn_throughput: 721.937\n",
      "    learn_time_ms: 5540.65\n",
      "    sample_throughput: 4416.282\n",
      "    sample_time_ms: 905.739\n",
      "    update_time_ms: 3.092\n",
      "  timestamp: 1619703698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -29.99999999999996\n",
      "  episode_reward_mean: -55.14300000000005\n",
      "  episode_reward_min: -79.5000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 480\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0945185422897339\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018036164343357086\n",
      "          model: {}\n",
      "          policy_loss: -0.0560496523976326\n",
      "          total_loss: 57.633670806884766\n",
      "          vf_explained_var: 0.20934708416461945\n",
      "          vf_loss: 57.67145919799805\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1102731227874756\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01773843541741371\n",
      "          model: {}\n",
      "          policy_loss: -0.059498101472854614\n",
      "          total_loss: 5.616543292999268\n",
      "          vf_explained_var: 0.5163945555686951\n",
      "          vf_loss: 5.658081531524658\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.824999999999996\n",
      "    ram_util_percent: 60.974999999999994\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -15.5\n",
      "    pol2: -7.899999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: -37.64\n",
      "    pol2: -17.50299999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -64.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41124491444313266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22718081713305893\n",
      "    mean_inference_ms: 1.7869205380455362\n",
      "    mean_raw_obs_processing_ms: 1.6922738191666542\n",
      "  time_since_restore: 78.42595148086548\n",
      "  time_this_iter_s: 5.754555940628052\n",
      "  time_total_s: 78.42595148086548\n",
      "  timers:\n",
      "    learn_throughput: 716.514\n",
      "    learn_time_ms: 5582.586\n",
      "    sample_throughput: 4684.135\n",
      "    sample_time_ms: 853.946\n",
      "    update_time_ms: 4.14\n",
      "  timestamp: 1619703698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.299999999999972\n",
      "  episode_reward_mean: -50.90700000000005\n",
      "  episode_reward_min: -72.30000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 520\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0949629545211792\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016567997634410858\n",
      "          model: {}\n",
      "          policy_loss: -0.046472858637571335\n",
      "          total_loss: 65.45336151123047\n",
      "          vf_explained_var: 0.12555423378944397\n",
      "          vf_loss: 65.48306274414062\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0943676233291626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015525644645094872\n",
      "          model: {}\n",
      "          policy_loss: -0.05354166775941849\n",
      "          total_loss: 9.690729141235352\n",
      "          vf_explained_var: 0.391116738319397\n",
      "          vf_loss: 9.728550910949707\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.160000000000004\n",
      "    ram_util_percent: 61.42999999999999\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -9.5\n",
      "    pol2: 2.0000000000000164\n",
      "  policy_reward_mean:\n",
      "    pol1: -34.625\n",
      "    pol2: -16.28199999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -55.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41271384040555176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2250576707164366\n",
      "    mean_inference_ms: 1.7906648836645456\n",
      "    mean_raw_obs_processing_ms: 1.7194252860493073\n",
      "  time_since_restore: 85.13884091377258\n",
      "  time_this_iter_s: 7.132088661193848\n",
      "  time_total_s: 85.13884091377258\n",
      "  timers:\n",
      "    learn_throughput: 721.923\n",
      "    learn_time_ms: 5540.758\n",
      "    sample_throughput: 4594.869\n",
      "    sample_time_ms: 870.536\n",
      "    update_time_ms: 4.337\n",
      "  timestamp: 1619703705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         78.6584</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -55.395</td><td style=\"text-align: right;\">               -34.2</td><td style=\"text-align: right;\">               -81  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         85.1388</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -50.907</td><td style=\"text-align: right;\">               -27.3</td><td style=\"text-align: right;\">               -72.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         78.426 </td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -55.143</td><td style=\"text-align: right;\">               -30  </td><td style=\"text-align: right;\">               -79.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         78.4573</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -56.43 </td><td style=\"text-align: right;\">               -35.4</td><td style=\"text-align: right;\">               -78  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.899999999999977\n",
      "  episode_reward_mean: -54.71400000000005\n",
      "  episode_reward_min: -81.0000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 520\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0471458435058594\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01586918532848358\n",
      "          model: {}\n",
      "          policy_loss: -0.0489836186170578\n",
      "          total_loss: 81.99269104003906\n",
      "          vf_explained_var: 0.16763535141944885\n",
      "          vf_loss: 82.02560424804688\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0523583889007568\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01609680987894535\n",
      "          model: {}\n",
      "          policy_loss: -0.04995109885931015\n",
      "          total_loss: 17.191287994384766\n",
      "          vf_explained_var: 0.3401585519313812\n",
      "          vf_loss: 17.22494125366211\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.15454545454546\n",
      "    ram_util_percent: 61.40909090909091\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -22.5\n",
      "    pol2: 19.600000000000023\n",
      "  policy_reward_mean:\n",
      "    pol1: -39.015\n",
      "    pol2: -15.69899999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -61.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.451139476373972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2485532663777167\n",
      "    mean_inference_ms: 1.9146449476842236\n",
      "    mean_raw_obs_processing_ms: 1.8738927092025466\n",
      "  time_since_restore: 86.00871658325195\n",
      "  time_this_iter_s: 7.350323915481567\n",
      "  time_total_s: 86.00871658325195\n",
      "  timers:\n",
      "    learn_throughput: 723.996\n",
      "    learn_time_ms: 5524.89\n",
      "    sample_throughput: 4151.577\n",
      "    sample_time_ms: 963.489\n",
      "    update_time_ms: 4.7\n",
      "  timestamp: 1619703705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.899999999999984\n",
      "  episode_reward_mean: -55.15500000000006\n",
      "  episode_reward_min: -88.80000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 520\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.089387059211731\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01636091247200966\n",
      "          model: {}\n",
      "          policy_loss: -0.056205615401268005\n",
      "          total_loss: 93.79116821289062\n",
      "          vf_explained_var: 0.1359013020992279\n",
      "          vf_loss: 93.830810546875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1144976615905762\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016052037477493286\n",
      "          model: {}\n",
      "          policy_loss: -0.048681098967790604\n",
      "          total_loss: 11.779009819030762\n",
      "          vf_explained_var: 0.4297510087490082\n",
      "          vf_loss: 11.81143856048584\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.4\n",
      "    ram_util_percent: 61.42999999999999\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -15.0\n",
      "    pol2: 10.800000000000013\n",
      "  policy_reward_mean:\n",
      "    pol1: -40.655\n",
      "    pol2: -14.499999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -76.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.43255937805752337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23429181952148126\n",
      "    mean_inference_ms: 1.8489178980899283\n",
      "    mean_raw_obs_processing_ms: 1.7885295916464687\n",
      "  time_since_restore: 85.73404216766357\n",
      "  time_this_iter_s: 7.2767839431762695\n",
      "  time_total_s: 85.73404216766357\n",
      "  timers:\n",
      "    learn_throughput: 723.038\n",
      "    learn_time_ms: 5532.213\n",
      "    sample_throughput: 4408.517\n",
      "    sample_time_ms: 907.335\n",
      "    update_time_ms: 3.097\n",
      "  timestamp: 1619703705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -29.99999999999996\n",
      "  episode_reward_mean: -54.59400000000006\n",
      "  episode_reward_min: -82.50000000000014\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 520\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0747621059417725\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017458677291870117\n",
      "          model: {}\n",
      "          policy_loss: -0.06040452420711517\n",
      "          total_loss: 70.20108032226562\n",
      "          vf_explained_var: 0.18703240156173706\n",
      "          vf_loss: 70.24381256103516\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0912792682647705\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017733074724674225\n",
      "          model: {}\n",
      "          policy_loss: -0.054618291556835175\n",
      "          total_loss: 6.166773319244385\n",
      "          vf_explained_var: 0.4288647174835205\n",
      "          vf_loss: 6.203436851501465\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.14\n",
      "    ram_util_percent: 61.42999999999999\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -15.5\n",
      "    pol2: -9.000000000000004\n",
      "  policy_reward_mean:\n",
      "    pol1: -36.97\n",
      "    pol2: -17.623999999999963\n",
      "  policy_reward_min:\n",
      "    pol1: -62.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40774175429687104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22514377751080997\n",
      "    mean_inference_ms: 1.7700606076532006\n",
      "    mean_raw_obs_processing_ms: 1.676832073711255\n",
      "  time_since_restore: 85.66488337516785\n",
      "  time_this_iter_s: 7.238931894302368\n",
      "  time_total_s: 85.66488337516785\n",
      "  timers:\n",
      "    learn_throughput: 716.331\n",
      "    learn_time_ms: 5584.013\n",
      "    sample_throughput: 4632.315\n",
      "    sample_time_ms: 863.499\n",
      "    update_time_ms: 4.116\n",
      "  timestamp: 1619703705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.299999999999972\n",
      "  episode_reward_mean: -49.02300000000003\n",
      "  episode_reward_min: -72.30000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 560\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0602049827575684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015645895153284073\n",
      "          model: {}\n",
      "          policy_loss: -0.044475257396698\n",
      "          total_loss: 57.514564514160156\n",
      "          vf_explained_var: 0.09111350774765015\n",
      "          vf_loss: 57.54319381713867\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0872372388839722\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01559516228735447\n",
      "          model: {}\n",
      "          policy_loss: -0.047281913459300995\n",
      "          total_loss: 7.618687629699707\n",
      "          vf_explained_var: 0.36540836095809937\n",
      "          vf_loss: 7.650179386138916\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.08888888888888\n",
      "    ram_util_percent: 60.955555555555556\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -9.5\n",
      "    pol2: 2.0000000000000164\n",
      "  policy_reward_mean:\n",
      "    pol1: -32.675\n",
      "    pol2: -16.347999999999967\n",
      "  policy_reward_min:\n",
      "    pol1: -55.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4139526161594161\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2256527815802165\n",
      "    mean_inference_ms: 1.8003370456025636\n",
      "    mean_raw_obs_processing_ms: 1.7170948182781565\n",
      "  time_since_restore: 91.46990013122559\n",
      "  time_this_iter_s: 6.331059217453003\n",
      "  time_total_s: 91.46990013122559\n",
      "  timers:\n",
      "    learn_throughput: 735.816\n",
      "    learn_time_ms: 5436.143\n",
      "    sample_throughput: 4882.712\n",
      "    sample_time_ms: 819.217\n",
      "    update_time_ms: 4.257\n",
      "  timestamp: 1619703711\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         86.0087</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -54.714</td><td style=\"text-align: right;\">               -27.9</td><td style=\"text-align: right;\">               -81  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         91.4699</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> -49.023</td><td style=\"text-align: right;\">               -27.3</td><td style=\"text-align: right;\">               -72.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         85.6649</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -54.594</td><td style=\"text-align: right;\">               -30  </td><td style=\"text-align: right;\">               -82.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         85.734 </td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -55.155</td><td style=\"text-align: right;\">               -33.9</td><td style=\"text-align: right;\">               -88.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.899999999999977\n",
      "  episode_reward_mean: -55.10400000000005\n",
      "  episode_reward_min: -82.50000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 560\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0497357845306396\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016114089637994766\n",
      "          model: {}\n",
      "          policy_loss: -0.06135372072458267\n",
      "          total_loss: 71.85824584960938\n",
      "          vf_explained_var: 0.1468425989151001\n",
      "          vf_loss: 71.90328216552734\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0292325019836426\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01615051180124283\n",
      "          model: {}\n",
      "          policy_loss: -0.04870440438389778\n",
      "          total_loss: 10.234282493591309\n",
      "          vf_explained_var: 0.3791511654853821\n",
      "          vf_loss: 10.266634941101074\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.5\n",
      "    ram_util_percent: 61.0\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -20.5\n",
      "    pol2: 19.600000000000023\n",
      "  policy_reward_mean:\n",
      "    pol1: -39.46\n",
      "    pol2: -15.643999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -62.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.44886763926002343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2474540388143764\n",
      "    mean_inference_ms: 1.9084609107004156\n",
      "    mean_raw_obs_processing_ms: 1.862450888949264\n",
      "  time_since_restore: 92.18791437149048\n",
      "  time_this_iter_s: 6.179197788238525\n",
      "  time_total_s: 92.18791437149048\n",
      "  timers:\n",
      "    learn_throughput: 740.228\n",
      "    learn_time_ms: 5403.74\n",
      "    sample_throughput: 4489.657\n",
      "    sample_time_ms: 890.937\n",
      "    update_time_ms: 4.556\n",
      "  timestamp: 1619703711\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.899999999999984\n",
      "  episode_reward_mean: -54.42600000000006\n",
      "  episode_reward_min: -88.80000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 560\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0750617980957031\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01615813374519348\n",
      "          model: {}\n",
      "          policy_loss: -0.04562841355800629\n",
      "          total_loss: 75.31156921386719\n",
      "          vf_explained_var: 0.16729065775871277\n",
      "          vf_loss: 75.34082794189453\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0972270965576172\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016658257693052292\n",
      "          model: {}\n",
      "          policy_loss: -0.053987979888916016\n",
      "          total_loss: 9.341804504394531\n",
      "          vf_explained_var: 0.44054746627807617\n",
      "          vf_loss: 9.378926277160645\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.77777777777777\n",
      "    ram_util_percent: 60.955555555555556\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -15.0\n",
      "    pol2: 10.800000000000013\n",
      "  policy_reward_mean:\n",
      "    pol1: -40.41\n",
      "    pol2: -14.015999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -76.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4302072176150024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2330405799363056\n",
      "    mean_inference_ms: 1.8410669823744468\n",
      "    mean_raw_obs_processing_ms: 1.7768661925315354\n",
      "  time_since_restore: 91.90375399589539\n",
      "  time_this_iter_s: 6.1697118282318115\n",
      "  time_total_s: 91.90375399589539\n",
      "  timers:\n",
      "    learn_throughput: 733.282\n",
      "    learn_time_ms: 5454.926\n",
      "    sample_throughput: 4843.989\n",
      "    sample_time_ms: 825.766\n",
      "    update_time_ms: 3.091\n",
      "  timestamp: 1619703711\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -34.79999999999998\n",
      "  episode_reward_mean: -54.93600000000005\n",
      "  episode_reward_min: -82.50000000000014\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 560\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0784955024719238\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01705094799399376\n",
      "          model: {}\n",
      "          policy_loss: -0.041010599583387375\n",
      "          total_loss: 73.74925994873047\n",
      "          vf_explained_var: 0.1418878436088562\n",
      "          vf_loss: 73.77301025390625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0797419548034668\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016128119081258774\n",
      "          model: {}\n",
      "          policy_loss: -0.05410649627447128\n",
      "          total_loss: 11.872899055480957\n",
      "          vf_explained_var: 0.35124996304512024\n",
      "          vf_loss: 11.910676002502441\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.099999999999994\n",
      "    ram_util_percent: 60.955555555555556\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -17.0\n",
      "    pol2: 9.700000000000024\n",
      "  policy_reward_mean:\n",
      "    pol1: -37.455\n",
      "    pol2: -17.480999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -62.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4062731261940996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22425002880119838\n",
      "    mean_inference_ms: 1.7657597361611153\n",
      "    mean_raw_obs_processing_ms: 1.6703033316366558\n",
      "  time_since_restore: 91.81869220733643\n",
      "  time_this_iter_s: 6.153808832168579\n",
      "  time_total_s: 91.81869220733643\n",
      "  timers:\n",
      "    learn_throughput: 730.266\n",
      "    learn_time_ms: 5477.455\n",
      "    sample_throughput: 5034.387\n",
      "    sample_time_ms: 794.536\n",
      "    update_time_ms: 3.959\n",
      "  timestamp: 1619703711\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -24.599999999999984\n",
      "  episode_reward_mean: -54.51000000000005\n",
      "  episode_reward_min: -90.00000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 600\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0249933004379272\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017628367990255356\n",
      "          model: {}\n",
      "          policy_loss: -0.06103980541229248\n",
      "          total_loss: 73.46014404296875\n",
      "          vf_explained_var: 0.2006087303161621\n",
      "          vf_loss: 73.50334167480469\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.016937255859375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018667059019207954\n",
      "          model: {}\n",
      "          policy_loss: -0.058953218162059784\n",
      "          total_loss: 6.993266582489014\n",
      "          vf_explained_var: 0.4778410792350769\n",
      "          vf_loss: 7.033319473266602\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.75\n",
      "    ram_util_percent: 60.75\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -13.0\n",
      "    pol2: 19.600000000000023\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.195\n",
      "    pol2: -16.31499999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -70.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4422227360578583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.24404354608890635\n",
      "    mean_inference_ms: 1.8862168626641034\n",
      "    mean_raw_obs_processing_ms: 1.8305325680647013\n",
      "  time_since_restore: 98.15051627159119\n",
      "  time_this_iter_s: 5.962601900100708\n",
      "  time_total_s: 98.15051627159119\n",
      "  timers:\n",
      "    learn_throughput: 737.472\n",
      "    learn_time_ms: 5423.938\n",
      "    sample_throughput: 4560.034\n",
      "    sample_time_ms: 877.186\n",
      "    update_time_ms: 4.441\n",
      "  timestamp: 1619703717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         98.1505</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> -54.51 </td><td style=\"text-align: right;\">               -24.6</td><td style=\"text-align: right;\">               -90  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         91.4699</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> -49.023</td><td style=\"text-align: right;\">               -27.3</td><td style=\"text-align: right;\">               -72.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         91.8187</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> -54.936</td><td style=\"text-align: right;\">               -34.8</td><td style=\"text-align: right;\">               -82.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         91.9038</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> -54.426</td><td style=\"text-align: right;\">               -33.9</td><td style=\"text-align: right;\">               -88.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.19999999999997\n",
      "  episode_reward_mean: -47.07000000000003\n",
      "  episode_reward_min: -71.10000000000012\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 600\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0645031929016113\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015343844890594482\n",
      "          model: {}\n",
      "          policy_loss: -0.056475937366485596\n",
      "          total_loss: 62.323326110839844\n",
      "          vf_explained_var: 0.08131785690784454\n",
      "          vf_loss: 62.36427307128906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.065307378768921\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015540984459221363\n",
      "          model: {}\n",
      "          policy_loss: -0.05209957808256149\n",
      "          total_loss: 7.982500076293945\n",
      "          vf_explained_var: 0.34516382217407227\n",
      "          vf_loss: 8.018864631652832\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.52222222222221\n",
      "    ram_util_percent: 60.75555555555555\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -11.5\n",
      "    pol2: -4.600000000000009\n",
      "  policy_reward_mean:\n",
      "    pol1: -31.195\n",
      "    pol2: -15.874999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -55.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41457481656751144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22598824206744478\n",
      "    mean_inference_ms: 1.8054671274615952\n",
      "    mean_raw_obs_processing_ms: 1.713124356738667\n",
      "  time_since_restore: 97.64039516448975\n",
      "  time_this_iter_s: 6.17049503326416\n",
      "  time_total_s: 97.64039516448975\n",
      "  timers:\n",
      "    learn_throughput: 733.341\n",
      "    learn_time_ms: 5454.486\n",
      "    sample_throughput: 4903.821\n",
      "    sample_time_ms: 815.69\n",
      "    update_time_ms: 4.146\n",
      "  timestamp: 1619703717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.59999999999999\n",
      "  episode_reward_mean: -54.03900000000005\n",
      "  episode_reward_min: -88.80000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 600\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0569945573806763\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016060510650277138\n",
      "          model: {}\n",
      "          policy_loss: -0.05283662676811218\n",
      "          total_loss: 80.32999420166016\n",
      "          vf_explained_var: 0.16963821649551392\n",
      "          vf_loss: 80.36656951904297\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0877697467803955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017102645710110664\n",
      "          model: {}\n",
      "          policy_loss: -0.04974979907274246\n",
      "          total_loss: 11.217954635620117\n",
      "          vf_explained_var: 0.3562209904193878\n",
      "          vf_loss: 11.250388145446777\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.37777777777778\n",
      "    ram_util_percent: 60.766666666666666\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -15.0\n",
      "    pol2: 2.00000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: -39.11\n",
      "    pol2: -14.928999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -76.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42587910515230815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23063796443319132\n",
      "    mean_inference_ms: 1.82375937398321\n",
      "    mean_raw_obs_processing_ms: 1.7560367315676892\n",
      "  time_since_restore: 97.9147777557373\n",
      "  time_this_iter_s: 6.011023759841919\n",
      "  time_total_s: 97.9147777557373\n",
      "  timers:\n",
      "    learn_throughput: 730.281\n",
      "    learn_time_ms: 5477.346\n",
      "    sample_throughput: 5127.486\n",
      "    sample_time_ms: 780.109\n",
      "    update_time_ms: 3.193\n",
      "  timestamp: 1619703717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-41-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.599999999999994\n",
      "  episode_reward_mean: -53.54100000000005\n",
      "  episode_reward_min: -82.50000000000014\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 600\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.046095609664917\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017542783170938492\n",
      "          model: {}\n",
      "          policy_loss: -0.055878378450870514\n",
      "          total_loss: 59.26243591308594\n",
      "          vf_explained_var: 0.17983770370483398\n",
      "          vf_loss: 59.3005485534668\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0591636896133423\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017429830506443977\n",
      "          model: {}\n",
      "          policy_loss: -0.04795834422111511\n",
      "          total_loss: 6.818698883056641\n",
      "          vf_explained_var: 0.4374050498008728\n",
      "          vf_loss: 6.8490095138549805\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.955555555555556\n",
      "    ram_util_percent: 60.75555555555555\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -17.0\n",
      "    pol2: 9.700000000000024\n",
      "  policy_reward_mean:\n",
      "    pol1: -36.115\n",
      "    pol2: -17.425999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -62.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40332438465618536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2225563769112114\n",
      "    mean_inference_ms: 1.7546629398155165\n",
      "    mean_raw_obs_processing_ms: 1.657656207717073\n",
      "  time_since_restore: 97.87169313430786\n",
      "  time_this_iter_s: 6.0530009269714355\n",
      "  time_total_s: 97.87169313430786\n",
      "  timers:\n",
      "    learn_throughput: 728.099\n",
      "    learn_time_ms: 5493.761\n",
      "    sample_throughput: 5107.194\n",
      "    sample_time_ms: 783.209\n",
      "    update_time_ms: 3.71\n",
      "  timestamp: 1619703717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -24.599999999999984\n",
      "  episode_reward_mean: -54.318000000000055\n",
      "  episode_reward_min: -90.00000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 640\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0205719470977783\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016780827194452286\n",
      "          model: {}\n",
      "          policy_loss: -0.0529618039727211\n",
      "          total_loss: 72.73287963867188\n",
      "          vf_explained_var: 0.15535081923007965\n",
      "          vf_loss: 72.76884460449219\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.016371488571167\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0163719579577446\n",
      "          model: {}\n",
      "          policy_loss: -0.053719669580459595\n",
      "          total_loss: 7.0073370933532715\n",
      "          vf_explained_var: 0.4281988739967346\n",
      "          vf_loss: 7.044480323791504\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.03333333333333\n",
      "    ram_util_percent: 60.8111111111111\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -13.0\n",
      "    pol2: -5.700000000000008\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.08\n",
      "    pol2: -16.237999999999968\n",
      "  policy_reward_min:\n",
      "    pol1: -70.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.434140089032411\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23987486282391524\n",
      "    mean_inference_ms: 1.8540239281641737\n",
      "    mean_raw_obs_processing_ms: 1.7941246054038613\n",
      "  time_since_restore: 104.60819816589355\n",
      "  time_this_iter_s: 6.457681894302368\n",
      "  time_total_s: 104.60819816589355\n",
      "  timers:\n",
      "    learn_throughput: 754.581\n",
      "    learn_time_ms: 5300.958\n",
      "    sample_throughput: 4715.187\n",
      "    sample_time_ms: 848.323\n",
      "    update_time_ms: 4.258\n",
      "  timestamp: 1619703724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        104.608 </td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> -54.318</td><td style=\"text-align: right;\">               -24.6</td><td style=\"text-align: right;\">               -90  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         97.6404</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> -47.07 </td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -71.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         97.8717</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> -53.541</td><td style=\"text-align: right;\">               -33.6</td><td style=\"text-align: right;\">               -82.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         97.9148</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> -54.039</td><td style=\"text-align: right;\">               -33.6</td><td style=\"text-align: right;\">               -88.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -25.199999999999978\n",
      "  episode_reward_mean: -46.87800000000003\n",
      "  episode_reward_min: -68.70000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 640\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0216400623321533\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0159014780074358\n",
      "          model: {}\n",
      "          policy_loss: -0.052468206733465195\n",
      "          total_loss: 64.10145568847656\n",
      "          vf_explained_var: 0.08231677114963531\n",
      "          vf_loss: 64.13783264160156\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.049309253692627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017238829284906387\n",
      "          model: {}\n",
      "          policy_loss: -0.0545152947306633\n",
      "          total_loss: 7.741045951843262\n",
      "          vf_explained_var: 0.3848620057106018\n",
      "          vf_loss: 7.778106689453125\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.544444444444444\n",
      "    ram_util_percent: 60.91111111111112\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -8.5\n",
      "    pol2: -4.599999999999999\n",
      "  policy_reward_mean:\n",
      "    pol1: -30.915\n",
      "    pol2: -15.962999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -56.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4115364876856662\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22446640195873735\n",
      "    mean_inference_ms: 1.790493230865148\n",
      "    mean_raw_obs_processing_ms: 1.6982056584323633\n",
      "  time_since_restore: 104.065012216568\n",
      "  time_this_iter_s: 6.424617052078247\n",
      "  time_total_s: 104.065012216568\n",
      "  timers:\n",
      "    learn_throughput: 754.297\n",
      "    learn_time_ms: 5302.954\n",
      "    sample_throughput: 5029.934\n",
      "    sample_time_ms: 795.239\n",
      "    update_time_ms: 4.187\n",
      "  timestamp: 1619703724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.599999999999994\n",
      "  episode_reward_mean: -52.842000000000056\n",
      "  episode_reward_min: -75.00000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 640\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0421088933944702\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01809798553586006\n",
      "          model: {}\n",
      "          policy_loss: -0.054905280470848083\n",
      "          total_loss: 72.49884033203125\n",
      "          vf_explained_var: 0.15515297651290894\n",
      "          vf_loss: 72.53541564941406\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0445401668548584\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01725916564464569\n",
      "          model: {}\n",
      "          policy_loss: -0.052018411457538605\n",
      "          total_loss: 7.011840343475342\n",
      "          vf_explained_var: 0.4178897440433502\n",
      "          vf_loss: 7.046383857727051\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.51111111111112\n",
      "    ram_util_percent: 60.91111111111112\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -14.5\n",
      "    pol2: 9.700000000000024\n",
      "  policy_reward_mean:\n",
      "    pol1: -35.625\n",
      "    pol2: -17.21699999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -62.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3983939829339733\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2198950089556469\n",
      "    mean_inference_ms: 1.7330803240698365\n",
      "    mean_raw_obs_processing_ms: 1.6369524439222642\n",
      "  time_since_restore: 104.38459277153015\n",
      "  time_this_iter_s: 6.51289963722229\n",
      "  time_total_s: 104.38459277153015\n",
      "  timers:\n",
      "    learn_throughput: 747.569\n",
      "    learn_time_ms: 5350.674\n",
      "    sample_throughput: 5194.412\n",
      "    sample_time_ms: 770.058\n",
      "    update_time_ms: 3.716\n",
      "  timestamp: 1619703724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.59999999999999\n",
      "  episode_reward_mean: -53.05800000000006\n",
      "  episode_reward_min: -73.5000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 640\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0313175916671753\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016968265175819397\n",
      "          model: {}\n",
      "          policy_loss: -0.04997505992650986\n",
      "          total_loss: 78.59606170654297\n",
      "          vf_explained_var: 0.15510568022727966\n",
      "          vf_loss: 78.62886047363281\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0685241222381592\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016354123130440712\n",
      "          model: {}\n",
      "          policy_loss: -0.052658069878816605\n",
      "          total_loss: 10.442925453186035\n",
      "          vf_explained_var: 0.4243896007537842\n",
      "          vf_loss: 10.479024887084961\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.54444444444444\n",
      "    ram_util_percent: 60.888888888888886\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -16.0\n",
      "    pol2: 3.100000000000016\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.03\n",
      "    pol2: -15.027999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -65.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42163426581800806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22821688942043145\n",
      "    mean_inference_ms: 1.803499393242784\n",
      "    mean_raw_obs_processing_ms: 1.7374351147746454\n",
      "  time_since_restore: 104.71123886108398\n",
      "  time_this_iter_s: 6.79646110534668\n",
      "  time_total_s: 104.71123886108398\n",
      "  timers:\n",
      "    learn_throughput: 747.233\n",
      "    learn_time_ms: 5353.083\n",
      "    sample_throughput: 5114.8\n",
      "    sample_time_ms: 782.044\n",
      "    update_time_ms: 3.234\n",
      "  timestamp: 1619703724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -23.09999999999996\n",
      "  episode_reward_mean: -45.189000000000036\n",
      "  episode_reward_min: -65.40000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 680\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9923430681228638\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01599808968603611\n",
      "          model: {}\n",
      "          policy_loss: -0.05250795558094978\n",
      "          total_loss: 42.999183654785156\n",
      "          vf_explained_var: 0.11616837978363037\n",
      "          vf_loss: 43.03549575805664\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0247224569320679\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01466913428157568\n",
      "          model: {}\n",
      "          policy_loss: -0.04331820458173752\n",
      "          total_loss: 8.399845123291016\n",
      "          vf_explained_var: 0.3262156844139099\n",
      "          vf_loss: 8.42831039428711\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.81249999999999\n",
      "    ram_util_percent: 60.625\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -7.5\n",
      "    pol2: -0.19999999999999263\n",
      "  policy_reward_mean:\n",
      "    pol1: -29.435\n",
      "    pol2: -15.753999999999971\n",
      "  policy_reward_min:\n",
      "    pol1: -56.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40816559959909476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22280220970006415\n",
      "    mean_inference_ms: 1.7747192396543903\n",
      "    mean_raw_obs_processing_ms: 1.6838378110795345\n",
      "  time_since_restore: 110.0454432964325\n",
      "  time_this_iter_s: 5.980431079864502\n",
      "  time_total_s: 110.0454432964325\n",
      "  timers:\n",
      "    learn_throughput: 772.91\n",
      "    learn_time_ms: 5175.243\n",
      "    sample_throughput: 5164.981\n",
      "    sample_time_ms: 774.446\n",
      "    update_time_ms: 3.908\n",
      "  timestamp: 1619703730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         104.608</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> -54.318</td><td style=\"text-align: right;\">               -24.6</td><td style=\"text-align: right;\">               -90  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         110.045</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> -45.189</td><td style=\"text-align: right;\">               -23.1</td><td style=\"text-align: right;\">               -65.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         104.385</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> -52.842</td><td style=\"text-align: right;\">               -33.6</td><td style=\"text-align: right;\">               -75  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         104.711</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> -53.058</td><td style=\"text-align: right;\">               -33.6</td><td style=\"text-align: right;\">               -73.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -24.599999999999984\n",
      "  episode_reward_mean: -52.91700000000005\n",
      "  episode_reward_min: -87.00000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 680\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0037667751312256\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015906378626823425\n",
      "          model: {}\n",
      "          policy_loss: -0.053139783442020416\n",
      "          total_loss: 54.807289123535156\n",
      "          vf_explained_var: 0.12878882884979248\n",
      "          vf_loss: 54.844322204589844\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.014993667602539\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014941203407943249\n",
      "          model: {}\n",
      "          policy_loss: -0.04723004251718521\n",
      "          total_loss: 6.960531234741211\n",
      "          vf_explained_var: 0.47192394733428955\n",
      "          vf_loss: 6.992632865905762\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.97777777777777\n",
      "    ram_util_percent: 60.62222222222223\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -14.5\n",
      "    pol2: -6.800000000000005\n",
      "  policy_reward_mean:\n",
      "    pol1: -37.075\n",
      "    pol2: -15.84199999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -67.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4289433123952955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23734390878301814\n",
      "    mean_inference_ms: 1.8329244551280226\n",
      "    mean_raw_obs_processing_ms: 1.7717244071561464\n",
      "  time_since_restore: 110.69611525535583\n",
      "  time_this_iter_s: 6.08791708946228\n",
      "  time_total_s: 110.69611525535583\n",
      "  timers:\n",
      "    learn_throughput: 770.353\n",
      "    learn_time_ms: 5192.426\n",
      "    sample_throughput: 4880.652\n",
      "    sample_time_ms: 819.563\n",
      "    update_time_ms: 4.111\n",
      "  timestamp: 1619703730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.599999999999994\n",
      "  episode_reward_mean: -52.206000000000046\n",
      "  episode_reward_min: -75.00000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 680\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0394439697265625\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016413599252700806\n",
      "          model: {}\n",
      "          policy_loss: -0.05550628900527954\n",
      "          total_loss: 58.98406982421875\n",
      "          vf_explained_var: 0.21161779761314392\n",
      "          vf_loss: 59.02295684814453\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0283560752868652\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016547992825508118\n",
      "          model: {}\n",
      "          policy_loss: -0.05447568744421005\n",
      "          total_loss: 5.721439361572266\n",
      "          vf_explained_var: 0.5048027038574219\n",
      "          vf_loss: 5.759159564971924\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.32222222222222\n",
      "    ram_util_percent: 60.57777777777778\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -14.5\n",
      "    pol2: -6.8000000000000105\n",
      "  policy_reward_mean:\n",
      "    pol1: -34.725\n",
      "    pol2: -17.480999999999966\n",
      "  policy_reward_min:\n",
      "    pol1: -56.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39575023277570254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2183283577196806\n",
      "    mean_inference_ms: 1.719110330442187\n",
      "    mean_raw_obs_processing_ms: 1.6244420019642982\n",
      "  time_since_restore: 110.44804883003235\n",
      "  time_this_iter_s: 6.063456058502197\n",
      "  time_total_s: 110.44804883003235\n",
      "  timers:\n",
      "    learn_throughput: 764.677\n",
      "    learn_time_ms: 5230.965\n",
      "    sample_throughput: 5352.934\n",
      "    sample_time_ms: 747.254\n",
      "    update_time_ms: 3.456\n",
      "  timestamp: 1619703730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.59999999999999\n",
      "  episode_reward_mean: -52.92300000000006\n",
      "  episode_reward_min: -73.5000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 680\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0214817523956299\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014511942863464355\n",
      "          model: {}\n",
      "          policy_loss: -0.042855776846408844\n",
      "          total_loss: 77.05363464355469\n",
      "          vf_explained_var: 0.10949154198169708\n",
      "          vf_loss: 77.08179473876953\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.047829270362854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01617169752717018\n",
      "          model: {}\n",
      "          policy_loss: -0.0477808341383934\n",
      "          total_loss: 10.974445343017578\n",
      "          vf_explained_var: 0.30187904834747314\n",
      "          vf_loss: 11.005851745605469\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.10000000000001\n",
      "    ram_util_percent: 60.599999999999994\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -19.0\n",
      "    pol2: 5.30000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -38.17\n",
      "    pol2: -14.752999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -65.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4192085849612004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22682604891428956\n",
      "    mean_inference_ms: 1.791502226915041\n",
      "    mean_raw_obs_processing_ms: 1.730482321237319\n",
      "  time_since_restore: 110.66914820671082\n",
      "  time_this_iter_s: 5.957909345626831\n",
      "  time_total_s: 110.66914820671082\n",
      "  timers:\n",
      "    learn_throughput: 764.85\n",
      "    learn_time_ms: 5229.787\n",
      "    sample_throughput: 5179.267\n",
      "    sample_time_ms: 772.31\n",
      "    update_time_ms: 3.207\n",
      "  timestamp: 1619703730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.3\n",
      "  episode_reward_mean: -51.36600000000005\n",
      "  episode_reward_min: -74.40000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 720\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9953262805938721\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015868954360485077\n",
      "          model: {}\n",
      "          policy_loss: -0.05026094987988472\n",
      "          total_loss: 60.89170455932617\n",
      "          vf_explained_var: 0.1229696199297905\n",
      "          vf_loss: 60.92589569091797\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9874749779701233\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016220219433307648\n",
      "          model: {}\n",
      "          policy_loss: -0.05621754378080368\n",
      "          total_loss: 9.048564910888672\n",
      "          vf_explained_var: 0.3613653779029846\n",
      "          vf_loss: 9.088360786437988\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.8375\n",
      "    ram_util_percent: 59.8\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -15.5\n",
      "    pol2: -0.19999999999999796\n",
      "  policy_reward_mean:\n",
      "    pol1: -36.03\n",
      "    pol2: -15.335999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -55.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4253937353323697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.235668518766782\n",
      "    mean_inference_ms: 1.8197660958842954\n",
      "    mean_raw_obs_processing_ms: 1.7586723857230298\n",
      "  time_since_restore: 116.52146530151367\n",
      "  time_this_iter_s: 5.825350046157837\n",
      "  time_total_s: 116.52146530151367\n",
      "  timers:\n",
      "    learn_throughput: 776.608\n",
      "    learn_time_ms: 5150.603\n",
      "    sample_throughput: 4949.042\n",
      "    sample_time_ms: 808.237\n",
      "    update_time_ms: 4.007\n",
      "  timestamp: 1619703736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         116.521</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\"> -51.366</td><td style=\"text-align: right;\">               -33.3</td><td style=\"text-align: right;\">               -74.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         110.045</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> -45.189</td><td style=\"text-align: right;\">               -23.1</td><td style=\"text-align: right;\">               -65.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         110.448</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> -52.206</td><td style=\"text-align: right;\">               -33.6</td><td style=\"text-align: right;\">               -75  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         110.669</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> -52.923</td><td style=\"text-align: right;\">               -33.6</td><td style=\"text-align: right;\">               -73.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -23.09999999999996\n",
      "  episode_reward_mean: -43.86000000000003\n",
      "  episode_reward_min: -64.50000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 720\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9882775545120239\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014036556705832481\n",
      "          model: {}\n",
      "          policy_loss: -0.04217071086168289\n",
      "          total_loss: 68.61019897460938\n",
      "          vf_explained_var: 0.06600187718868256\n",
      "          vf_loss: 68.63815307617188\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0157723426818848\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014514646492898464\n",
      "          model: {}\n",
      "          policy_loss: -0.0384606197476387\n",
      "          total_loss: 10.28648567199707\n",
      "          vf_explained_var: 0.36461055278778076\n",
      "          vf_loss: 10.310251235961914\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.65555555555555\n",
      "    ram_util_percent: 59.81111111111112\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -7.5\n",
      "    pol2: 7.499999999999994\n",
      "  policy_reward_mean:\n",
      "    pol1: -28.425\n",
      "    pol2: -15.434999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -58.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4073083332896866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22248161465135496\n",
      "    mean_inference_ms: 1.7684665111948008\n",
      "    mean_raw_obs_processing_ms: 1.6807897150396378\n",
      "  time_since_restore: 116.04890418052673\n",
      "  time_this_iter_s: 6.003460884094238\n",
      "  time_total_s: 116.04890418052673\n",
      "  timers:\n",
      "    learn_throughput: 778.524\n",
      "    learn_time_ms: 5137.925\n",
      "    sample_throughput: 5139.131\n",
      "    sample_time_ms: 778.342\n",
      "    update_time_ms: 3.914\n",
      "  timestamp: 1619703736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -32.09999999999997\n",
      "  episode_reward_mean: -52.23900000000006\n",
      "  episode_reward_min: -73.2000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 720\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0211730003356934\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015125257894396782\n",
      "          model: {}\n",
      "          policy_loss: -0.046872444450855255\n",
      "          total_loss: 68.65573120117188\n",
      "          vf_explained_var: 0.09474287182092667\n",
      "          vf_loss: 68.68728637695312\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0264091491699219\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014292564243078232\n",
      "          model: {}\n",
      "          policy_loss: -0.03887992352247238\n",
      "          total_loss: 12.44471263885498\n",
      "          vf_explained_var: 0.351535439491272\n",
      "          vf_loss: 12.469121932983398\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.2875\n",
      "    ram_util_percent: 59.75\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -17.0\n",
      "    pol2: 17.399999999999995\n",
      "  policy_reward_mean:\n",
      "    pol1: -37.695\n",
      "    pol2: -14.543999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -65.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41675255365531316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22558363843068002\n",
      "    mean_inference_ms: 1.7805062290150224\n",
      "    mean_raw_obs_processing_ms: 1.723419258262445\n",
      "  time_since_restore: 116.4862654209137\n",
      "  time_this_iter_s: 5.817117214202881\n",
      "  time_total_s: 116.4862654209137\n",
      "  timers:\n",
      "    learn_throughput: 770.459\n",
      "    learn_time_ms: 5191.713\n",
      "    sample_throughput: 5335.762\n",
      "    sample_time_ms: 749.659\n",
      "    update_time_ms: 3.239\n",
      "  timestamp: 1619703736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -31.19999999999997\n",
      "  episode_reward_mean: -50.445000000000036\n",
      "  episode_reward_min: -75.00000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 720\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0256812572479248\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017384663224220276\n",
      "          model: {}\n",
      "          policy_loss: -0.05849197134375572\n",
      "          total_loss: 50.99622344970703\n",
      "          vf_explained_var: 0.21777179837226868\n",
      "          vf_loss: 51.03711700439453\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0030816793441772\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01575716957449913\n",
      "          model: {}\n",
      "          policy_loss: -0.04907286912202835\n",
      "          total_loss: 6.449047565460205\n",
      "          vf_explained_var: 0.4668678045272827\n",
      "          vf_loss: 6.482166290283203\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.32499999999999\n",
      "    ram_util_percent: 59.7625\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -14.5\n",
      "    pol2: -6.8000000000000105\n",
      "  policy_reward_mean:\n",
      "    pol1: -33.25\n",
      "    pol2: -17.194999999999965\n",
      "  policy_reward_min:\n",
      "    pol1: -55.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39603226397860836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21830437418449203\n",
      "    mean_inference_ms: 1.714985112918626\n",
      "    mean_raw_obs_processing_ms: 1.6246964041854255\n",
      "  time_since_restore: 116.43772172927856\n",
      "  time_this_iter_s: 5.989672899246216\n",
      "  time_total_s: 116.43772172927856\n",
      "  timers:\n",
      "    learn_throughput: 770.024\n",
      "    learn_time_ms: 5194.646\n",
      "    sample_throughput: 5333.83\n",
      "    sample_time_ms: 749.93\n",
      "    update_time_ms: 3.275\n",
      "  timestamp: 1619703736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.3\n",
      "  episode_reward_mean: -49.500000000000036\n",
      "  episode_reward_min: -72.00000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 760\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9880740642547607\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01540043018758297\n",
      "          model: {}\n",
      "          policy_loss: -0.05463040992617607\n",
      "          total_loss: 58.954833984375\n",
      "          vf_explained_var: 0.11092649400234222\n",
      "          vf_loss: 58.993873596191406\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9765592217445374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015361225232481956\n",
      "          model: {}\n",
      "          policy_loss: -0.05094846710562706\n",
      "          total_loss: 6.9313554763793945\n",
      "          vf_explained_var: 0.44430506229400635\n",
      "          vf_loss: 6.966750144958496\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.97777777777777\n",
      "    ram_util_percent: 60.01111111111111\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -15.5\n",
      "    pol2: -0.19999999999999796\n",
      "  policy_reward_mean:\n",
      "    pol1: -34.395\n",
      "    pol2: -15.104999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -52.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.42296004576302176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23456672683238408\n",
      "    mean_inference_ms: 1.808239988655452\n",
      "    mean_raw_obs_processing_ms: 1.7510922672300167\n",
      "  time_since_restore: 122.23188948631287\n",
      "  time_this_iter_s: 5.710424184799194\n",
      "  time_total_s: 122.23188948631287\n",
      "  timers:\n",
      "    learn_throughput: 781.431\n",
      "    learn_time_ms: 5118.816\n",
      "    sample_throughput: 5041.534\n",
      "    sample_time_ms: 793.409\n",
      "    update_time_ms: 3.945\n",
      "  timestamp: 1619703741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         122.232</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> -49.5  </td><td style=\"text-align: right;\">               -33.3</td><td style=\"text-align: right;\">               -72  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         116.049</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\"> -43.86 </td><td style=\"text-align: right;\">               -23.1</td><td style=\"text-align: right;\">               -64.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         116.438</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\"> -50.445</td><td style=\"text-align: right;\">               -31.2</td><td style=\"text-align: right;\">               -75  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         116.486</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\"> -52.239</td><td style=\"text-align: right;\">               -32.1</td><td style=\"text-align: right;\">               -73.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -18.899999999999974\n",
      "  episode_reward_mean: -41.89500000000002\n",
      "  episode_reward_min: -64.50000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 760\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9670689702033997\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014392219483852386\n",
      "          model: {}\n",
      "          policy_loss: -0.04987628012895584\n",
      "          total_loss: 48.62340545654297\n",
      "          vf_explained_var: 0.08940410614013672\n",
      "          vf_loss: 48.65870666503906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9964154958724976\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013685173355042934\n",
      "          model: {}\n",
      "          policy_loss: -0.052045971155166626\n",
      "          total_loss: 7.846096038818359\n",
      "          vf_explained_var: 0.2832563519477844\n",
      "          vf_loss: 7.884285926818848\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.6\n",
      "    ram_util_percent: 60.0625\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -5.5\n",
      "    pol2: 7.499999999999994\n",
      "  policy_reward_mean:\n",
      "    pol1: -26.405\n",
      "    pol2: -15.489999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -58.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40640619755849555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2221265416918086\n",
      "    mean_inference_ms: 1.760741035028737\n",
      "    mean_raw_obs_processing_ms: 1.6784134076649997\n",
      "  time_since_restore: 121.64547228813171\n",
      "  time_this_iter_s: 5.5965681076049805\n",
      "  time_total_s: 121.64547228813171\n",
      "  timers:\n",
      "    learn_throughput: 785.118\n",
      "    learn_time_ms: 5094.775\n",
      "    sample_throughput: 5195.874\n",
      "    sample_time_ms: 769.842\n",
      "    update_time_ms: 3.868\n",
      "  timestamp: 1619703741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -29.999999999999954\n",
      "  episode_reward_mean: -50.59800000000003\n",
      "  episode_reward_min: -73.50000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 760\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.013639211654663\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01587092876434326\n",
      "          model: {}\n",
      "          policy_loss: -0.05244454741477966\n",
      "          total_loss: 69.70491790771484\n",
      "          vf_explained_var: 0.17460191249847412\n",
      "          vf_loss: 69.74130249023438\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9915174245834351\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016483809798955917\n",
      "          model: {}\n",
      "          policy_loss: -0.05826115608215332\n",
      "          total_loss: 8.541971206665039\n",
      "          vf_explained_var: 0.35866743326187134\n",
      "          vf_loss: 8.58354377746582\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.83749999999999\n",
      "    ram_util_percent: 60.0625\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -10.0\n",
      "    pol2: -1.3000000000000054\n",
      "  policy_reward_mean:\n",
      "    pol1: -33.7\n",
      "    pol2: -16.89799999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -64.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39542142023166077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21787031467922482\n",
      "    mean_inference_ms: 1.7079583011714106\n",
      "    mean_raw_obs_processing_ms: 1.6236342578626368\n",
      "  time_since_restore: 121.99913549423218\n",
      "  time_this_iter_s: 5.561413764953613\n",
      "  time_total_s: 121.99913549423218\n",
      "  timers:\n",
      "    learn_throughput: 777.01\n",
      "    learn_time_ms: 5147.939\n",
      "    sample_throughput: 5409.027\n",
      "    sample_time_ms: 739.505\n",
      "    update_time_ms: 3.21\n",
      "  timestamp: 1619703742\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -32.09999999999997\n",
      "  episode_reward_mean: -51.81900000000005\n",
      "  episode_reward_min: -73.2000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 760\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9825829267501831\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01610841229557991\n",
      "          model: {}\n",
      "          policy_loss: -0.05238506942987442\n",
      "          total_loss: 76.29595947265625\n",
      "          vf_explained_var: 0.124407559633255\n",
      "          vf_loss: 76.33203887939453\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0056498050689697\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015260186046361923\n",
      "          model: {}\n",
      "          policy_loss: -0.0429622083902359\n",
      "          total_loss: 10.005727767944336\n",
      "          vf_explained_var: 0.44299551844596863\n",
      "          vf_loss: 10.033239364624023\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.1\n",
      "    ram_util_percent: 60.0625\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -15.5\n",
      "    pol2: 17.399999999999995\n",
      "  policy_reward_mean:\n",
      "    pol1: -37.11\n",
      "    pol2: -14.708999999999971\n",
      "  policy_reward_min:\n",
      "    pol1: -62.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41398740583598553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22423863962050128\n",
      "    mean_inference_ms: 1.7678179010524042\n",
      "    mean_raw_obs_processing_ms: 1.7157064616207591\n",
      "  time_since_restore: 122.15954542160034\n",
      "  time_this_iter_s: 5.6732800006866455\n",
      "  time_total_s: 122.15954542160034\n",
      "  timers:\n",
      "    learn_throughput: 777.44\n",
      "    learn_time_ms: 5145.09\n",
      "    sample_throughput: 5260.874\n",
      "    sample_time_ms: 760.33\n",
      "    update_time_ms: 3.214\n",
      "  timestamp: 1619703742\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -29.99999999999996\n",
      "  episode_reward_mean: -49.11900000000004\n",
      "  episode_reward_min: -72.00000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 800\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9767965078353882\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015112707391381264\n",
      "          model: {}\n",
      "          policy_loss: -0.04579835385084152\n",
      "          total_loss: 75.87159729003906\n",
      "          vf_explained_var: 0.08759167790412903\n",
      "          vf_loss: 75.902099609375\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9628167152404785\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015457618050277233\n",
      "          model: {}\n",
      "          policy_loss: -0.05327465385198593\n",
      "          total_loss: 14.72414779663086\n",
      "          vf_explained_var: 0.28037163615226746\n",
      "          vf_loss: 14.761771202087402\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.475\n",
      "    ram_util_percent: 61.0625\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -11.5\n",
      "    pol2: 9.700000000000019\n",
      "  policy_reward_mean:\n",
      "    pol1: -35.125\n",
      "    pol2: -13.99399999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -60.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4222967425869868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23437269724235762\n",
      "    mean_inference_ms: 1.8019639279538981\n",
      "    mean_raw_obs_processing_ms: 1.7501635688264718\n",
      "  time_since_restore: 128.1814615726471\n",
      "  time_this_iter_s: 5.9495720863342285\n",
      "  time_total_s: 128.1814615726471\n",
      "  timers:\n",
      "    learn_throughput: 783.969\n",
      "    learn_time_ms: 5102.246\n",
      "    sample_throughput: 4987.429\n",
      "    sample_time_ms: 802.016\n",
      "    update_time_ms: 3.76\n",
      "  timestamp: 1619703747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         128.181</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -49.119</td><td style=\"text-align: right;\">               -30  </td><td style=\"text-align: right;\">               -72  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         121.645</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> -41.895</td><td style=\"text-align: right;\">               -18.9</td><td style=\"text-align: right;\">               -64.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         121.999</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> -50.598</td><td style=\"text-align: right;\">               -30  </td><td style=\"text-align: right;\">               -73.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         122.16 </td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> -51.819</td><td style=\"text-align: right;\">               -32.1</td><td style=\"text-align: right;\">               -73.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -18.899999999999974\n",
      "  episode_reward_mean: -41.16600000000001\n",
      "  episode_reward_min: -62.10000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 800\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9520997405052185\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015064072795212269\n",
      "          model: {}\n",
      "          policy_loss: -0.05516812205314636\n",
      "          total_loss: 50.42054748535156\n",
      "          vf_explained_var: 0.1345233917236328\n",
      "          vf_loss: 50.46046447753906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9748488664627075\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014034239575266838\n",
      "          model: {}\n",
      "          policy_loss: -0.04444349557161331\n",
      "          total_loss: 11.025846481323242\n",
      "          vf_explained_var: 0.279818594455719\n",
      "          vf_loss: 11.05608081817627\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.70000000000001\n",
      "    ram_util_percent: 61.044444444444444\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -5.5\n",
      "    pol2: 7.499999999999994\n",
      "  policy_reward_mean:\n",
      "    pol1: -26.05\n",
      "    pol2: -15.11599999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -58.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40560685303829813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2217573472731528\n",
      "    mean_inference_ms: 1.7556805508052746\n",
      "    mean_raw_obs_processing_ms: 1.6776766077552339\n",
      "  time_since_restore: 127.56332921981812\n",
      "  time_this_iter_s: 5.917856931686401\n",
      "  time_total_s: 127.56332921981812\n",
      "  timers:\n",
      "    learn_throughput: 788.722\n",
      "    learn_time_ms: 5071.496\n",
      "    sample_throughput: 5062.48\n",
      "    sample_time_ms: 790.127\n",
      "    update_time_ms: 3.918\n",
      "  timestamp: 1619703747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -19.199999999999974\n",
      "  episode_reward_mean: -48.98100000000005\n",
      "  episode_reward_min: -73.50000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 800\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0019094944000244\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017264746129512787\n",
      "          model: {}\n",
      "          policy_loss: -0.05584390461444855\n",
      "          total_loss: 66.76199340820312\n",
      "          vf_explained_var: 0.2212243378162384\n",
      "          vf_loss: 66.80036926269531\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9801065921783447\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013327330350875854\n",
      "          model: {}\n",
      "          policy_loss: -0.038455408066511154\n",
      "          total_loss: 24.320926666259766\n",
      "          vf_explained_var: 0.27785614132881165\n",
      "          vf_loss: 24.34588623046875\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.42222222222222\n",
      "    ram_util_percent: 61.044444444444444\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -10.0\n",
      "    pol2: 38.29999999999988\n",
      "  policy_reward_mean:\n",
      "    pol1: -33.48\n",
      "    pol2: -15.500999999999971\n",
      "  policy_reward_min:\n",
      "    pol1: -64.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39417717435997945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21712048110927612\n",
      "    mean_inference_ms: 1.7012393862634578\n",
      "    mean_raw_obs_processing_ms: 1.6205657790301542\n",
      "  time_since_restore: 127.90232539176941\n",
      "  time_this_iter_s: 5.9031898975372314\n",
      "  time_total_s: 127.90232539176941\n",
      "  timers:\n",
      "    learn_throughput: 779.825\n",
      "    learn_time_ms: 5129.358\n",
      "    sample_throughput: 5329.668\n",
      "    sample_time_ms: 750.516\n",
      "    update_time_ms: 3.074\n",
      "  timestamp: 1619703748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -32.10000000000008\n",
      "  episode_reward_mean: -51.38700000000005\n",
      "  episode_reward_min: -66.90000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 800\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9863250851631165\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013948586769402027\n",
      "          model: {}\n",
      "          policy_loss: -0.03861384466290474\n",
      "          total_loss: 93.5748291015625\n",
      "          vf_explained_var: 0.10631035268306732\n",
      "          vf_loss: 93.59931945800781\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.995315670967102\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014918142929673195\n",
      "          model: {}\n",
      "          policy_loss: -0.04401098191738129\n",
      "          total_loss: 18.07611846923828\n",
      "          vf_explained_var: 0.3713221251964569\n",
      "          vf_loss: 18.105024337768555\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.94444444444444\n",
      "    ram_util_percent: 61.044444444444444\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -15.5\n",
      "    pol2: 22.900000000000013\n",
      "  policy_reward_mean:\n",
      "    pol1: -37.47\n",
      "    pol2: -13.916999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -74.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4118451929603742\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22309254502299417\n",
      "    mean_inference_ms: 1.7589794178839957\n",
      "    mean_raw_obs_processing_ms: 1.7091810200302546\n",
      "  time_since_restore: 128.0260362625122\n",
      "  time_this_iter_s: 5.866490840911865\n",
      "  time_total_s: 128.0260362625122\n",
      "  timers:\n",
      "    learn_throughput: 779.391\n",
      "    learn_time_ms: 5132.215\n",
      "    sample_throughput: 5242.708\n",
      "    sample_time_ms: 762.964\n",
      "    update_time_ms: 2.929\n",
      "  timestamp: 1619703748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -29.99999999999996\n",
      "  episode_reward_mean: -48.120000000000026\n",
      "  episode_reward_min: -72.00000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 840\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.966594934463501\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014958808198571205\n",
      "          model: {}\n",
      "          policy_loss: -0.0547465980052948\n",
      "          total_loss: 69.80583953857422\n",
      "          vf_explained_var: 0.08772227168083191\n",
      "          vf_loss: 69.8454360961914\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9314014911651611\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016816196963191032\n",
      "          model: {}\n",
      "          policy_loss: -0.056148823350667953\n",
      "          total_loss: 9.327779769897461\n",
      "          vf_explained_var: 0.38132932782173157\n",
      "          vf_loss: 9.366902351379395\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.025\n",
      "    ram_util_percent: 61.1375\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -11.5\n",
      "    pol2: 9.700000000000019\n",
      "  policy_reward_mean:\n",
      "    pol1: -34.17\n",
      "    pol2: -13.949999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -60.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4215900169753941\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.23415740915461675\n",
      "    mean_inference_ms: 1.7966766882342853\n",
      "    mean_raw_obs_processing_ms: 1.748044181071613\n",
      "  time_since_restore: 134.0360016822815\n",
      "  time_this_iter_s: 5.854540109634399\n",
      "  time_total_s: 134.0360016822815\n",
      "  timers:\n",
      "    learn_throughput: 783.698\n",
      "    learn_time_ms: 5104.007\n",
      "    sample_throughput: 5060.593\n",
      "    sample_time_ms: 790.421\n",
      "    update_time_ms: 3.713\n",
      "  timestamp: 1619703753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         134.036</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\"> -48.12 </td><td style=\"text-align: right;\">               -30  </td><td style=\"text-align: right;\">               -72  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         127.563</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -41.166</td><td style=\"text-align: right;\">               -18.9</td><td style=\"text-align: right;\">               -62.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         127.902</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -48.981</td><td style=\"text-align: right;\">               -19.2</td><td style=\"text-align: right;\">               -73.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         128.026</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -51.387</td><td style=\"text-align: right;\">               -32.1</td><td style=\"text-align: right;\">               -66.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -18.899999999999974\n",
      "  episode_reward_mean: -39.62700000000001\n",
      "  episode_reward_min: -60.90000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 840\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9443992972373962\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014932885766029358\n",
      "          model: {}\n",
      "          policy_loss: -0.05163469910621643\n",
      "          total_loss: 33.023468017578125\n",
      "          vf_explained_var: 0.13632924854755402\n",
      "          vf_loss: 33.05998229980469\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9565864205360413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016174882650375366\n",
      "          model: {}\n",
      "          policy_loss: -0.04661092534661293\n",
      "          total_loss: 8.385028839111328\n",
      "          vf_explained_var: 0.2841077446937561\n",
      "          vf_loss: 8.415263175964355\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.7375\n",
      "    ram_util_percent: 61.1875\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -5.0\n",
      "    pol2: 3.099999999999991\n",
      "  policy_reward_mean:\n",
      "    pol1: -24.28\n",
      "    pol2: -15.346999999999971\n",
      "  policy_reward_min:\n",
      "    pol1: -53.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40389271712046065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2209209523639163\n",
      "    mean_inference_ms: 1.748116669877894\n",
      "    mean_raw_obs_processing_ms: 1.6707078570958374\n",
      "  time_since_restore: 133.373553276062\n",
      "  time_this_iter_s: 5.8102240562438965\n",
      "  time_total_s: 133.373553276062\n",
      "  timers:\n",
      "    learn_throughput: 786.261\n",
      "    learn_time_ms: 5087.367\n",
      "    sample_throughput: 5087.15\n",
      "    sample_time_ms: 786.295\n",
      "    update_time_ms: 3.927\n",
      "  timestamp: 1619703753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -32.10000000000008\n",
      "  episode_reward_mean: -50.24100000000004\n",
      "  episode_reward_min: -69.00000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 840\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9638047218322754\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014680455438792706\n",
      "          model: {}\n",
      "          policy_loss: -0.03986230492591858\n",
      "          total_loss: 79.18544006347656\n",
      "          vf_explained_var: 0.10236646980047226\n",
      "          vf_loss: 79.21044158935547\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9819333553314209\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013715706765651703\n",
      "          model: {}\n",
      "          policy_loss: -0.051224276423454285\n",
      "          total_loss: 25.609371185302734\n",
      "          vf_explained_var: 0.3732236623764038\n",
      "          vf_loss: 25.646709442138672\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.65\n",
      "    ram_util_percent: 61.175\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -15.5\n",
      "    pol2: 22.900000000000013\n",
      "  policy_reward_mean:\n",
      "    pol1: -37.215\n",
      "    pol2: -13.025999999999977\n",
      "  policy_reward_min:\n",
      "    pol1: -74.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4099396820468004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2219630470158763\n",
      "    mean_inference_ms: 1.7514601998132902\n",
      "    mean_raw_obs_processing_ms: 1.702764770611705\n",
      "  time_since_restore: 133.75228834152222\n",
      "  time_this_iter_s: 5.72625207901001\n",
      "  time_total_s: 133.75228834152222\n",
      "  timers:\n",
      "    learn_throughput: 781.347\n",
      "    learn_time_ms: 5119.365\n",
      "    sample_throughput: 5213.68\n",
      "    sample_time_ms: 767.212\n",
      "    update_time_ms: 3.162\n",
      "  timestamp: 1619703753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -19.199999999999974\n",
      "  episode_reward_mean: -48.24600000000005\n",
      "  episode_reward_min: -66.90000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 840\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9783563017845154\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017409387975931168\n",
      "          model: {}\n",
      "          policy_loss: -0.05544019490480423\n",
      "          total_loss: 54.954833984375\n",
      "          vf_explained_var: 0.2432975023984909\n",
      "          vf_loss: 54.992652893066406\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9649791717529297\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016781438142061234\n",
      "          model: {}\n",
      "          policy_loss: -0.0504220649600029\n",
      "          total_loss: 6.1213507652282715\n",
      "          vf_explained_var: 0.49065637588500977\n",
      "          vf_loss: 6.154781341552734\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.0625\n",
      "    ram_util_percent: 61.1875\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -7.0\n",
      "    pol2: 38.29999999999988\n",
      "  policy_reward_mean:\n",
      "    pol1: -32.415\n",
      "    pol2: -15.830999999999971\n",
      "  policy_reward_min:\n",
      "    pol1: -60.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3924941762349656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2161619037881222\n",
      "    mean_inference_ms: 1.6940818178816688\n",
      "    mean_raw_obs_processing_ms: 1.615861609113615\n",
      "  time_since_restore: 133.74311923980713\n",
      "  time_this_iter_s: 5.84079384803772\n",
      "  time_total_s: 133.74311923980713\n",
      "  timers:\n",
      "    learn_throughput: 780.362\n",
      "    learn_time_ms: 5125.827\n",
      "    sample_throughput: 5285.991\n",
      "    sample_time_ms: 756.717\n",
      "    update_time_ms: 3.176\n",
      "  timestamp: 1619703754\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -22.49999999999997\n",
      "  episode_reward_mean: -38.59799999999999\n",
      "  episode_reward_min: -57.90000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 880\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9266906976699829\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013840598054230213\n",
      "          model: {}\n",
      "          policy_loss: -0.04020845517516136\n",
      "          total_loss: 39.30078887939453\n",
      "          vf_explained_var: 0.1618736982345581\n",
      "          vf_loss: 39.32698440551758\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9477399587631226\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013988479971885681\n",
      "          model: {}\n",
      "          policy_loss: -0.04264369606971741\n",
      "          total_loss: 7.973800182342529\n",
      "          vf_explained_var: 0.38087645173072815\n",
      "          vf_loss: 8.002280235290527\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.1\n",
      "    ram_util_percent: 61.8875\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -5.0\n",
      "    pol2: 0.8999999999999908\n",
      "  policy_reward_mean:\n",
      "    pol1: -23.35\n",
      "    pol2: -15.247999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -44.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40079085608969434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21942860030077838\n",
      "    mean_inference_ms: 1.733980808939328\n",
      "    mean_raw_obs_processing_ms: 1.6571392940703467\n",
      "  time_since_restore: 138.73743414878845\n",
      "  time_this_iter_s: 5.36388087272644\n",
      "  time_total_s: 138.73743414878845\n",
      "  timers:\n",
      "    learn_throughput: 787.763\n",
      "    learn_time_ms: 5077.667\n",
      "    sample_throughput: 5148.062\n",
      "    sample_time_ms: 776.991\n",
      "    update_time_ms: 3.659\n",
      "  timestamp: 1619703759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         134.036</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\"> -48.12 </td><td style=\"text-align: right;\">               -30  </td><td style=\"text-align: right;\">               -72  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         138.737</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\"> -38.598</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               -57.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         133.743</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\"> -48.246</td><td style=\"text-align: right;\">               -19.2</td><td style=\"text-align: right;\">               -66.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         133.752</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\"> -50.241</td><td style=\"text-align: right;\">               -32.1</td><td style=\"text-align: right;\">               -69  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -26.69999999999997\n",
      "  episode_reward_mean: -47.15700000000003\n",
      "  episode_reward_min: -71.70000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 880\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9587459564208984\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01614401675760746\n",
      "          model: {}\n",
      "          policy_loss: -0.0503421388566494\n",
      "          total_loss: 65.3270492553711\n",
      "          vf_explained_var: 0.10377812385559082\n",
      "          vf_loss: 65.36104583740234\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9224126935005188\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013802655041217804\n",
      "          model: {}\n",
      "          policy_loss: -0.03384111821651459\n",
      "          total_loss: 10.009943008422852\n",
      "          vf_explained_var: 0.4086005687713623\n",
      "          vf_loss: 10.029808044433594\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.9125\n",
      "    ram_util_percent: 61.875\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -10.0\n",
      "    pol2: 7.499999999999992\n",
      "  policy_reward_mean:\n",
      "    pol1: -33.13\n",
      "    pol2: -14.026999999999976\n",
      "  policy_reward_min:\n",
      "    pol1: -60.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41885637571622847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.232847432618255\n",
      "    mean_inference_ms: 1.7847429688030194\n",
      "    mean_raw_obs_processing_ms: 1.7372541916523823\n",
      "  time_since_restore: 139.4779498577118\n",
      "  time_this_iter_s: 5.441948175430298\n",
      "  time_total_s: 139.4779498577118\n",
      "  timers:\n",
      "    learn_throughput: 786.367\n",
      "    learn_time_ms: 5086.684\n",
      "    sample_throughput: 5179.064\n",
      "    sample_time_ms: 772.34\n",
      "    update_time_ms: 3.668\n",
      "  timestamp: 1619703759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -23.699999999999974\n",
      "  episode_reward_mean: -48.43500000000003\n",
      "  episode_reward_min: -72.90000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 880\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9702227115631104\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016380881890654564\n",
      "          model: {}\n",
      "          policy_loss: -0.05217157304286957\n",
      "          total_loss: 65.66386413574219\n",
      "          vf_explained_var: 0.15085838735103607\n",
      "          vf_loss: 65.69944763183594\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9452311396598816\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01574844866991043\n",
      "          model: {}\n",
      "          policy_loss: -0.050195470452308655\n",
      "          total_loss: 6.343968391418457\n",
      "          vf_explained_var: 0.46855753660202026\n",
      "          vf_loss: 6.378218650817871\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.125\n",
      "    ram_util_percent: 61.8875\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -7.0\n",
      "    pol2: 18.500000000000018\n",
      "  policy_reward_mean:\n",
      "    pol1: -31.405\n",
      "    pol2: -17.02999999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -54.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3909248089437476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21521629975897377\n",
      "    mean_inference_ms: 1.6865019385478748\n",
      "    mean_raw_obs_processing_ms: 1.6110826488126826\n",
      "  time_since_restore: 139.28543329238892\n",
      "  time_this_iter_s: 5.542314052581787\n",
      "  time_total_s: 139.28543329238892\n",
      "  timers:\n",
      "    learn_throughput: 783.772\n",
      "    learn_time_ms: 5103.527\n",
      "    sample_throughput: 5270.869\n",
      "    sample_time_ms: 758.888\n",
      "    update_time_ms: 3.168\n",
      "  timestamp: 1619703759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -32.10000000000008\n",
      "  episode_reward_mean: -49.632000000000055\n",
      "  episode_reward_min: -69.00000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 880\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9634628295898438\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014697089791297913\n",
      "          model: {}\n",
      "          policy_loss: -0.038513630628585815\n",
      "          total_loss: 81.20675659179688\n",
      "          vf_explained_var: 0.13038428127765656\n",
      "          vf_loss: 81.23039245605469\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9626083374023438\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013884304091334343\n",
      "          model: {}\n",
      "          policy_loss: -0.04177999496459961\n",
      "          total_loss: 25.29852294921875\n",
      "          vf_explained_var: 0.2790951728820801\n",
      "          vf_loss: 25.326244354248047\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.025\n",
      "    ram_util_percent: 61.8875\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -16.0\n",
      "    pol2: 22.900000000000013\n",
      "  policy_reward_mean:\n",
      "    pol1: -37.97\n",
      "    pol2: -11.661999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -74.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40870411588082817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22113398748409324\n",
      "    mean_inference_ms: 1.7441856724206601\n",
      "    mean_raw_obs_processing_ms: 1.6980437947549416\n",
      "  time_since_restore: 139.3943612575531\n",
      "  time_this_iter_s: 5.642072916030884\n",
      "  time_total_s: 139.3943612575531\n",
      "  timers:\n",
      "    learn_throughput: 784.905\n",
      "    learn_time_ms: 5096.159\n",
      "    sample_throughput: 5121.691\n",
      "    sample_time_ms: 780.992\n",
      "    update_time_ms: 3.17\n",
      "  timestamp: 1619703759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -26.69999999999997\n",
      "  episode_reward_mean: -48.10800000000005\n",
      "  episode_reward_min: -81.6000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 920\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9417396783828735\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01441585086286068\n",
      "          model: {}\n",
      "          policy_loss: -0.047279175370931625\n",
      "          total_loss: 70.12178039550781\n",
      "          vf_explained_var: 0.0944022536277771\n",
      "          vf_loss: 70.15446472167969\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8958992958068848\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015011853538453579\n",
      "          model: {}\n",
      "          policy_loss: -0.05150451138615608\n",
      "          total_loss: 8.460960388183594\n",
      "          vf_explained_var: 0.39871346950531006\n",
      "          vf_loss: 8.497264862060547\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.9625\n",
      "    ram_util_percent: 61.912499999999994\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -10.0\n",
      "    pol2: 7.499999999999992\n",
      "  policy_reward_mean:\n",
      "    pol1: -34.235\n",
      "    pol2: -13.872999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -77.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41557614544433025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.231116351973534\n",
      "    mean_inference_ms: 1.770640423759616\n",
      "    mean_raw_obs_processing_ms: 1.7246525730378535\n",
      "  time_since_restore: 144.8277108669281\n",
      "  time_this_iter_s: 5.349761009216309\n",
      "  time_total_s: 144.8277108669281\n",
      "  timers:\n",
      "    learn_throughput: 812.716\n",
      "    learn_time_ms: 4921.771\n",
      "    sample_throughput: 5359.96\n",
      "    sample_time_ms: 746.274\n",
      "    update_time_ms: 3.678\n",
      "  timestamp: 1619703764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         144.828</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\"> -48.108</td><td style=\"text-align: right;\">               -26.7</td><td style=\"text-align: right;\">               -81.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         138.737</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\"> -38.598</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               -57.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         139.285</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\"> -48.435</td><td style=\"text-align: right;\">               -23.7</td><td style=\"text-align: right;\">               -72.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         139.394</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\"> -49.632</td><td style=\"text-align: right;\">               -32.1</td><td style=\"text-align: right;\">               -69  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -21.599999999999977\n",
      "  episode_reward_mean: -38.175000000000004\n",
      "  episode_reward_min: -57.90000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 920\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9347695708274841\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013316778466105461\n",
      "          model: {}\n",
      "          policy_loss: -0.04786767065525055\n",
      "          total_loss: 43.17835998535156\n",
      "          vf_explained_var: 0.11864691972732544\n",
      "          vf_loss: 43.212745666503906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9400485157966614\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01423792913556099\n",
      "          model: {}\n",
      "          policy_loss: -0.046732183545827866\n",
      "          total_loss: 10.795941352844238\n",
      "          vf_explained_var: 0.23283152282238007\n",
      "          vf_loss: 10.828258514404297\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.3625\n",
      "    ram_util_percent: 61.925\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -5.0\n",
      "    pol2: -1.2999999999999687\n",
      "  policy_reward_mean:\n",
      "    pol1: -23.235\n",
      "    pol2: -14.939999999999975\n",
      "  policy_reward_min:\n",
      "    pol1: -40.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3980238747663766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21810949396057744\n",
      "    mean_inference_ms: 1.7206156263627748\n",
      "    mean_raw_obs_processing_ms: 1.6452744415356912\n",
      "  time_since_restore: 144.14917922019958\n",
      "  time_this_iter_s: 5.411745071411133\n",
      "  time_total_s: 144.14917922019958\n",
      "  timers:\n",
      "    learn_throughput: 813.626\n",
      "    learn_time_ms: 4916.264\n",
      "    sample_throughput: 5227.528\n",
      "    sample_time_ms: 765.18\n",
      "    update_time_ms: 3.622\n",
      "  timestamp: 1619703764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -24.29999999999999\n",
      "  episode_reward_mean: -47.772000000000034\n",
      "  episode_reward_min: -72.90000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 920\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9537280797958374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016913816332817078\n",
      "          model: {}\n",
      "          policy_loss: -0.04774463176727295\n",
      "          total_loss: 53.926605224609375\n",
      "          vf_explained_var: 0.16469570994377136\n",
      "          vf_loss: 53.95722579956055\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9338424205780029\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015330919064581394\n",
      "          model: {}\n",
      "          policy_loss: -0.05303322523832321\n",
      "          total_loss: 7.064981460571289\n",
      "          vf_explained_var: 0.4749022126197815\n",
      "          vf_loss: 7.102491855621338\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.3625\n",
      "    ram_util_percent: 61.925\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -6.5\n",
      "    pol2: -4.599999999999966\n",
      "  policy_reward_mean:\n",
      "    pol1: -30.555\n",
      "    pol2: -17.216999999999967\n",
      "  policy_reward_min:\n",
      "    pol1: -54.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.389641215572851\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21442921510111104\n",
      "    mean_inference_ms: 1.6788368008532868\n",
      "    mean_raw_obs_processing_ms: 1.607034715956025\n",
      "  time_since_restore: 144.66720366477966\n",
      "  time_this_iter_s: 5.381770372390747\n",
      "  time_total_s: 144.66720366477966\n",
      "  timers:\n",
      "    learn_throughput: 809.256\n",
      "    learn_time_ms: 4942.811\n",
      "    sample_throughput: 5341.006\n",
      "    sample_time_ms: 748.923\n",
      "    update_time_ms: 3.159\n",
      "  timestamp: 1619703765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -29.39999999999997\n",
      "  episode_reward_mean: -47.88000000000002\n",
      "  episode_reward_min: -67.5000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 920\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9478334188461304\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014486778527498245\n",
      "          model: {}\n",
      "          policy_loss: -0.046899788081645966\n",
      "          total_loss: 81.86964416503906\n",
      "          vf_explained_var: 0.11323902010917664\n",
      "          vf_loss: 81.9018783569336\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.944182813167572\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012851613573729992\n",
      "          model: {}\n",
      "          policy_loss: -0.04639533907175064\n",
      "          total_loss: 28.388490676879883\n",
      "          vf_explained_var: 0.3559504449367523\n",
      "          vf_loss: 28.421875\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.65\n",
      "    ram_util_percent: 61.925\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -16.0\n",
      "    pol2: 24.00000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -37.835\n",
      "    pol2: -10.044999999999977\n",
      "  policy_reward_min:\n",
      "    pol1: -67.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40931272870561924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22123972746109175\n",
      "    mean_inference_ms: 1.7411947353950767\n",
      "    mean_raw_obs_processing_ms: 1.6996721685147238\n",
      "  time_since_restore: 144.98941326141357\n",
      "  time_this_iter_s: 5.595052003860474\n",
      "  time_total_s: 144.98941326141357\n",
      "  timers:\n",
      "    learn_throughput: 810.907\n",
      "    learn_time_ms: 4932.751\n",
      "    sample_throughput: 5052.2\n",
      "    sample_time_ms: 791.734\n",
      "    update_time_ms: 3.127\n",
      "  timestamp: 1619703765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.3\n",
      "  episode_reward_mean: -47.52600000000004\n",
      "  episode_reward_min: -81.6000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 960\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9363123178482056\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013352429494261742\n",
      "          model: {}\n",
      "          policy_loss: -0.04502127692103386\n",
      "          total_loss: 63.663902282714844\n",
      "          vf_explained_var: 0.11999409645795822\n",
      "          vf_loss: 63.69540023803711\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8901071548461914\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013919470831751823\n",
      "          model: {}\n",
      "          policy_loss: -0.041101813316345215\n",
      "          total_loss: 11.388019561767578\n",
      "          vf_explained_var: 0.312308132648468\n",
      "          vf_loss: 11.415027618408203\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.52727272727272\n",
      "    ram_util_percent: 63.35454545454545\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -15.5\n",
      "    pol2: 9.700000000000008\n",
      "  policy_reward_mean:\n",
      "    pol1: -34.005\n",
      "    pol2: -13.520999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -77.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41315471800630943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22972704407352015\n",
      "    mean_inference_ms: 1.7586933807963703\n",
      "    mean_raw_obs_processing_ms: 1.7151015284774696\n",
      "  time_since_restore: 152.45634365081787\n",
      "  time_this_iter_s: 7.6286327838897705\n",
      "  time_total_s: 152.45634365081787\n",
      "  timers:\n",
      "    learn_throughput: 791.475\n",
      "    learn_time_ms: 5053.857\n",
      "    sample_throughput: 5414.465\n",
      "    sample_time_ms: 738.762\n",
      "    update_time_ms: 3.809\n",
      "  timestamp: 1619703772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         152.456</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\"> -47.526</td><td style=\"text-align: right;\">               -33.3</td><td style=\"text-align: right;\">               -81.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         144.149</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\"> -38.175</td><td style=\"text-align: right;\">               -21.6</td><td style=\"text-align: right;\">               -57.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         144.667</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\"> -47.772</td><td style=\"text-align: right;\">               -24.3</td><td style=\"text-align: right;\">               -72.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         144.989</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\"> -47.88 </td><td style=\"text-align: right;\">               -29.4</td><td style=\"text-align: right;\">               -67.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -21.599999999999977\n",
      "  episode_reward_mean: -37.641000000000005\n",
      "  episode_reward_min: -57.600000000000115\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 960\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9331375360488892\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014219926670193672\n",
      "          model: {}\n",
      "          policy_loss: -0.04750514775514603\n",
      "          total_loss: 55.11891555786133\n",
      "          vf_explained_var: 0.143527090549469\n",
      "          vf_loss: 55.15201950073242\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9134708642959595\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013876590877771378\n",
      "          model: {}\n",
      "          policy_loss: -0.05445338785648346\n",
      "          total_loss: 12.18422794342041\n",
      "          vf_explained_var: 0.24284663796424866\n",
      "          vf_loss: 12.224630355834961\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.51818181818183\n",
      "    ram_util_percent: 63.22727272727274\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -2.5\n",
      "    pol2: 4.200000000000019\n",
      "  policy_reward_mean:\n",
      "    pol1: -23.35\n",
      "    pol2: -14.290999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -58.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39650460531725557\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2173682284790384\n",
      "    mean_inference_ms: 1.7114516335063592\n",
      "    mean_raw_obs_processing_ms: 1.6396397717967681\n",
      "  time_since_restore: 151.83016324043274\n",
      "  time_this_iter_s: 7.680984020233154\n",
      "  time_total_s: 151.83016324043274\n",
      "  timers:\n",
      "    learn_throughput: 791.226\n",
      "    learn_time_ms: 5055.444\n",
      "    sample_throughput: 5368.219\n",
      "    sample_time_ms: 745.126\n",
      "    update_time_ms: 3.691\n",
      "  timestamp: 1619703772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -22.499999999999982\n",
      "  episode_reward_mean: -46.93200000000004\n",
      "  episode_reward_min: -68.40000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 960\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9299414753913879\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014846532605588436\n",
      "          model: {}\n",
      "          policy_loss: -0.042955316603183746\n",
      "          total_loss: 59.372474670410156\n",
      "          vf_explained_var: 0.23838233947753906\n",
      "          vf_loss: 59.40039825439453\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.935820460319519\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01484526228159666\n",
      "          model: {}\n",
      "          policy_loss: -0.050429943948984146\n",
      "          total_loss: 7.184126853942871\n",
      "          vf_explained_var: 0.4479959309101105\n",
      "          vf_loss: 7.219525337219238\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.78181818181818\n",
      "    ram_util_percent: 63.22727272727274\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -2.5\n",
      "    pol2: -4.599999999999966\n",
      "  policy_reward_mean:\n",
      "    pol1: -30.155\n",
      "    pol2: -16.77699999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -54.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3893733823314436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21410617563089765\n",
      "    mean_inference_ms: 1.672762775517324\n",
      "    mean_raw_obs_processing_ms: 1.6063654774119018\n",
      "  time_since_restore: 152.44846987724304\n",
      "  time_this_iter_s: 7.781266212463379\n",
      "  time_total_s: 152.44846987724304\n",
      "  timers:\n",
      "    learn_throughput: 786.743\n",
      "    learn_time_ms: 5084.254\n",
      "    sample_throughput: 5362.113\n",
      "    sample_time_ms: 745.975\n",
      "    update_time_ms: 3.547\n",
      "  timestamp: 1619703772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -26.999999999999986\n",
      "  episode_reward_mean: -46.64400000000004\n",
      "  episode_reward_min: -70.20000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 960\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9175825715065002\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015217426232993603\n",
      "          model: {}\n",
      "          policy_loss: -0.04181092232465744\n",
      "          total_loss: 76.72297668457031\n",
      "          vf_explained_var: 0.17453530430793762\n",
      "          vf_loss: 76.74938201904297\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9423220157623291\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014448419213294983\n",
      "          model: {}\n",
      "          policy_loss: -0.0533280074596405\n",
      "          total_loss: 20.842082977294922\n",
      "          vf_explained_var: 0.2968999445438385\n",
      "          vf_loss: 20.880783081054688\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.16666666666667\n",
      "    ram_util_percent: 63.150000000000006\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -12.0\n",
      "    pol2: 24.00000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -35.565\n",
      "    pol2: -11.078999999999978\n",
      "  policy_reward_min:\n",
      "    pol1: -72.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41149278790685484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22180745664117751\n",
      "    mean_inference_ms: 1.7420786525396221\n",
      "    mean_raw_obs_processing_ms: 1.7103993488519649\n",
      "  time_since_restore: 152.90794229507446\n",
      "  time_this_iter_s: 7.918529033660889\n",
      "  time_total_s: 152.90794229507446\n",
      "  timers:\n",
      "    learn_throughput: 785.125\n",
      "    learn_time_ms: 5094.732\n",
      "    sample_throughput: 4967.159\n",
      "    sample_time_ms: 805.289\n",
      "    update_time_ms: 3.109\n",
      "  timestamp: 1619703773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.499999999999968\n",
      "  episode_reward_mean: -45.65700000000003\n",
      "  episode_reward_min: -70.50000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1000\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9110726118087769\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013727908954024315\n",
      "          model: {}\n",
      "          policy_loss: -0.045359522104263306\n",
      "          total_loss: 57.83357620239258\n",
      "          vf_explained_var: 0.12834873795509338\n",
      "          vf_loss: 57.86503601074219\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8452452421188354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015132013708353043\n",
      "          model: {}\n",
      "          policy_loss: -0.05124448984861374\n",
      "          total_loss: 7.062633991241455\n",
      "          vf_explained_var: 0.3907266855239868\n",
      "          vf_loss: 7.098557472229004\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.1\n",
      "    ram_util_percent: 63.14444444444444\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -14.0\n",
      "    pol2: 9.700000000000008\n",
      "  policy_reward_mean:\n",
      "    pol1: -32.07\n",
      "    pol2: -13.586999999999975\n",
      "  policy_reward_min:\n",
      "    pol1: -56.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4136471726189565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22983485990924243\n",
      "    mean_inference_ms: 1.7618423567245176\n",
      "    mean_raw_obs_processing_ms: 1.7157151312967154\n",
      "  time_since_restore: 158.63415050506592\n",
      "  time_this_iter_s: 6.177806854248047\n",
      "  time_total_s: 158.63415050506592\n",
      "  timers:\n",
      "    learn_throughput: 795.376\n",
      "    learn_time_ms: 5029.066\n",
      "    sample_throughput: 5090.716\n",
      "    sample_time_ms: 785.744\n",
      "    update_time_ms: 3.652\n",
      "  timestamp: 1619703778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         158.634</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\"> -45.657</td><td style=\"text-align: right;\">               -28.5</td><td style=\"text-align: right;\">               -70.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         151.83 </td><td style=\"text-align: right;\"> 96000</td><td style=\"text-align: right;\"> -37.641</td><td style=\"text-align: right;\">               -21.6</td><td style=\"text-align: right;\">               -57.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         152.448</td><td style=\"text-align: right;\"> 96000</td><td style=\"text-align: right;\"> -46.932</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               -68.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         152.908</td><td style=\"text-align: right;\"> 96000</td><td style=\"text-align: right;\"> -46.644</td><td style=\"text-align: right;\">               -27  </td><td style=\"text-align: right;\">               -70.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20.999999999999964\n",
      "  episode_reward_mean: -36.309\n",
      "  episode_reward_min: -58.20000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9093656539916992\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015630370005965233\n",
      "          model: {}\n",
      "          policy_loss: -0.04836378991603851\n",
      "          total_loss: 38.12422180175781\n",
      "          vf_explained_var: 0.11646772176027298\n",
      "          vf_loss: 38.15675354003906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9031468629837036\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015761232003569603\n",
      "          model: {}\n",
      "          policy_loss: -0.043126270174980164\n",
      "          total_loss: 7.499978065490723\n",
      "          vf_explained_var: 0.2422107756137848\n",
      "          vf_loss: 7.527146339416504\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.0375\n",
      "    ram_util_percent: 63.1625\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -2.5\n",
      "    pol2: 4.200000000000019\n",
      "  policy_reward_mean:\n",
      "    pol1: -22.37\n",
      "    pol2: -13.938999999999979\n",
      "  policy_reward_min:\n",
      "    pol1: -58.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3976533702563397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21786251570990295\n",
      "    mean_inference_ms: 1.7137087893209144\n",
      "    mean_raw_obs_processing_ms: 1.6440875187934985\n",
      "  time_since_restore: 157.90734338760376\n",
      "  time_this_iter_s: 6.0771801471710205\n",
      "  time_total_s: 157.90734338760376\n",
      "  timers:\n",
      "    learn_throughput: 796.071\n",
      "    learn_time_ms: 5024.675\n",
      "    sample_throughput: 5207.517\n",
      "    sample_time_ms: 768.12\n",
      "    update_time_ms: 3.603\n",
      "  timestamp: 1619703778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -22.499999999999982\n",
      "  episode_reward_mean: -44.76900000000003\n",
      "  episode_reward_min: -67.5000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1000\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.902560293674469\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016256898641586304\n",
      "          model: {}\n",
      "          policy_loss: -0.055917948484420776\n",
      "          total_loss: 51.89548873901367\n",
      "          vf_explained_var: 0.2539639174938202\n",
      "          vf_loss: 51.93494415283203\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9095312356948853\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015360580757260323\n",
      "          model: {}\n",
      "          policy_loss: -0.04668129235506058\n",
      "          total_loss: 7.812265396118164\n",
      "          vf_explained_var: 0.3546315133571625\n",
      "          vf_loss: 7.8433942794799805\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.1888888888889\n",
      "    ram_util_percent: 63.22222222222222\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -2.5\n",
      "    pol2: 0.8999999999999919\n",
      "  policy_reward_mean:\n",
      "    pol1: -28.245\n",
      "    pol2: -16.523999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -53.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3905118200701645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21440859808894575\n",
      "    mean_inference_ms: 1.6717506197126988\n",
      "    mean_raw_obs_processing_ms: 1.6132169774489358\n",
      "  time_since_restore: 158.48524689674377\n",
      "  time_this_iter_s: 6.036777019500732\n",
      "  time_total_s: 158.48524689674377\n",
      "  timers:\n",
      "    learn_throughput: 793.13\n",
      "    learn_time_ms: 5043.31\n",
      "    sample_throughput: 5103.632\n",
      "    sample_time_ms: 783.756\n",
      "    update_time_ms: 3.603\n",
      "  timestamp: 1619703778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-42-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -26.999999999999986\n",
      "  episode_reward_mean: -46.71900000000003\n",
      "  episode_reward_min: -70.20000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9102777242660522\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014085078611969948\n",
      "          model: {}\n",
      "          policy_loss: -0.03399927169084549\n",
      "          total_loss: 89.39308166503906\n",
      "          vf_explained_var: 0.09842950105667114\n",
      "          vf_loss: 89.41282653808594\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9068981409072876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012342226691544056\n",
      "          model: {}\n",
      "          policy_loss: -0.036592647433280945\n",
      "          total_loss: 29.027385711669922\n",
      "          vf_explained_var: 0.30777761340141296\n",
      "          vf_loss: 29.05147933959961\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.575\n",
      "    ram_util_percent: 63.3125\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -12.0\n",
      "    pol2: 38.29999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: -36.245\n",
      "    pol2: -10.473999999999979\n",
      "  policy_reward_min:\n",
      "    pol1: -72.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4126989157671923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2219018473169195\n",
      "    mean_inference_ms: 1.7411685437724407\n",
      "    mean_raw_obs_processing_ms: 1.7178167915805866\n",
      "  time_since_restore: 158.63827538490295\n",
      "  time_this_iter_s: 5.730333089828491\n",
      "  time_total_s: 158.63827538490295\n",
      "  timers:\n",
      "    learn_throughput: 791.241\n",
      "    learn_time_ms: 5055.35\n",
      "    sample_throughput: 4891.143\n",
      "    sample_time_ms: 817.805\n",
      "    update_time_ms: 3.048\n",
      "  timestamp: 1619703779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -26.399999999999977\n",
      "  episode_reward_mean: -45.309000000000026\n",
      "  episode_reward_min: -66.30000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1040\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8984600305557251\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013633957132697105\n",
      "          model: {}\n",
      "          policy_loss: -0.052944086492061615\n",
      "          total_loss: 68.3830337524414\n",
      "          vf_explained_var: 0.13159221410751343\n",
      "          vf_loss: 68.42218017578125\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8388519287109375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01595274731516838\n",
      "          model: {}\n",
      "          policy_loss: -0.044688668102025986\n",
      "          total_loss: 7.778470039367676\n",
      "          vf_explained_var: 0.38208773732185364\n",
      "          vf_loss: 7.807006359100342\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.48750000000001\n",
      "    ram_util_percent: 63.5875\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -13.0\n",
      "    pol2: 9.700000000000008\n",
      "  policy_reward_mean:\n",
      "    pol1: -32.14\n",
      "    pol2: -13.168999999999977\n",
      "  policy_reward_min:\n",
      "    pol1: -57.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4139118684957937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22991310503926332\n",
      "    mean_inference_ms: 1.7644240444356114\n",
      "    mean_raw_obs_processing_ms: 1.7157806351653289\n",
      "  time_since_restore: 164.39387845993042\n",
      "  time_this_iter_s: 5.759727954864502\n",
      "  time_total_s: 164.39387845993042\n",
      "  timers:\n",
      "    learn_throughput: 808.073\n",
      "    learn_time_ms: 4950.046\n",
      "    sample_throughput: 5002.289\n",
      "    sample_time_ms: 799.634\n",
      "    update_time_ms: 3.592\n",
      "  timestamp: 1619703784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         164.394</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\"> -45.309</td><td style=\"text-align: right;\">               -26.4</td><td style=\"text-align: right;\">               -66.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         157.907</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\"> -36.309</td><td style=\"text-align: right;\">               -21  </td><td style=\"text-align: right;\">               -58.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         158.485</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\"> -44.769</td><td style=\"text-align: right;\">               -22.5</td><td style=\"text-align: right;\">               -67.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         158.638</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\"> -46.719</td><td style=\"text-align: right;\">               -27  </td><td style=\"text-align: right;\">               -70.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -23.999999999999993\n",
      "  episode_reward_mean: -42.870000000000026\n",
      "  episode_reward_min: -67.5000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1040\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8917433023452759\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01605263352394104\n",
      "          model: {}\n",
      "          policy_loss: -0.04632389545440674\n",
      "          total_loss: 43.24937438964844\n",
      "          vf_explained_var: 0.17000192403793335\n",
      "          vf_loss: 43.279441833496094\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.905520498752594\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014447236433625221\n",
      "          model: {}\n",
      "          policy_loss: -0.0470738485455513\n",
      "          total_loss: 7.805529594421387\n",
      "          vf_explained_var: 0.43209898471832275\n",
      "          vf_loss: 7.83797550201416\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.85000000000001\n",
      "    ram_util_percent: 63.55\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -4.0\n",
      "    pol2: 0.8999999999999919\n",
      "  policy_reward_mean:\n",
      "    pol1: -26.445\n",
      "    pol2: -16.42499999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -55.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39066163860251124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21442713668089347\n",
      "    mean_inference_ms: 1.6677583310972457\n",
      "    mean_raw_obs_processing_ms: 1.618027683679335\n",
      "  time_since_restore: 164.10899782180786\n",
      "  time_this_iter_s: 5.623750925064087\n",
      "  time_total_s: 164.10899782180786\n",
      "  timers:\n",
      "    learn_throughput: 807.554\n",
      "    learn_time_ms: 4953.229\n",
      "    sample_throughput: 5064.193\n",
      "    sample_time_ms: 789.859\n",
      "    update_time_ms: 3.513\n",
      "  timestamp: 1619703784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20.999999999999964\n",
      "  episode_reward_mean: -34.983000000000004\n",
      "  episode_reward_min: -58.20000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1040\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.890700101852417\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014167441986501217\n",
      "          model: {}\n",
      "          policy_loss: -0.04134555160999298\n",
      "          total_loss: 40.558902740478516\n",
      "          vf_explained_var: 0.11146275699138641\n",
      "          vf_loss: 40.58590316772461\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8845527768135071\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013819655403494835\n",
      "          model: {}\n",
      "          policy_loss: -0.05307955667376518\n",
      "          total_loss: 10.95443344116211\n",
      "          vf_explained_var: 0.26813510060310364\n",
      "          vf_loss: 10.993520736694336\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.422222222222224\n",
      "    ram_util_percent: 63.55555555555556\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -2.5\n",
      "    pol2: 1.9999999999999967\n",
      "  policy_reward_mean:\n",
      "    pol1: -20.67\n",
      "    pol2: -14.312999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -42.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3991372200098462\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21859332647509142\n",
      "    mean_inference_ms: 1.7170579401430894\n",
      "    mean_raw_obs_processing_ms: 1.6509396386518123\n",
      "  time_since_restore: 163.7657995223999\n",
      "  time_this_iter_s: 5.858456134796143\n",
      "  time_total_s: 163.7657995223999\n",
      "  timers:\n",
      "    learn_throughput: 807.63\n",
      "    learn_time_ms: 4952.761\n",
      "    sample_throughput: 5074.882\n",
      "    sample_time_ms: 788.196\n",
      "    update_time_ms: 3.512\n",
      "  timestamp: 1619703784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -26.999999999999986\n",
      "  episode_reward_mean: -47.25300000000003\n",
      "  episode_reward_min: -69.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1040\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8962021470069885\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014525264501571655\n",
      "          model: {}\n",
      "          policy_loss: -0.046054400503635406\n",
      "          total_loss: 69.69686889648438\n",
      "          vf_explained_var: 0.16898751258850098\n",
      "          vf_loss: 69.72821044921875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.897473931312561\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015123259276151657\n",
      "          model: {}\n",
      "          policy_loss: -0.055186741054058075\n",
      "          total_loss: 13.756998062133789\n",
      "          vf_explained_var: 0.34852418303489685\n",
      "          vf_loss: 13.796873092651367\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.775\n",
      "    ram_util_percent: 63.5625\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -12.0\n",
      "    pol2: 38.29999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: -35.855\n",
      "    pol2: -11.397999999999977\n",
      "  policy_reward_min:\n",
      "    pol1: -69.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.41196636012555854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22133236131323458\n",
      "    mean_inference_ms: 1.734761403885321\n",
      "    mean_raw_obs_processing_ms: 1.7179509895964238\n",
      "  time_since_restore: 164.21825242042542\n",
      "  time_this_iter_s: 5.579977035522461\n",
      "  time_total_s: 164.21825242042542\n",
      "  timers:\n",
      "    learn_throughput: 809.487\n",
      "    learn_time_ms: 4941.4\n",
      "    sample_throughput: 4919.539\n",
      "    sample_time_ms: 813.084\n",
      "    update_time_ms: 3.002\n",
      "  timestamp: 1619703784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -26.399999999999977\n",
      "  episode_reward_mean: -46.03800000000002\n",
      "  episode_reward_min: -70.8000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1080\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8889001607894897\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013041079044342041\n",
      "          model: {}\n",
      "          policy_loss: -0.041091032326221466\n",
      "          total_loss: 63.51601791381836\n",
      "          vf_explained_var: 0.1394766867160797\n",
      "          vf_loss: 63.54390335083008\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8297728896141052\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01341041550040245\n",
      "          model: {}\n",
      "          policy_loss: -0.04282525181770325\n",
      "          total_loss: 8.453932762145996\n",
      "          vf_explained_var: 0.36919423937797546\n",
      "          vf_loss: 8.483180046081543\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.38888888888889\n",
      "    ram_util_percent: 63.400000000000006\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -13.0\n",
      "    pol2: 0.9000000000000175\n",
      "  policy_reward_mean:\n",
      "    pol1: -32.55\n",
      "    pol2: -13.487999999999975\n",
      "  policy_reward_min:\n",
      "    pol1: -58.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.412715654849651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2293136701295241\n",
      "    mean_inference_ms: 1.7592308808786619\n",
      "    mean_raw_obs_processing_ms: 1.7117912109741098\n",
      "  time_since_restore: 170.67300963401794\n",
      "  time_this_iter_s: 6.279131174087524\n",
      "  time_total_s: 170.67300963401794\n",
      "  timers:\n",
      "    learn_throughput: 801.869\n",
      "    learn_time_ms: 4988.344\n",
      "    sample_throughput: 5105.117\n",
      "    sample_time_ms: 783.528\n",
      "    update_time_ms: 3.532\n",
      "  timestamp: 1619703790\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         170.673</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\"> -46.038</td><td style=\"text-align: right;\">               -26.4</td><td style=\"text-align: right;\">               -70.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         163.766</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\"> -34.983</td><td style=\"text-align: right;\">               -21  </td><td style=\"text-align: right;\">               -58.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         164.109</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\"> -42.87 </td><td style=\"text-align: right;\">               -24  </td><td style=\"text-align: right;\">               -67.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         164.218</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\"> -47.253</td><td style=\"text-align: right;\">               -27  </td><td style=\"text-align: right;\">               -69  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -22.799999999999994\n",
      "  episode_reward_mean: -46.914000000000044\n",
      "  episode_reward_min: -67.5000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1080\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8817446231842041\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0162050724029541\n",
      "          model: {}\n",
      "          policy_loss: -0.05816159024834633\n",
      "          total_loss: 72.2748794555664\n",
      "          vf_explained_var: 0.15604007244110107\n",
      "          vf_loss: 72.31663513183594\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8830374479293823\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012672626413404942\n",
      "          model: {}\n",
      "          policy_loss: -0.04613490402698517\n",
      "          total_loss: 18.129867553710938\n",
      "          vf_explained_var: 0.2768574655056\n",
      "          vf_loss: 18.163169860839844\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.91111111111111\n",
      "    ram_util_percent: 63.41111111111112\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -10.5\n",
      "    pol2: 20.700000000000014\n",
      "  policy_reward_mean:\n",
      "    pol1: -34.845\n",
      "    pol2: -12.068999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -59.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.409991013034652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22043993130173725\n",
      "    mean_inference_ms: 1.725149332901433\n",
      "    mean_raw_obs_processing_ms: 1.7111290265770287\n",
      "  time_since_restore: 170.50917053222656\n",
      "  time_this_iter_s: 6.2909181118011475\n",
      "  time_total_s: 170.50917053222656\n",
      "  timers:\n",
      "    learn_throughput: 802.434\n",
      "    learn_time_ms: 4984.835\n",
      "    sample_throughput: 5028.944\n",
      "    sample_time_ms: 795.396\n",
      "    update_time_ms: 2.999\n",
      "  timestamp: 1619703791\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -17.999999999999982\n",
      "  episode_reward_mean: -34.272\n",
      "  episode_reward_min: -58.20000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1080\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.891139030456543\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01426954846829176\n",
      "          model: {}\n",
      "          policy_loss: -0.05310564115643501\n",
      "          total_loss: 35.776161193847656\n",
      "          vf_explained_var: 0.1587621569633484\n",
      "          vf_loss: 35.8148193359375\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.883638322353363\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014155149459838867\n",
      "          model: {}\n",
      "          policy_loss: -0.03951254114508629\n",
      "          total_loss: 11.272812843322754\n",
      "          vf_explained_var: 0.252159059047699\n",
      "          vf_loss: 11.297991752624512\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.08888888888889\n",
      "    ram_util_percent: 63.41111111111112\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -3.5\n",
      "    pol2: 4.200000000000005\n",
      "  policy_reward_mean:\n",
      "    pol1: -19.585\n",
      "    pol2: -14.686999999999976\n",
      "  policy_reward_min:\n",
      "    pol1: -42.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39921623521512245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21871584692508114\n",
      "    mean_inference_ms: 1.7144716052299998\n",
      "    mean_raw_obs_processing_ms: 1.652649818229108\n",
      "  time_since_restore: 170.13397574424744\n",
      "  time_this_iter_s: 6.368176221847534\n",
      "  time_total_s: 170.13397574424744\n",
      "  timers:\n",
      "    learn_throughput: 800.104\n",
      "    learn_time_ms: 4999.348\n",
      "    sample_throughput: 5147.69\n",
      "    sample_time_ms: 777.048\n",
      "    update_time_ms: 3.61\n",
      "  timestamp: 1619703791\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20.39999999999996\n",
      "  episode_reward_mean: -41.655000000000015\n",
      "  episode_reward_min: -68.4000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1080\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8848386406898499\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015018774196505547\n",
      "          model: {}\n",
      "          policy_loss: -0.04337381571531296\n",
      "          total_loss: 66.1082534790039\n",
      "          vf_explained_var: 0.12730219960212708\n",
      "          vf_loss: 66.13642120361328\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8908143043518066\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01325478870421648\n",
      "          model: {}\n",
      "          policy_loss: -0.04005183279514313\n",
      "          total_loss: 12.99460220336914\n",
      "          vf_explained_var: 0.36305975914001465\n",
      "          vf_loss: 13.021232604980469\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.85555555555555\n",
      "    ram_util_percent: 63.41111111111112\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -4.0\n",
      "    pol2: 16.300000000000015\n",
      "  policy_reward_mean:\n",
      "    pol1: -24.9\n",
      "    pol2: -16.754999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -61.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38982175437462346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21400827061201733\n",
      "    mean_inference_ms: 1.6606631687900886\n",
      "    mean_raw_obs_processing_ms: 1.6176930481956828\n",
      "  time_since_restore: 170.54120874404907\n",
      "  time_this_iter_s: 6.432210922241211\n",
      "  time_total_s: 170.54120874404907\n",
      "  timers:\n",
      "    learn_throughput: 800.733\n",
      "    learn_time_ms: 4995.425\n",
      "    sample_throughput: 5098.226\n",
      "    sample_time_ms: 784.587\n",
      "    update_time_ms: 3.57\n",
      "  timestamp: 1619703791\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -26.099999999999994\n",
      "  episode_reward_mean: -46.65000000000005\n",
      "  episode_reward_min: -70.8000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1120\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8656289577484131\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014143843203783035\n",
      "          model: {}\n",
      "          policy_loss: -0.05098909139633179\n",
      "          total_loss: 72.95634460449219\n",
      "          vf_explained_var: 0.10616059601306915\n",
      "          vf_loss: 72.99301147460938\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8144806623458862\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014495890587568283\n",
      "          model: {}\n",
      "          policy_loss: -0.038784630596637726\n",
      "          total_loss: 8.534905433654785\n",
      "          vf_explained_var: 0.32660162448883057\n",
      "          vf_loss: 8.559013366699219\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.9\n",
      "    ram_util_percent: 63.12222222222223\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -10.5\n",
      "    pol2: 0.9000000000000175\n",
      "  policy_reward_mean:\n",
      "    pol1: -32.975\n",
      "    pol2: -13.674999999999976\n",
      "  policy_reward_min:\n",
      "    pol1: -58.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.411159338427754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22842935783599302\n",
      "    mean_inference_ms: 1.7517256048825203\n",
      "    mean_raw_obs_processing_ms: 1.7051677774598293\n",
      "  time_since_restore: 176.6203806400299\n",
      "  time_this_iter_s: 5.947371006011963\n",
      "  time_total_s: 176.6203806400299\n",
      "  timers:\n",
      "    learn_throughput: 801.054\n",
      "    learn_time_ms: 4993.419\n",
      "    sample_throughput: 5059.647\n",
      "    sample_time_ms: 790.569\n",
      "    update_time_ms: 3.567\n",
      "  timestamp: 1619703796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         176.62 </td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\"> -46.65 </td><td style=\"text-align: right;\">               -26.1</td><td style=\"text-align: right;\">               -70.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         170.134</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\"> -34.272</td><td style=\"text-align: right;\">               -18  </td><td style=\"text-align: right;\">               -58.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         170.541</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\"> -41.655</td><td style=\"text-align: right;\">               -20.4</td><td style=\"text-align: right;\">               -68.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         170.509</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\"> -46.914</td><td style=\"text-align: right;\">               -22.8</td><td style=\"text-align: right;\">               -67.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -22.799999999999994\n",
      "  episode_reward_mean: -45.43800000000002\n",
      "  episode_reward_min: -67.50000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1120\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8651791214942932\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014631852507591248\n",
      "          model: {}\n",
      "          policy_loss: -0.04263238608837128\n",
      "          total_loss: 70.45223999023438\n",
      "          vf_explained_var: 0.10022768378257751\n",
      "          vf_loss: 70.48005676269531\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8581981658935547\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013725592754781246\n",
      "          model: {}\n",
      "          policy_loss: -0.04349265992641449\n",
      "          total_loss: 10.581472396850586\n",
      "          vf_explained_var: 0.36021050810813904\n",
      "          vf_loss: 10.611066818237305\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.6\n",
      "    ram_util_percent: 63.0125\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -9.0\n",
      "    pol2: 20.700000000000014\n",
      "  policy_reward_mean:\n",
      "    pol1: -32.72\n",
      "    pol2: -12.717999999999975\n",
      "  policy_reward_min:\n",
      "    pol1: -59.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4080849018324655\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21946196307428525\n",
      "    mean_inference_ms: 1.717127743155338\n",
      "    mean_raw_obs_processing_ms: 1.7022008356543388\n",
      "  time_since_restore: 176.4933545589447\n",
      "  time_this_iter_s: 5.98418402671814\n",
      "  time_total_s: 176.4933545589447\n",
      "  timers:\n",
      "    learn_throughput: 800.772\n",
      "    learn_time_ms: 4995.179\n",
      "    sample_throughput: 4984.944\n",
      "    sample_time_ms: 802.416\n",
      "    update_time_ms: 3.169\n",
      "  timestamp: 1619703797\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20.099999999999966\n",
      "  episode_reward_mean: -41.79900000000001\n",
      "  episode_reward_min: -68.4000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1120\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8698265552520752\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01474808156490326\n",
      "          model: {}\n",
      "          policy_loss: -0.0580279566347599\n",
      "          total_loss: 47.27272415161133\n",
      "          vf_explained_var: 0.19254475831985474\n",
      "          vf_loss: 47.315818786621094\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8940138816833496\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015243537724018097\n",
      "          model: {}\n",
      "          policy_loss: -0.05010654404759407\n",
      "          total_loss: 8.452831268310547\n",
      "          vf_explained_var: 0.39823973178863525\n",
      "          vf_loss: 8.487503051757812\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.7125\n",
      "    ram_util_percent: 63.0125\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -5.5\n",
      "    pol2: 16.300000000000015\n",
      "  policy_reward_mean:\n",
      "    pol1: -25.495\n",
      "    pol2: -16.30399999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -61.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3889686785268498\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21358228775738985\n",
      "    mean_inference_ms: 1.6549356889768814\n",
      "    mean_raw_obs_processing_ms: 1.615110673548812\n",
      "  time_since_restore: 176.46325993537903\n",
      "  time_this_iter_s: 5.922051191329956\n",
      "  time_total_s: 176.46325993537903\n",
      "  timers:\n",
      "    learn_throughput: 800.409\n",
      "    learn_time_ms: 4997.445\n",
      "    sample_throughput: 5166.136\n",
      "    sample_time_ms: 774.273\n",
      "    update_time_ms: 3.71\n",
      "  timestamp: 1619703797\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16.500000000000004\n",
      "  episode_reward_mean: -34.062\n",
      "  episode_reward_min: -55.800000000000054\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1120\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8681994676589966\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013980314135551453\n",
      "          model: {}\n",
      "          policy_loss: -0.040492523461580276\n",
      "          total_loss: 37.81007385253906\n",
      "          vf_explained_var: 0.1122586578130722\n",
      "          vf_loss: 37.83641052246094\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8569180965423584\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01361420750617981\n",
      "          model: {}\n",
      "          policy_loss: -0.04163695126771927\n",
      "          total_loss: 7.9996137619018555\n",
      "          vf_explained_var: 0.3144175708293915\n",
      "          vf_loss: 8.0274658203125\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.7125\n",
      "    ram_util_percent: 63.0125\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -2.5\n",
      "    pol2: 4.200000000000005\n",
      "  policy_reward_mean:\n",
      "    pol1: -19.1\n",
      "    pol2: -14.961999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -39.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3978569161111894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2181307897838489\n",
      "    mean_inference_ms: 1.707955121160074\n",
      "    mean_raw_obs_processing_ms: 1.6475889968642718\n",
      "  time_since_restore: 176.10620379447937\n",
      "  time_this_iter_s: 5.972228050231934\n",
      "  time_total_s: 176.10620379447937\n",
      "  timers:\n",
      "    learn_throughput: 798.027\n",
      "    learn_time_ms: 5012.359\n",
      "    sample_throughput: 5245.201\n",
      "    sample_time_ms: 762.602\n",
      "    update_time_ms: 3.641\n",
      "  timestamp: 1619703797\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -17.399999999999977\n",
      "  episode_reward_mean: -43.98000000000002\n",
      "  episode_reward_min: -69.30000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1160\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8503374457359314\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011887435801327229\n",
      "          model: {}\n",
      "          policy_loss: -0.030735738575458527\n",
      "          total_loss: 52.41538619995117\n",
      "          vf_explained_var: 0.09316299855709076\n",
      "          vf_loss: 52.434085845947266\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7932935953140259\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013105650432407856\n",
      "          model: {}\n",
      "          policy_loss: -0.03568539023399353\n",
      "          total_loss: 11.186454772949219\n",
      "          vf_explained_var: 0.27539584040641785\n",
      "          vf_loss: 11.208870887756348\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.9\n",
      "    ram_util_percent: 64.13749999999999\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -9.5\n",
      "    pol2: 10.800000000000043\n",
      "  policy_reward_mean:\n",
      "    pol1: -30.965\n",
      "    pol2: -13.014999999999976\n",
      "  policy_reward_min:\n",
      "    pol1: -51.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4090745715829543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22729257453348187\n",
      "    mean_inference_ms: 1.7426571884592554\n",
      "    mean_raw_obs_processing_ms: 1.6946396703374313\n",
      "  time_since_restore: 182.30657243728638\n",
      "  time_this_iter_s: 5.68619179725647\n",
      "  time_total_s: 182.30657243728638\n",
      "  timers:\n",
      "    learn_throughput: 800.583\n",
      "    learn_time_ms: 4996.359\n",
      "    sample_throughput: 5165.731\n",
      "    sample_time_ms: 774.334\n",
      "    update_time_ms: 3.673\n",
      "  timestamp: 1619703802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         182.307</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\"> -43.98 </td><td style=\"text-align: right;\">               -17.4</td><td style=\"text-align: right;\">               -69.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         176.106</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\"> -34.062</td><td style=\"text-align: right;\">               -16.5</td><td style=\"text-align: right;\">               -55.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         176.463</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\"> -41.799</td><td style=\"text-align: right;\">               -20.1</td><td style=\"text-align: right;\">               -68.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         176.493</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\"> -45.438</td><td style=\"text-align: right;\">               -22.8</td><td style=\"text-align: right;\">               -67.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -23.099999999999973\n",
      "  episode_reward_mean: -44.385000000000026\n",
      "  episode_reward_min: -67.50000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1160\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8432906866073608\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011795462109148502\n",
      "          model: {}\n",
      "          policy_loss: -0.03927554935216904\n",
      "          total_loss: 79.24305725097656\n",
      "          vf_explained_var: 0.0963183119893074\n",
      "          vf_loss: 79.2703857421875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8555780649185181\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014676671475172043\n",
      "          model: {}\n",
      "          policy_loss: -0.05327147617936134\n",
      "          total_loss: 13.18582820892334\n",
      "          vf_explained_var: 0.26865914463996887\n",
      "          vf_loss: 13.224239349365234\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.14444444444445\n",
      "    ram_util_percent: 64.15555555555554\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -9.0\n",
      "    pol2: 4.2\n",
      "  policy_reward_mean:\n",
      "    pol1: -31.37\n",
      "    pol2: -13.014999999999977\n",
      "  policy_reward_min:\n",
      "    pol1: -56.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4057903131124692\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2182326833195681\n",
      "    mean_inference_ms: 1.7084927492695121\n",
      "    mean_raw_obs_processing_ms: 1.690875616424096\n",
      "  time_since_restore: 182.37877655029297\n",
      "  time_this_iter_s: 5.885421991348267\n",
      "  time_total_s: 182.37877655029297\n",
      "  timers:\n",
      "    learn_throughput: 794.813\n",
      "    learn_time_ms: 5032.632\n",
      "    sample_throughput: 5076.43\n",
      "    sample_time_ms: 787.955\n",
      "    update_time_ms: 3.197\n",
      "  timestamp: 1619703802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20.099999999999966\n",
      "  episode_reward_mean: -40.41300000000002\n",
      "  episode_reward_min: -68.4000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1160\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.844560980796814\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014630130492150784\n",
      "          model: {}\n",
      "          policy_loss: -0.04550054669380188\n",
      "          total_loss: 46.341983795166016\n",
      "          vf_explained_var: 0.11098030209541321\n",
      "          vf_loss: 46.37267303466797\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8877896070480347\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014621649868786335\n",
      "          model: {}\n",
      "          policy_loss: -0.05612991750240326\n",
      "          total_loss: 7.572812080383301\n",
      "          vf_explained_var: 0.382516473531723\n",
      "          vf_loss: 7.614137649536133\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.8888888888889\n",
      "    ram_util_percent: 64.17777777777778\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -4.5\n",
      "    pol2: 16.300000000000015\n",
      "  policy_reward_mean:\n",
      "    pol1: -23.845\n",
      "    pol2: -16.56799999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -56.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38800567813247555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21302512092165946\n",
      "    mean_inference_ms: 1.649611666360291\n",
      "    mean_raw_obs_processing_ms: 1.6096915452267115\n",
      "  time_since_restore: 182.36900997161865\n",
      "  time_this_iter_s: 5.905750036239624\n",
      "  time_total_s: 182.36900997161865\n",
      "  timers:\n",
      "    learn_throughput: 794.95\n",
      "    learn_time_ms: 5031.762\n",
      "    sample_throughput: 5166.814\n",
      "    sample_time_ms: 774.171\n",
      "    update_time_ms: 3.802\n",
      "  timestamp: 1619703803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16.500000000000004\n",
      "  episode_reward_mean: -33.71999999999999\n",
      "  episode_reward_min: -55.800000000000054\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1160\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8415623903274536\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013229601085186005\n",
      "          model: {}\n",
      "          policy_loss: -0.036734092980623245\n",
      "          total_loss: 44.897727966308594\n",
      "          vf_explained_var: 0.14318028092384338\n",
      "          vf_loss: 44.92106246948242\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.831716001033783\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012740140780806541\n",
      "          model: {}\n",
      "          policy_loss: -0.041307516396045685\n",
      "          total_loss: 8.645279884338379\n",
      "          vf_explained_var: 0.3019879162311554\n",
      "          vf_loss: 8.673687934875488\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.91111111111111\n",
      "    ram_util_percent: 64.17777777777778\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -1.0\n",
      "    pol2: 4.200000000000005\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.175\n",
      "    pol2: -15.544999999999975\n",
      "  policy_reward_min:\n",
      "    pol1: -38.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.396577551224566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2173594057231765\n",
      "    mean_inference_ms: 1.701623705552263\n",
      "    mean_raw_obs_processing_ms: 1.6403807992309458\n",
      "  time_since_restore: 182.03046703338623\n",
      "  time_this_iter_s: 5.92426323890686\n",
      "  time_total_s: 182.03046703338623\n",
      "  timers:\n",
      "    learn_throughput: 793.287\n",
      "    learn_time_ms: 5042.31\n",
      "    sample_throughput: 5227.573\n",
      "    sample_time_ms: 765.173\n",
      "    update_time_ms: 3.623\n",
      "  timestamp: 1619703803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -17.399999999999977\n",
      "  episode_reward_mean: -43.38300000000002\n",
      "  episode_reward_min: -67.50000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1200\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8236138224601746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013544507324695587\n",
      "          model: {}\n",
      "          policy_loss: -0.04225869104266167\n",
      "          total_loss: 67.05473327636719\n",
      "          vf_explained_var: 0.10390971601009369\n",
      "          vf_loss: 67.08328247070312\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7731021642684937\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0137090552598238\n",
      "          model: {}\n",
      "          policy_loss: -0.03965657576918602\n",
      "          total_loss: 10.545178413391113\n",
      "          vf_explained_var: 0.27332818508148193\n",
      "          vf_loss: 10.570955276489258\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.9125\n",
      "    ram_util_percent: 65.89999999999999\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -9.5\n",
      "    pol2: 10.800000000000043\n",
      "  policy_reward_mean:\n",
      "    pol1: -30.775\n",
      "    pol2: -12.60799999999998\n",
      "  policy_reward_min:\n",
      "    pol1: -61.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.407508789068105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22645500856181563\n",
      "    mean_inference_ms: 1.7359947423791937\n",
      "    mean_raw_obs_processing_ms: 1.686465992682516\n",
      "  time_since_restore: 188.0907325744629\n",
      "  time_this_iter_s: 5.784160137176514\n",
      "  time_total_s: 188.0907325744629\n",
      "  timers:\n",
      "    learn_throughput: 800.384\n",
      "    learn_time_ms: 4997.603\n",
      "    sample_throughput: 5253.588\n",
      "    sample_time_ms: 761.384\n",
      "    update_time_ms: 3.742\n",
      "  timestamp: 1619703808\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         188.091</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\"> -43.383</td><td style=\"text-align: right;\">               -17.4</td><td style=\"text-align: right;\">               -67.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         182.03 </td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\"> -33.72 </td><td style=\"text-align: right;\">               -16.5</td><td style=\"text-align: right;\">               -55.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         182.369</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\"> -40.413</td><td style=\"text-align: right;\">               -20.1</td><td style=\"text-align: right;\">               -68.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         182.379</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\"> -44.385</td><td style=\"text-align: right;\">               -23.1</td><td style=\"text-align: right;\">               -67.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -26.399999999999995\n",
      "  episode_reward_mean: -44.505000000000024\n",
      "  episode_reward_min: -64.20000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1200\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8319759368896484\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012361745350062847\n",
      "          model: {}\n",
      "          policy_loss: -0.036920759826898575\n",
      "          total_loss: 54.914268493652344\n",
      "          vf_explained_var: 0.1454734206199646\n",
      "          vf_loss: 54.93867111206055\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.835668683052063\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013907643035054207\n",
      "          model: {}\n",
      "          policy_loss: -0.04998666048049927\n",
      "          total_loss: 13.409943580627441\n",
      "          vf_explained_var: 0.27629998326301575\n",
      "          vf_loss: 13.44584846496582\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.07499999999999\n",
      "    ram_util_percent: 66.025\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -10.0\n",
      "    pol2: 10.800000000000033\n",
      "  policy_reward_mean:\n",
      "    pol1: -31.765\n",
      "    pol2: -12.739999999999975\n",
      "  policy_reward_min:\n",
      "    pol1: -56.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40437353340512966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.217384656889866\n",
      "    mean_inference_ms: 1.7025949095843345\n",
      "    mean_raw_obs_processing_ms: 1.6835978100399434\n",
      "  time_since_restore: 188.25843143463135\n",
      "  time_this_iter_s: 5.879654884338379\n",
      "  time_total_s: 188.25843143463135\n",
      "  timers:\n",
      "    learn_throughput: 795.104\n",
      "    learn_time_ms: 5030.788\n",
      "    sample_throughput: 5069.514\n",
      "    sample_time_ms: 789.03\n",
      "    update_time_ms: 3.284\n",
      "  timestamp: 1619703808\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -9.000000000000009\n",
      "  episode_reward_mean: -38.74500000000001\n",
      "  episode_reward_min: -58.800000000000075\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1200\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8431028723716736\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013327788561582565\n",
      "          model: {}\n",
      "          policy_loss: -0.04103035479784012\n",
      "          total_loss: 55.121253967285156\n",
      "          vf_explained_var: 0.13951635360717773\n",
      "          vf_loss: 55.14878845214844\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8703504800796509\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015895333141088486\n",
      "          model: {}\n",
      "          policy_loss: -0.060879193246364594\n",
      "          total_loss: 6.797905921936035\n",
      "          vf_explained_var: 0.4037157893180847\n",
      "          vf_loss: 6.842691421508789\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.05\n",
      "    ram_util_percent: 66.025\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 5.5\n",
      "    pol2: -4.600000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -21.99\n",
      "    pol2: -16.754999999999967\n",
      "  policy_reward_min:\n",
      "    pol1: -46.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3873847990370628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21257541748698272\n",
      "    mean_inference_ms: 1.6461656757926628\n",
      "    mean_raw_obs_processing_ms: 1.6062646409598402\n",
      "  time_since_restore: 188.26499485969543\n",
      "  time_this_iter_s: 5.895984888076782\n",
      "  time_total_s: 188.26499485969543\n",
      "  timers:\n",
      "    learn_throughput: 795.103\n",
      "    learn_time_ms: 5030.796\n",
      "    sample_throughput: 5165.725\n",
      "    sample_time_ms: 774.335\n",
      "    update_time_ms: 3.919\n",
      "  timestamp: 1619703808\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16.500000000000004\n",
      "  episode_reward_mean: -33.87\n",
      "  episode_reward_min: -57.00000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1200\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8466673493385315\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012608750723302364\n",
      "          model: {}\n",
      "          policy_loss: -0.04117632657289505\n",
      "          total_loss: 36.3887939453125\n",
      "          vf_explained_var: 0.16936837136745453\n",
      "          vf_loss: 36.417205810546875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8352928161621094\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012531801126897335\n",
      "          model: {}\n",
      "          policy_loss: -0.03881731256842613\n",
      "          total_loss: 8.967702865600586\n",
      "          vf_explained_var: 0.24508824944496155\n",
      "          vf_loss: 8.993831634521484\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.388888888888886\n",
      "    ram_util_percent: 66.03333333333335\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -1.0\n",
      "    pol2: -2.4000000000000026\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.6\n",
      "    pol2: -15.269999999999975\n",
      "  policy_reward_min:\n",
      "    pol1: -42.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3967789693991246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2172827000274836\n",
      "    mean_inference_ms: 1.6997125051774682\n",
      "    mean_raw_obs_processing_ms: 1.642148286582639\n",
      "  time_since_restore: 187.98265385627747\n",
      "  time_this_iter_s: 5.952186822891235\n",
      "  time_total_s: 187.98265385627747\n",
      "  timers:\n",
      "    learn_throughput: 793.179\n",
      "    learn_time_ms: 5043.0\n",
      "    sample_throughput: 5177.637\n",
      "    sample_time_ms: 772.553\n",
      "    update_time_ms: 3.454\n",
      "  timestamp: 1619703809\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20.399999999999967\n",
      "  episode_reward_mean: -43.57500000000001\n",
      "  episode_reward_min: -67.50000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1240\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.818879246711731\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013378150761127472\n",
      "          model: {}\n",
      "          policy_loss: -0.04247002303600311\n",
      "          total_loss: 73.43601989746094\n",
      "          vf_explained_var: 0.13970719277858734\n",
      "          vf_loss: 73.4649429321289\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7811737656593323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014073286205530167\n",
      "          model: {}\n",
      "          policy_loss: -0.04015151411294937\n",
      "          total_loss: 9.73468017578125\n",
      "          vf_explained_var: 0.28431612253189087\n",
      "          vf_loss: 9.760581970214844\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.1125\n",
      "    ram_util_percent: 66.1875\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -12.5\n",
      "    pol2: 10.800000000000043\n",
      "  policy_reward_mean:\n",
      "    pol1: -32.155\n",
      "    pol2: -11.41999999999998\n",
      "  policy_reward_min:\n",
      "    pol1: -64.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40674349021744294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2260590748350208\n",
      "    mean_inference_ms: 1.7320126022777276\n",
      "    mean_raw_obs_processing_ms: 1.683062000803202\n",
      "  time_since_restore: 193.90267968177795\n",
      "  time_this_iter_s: 5.8119471073150635\n",
      "  time_total_s: 193.90267968177795\n",
      "  timers:\n",
      "    learn_throughput: 804.846\n",
      "    learn_time_ms: 4969.897\n",
      "    sample_throughput: 5190.082\n",
      "    sample_time_ms: 770.701\n",
      "    update_time_ms: 3.738\n",
      "  timestamp: 1619703814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         193.903</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\"> -43.575</td><td style=\"text-align: right;\">               -20.4</td><td style=\"text-align: right;\">               -67.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         187.983</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\"> -33.87 </td><td style=\"text-align: right;\">               -16.5</td><td style=\"text-align: right;\">               -57  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         188.265</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\"> -38.745</td><td style=\"text-align: right;\">                -9  </td><td style=\"text-align: right;\">               -58.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         188.258</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\"> -44.505</td><td style=\"text-align: right;\">               -26.4</td><td style=\"text-align: right;\">               -64.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -26.69999999999999\n",
      "  episode_reward_mean: -43.56900000000002\n",
      "  episode_reward_min: -64.20000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1240\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8291501998901367\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012866836041212082\n",
      "          model: {}\n",
      "          policy_loss: -0.04232071712613106\n",
      "          total_loss: 65.07513427734375\n",
      "          vf_explained_var: 0.13710403442382812\n",
      "          vf_loss: 65.10442352294922\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.800458550453186\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014147497713565826\n",
      "          model: {}\n",
      "          policy_loss: -0.0512240007519722\n",
      "          total_loss: 14.0069580078125\n",
      "          vf_explained_var: 0.31536275148391724\n",
      "          vf_loss: 14.04385757446289\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.4375\n",
      "    ram_util_percent: 66.17500000000001\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -10.0\n",
      "    pol2: 10.800000000000033\n",
      "  policy_reward_mean:\n",
      "    pol1: -30.895\n",
      "    pol2: -12.67399999999998\n",
      "  policy_reward_min:\n",
      "    pol1: -56.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40290780988240166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21649141824202622\n",
      "    mean_inference_ms: 1.6960548319838387\n",
      "    mean_raw_obs_processing_ms: 1.6762322002777768\n",
      "  time_since_restore: 193.8319342136383\n",
      "  time_this_iter_s: 5.573502779006958\n",
      "  time_total_s: 193.8319342136383\n",
      "  timers:\n",
      "    learn_throughput: 796.762\n",
      "    learn_time_ms: 5020.323\n",
      "    sample_throughput: 5111.773\n",
      "    sample_time_ms: 782.507\n",
      "    update_time_ms: 3.087\n",
      "  timestamp: 1619703814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -9.000000000000009\n",
      "  episode_reward_mean: -38.226000000000006\n",
      "  episode_reward_min: -63.00000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1240\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.816243588924408\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012546218931674957\n",
      "          model: {}\n",
      "          policy_loss: -0.046920858323574066\n",
      "          total_loss: 39.79175567626953\n",
      "          vf_explained_var: 0.18833623826503754\n",
      "          vf_loss: 39.82597732543945\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8435211181640625\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01490806881338358\n",
      "          model: {}\n",
      "          policy_loss: -0.050167016685009\n",
      "          total_loss: 11.755605697631836\n",
      "          vf_explained_var: 0.29604482650756836\n",
      "          vf_loss: 11.790678024291992\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.1375\n",
      "    ram_util_percent: 66.17500000000001\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 5.5\n",
      "    pol2: 4.2000000000000215\n",
      "  policy_reward_mean:\n",
      "    pol1: -22.12\n",
      "    pol2: -16.105999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -46.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38664932974160526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21208666951872496\n",
      "    mean_inference_ms: 1.6422847329522958\n",
      "    mean_raw_obs_processing_ms: 1.6026880157812626\n",
      "  time_since_restore: 193.85310769081116\n",
      "  time_this_iter_s: 5.588112831115723\n",
      "  time_total_s: 193.85310769081116\n",
      "  timers:\n",
      "    learn_throughput: 798.454\n",
      "    learn_time_ms: 5009.681\n",
      "    sample_throughput: 5189.138\n",
      "    sample_time_ms: 770.841\n",
      "    update_time_ms: 3.85\n",
      "  timestamp: 1619703814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -18.600000000000005\n",
      "  episode_reward_mean: -33.663\n",
      "  episode_reward_min: -57.00000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1240\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8165843486785889\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012482158839702606\n",
      "          model: {}\n",
      "          policy_loss: -0.04031267762184143\n",
      "          total_loss: 52.22767639160156\n",
      "          vf_explained_var: 0.14976447820663452\n",
      "          vf_loss: 52.255348205566406\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8154076933860779\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012742455117404461\n",
      "          model: {}\n",
      "          policy_loss: -0.03632824867963791\n",
      "          total_loss: 12.490227699279785\n",
      "          vf_explained_var: 0.28810930252075195\n",
      "          vf_loss: 12.513654708862305\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.4875\n",
      "    ram_util_percent: 66.17500000000001\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -1.0\n",
      "    pol2: 17.400000000000023\n",
      "  policy_reward_mean:\n",
      "    pol1: -19.35\n",
      "    pol2: -14.312999999999978\n",
      "  policy_reward_min:\n",
      "    pol1: -61.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3971032924505161\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21721538842537869\n",
      "    mean_inference_ms: 1.697728836913507\n",
      "    mean_raw_obs_processing_ms: 1.644397379296396\n",
      "  time_since_restore: 193.57467484474182\n",
      "  time_this_iter_s: 5.5920209884643555\n",
      "  time_total_s: 193.57467484474182\n",
      "  timers:\n",
      "    learn_throughput: 796.751\n",
      "    learn_time_ms: 5020.392\n",
      "    sample_throughput: 5144.281\n",
      "    sample_time_ms: 777.562\n",
      "    update_time_ms: 3.296\n",
      "  timestamp: 1619703814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12.3\n",
      "  episode_reward_mean: -42.08700000000001\n",
      "  episode_reward_min: -67.50000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1280\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8203696012496948\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013328020460903645\n",
      "          model: {}\n",
      "          policy_loss: -0.04224826395511627\n",
      "          total_loss: 52.096900939941406\n",
      "          vf_explained_var: 0.11797308921813965\n",
      "          vf_loss: 52.12565612792969\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7563791275024414\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012877469882369041\n",
      "          model: {}\n",
      "          policy_loss: -0.03753257915377617\n",
      "          total_loss: 9.348865509033203\n",
      "          vf_explained_var: 0.26533445715904236\n",
      "          vf_loss: 9.373360633850098\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.4625\n",
      "    ram_util_percent: 66.15\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -11.0\n",
      "    pol2: 0.9000000000000163\n",
      "  policy_reward_mean:\n",
      "    pol1: -30.37\n",
      "    pol2: -11.71699999999998\n",
      "  policy_reward_min:\n",
      "    pol1: -64.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4063234354185695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22597265129738445\n",
      "    mean_inference_ms: 1.7294320745248637\n",
      "    mean_raw_obs_processing_ms: 1.6826887777527941\n",
      "  time_since_restore: 199.32158160209656\n",
      "  time_this_iter_s: 5.4189019203186035\n",
      "  time_total_s: 199.32158160209656\n",
      "  timers:\n",
      "    learn_throughput: 807.26\n",
      "    learn_time_ms: 4955.036\n",
      "    sample_throughput: 5099.548\n",
      "    sample_time_ms: 784.383\n",
      "    update_time_ms: 3.69\n",
      "  timestamp: 1619703819\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         199.322</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\"> -42.087</td><td style=\"text-align: right;\">               -12.3</td><td style=\"text-align: right;\">               -67.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         193.575</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\"> -33.663</td><td style=\"text-align: right;\">               -18.6</td><td style=\"text-align: right;\">               -57  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         193.853</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\"> -38.226</td><td style=\"text-align: right;\">                -9  </td><td style=\"text-align: right;\">               -63  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         193.832</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\"> -43.569</td><td style=\"text-align: right;\">               -26.7</td><td style=\"text-align: right;\">               -64.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -22.199999999999964\n",
      "  episode_reward_mean: -42.774000000000015\n",
      "  episode_reward_min: -61.50000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1280\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8071923851966858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016047891229391098\n",
      "          model: {}\n",
      "          policy_loss: -0.05564795434474945\n",
      "          total_loss: 47.105125427246094\n",
      "          vf_explained_var: 0.1270139366388321\n",
      "          vf_loss: 47.14452362060547\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8003455400466919\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014044013805687428\n",
      "          model: {}\n",
      "          policy_loss: -0.04876536875963211\n",
      "          total_loss: 8.14301872253418\n",
      "          vf_explained_var: 0.3504573106765747\n",
      "          vf_loss: 8.17756462097168\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.5\n",
      "    ram_util_percent: 66.1375\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -11.0\n",
      "    pol2: 10.800000000000033\n",
      "  policy_reward_mean:\n",
      "    pol1: -29.77\n",
      "    pol2: -13.003999999999976\n",
      "  policy_reward_min:\n",
      "    pol1: -51.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.401389419586471\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21576290231856915\n",
      "    mean_inference_ms: 1.689036166558236\n",
      "    mean_raw_obs_processing_ms: 1.669994229177616\n",
      "  time_since_restore: 199.29198026657104\n",
      "  time_this_iter_s: 5.460046052932739\n",
      "  time_total_s: 199.29198026657104\n",
      "  timers:\n",
      "    learn_throughput: 798.15\n",
      "    learn_time_ms: 5011.589\n",
      "    sample_throughput: 5170.647\n",
      "    sample_time_ms: 773.598\n",
      "    update_time_ms: 3.068\n",
      "  timestamp: 1619703820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -13.800000000000002\n",
      "  episode_reward_mean: -38.457\n",
      "  episode_reward_min: -63.00000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1280\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.820742130279541\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014902661554515362\n",
      "          model: {}\n",
      "          policy_loss: -0.054597415030002594\n",
      "          total_loss: 45.69700622558594\n",
      "          vf_explained_var: 0.16414082050323486\n",
      "          vf_loss: 45.73651123046875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8245238065719604\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014154711738228798\n",
      "          model: {}\n",
      "          policy_loss: -0.04424315690994263\n",
      "          total_loss: 6.594296455383301\n",
      "          vf_explained_var: 0.38919126987457275\n",
      "          vf_loss: 6.624207973480225\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.2\n",
      "    ram_util_percent: 66.1375\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -7.0\n",
      "    pol2: 4.2000000000000215\n",
      "  policy_reward_mean:\n",
      "    pol1: -22.395\n",
      "    pol2: -16.06199999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -46.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3855154876175111\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2115236106682714\n",
      "    mean_inference_ms: 1.6363946768239273\n",
      "    mean_raw_obs_processing_ms: 1.5986372281372712\n",
      "  time_since_restore: 199.3159236907959\n",
      "  time_this_iter_s: 5.462815999984741\n",
      "  time_total_s: 199.3159236907959\n",
      "  timers:\n",
      "    learn_throughput: 799.145\n",
      "    learn_time_ms: 5005.347\n",
      "    sample_throughput: 5197.079\n",
      "    sample_time_ms: 769.663\n",
      "    update_time_ms: 3.831\n",
      "  timestamp: 1619703820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -17.7\n",
      "  episode_reward_mean: -33.032999999999994\n",
      "  episode_reward_min: -57.00000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1280\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8056759834289551\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013275843113660812\n",
      "          model: {}\n",
      "          policy_loss: -0.04100334644317627\n",
      "          total_loss: 37.87158966064453\n",
      "          vf_explained_var: 0.16157054901123047\n",
      "          vf_loss: 37.89915466308594\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8076176643371582\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01352626457810402\n",
      "          model: {}\n",
      "          policy_loss: -0.03710151091217995\n",
      "          total_loss: 10.648439407348633\n",
      "          vf_explained_var: 0.2796924114227295\n",
      "          vf_loss: 10.671844482421875\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.87142857142857\n",
      "    ram_util_percent: 66.14285714285715\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 0.0\n",
      "    pol2: 17.400000000000023\n",
      "  policy_reward_mean:\n",
      "    pol1: -19.38\n",
      "    pol2: -13.65299999999998\n",
      "  policy_reward_min:\n",
      "    pol1: -61.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.397087727258195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21705289300068004\n",
      "    mean_inference_ms: 1.6945754661563797\n",
      "    mean_raw_obs_processing_ms: 1.645836055467276\n",
      "  time_since_restore: 199.0258538722992\n",
      "  time_this_iter_s: 5.451179027557373\n",
      "  time_total_s: 199.0258538722992\n",
      "  timers:\n",
      "    learn_throughput: 798.307\n",
      "    learn_time_ms: 5010.607\n",
      "    sample_throughput: 5011.374\n",
      "    sample_time_ms: 798.184\n",
      "    update_time_ms: 3.161\n",
      "  timestamp: 1619703820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12.3\n",
      "  episode_reward_mean: -40.59000000000002\n",
      "  episode_reward_min: -64.20000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1320\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8017166256904602\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013416459783911705\n",
      "          model: {}\n",
      "          policy_loss: -0.041583120822906494\n",
      "          total_loss: 55.47414779663086\n",
      "          vf_explained_var: 0.08840664476156235\n",
      "          vf_loss: 55.50214385986328\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7449804544448853\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011818531900644302\n",
      "          model: {}\n",
      "          policy_loss: -0.03674031049013138\n",
      "          total_loss: 12.508499145507812\n",
      "          vf_explained_var: 0.2292952686548233\n",
      "          vf_loss: 12.533273696899414\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.325\n",
      "    ram_util_percent: 65.8125\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -10.0\n",
      "    pol2: 6.40000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -29.445\n",
      "    pol2: -11.14499999999998\n",
      "  policy_reward_min:\n",
      "    pol1: -64.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40581349867271393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22586658663947012\n",
      "    mean_inference_ms: 1.7259582898182697\n",
      "    mean_raw_obs_processing_ms: 1.6831430730199617\n",
      "  time_since_restore: 204.98609161376953\n",
      "  time_this_iter_s: 5.664510011672974\n",
      "  time_total_s: 204.98609161376953\n",
      "  timers:\n",
      "    learn_throughput: 802.543\n",
      "    learn_time_ms: 4984.154\n",
      "    sample_throughput: 5073.444\n",
      "    sample_time_ms: 788.419\n",
      "    update_time_ms: 3.759\n",
      "  timestamp: 1619703825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         204.986</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\"> -40.59 </td><td style=\"text-align: right;\">               -12.3</td><td style=\"text-align: right;\">               -64.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         199.026</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\"> -33.033</td><td style=\"text-align: right;\">               -17.7</td><td style=\"text-align: right;\">               -57  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         199.316</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\"> -38.457</td><td style=\"text-align: right;\">               -13.8</td><td style=\"text-align: right;\">               -63  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         199.292</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\"> -42.774</td><td style=\"text-align: right;\">               -22.2</td><td style=\"text-align: right;\">               -61.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -19.799999999999972\n",
      "  episode_reward_mean: -42.47400000000002\n",
      "  episode_reward_min: -64.50000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1320\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7916695475578308\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014242918230593204\n",
      "          model: {}\n",
      "          policy_loss: -0.041739337146282196\n",
      "          total_loss: 71.66497802734375\n",
      "          vf_explained_var: 0.14963850378990173\n",
      "          vf_loss: 71.69229888916016\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7881560325622559\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014646313153207302\n",
      "          model: {}\n",
      "          policy_loss: -0.05631072819232941\n",
      "          total_loss: 10.876081466674805\n",
      "          vf_explained_var: 0.2928590178489685\n",
      "          vf_loss: 10.917562484741211\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.96249999999999\n",
      "    ram_util_percent: 65.85\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -2.0\n",
      "    pol2: 8.599999999999998\n",
      "  policy_reward_mean:\n",
      "    pol1: -29.855\n",
      "    pol2: -12.618999999999982\n",
      "  policy_reward_min:\n",
      "    pol1: -55.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.399010337276954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2147190760282568\n",
      "    mean_inference_ms: 1.6789423126787832\n",
      "    mean_raw_obs_processing_ms: 1.6612151211160386\n",
      "  time_since_restore: 204.75050330162048\n",
      "  time_this_iter_s: 5.4585230350494385\n",
      "  time_total_s: 204.75050330162048\n",
      "  timers:\n",
      "    learn_throughput: 794.653\n",
      "    learn_time_ms: 5033.646\n",
      "    sample_throughput: 5458.647\n",
      "    sample_time_ms: 732.782\n",
      "    update_time_ms: 3.212\n",
      "  timestamp: 1619703825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -15.89999999999997\n",
      "  episode_reward_mean: -38.57700000000001\n",
      "  episode_reward_min: -63.00000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1320\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8154048919677734\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012923795729875565\n",
      "          model: {}\n",
      "          policy_loss: -0.035063933581113815\n",
      "          total_loss: 53.92674255371094\n",
      "          vf_explained_var: 0.15890486538410187\n",
      "          vf_loss: 53.948726654052734\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8226008415222168\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012624697759747505\n",
      "          model: {}\n",
      "          policy_loss: -0.044448837637901306\n",
      "          total_loss: 16.28326416015625\n",
      "          vf_explained_var: 0.3503720164299011\n",
      "          vf_loss: 16.31493377685547\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.975\n",
      "    ram_util_percent: 65.8375\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 0.5\n",
      "    pol2: 19.6\n",
      "  policy_reward_mean:\n",
      "    pol1: -22.35\n",
      "    pol2: -16.226999999999965\n",
      "  policy_reward_min:\n",
      "    pol1: -51.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38433015319068065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21101644610146544\n",
      "    mean_inference_ms: 1.6296544162879598\n",
      "    mean_raw_obs_processing_ms: 1.5948886417495538\n",
      "  time_since_restore: 204.86766076087952\n",
      "  time_this_iter_s: 5.551737070083618\n",
      "  time_total_s: 204.86766076087952\n",
      "  timers:\n",
      "    learn_throughput: 796.964\n",
      "    learn_time_ms: 5019.047\n",
      "    sample_throughput: 5201.33\n",
      "    sample_time_ms: 769.034\n",
      "    update_time_ms: 3.822\n",
      "  timestamp: 1619703825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16.8\n",
      "  episode_reward_mean: -32.51099999999999\n",
      "  episode_reward_min: -57.300000000000075\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1320\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7944858074188232\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013274289667606354\n",
      "          model: {}\n",
      "          policy_loss: -0.04821787774562836\n",
      "          total_loss: 40.51319885253906\n",
      "          vf_explained_var: 0.11748351901769638\n",
      "          vf_loss: 40.54798126220703\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7877683639526367\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012855453416705132\n",
      "          model: {}\n",
      "          policy_loss: -0.05049837380647659\n",
      "          total_loss: 11.645368576049805\n",
      "          vf_explained_var: 0.24786913394927979\n",
      "          vf_loss: 11.682851791381836\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.62222222222222\n",
      "    ram_util_percent: 65.71111111111111\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 0.0\n",
      "    pol2: 17.400000000000023\n",
      "  policy_reward_mean:\n",
      "    pol1: -19.43\n",
      "    pol2: -13.080999999999978\n",
      "  policy_reward_min:\n",
      "    pol1: -61.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3978076261835443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.217277665013227\n",
      "    mean_inference_ms: 1.6928673064307669\n",
      "    mean_raw_obs_processing_ms: 1.6496337882482197\n",
      "  time_since_restore: 204.7192108631134\n",
      "  time_this_iter_s: 5.693356990814209\n",
      "  time_total_s: 204.7192108631134\n",
      "  timers:\n",
      "    learn_throughput: 796.622\n",
      "    learn_time_ms: 5021.2\n",
      "    sample_throughput: 4857.768\n",
      "    sample_time_ms: 823.423\n",
      "    update_time_ms: 3.068\n",
      "  timestamp: 1619703825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -17.09999999999999\n",
      "  episode_reward_mean: -40.338000000000015\n",
      "  episode_reward_min: -58.50000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1360\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.781320333480835\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01167747937142849\n",
      "          model: {}\n",
      "          policy_loss: -0.031889863312244415\n",
      "          total_loss: 55.71376037597656\n",
      "          vf_explained_var: 0.13557013869285583\n",
      "          vf_loss: 55.73382568359375\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7399228811264038\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013682413846254349\n",
      "          model: {}\n",
      "          policy_loss: -0.046675145626068115\n",
      "          total_loss: 10.88204574584961\n",
      "          vf_explained_var: 0.24185073375701904\n",
      "          vf_loss: 10.914867401123047\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.35\n",
      "    ram_util_percent: 64.6875\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -10.0\n",
      "    pol2: 8.600000000000025\n",
      "  policy_reward_mean:\n",
      "    pol1: -29.71\n",
      "    pol2: -10.627999999999984\n",
      "  policy_reward_min:\n",
      "    pol1: -49.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4047163010106026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22533799863724308\n",
      "    mean_inference_ms: 1.7201526479543037\n",
      "    mean_raw_obs_processing_ms: 1.6795607492452438\n",
      "  time_since_restore: 210.3850917816162\n",
      "  time_this_iter_s: 5.39900016784668\n",
      "  time_total_s: 210.3850917816162\n",
      "  timers:\n",
      "    learn_throughput: 836.328\n",
      "    learn_time_ms: 4782.81\n",
      "    sample_throughput: 5113.023\n",
      "    sample_time_ms: 782.316\n",
      "    update_time_ms: 3.802\n",
      "  timestamp: 1619703830\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         210.385</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\"> -40.338</td><td style=\"text-align: right;\">               -17.1</td><td style=\"text-align: right;\">               -58.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         204.719</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\"> -32.511</td><td style=\"text-align: right;\">               -16.8</td><td style=\"text-align: right;\">               -57.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         204.868</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\"> -38.577</td><td style=\"text-align: right;\">               -15.9</td><td style=\"text-align: right;\">               -63  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         204.751</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\"> -42.474</td><td style=\"text-align: right;\">               -19.8</td><td style=\"text-align: right;\">               -64.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -18.299999999999994\n",
      "  episode_reward_mean: -41.19600000000001\n",
      "  episode_reward_min: -64.50000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1360\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7697420716285706\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01435716450214386\n",
      "          model: {}\n",
      "          policy_loss: -0.04923272505402565\n",
      "          total_loss: 46.393985748291016\n",
      "          vf_explained_var: 0.11645837128162384\n",
      "          vf_loss: 46.428680419921875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7675232887268066\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014133810065686703\n",
      "          model: {}\n",
      "          policy_loss: -0.05395801365375519\n",
      "          total_loss: 9.73379898071289\n",
      "          vf_explained_var: 0.32542842626571655\n",
      "          vf_loss: 9.773447036743164\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.0125\n",
      "    ram_util_percent: 64.58749999999999\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -2.0\n",
      "    pol2: 5.2999999999999945\n",
      "  policy_reward_mean:\n",
      "    pol1: -27.84\n",
      "    pol2: -13.35599999999998\n",
      "  policy_reward_min:\n",
      "    pol1: -55.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39677454161967435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21370537958985308\n",
      "    mean_inference_ms: 1.6696159078561648\n",
      "    mean_raw_obs_processing_ms: 1.6523168299269388\n",
      "  time_since_restore: 210.1910274028778\n",
      "  time_this_iter_s: 5.440524101257324\n",
      "  time_total_s: 210.1910274028778\n",
      "  timers:\n",
      "    learn_throughput: 831.441\n",
      "    learn_time_ms: 4810.924\n",
      "    sample_throughput: 5664.386\n",
      "    sample_time_ms: 706.167\n",
      "    update_time_ms: 3.244\n",
      "  timestamp: 1619703831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -18.599999999999998\n",
      "  episode_reward_mean: -36.894000000000005\n",
      "  episode_reward_min: -58.50000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1360\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8104268312454224\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014524035155773163\n",
      "          model: {}\n",
      "          policy_loss: -0.05124414712190628\n",
      "          total_loss: 44.01627731323242\n",
      "          vf_explained_var: 0.174615278840065\n",
      "          vf_loss: 44.052818298339844\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.8131453394889832\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014199279248714447\n",
      "          model: {}\n",
      "          policy_loss: -0.05034736171364784\n",
      "          total_loss: 12.921366691589355\n",
      "          vf_explained_var: 0.33079105615615845\n",
      "          vf_loss: 12.957337379455566\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.400000000000006\n",
      "    ram_util_percent: 64.5625\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 0.5\n",
      "    pol2: 19.6\n",
      "  policy_reward_mean:\n",
      "    pol1: -20.92\n",
      "    pol2: -15.973999999999968\n",
      "  policy_reward_min:\n",
      "    pol1: -51.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38341666414899583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2105815707023859\n",
      "    mean_inference_ms: 1.6241452767293532\n",
      "    mean_raw_obs_processing_ms: 1.5908108730389228\n",
      "  time_since_restore: 210.33003282546997\n",
      "  time_this_iter_s: 5.462372064590454\n",
      "  time_total_s: 210.33003282546997\n",
      "  timers:\n",
      "    learn_throughput: 830.257\n",
      "    learn_time_ms: 4817.784\n",
      "    sample_throughput: 5255.42\n",
      "    sample_time_ms: 761.119\n",
      "    update_time_ms: 3.332\n",
      "  timestamp: 1619703831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12.899999999999999\n",
      "  episode_reward_mean: -32.973\n",
      "  episode_reward_min: -57.300000000000075\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1360\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7942490577697754\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012760965153574944\n",
      "          model: {}\n",
      "          policy_loss: -0.03594350069761276\n",
      "          total_loss: 49.634178161621094\n",
      "          vf_explained_var: 0.11693792045116425\n",
      "          vf_loss: 49.65719985961914\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7753115892410278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012941928580403328\n",
      "          model: {}\n",
      "          policy_loss: -0.04491295665502548\n",
      "          total_loss: 8.772916793823242\n",
      "          vf_explained_var: 0.2528006136417389\n",
      "          vf_loss: 8.804725646972656\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.6\n",
      "    ram_util_percent: 64.55714285714286\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 0.0\n",
      "    pol2: 5.3000000000000105\n",
      "  policy_reward_mean:\n",
      "    pol1: -19.54\n",
      "    pol2: -13.432999999999975\n",
      "  policy_reward_min:\n",
      "    pol1: -39.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39843866544716405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2174452768759708\n",
      "    mean_inference_ms: 1.6923303505220246\n",
      "    mean_raw_obs_processing_ms: 1.6531846401269905\n",
      "  time_since_restore: 210.09477877616882\n",
      "  time_this_iter_s: 5.37556791305542\n",
      "  time_total_s: 210.09477877616882\n",
      "  timers:\n",
      "    learn_throughput: 831.458\n",
      "    learn_time_ms: 4810.827\n",
      "    sample_throughput: 4841.742\n",
      "    sample_time_ms: 826.149\n",
      "    update_time_ms: 2.964\n",
      "  timestamp: 1619703831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -17.09999999999999\n",
      "  episode_reward_mean: -39.80700000000001\n",
      "  episode_reward_min: -60.300000000000054\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1400\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7596969604492188\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011533157899975777\n",
      "          model: {}\n",
      "          policy_loss: -0.03621990233659744\n",
      "          total_loss: 52.4654426574707\n",
      "          vf_explained_var: 0.10827388614416122\n",
      "          vf_loss: 52.489986419677734\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.716363787651062\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011828500777482986\n",
      "          model: {}\n",
      "          policy_loss: -0.04039549455046654\n",
      "          total_loss: 11.586543083190918\n",
      "          vf_explained_var: 0.23516222834587097\n",
      "          vf_loss: 11.614962577819824\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.65\n",
      "    ram_util_percent: 64.875\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -7.5\n",
      "    pol2: 10.799999999999994\n",
      "  policy_reward_mean:\n",
      "    pol1: -29.355\n",
      "    pol2: -10.451999999999984\n",
      "  policy_reward_min:\n",
      "    pol1: -49.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40350607790211007\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22469961105480502\n",
      "    mean_inference_ms: 1.7150948476511803\n",
      "    mean_raw_obs_processing_ms: 1.6742395071784768\n",
      "  time_since_restore: 216.1529245376587\n",
      "  time_this_iter_s: 5.7678327560424805\n",
      "  time_total_s: 216.1529245376587\n",
      "  timers:\n",
      "    learn_throughput: 838.448\n",
      "    learn_time_ms: 4770.717\n",
      "    sample_throughput: 5303.898\n",
      "    sample_time_ms: 754.162\n",
      "    update_time_ms: 3.789\n",
      "  timestamp: 1619703836\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         216.153</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> -39.807</td><td style=\"text-align: right;\">               -17.1</td><td style=\"text-align: right;\">               -60.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         210.095</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\"> -32.973</td><td style=\"text-align: right;\">               -12.9</td><td style=\"text-align: right;\">               -57.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         210.33 </td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\"> -36.894</td><td style=\"text-align: right;\">               -18.6</td><td style=\"text-align: right;\">               -58.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         210.191</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\"> -41.196</td><td style=\"text-align: right;\">               -18.3</td><td style=\"text-align: right;\">               -64.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -18.299999999999994\n",
      "  episode_reward_mean: -39.51900000000001\n",
      "  episode_reward_min: -61.20000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1400\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7619023323059082\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014338860288262367\n",
      "          model: {}\n",
      "          policy_loss: -0.044018156826496124\n",
      "          total_loss: 62.76490783691406\n",
      "          vf_explained_var: 0.2097339928150177\n",
      "          vf_loss: 62.79440689086914\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.761864960193634\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01374901831150055\n",
      "          model: {}\n",
      "          policy_loss: -0.04095703735947609\n",
      "          total_loss: 21.581539154052734\n",
      "          vf_explained_var: 0.29967761039733887\n",
      "          vf_loss: 21.608577728271484\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.73750000000001\n",
      "    ram_util_percent: 64.9\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -5.0\n",
      "    pol2: 24.00000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: -26.845\n",
      "    pol2: -12.673999999999976\n",
      "  policy_reward_min:\n",
      "    pol1: -57.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39524729299997646\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21292510891743852\n",
      "    mean_inference_ms: 1.6632373197136234\n",
      "    mean_raw_obs_processing_ms: 1.6446533377561707\n",
      "  time_since_restore: 215.94747924804688\n",
      "  time_this_iter_s: 5.756451845169067\n",
      "  time_total_s: 215.94747924804688\n",
      "  timers:\n",
      "    learn_throughput: 831.421\n",
      "    learn_time_ms: 4811.041\n",
      "    sample_throughput: 5653.741\n",
      "    sample_time_ms: 707.496\n",
      "    update_time_ms: 3.252\n",
      "  timestamp: 1619703836\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -17.699999999999992\n",
      "  episode_reward_mean: -36.162000000000006\n",
      "  episode_reward_min: -58.50000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1400\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8138139247894287\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013656836934387684\n",
      "          model: {}\n",
      "          policy_loss: -0.038415227085351944\n",
      "          total_loss: 49.16576385498047\n",
      "          vf_explained_var: 0.1563236564397812\n",
      "          vf_loss: 49.190345764160156\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7895793318748474\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01418900117278099\n",
      "          model: {}\n",
      "          policy_loss: -0.061693549156188965\n",
      "          total_loss: 10.92877197265625\n",
      "          vf_explained_var: 0.3606082499027252\n",
      "          vf_loss: 10.97609806060791\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.4375\n",
      "    ram_util_percent: 64.9\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 0.5\n",
      "    pol2: 19.6\n",
      "  policy_reward_mean:\n",
      "    pol1: -21.09\n",
      "    pol2: -15.07199999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -51.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.383042478031217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21030729988272243\n",
      "    mean_inference_ms: 1.621455161329094\n",
      "    mean_raw_obs_processing_ms: 1.5878015522790665\n",
      "  time_since_restore: 215.97863459587097\n",
      "  time_this_iter_s: 5.648601770401001\n",
      "  time_total_s: 215.97863459587097\n",
      "  timers:\n",
      "    learn_throughput: 832.212\n",
      "    learn_time_ms: 4806.468\n",
      "    sample_throughput: 5454.99\n",
      "    sample_time_ms: 733.274\n",
      "    update_time_ms: 3.327\n",
      "  timestamp: 1619703836\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-43-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.999999999999996\n",
      "  episode_reward_mean: -31.940999999999995\n",
      "  episode_reward_min: -57.300000000000075\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1400\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7780563235282898\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01343735121190548\n",
      "          model: {}\n",
      "          policy_loss: -0.03954906016588211\n",
      "          total_loss: 49.995086669921875\n",
      "          vf_explained_var: 0.18689501285552979\n",
      "          vf_loss: 50.021034240722656\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7517547607421875\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00956245418637991\n",
      "          model: {}\n",
      "          policy_loss: -0.028807369992136955\n",
      "          total_loss: 30.58336639404297\n",
      "          vf_explained_var: 0.22532182931900024\n",
      "          vf_loss: 30.602489471435547\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.2375\n",
      "    ram_util_percent: 64.9\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -3.0\n",
      "    pol2: 45.99999999999993\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.75\n",
      "    pol2: -13.190999999999976\n",
      "  policy_reward_min:\n",
      "    pol1: -55.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3995238413378786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2177744778827637\n",
      "    mean_inference_ms: 1.6943661005954898\n",
      "    mean_raw_obs_processing_ms: 1.6582038869348616\n",
      "  time_since_restore: 215.81767797470093\n",
      "  time_this_iter_s: 5.7228991985321045\n",
      "  time_total_s: 215.81767797470093\n",
      "  timers:\n",
      "    learn_throughput: 835.642\n",
      "    learn_time_ms: 4786.74\n",
      "    sample_throughput: 4914.006\n",
      "    sample_time_ms: 814.0\n",
      "    update_time_ms: 2.987\n",
      "  timestamp: 1619703837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20.099999999999966\n",
      "  episode_reward_mean: -39.18900000000001\n",
      "  episode_reward_min: -60.300000000000054\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1440\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7454060316085815\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011229898780584335\n",
      "          model: {}\n",
      "          policy_loss: -0.03763540834188461\n",
      "          total_loss: 63.18269348144531\n",
      "          vf_explained_var: 0.10619394481182098\n",
      "          vf_loss: 63.208961486816406\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7014319896697998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013366684317588806\n",
      "          model: {}\n",
      "          policy_loss: -0.042815834283828735\n",
      "          total_loss: 12.968432426452637\n",
      "          vf_explained_var: 0.19236421585083008\n",
      "          vf_loss: 12.99771499633789\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.76666666666668\n",
      "    ram_util_percent: 64.98888888888888\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -7.0\n",
      "    pol2: 10.799999999999994\n",
      "  policy_reward_mean:\n",
      "    pol1: -28.77\n",
      "    pol2: -10.418999999999984\n",
      "  policy_reward_min:\n",
      "    pol1: -46.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4029778660198802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22438737211571053\n",
      "    mean_inference_ms: 1.7123744696508565\n",
      "    mean_raw_obs_processing_ms: 1.6710337697748165\n",
      "  time_since_restore: 221.88962936401367\n",
      "  time_this_iter_s: 5.7367048263549805\n",
      "  time_total_s: 221.88962936401367\n",
      "  timers:\n",
      "    learn_throughput: 839.845\n",
      "    learn_time_ms: 4762.782\n",
      "    sample_throughput: 5269.528\n",
      "    sample_time_ms: 759.081\n",
      "    update_time_ms: 3.733\n",
      "  timestamp: 1619703842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         221.89 </td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\"> -39.189</td><td style=\"text-align: right;\">               -20.1</td><td style=\"text-align: right;\">               -60.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         215.818</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> -31.941</td><td style=\"text-align: right;\">                -9  </td><td style=\"text-align: right;\">               -57.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         215.979</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> -36.162</td><td style=\"text-align: right;\">               -17.7</td><td style=\"text-align: right;\">               -58.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         215.947</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> -39.519</td><td style=\"text-align: right;\">               -18.3</td><td style=\"text-align: right;\">               -61.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -17.699999999999992\n",
      "  episode_reward_mean: -35.44499999999999\n",
      "  episode_reward_min: -57.600000000000044\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1440\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7972707748413086\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013321878388524055\n",
      "          model: {}\n",
      "          policy_loss: -0.045407045632600784\n",
      "          total_loss: 44.729469299316406\n",
      "          vf_explained_var: 0.13115015625953674\n",
      "          vf_loss: 44.76138687133789\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7949051856994629\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015360848978161812\n",
      "          model: {}\n",
      "          policy_loss: -0.06238534301519394\n",
      "          total_loss: 8.635727882385254\n",
      "          vf_explained_var: 0.38310590386390686\n",
      "          vf_loss: 8.682560920715332\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.9\n",
      "    ram_util_percent: 65.0\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -2.5\n",
      "    pol2: 5.300000000000031\n",
      "  policy_reward_mean:\n",
      "    pol1: -20.285\n",
      "    pol2: -15.159999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -47.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38201206641253727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20970893503104296\n",
      "    mean_inference_ms: 1.6172446149238817\n",
      "    mean_raw_obs_processing_ms: 1.5825950871402334\n",
      "  time_since_restore: 221.5566565990448\n",
      "  time_this_iter_s: 5.578022003173828\n",
      "  time_total_s: 221.5566565990448\n",
      "  timers:\n",
      "    learn_throughput: 831.827\n",
      "    learn_time_ms: 4808.69\n",
      "    sample_throughput: 5505.176\n",
      "    sample_time_ms: 726.589\n",
      "    update_time_ms: 3.248\n",
      "  timestamp: 1619703842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20.999999999999982\n",
      "  episode_reward_mean: -39.45300000000002\n",
      "  episode_reward_min: -61.20000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1440\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.75421142578125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012890078127384186\n",
      "          model: {}\n",
      "          policy_loss: -0.0470639243721962\n",
      "          total_loss: 65.76118469238281\n",
      "          vf_explained_var: 0.1853245496749878\n",
      "          vf_loss: 65.79519653320312\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7399790287017822\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012876348569989204\n",
      "          model: {}\n",
      "          policy_loss: -0.04389083757996559\n",
      "          total_loss: 23.271793365478516\n",
      "          vf_explained_var: 0.26072248816490173\n",
      "          vf_loss: 23.302644729614258\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.375\n",
      "    ram_util_percent: 65.0\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -6.0\n",
      "    pol2: 29.499999999999947\n",
      "  policy_reward_mean:\n",
      "    pol1: -27.615\n",
      "    pol2: -11.837999999999976\n",
      "  policy_reward_min:\n",
      "    pol1: -61.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39447709636183576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21250976471636004\n",
      "    mean_inference_ms: 1.6596579986975897\n",
      "    mean_raw_obs_processing_ms: 1.6398172977314076\n",
      "  time_since_restore: 221.63056921958923\n",
      "  time_this_iter_s: 5.683089971542358\n",
      "  time_total_s: 221.63056921958923\n",
      "  timers:\n",
      "    learn_throughput: 829.237\n",
      "    learn_time_ms: 4823.711\n",
      "    sample_throughput: 5696.605\n",
      "    sample_time_ms: 702.173\n",
      "    update_time_ms: 3.41\n",
      "  timestamp: 1619703842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.999999999999996\n",
      "  episode_reward_mean: -30.893999999999995\n",
      "  episode_reward_min: -54.60000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1440\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.768281877040863\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011662647128105164\n",
      "          model: {}\n",
      "          policy_loss: -0.044674552977085114\n",
      "          total_loss: 62.467403411865234\n",
      "          vf_explained_var: 0.1893349289894104\n",
      "          vf_loss: 62.500267028808594\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7365227937698364\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009716806933283806\n",
      "          model: {}\n",
      "          policy_loss: -0.03518659621477127\n",
      "          total_loss: 28.00693130493164\n",
      "          vf_explained_var: 0.3203393816947937\n",
      "          vf_loss: 28.032278060913086\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.875\n",
      "    ram_util_percent: 65.0\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 3.0\n",
      "    pol2: 45.99999999999993\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.825\n",
      "    pol2: -12.068999999999978\n",
      "  policy_reward_min:\n",
      "    pol1: -65.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3992308537630116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2174880166080036\n",
      "    mean_inference_ms: 1.6931789446383436\n",
      "    mean_raw_obs_processing_ms: 1.657975557537548\n",
      "  time_since_restore: 221.3736560344696\n",
      "  time_this_iter_s: 5.555978059768677\n",
      "  time_total_s: 221.3736560344696\n",
      "  timers:\n",
      "    learn_throughput: 838.953\n",
      "    learn_time_ms: 4767.85\n",
      "    sample_throughput: 4980.826\n",
      "    sample_time_ms: 803.08\n",
      "    update_time_ms: 2.904\n",
      "  timestamp: 1619703842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20.099999999999966\n",
      "  episode_reward_mean: -38.06100000000001\n",
      "  episode_reward_min: -61.50000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1480\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7358880639076233\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012636533938348293\n",
      "          model: {}\n",
      "          policy_loss: -0.04066472500562668\n",
      "          total_loss: 78.37477111816406\n",
      "          vf_explained_var: 0.12605933845043182\n",
      "          vf_loss: 78.40263366699219\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6878780126571655\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012476812116801739\n",
      "          model: {}\n",
      "          policy_loss: -0.04366454482078552\n",
      "          total_loss: 17.69314956665039\n",
      "          vf_explained_var: 0.23245419561862946\n",
      "          vf_loss: 17.724178314208984\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.65\n",
      "    ram_util_percent: 62.8875\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -7.0\n",
      "    pol2: 18.50000000000003\n",
      "  policy_reward_mean:\n",
      "    pol1: -29.105\n",
      "    pol2: -8.955999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -58.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4024711611852409\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22413395716265277\n",
      "    mean_inference_ms: 1.7092183506886938\n",
      "    mean_raw_obs_processing_ms: 1.6691264457475918\n",
      "  time_since_restore: 227.62539744377136\n",
      "  time_this_iter_s: 5.73576807975769\n",
      "  time_total_s: 227.62539744377136\n",
      "  timers:\n",
      "    learn_throughput: 850.534\n",
      "    learn_time_ms: 4702.928\n",
      "    sample_throughput: 5259.978\n",
      "    sample_time_ms: 760.459\n",
      "    update_time_ms: 3.709\n",
      "  timestamp: 1619703848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         227.625</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\"> -38.061</td><td style=\"text-align: right;\">               -20.1</td><td style=\"text-align: right;\">               -61.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         221.374</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\"> -30.894</td><td style=\"text-align: right;\">                -9  </td><td style=\"text-align: right;\">               -54.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         221.557</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\"> -35.445</td><td style=\"text-align: right;\">               -17.7</td><td style=\"text-align: right;\">               -57.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         221.631</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\"> -39.453</td><td style=\"text-align: right;\">               -21  </td><td style=\"text-align: right;\">               -61.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20.99999999999997\n",
      "  episode_reward_mean: -38.661000000000016\n",
      "  episode_reward_min: -77.40000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1480\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.745100200176239\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01402284111827612\n",
      "          model: {}\n",
      "          policy_loss: -0.04022061824798584\n",
      "          total_loss: 71.06468963623047\n",
      "          vf_explained_var: 0.14604327082633972\n",
      "          vf_loss: 71.09071350097656\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7393367290496826\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01324697770178318\n",
      "          model: {}\n",
      "          policy_loss: -0.05072203278541565\n",
      "          total_loss: 16.55243682861328\n",
      "          vf_explained_var: 0.30594420433044434\n",
      "          vf_loss: 16.589744567871094\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.7125\n",
      "    ram_util_percent: 63.550000000000004\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -2.5\n",
      "    pol2: 29.499999999999947\n",
      "  policy_reward_mean:\n",
      "    pol1: -26.9\n",
      "    pol2: -11.760999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -75.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39370040816403185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2121369392615443\n",
      "    mean_inference_ms: 1.6556875771959723\n",
      "    mean_raw_obs_processing_ms: 1.6358327751596995\n",
      "  time_since_restore: 227.31778120994568\n",
      "  time_this_iter_s: 5.687211990356445\n",
      "  time_total_s: 227.31778120994568\n",
      "  timers:\n",
      "    learn_throughput: 839.222\n",
      "    learn_time_ms: 4766.317\n",
      "    sample_throughput: 5667.08\n",
      "    sample_time_ms: 705.831\n",
      "    update_time_ms: 3.384\n",
      "  timestamp: 1619703848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -15.89999999999996\n",
      "  episode_reward_mean: -30.98399999999999\n",
      "  episode_reward_min: -46.80000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1480\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7518327236175537\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012522662058472633\n",
      "          model: {}\n",
      "          policy_loss: -0.04433324187994003\n",
      "          total_loss: 53.735328674316406\n",
      "          vf_explained_var: 0.14745765924453735\n",
      "          vf_loss: 53.76698684692383\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.742668628692627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012862127274274826\n",
      "          model: {}\n",
      "          policy_loss: -0.04450099170207977\n",
      "          total_loss: 13.186033248901367\n",
      "          vf_explained_var: 0.2811301648616791\n",
      "          vf_loss: 13.217510223388672\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.43333333333333\n",
      "    ram_util_percent: 62.62222222222223\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 3.0\n",
      "    pol2: 39.39999999999994\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.585\n",
      "    pol2: -12.39899999999998\n",
      "  policy_reward_min:\n",
      "    pol1: -65.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3979756390494775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2168464882431458\n",
      "    mean_inference_ms: 1.6881144842829894\n",
      "    mean_raw_obs_processing_ms: 1.6534619257687906\n",
      "  time_since_restore: 227.0137972831726\n",
      "  time_this_iter_s: 5.640141248703003\n",
      "  time_total_s: 227.0137972831726\n",
      "  timers:\n",
      "    learn_throughput: 849.762\n",
      "    learn_time_ms: 4707.203\n",
      "    sample_throughput: 5014.806\n",
      "    sample_time_ms: 797.638\n",
      "    update_time_ms: 2.871\n",
      "  timestamp: 1619703848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -18.299999999999972\n",
      "  episode_reward_mean: -35.784\n",
      "  episode_reward_min: -60.30000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1480\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7850490808486938\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01346583291888237\n",
      "          model: {}\n",
      "          policy_loss: -0.042475469410419464\n",
      "          total_loss: 50.401954650878906\n",
      "          vf_explained_var: 0.1539224535226822\n",
      "          vf_loss: 50.4307975769043\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7871721982955933\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015077076852321625\n",
      "          model: {}\n",
      "          policy_loss: -0.05078810825943947\n",
      "          total_loss: 9.044267654418945\n",
      "          vf_explained_var: 0.3466566801071167\n",
      "          vf_loss: 9.079791069030762\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.32222222222222\n",
      "    ram_util_percent: 62.62222222222223\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -0.5\n",
      "    pol2: 4.20000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -20.25\n",
      "    pol2: -15.533999999999969\n",
      "  policy_reward_min:\n",
      "    pol1: -45.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3815302113678419\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20944959024348897\n",
      "    mean_inference_ms: 1.613805948859871\n",
      "    mean_raw_obs_processing_ms: 1.5802348241849742\n",
      "  time_since_restore: 227.3941044807434\n",
      "  time_this_iter_s: 5.837447881698608\n",
      "  time_total_s: 227.3941044807434\n",
      "  timers:\n",
      "    learn_throughput: 842.086\n",
      "    learn_time_ms: 4750.11\n",
      "    sample_throughput: 5460.725\n",
      "    sample_time_ms: 732.503\n",
      "    update_time_ms: 3.091\n",
      "  timestamp: 1619703848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -15.89999999999996\n",
      "  episode_reward_mean: -30.516\n",
      "  episode_reward_min: -46.200000000000024\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1520\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7357374429702759\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013092800974845886\n",
      "          model: {}\n",
      "          policy_loss: -0.042856570333242416\n",
      "          total_loss: 73.24563598632812\n",
      "          vf_explained_var: 0.1566706895828247\n",
      "          vf_loss: 73.27523803710938\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.711794376373291\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011898646131157875\n",
      "          model: {}\n",
      "          policy_loss: -0.04436252638697624\n",
      "          total_loss: 39.84980010986328\n",
      "          vf_explained_var: 0.2558647692203522\n",
      "          vf_loss: 39.882110595703125\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.6\n",
      "    ram_util_percent: 55.04285714285715\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 3.0\n",
      "    pol2: 33.89999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: -19.415\n",
      "    pol2: -11.100999999999981\n",
      "  policy_reward_min:\n",
      "    pol1: -57.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3957330203130171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21576357630318369\n",
      "    mean_inference_ms: 1.6791445250481305\n",
      "    mean_raw_obs_processing_ms: 1.6440864353449518\n",
      "  time_since_restore: 232.22285413742065\n",
      "  time_this_iter_s: 5.209056854248047\n",
      "  time_total_s: 232.22285413742065\n",
      "  timers:\n",
      "    learn_throughput: 861.435\n",
      "    learn_time_ms: 4643.417\n",
      "    sample_throughput: 5111.831\n",
      "    sample_time_ms: 782.499\n",
      "    update_time_ms: 2.846\n",
      "  timestamp: 1619703853\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         227.625</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\"> -38.061</td><td style=\"text-align: right;\">               -20.1</td><td style=\"text-align: right;\">               -61.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         232.223</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\"> -30.516</td><td style=\"text-align: right;\">               -15.9</td><td style=\"text-align: right;\">               -46.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         227.394</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\"> -35.784</td><td style=\"text-align: right;\">               -18.3</td><td style=\"text-align: right;\">               -60.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         227.318</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\"> -38.661</td><td style=\"text-align: right;\">               -21  </td><td style=\"text-align: right;\">               -77.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20.99999999999997\n",
      "  episode_reward_mean: -39.09000000000001\n",
      "  episode_reward_min: -77.40000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1520\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7454019784927368\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014348085969686508\n",
      "          model: {}\n",
      "          policy_loss: -0.03885621950030327\n",
      "          total_loss: 70.56571197509766\n",
      "          vf_explained_var: 0.14796730875968933\n",
      "          vf_loss: 70.59004211425781\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7138841152191162\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013648273423314095\n",
      "          model: {}\n",
      "          policy_loss: -0.04282220080494881\n",
      "          total_loss: 22.401691436767578\n",
      "          vf_explained_var: 0.2789841294288635\n",
      "          vf_loss: 22.430694580078125\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.27499999999999\n",
      "    ram_util_percent: 55.0625\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -2.5\n",
      "    pol2: 29.499999999999947\n",
      "  policy_reward_mean:\n",
      "    pol1: -27.835\n",
      "    pol2: -11.254999999999978\n",
      "  policy_reward_min:\n",
      "    pol1: -75.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39267789073358883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.211721568855313\n",
      "    mean_inference_ms: 1.6505301733648117\n",
      "    mean_raw_obs_processing_ms: 1.6318201625060487\n",
      "  time_since_restore: 232.69086408615112\n",
      "  time_this_iter_s: 5.373082876205444\n",
      "  time_total_s: 232.69086408615112\n",
      "  timers:\n",
      "    learn_throughput: 849.24\n",
      "    learn_time_ms: 4710.093\n",
      "    sample_throughput: 5684.535\n",
      "    sample_time_ms: 703.664\n",
      "    update_time_ms: 3.254\n",
      "  timestamp: 1619703853\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20.099999999999966\n",
      "  episode_reward_mean: -37.88400000000001\n",
      "  episode_reward_min: -61.50000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1520\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7244240045547485\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012149590998888016\n",
      "          model: {}\n",
      "          policy_loss: -0.0384342297911644\n",
      "          total_loss: 78.6996078491211\n",
      "          vf_explained_var: 0.10975843667984009\n",
      "          vf_loss: 78.72573852539062\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6646976470947266\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011457061395049095\n",
      "          model: {}\n",
      "          policy_loss: -0.04076056182384491\n",
      "          total_loss: 31.52444076538086\n",
      "          vf_explained_var: 0.20706328749656677\n",
      "          vf_loss: 31.553600311279297\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.77499999999999\n",
      "    ram_util_percent: 55.075\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -8.5\n",
      "    pol2: 28.400000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -30.38\n",
      "    pol2: -7.503999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -69.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40221585564000706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22392410155939635\n",
      "    mean_inference_ms: 1.706048517956549\n",
      "    mean_raw_obs_processing_ms: 1.669405734038545\n",
      "  time_since_restore: 233.14884757995605\n",
      "  time_this_iter_s: 5.523450136184692\n",
      "  time_total_s: 233.14884757995605\n",
      "  timers:\n",
      "    learn_throughput: 859.617\n",
      "    learn_time_ms: 4653.234\n",
      "    sample_throughput: 5219.769\n",
      "    sample_time_ms: 766.317\n",
      "    update_time_ms: 3.665\n",
      "  timestamp: 1619703853\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.800000000000006\n",
      "  episode_reward_mean: -34.224\n",
      "  episode_reward_min: -60.30000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1520\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7686713337898254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013910641893744469\n",
      "          model: {}\n",
      "          policy_loss: -0.0444229394197464\n",
      "          total_loss: 42.863529205322266\n",
      "          vf_explained_var: 0.15093712508678436\n",
      "          vf_loss: 42.89386749267578\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7547003626823425\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013324588537216187\n",
      "          model: {}\n",
      "          policy_loss: -0.04776551574468613\n",
      "          total_loss: 17.178977966308594\n",
      "          vf_explained_var: 0.31085005402565\n",
      "          vf_loss: 17.213253021240234\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.371428571428574\n",
      "    ram_util_percent: 55.04285714285715\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -0.5\n",
      "    pol2: 20.70000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.745\n",
      "    pol2: -15.47899999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -45.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3805301674853335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20895524195077758\n",
      "    mean_inference_ms: 1.608458164596441\n",
      "    mean_raw_obs_processing_ms: 1.5759358375138302\n",
      "  time_since_restore: 232.7370285987854\n",
      "  time_this_iter_s: 5.342924118041992\n",
      "  time_total_s: 232.7370285987854\n",
      "  timers:\n",
      "    learn_throughput: 850.171\n",
      "    learn_time_ms: 4704.935\n",
      "    sample_throughput: 5567.036\n",
      "    sample_time_ms: 718.515\n",
      "    update_time_ms: 3.03\n",
      "  timestamp: 1619703853\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -15.899999999999974\n",
      "  episode_reward_mean: -31.274999999999995\n",
      "  episode_reward_min: -46.200000000000024\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1560\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7296280860900879\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013442829251289368\n",
      "          model: {}\n",
      "          policy_loss: -0.04293930530548096\n",
      "          total_loss: 41.126731872558594\n",
      "          vf_explained_var: 0.19247539341449738\n",
      "          vf_loss: 41.15605926513672\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7198233008384705\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012599743902683258\n",
      "          model: {}\n",
      "          policy_loss: -0.04338322579860687\n",
      "          total_loss: 10.811213493347168\n",
      "          vf_explained_var: 0.24265322089195251\n",
      "          vf_loss: 10.841839790344238\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.50000000000001\n",
      "    ram_util_percent: 54.89999999999999\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -2.0\n",
      "    pol2: 33.89999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: -19.91\n",
      "    pol2: -11.364999999999979\n",
      "  policy_reward_min:\n",
      "    pol1: -57.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39374086777888373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21480032213833675\n",
      "    mean_inference_ms: 1.6699112745688893\n",
      "    mean_raw_obs_processing_ms: 1.6355205141940032\n",
      "  time_since_restore: 237.39730715751648\n",
      "  time_this_iter_s: 5.174453020095825\n",
      "  time_total_s: 237.39730715751648\n",
      "  timers:\n",
      "    learn_throughput: 874.835\n",
      "    learn_time_ms: 4572.289\n",
      "    sample_throughput: 5125.32\n",
      "    sample_time_ms: 780.439\n",
      "    update_time_ms: 2.794\n",
      "  timestamp: 1619703858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         233.149</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\"> -37.884</td><td style=\"text-align: right;\">               -20.1</td><td style=\"text-align: right;\">               -61.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         237.397</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\"> -31.275</td><td style=\"text-align: right;\">               -15.9</td><td style=\"text-align: right;\">               -46.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         232.737</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\"> -34.224</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               -60.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         232.691</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\"> -39.09 </td><td style=\"text-align: right;\">               -21  </td><td style=\"text-align: right;\">               -77.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16.79999999999998\n",
      "  episode_reward_mean: -34.848\n",
      "  episode_reward_min: -57.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1560\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7127864360809326\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011409412138164043\n",
      "          model: {}\n",
      "          policy_loss: -0.03785654157400131\n",
      "          total_loss: 51.31982421875\n",
      "          vf_explained_var: 0.14388220012187958\n",
      "          vf_loss: 51.346126556396484\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6519321203231812\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011521369218826294\n",
      "          model: {}\n",
      "          policy_loss: -0.042906902730464935\n",
      "          total_loss: 16.132232666015625\n",
      "          vf_explained_var: 0.21823737025260925\n",
      "          vf_loss: 16.16347312927246\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.47142857142858\n",
      "    ram_util_percent: 54.85714285714285\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -8.5\n",
      "    pol2: 28.400000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -27.795\n",
      "    pol2: -7.052999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -69.5\n",
      "    pol2: -18.899999999999963\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40137696548053137\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2234098180442443\n",
      "    mean_inference_ms: 1.7006224653559656\n",
      "    mean_raw_obs_processing_ms: 1.667309084319803\n",
      "  time_since_restore: 238.335289478302\n",
      "  time_this_iter_s: 5.186441898345947\n",
      "  time_total_s: 238.335289478302\n",
      "  timers:\n",
      "    learn_throughput: 868.394\n",
      "    learn_time_ms: 4606.205\n",
      "    sample_throughput: 5200.403\n",
      "    sample_time_ms: 769.171\n",
      "    update_time_ms: 3.666\n",
      "  timestamp: 1619703859\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -21.299999999999994\n",
      "  episode_reward_mean: -39.14400000000001\n",
      "  episode_reward_min: -58.50000000000004\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1560\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7287638187408447\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011093378067016602\n",
      "          model: {}\n",
      "          policy_loss: -0.028615636751055717\n",
      "          total_loss: 62.68482208251953\n",
      "          vf_explained_var: 0.11469269543886185\n",
      "          vf_loss: 62.70220947265625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7269384860992432\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011907831765711308\n",
      "          model: {}\n",
      "          policy_loss: -0.04016541317105293\n",
      "          total_loss: 14.537214279174805\n",
      "          vf_explained_var: 0.2766701281070709\n",
      "          vf_loss: 14.565322875976562\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.724999999999994\n",
      "    ram_util_percent: 54.8875\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -3.0\n",
      "    pol2: 15.200000000000003\n",
      "  policy_reward_mean:\n",
      "    pol1: -26.69\n",
      "    pol2: -12.453999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -57.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3917186272072422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2113167501692562\n",
      "    mean_inference_ms: 1.6451446271910652\n",
      "    mean_raw_obs_processing_ms: 1.6278278297954416\n",
      "  time_since_restore: 237.99371814727783\n",
      "  time_this_iter_s: 5.302854061126709\n",
      "  time_total_s: 237.99371814727783\n",
      "  timers:\n",
      "    learn_throughput: 860.885\n",
      "    learn_time_ms: 4646.379\n",
      "    sample_throughput: 5642.284\n",
      "    sample_time_ms: 708.933\n",
      "    update_time_ms: 3.266\n",
      "  timestamp: 1619703859\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.100000000000005\n",
      "  episode_reward_mean: -33.564\n",
      "  episode_reward_min: -60.30000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1560\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7693251967430115\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013948872685432434\n",
      "          model: {}\n",
      "          policy_loss: -0.0494953915476799\n",
      "          total_loss: 57.865501403808594\n",
      "          vf_explained_var: 0.13591942191123962\n",
      "          vf_loss: 57.90087890625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7376946210861206\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012466578744351864\n",
      "          model: {}\n",
      "          policy_loss: -0.03677617013454437\n",
      "          total_loss: 44.69512176513672\n",
      "          vf_explained_var: 0.2629706859588623\n",
      "          vf_loss: 44.719268798828125\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.25\n",
      "    ram_util_percent: 54.8875\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: -0.5\n",
      "    pol2: 55.89999999999987\n",
      "  policy_reward_mean:\n",
      "    pol1: -20.01\n",
      "    pol2: -13.553999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -71.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3791394736697001\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20826987535181574\n",
      "    mean_inference_ms: 1.6019084606097747\n",
      "    mean_raw_obs_processing_ms: 1.5700479924146244\n",
      "  time_since_restore: 237.96912455558777\n",
      "  time_this_iter_s: 5.232095956802368\n",
      "  time_total_s: 237.96912455558777\n",
      "  timers:\n",
      "    learn_throughput: 862.199\n",
      "    learn_time_ms: 4639.303\n",
      "    sample_throughput: 5569.351\n",
      "    sample_time_ms: 718.217\n",
      "    update_time_ms: 3.07\n",
      "  timestamp: 1619703859\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -15.899999999999974\n",
      "  episode_reward_mean: -29.951999999999998\n",
      "  episode_reward_min: -57.30000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1600\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7124708890914917\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011882206425070763\n",
      "          model: {}\n",
      "          policy_loss: -0.033381834626197815\n",
      "          total_loss: 34.500213623046875\n",
      "          vf_explained_var: 0.19664901494979858\n",
      "          vf_loss: 34.52156066894531\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6980897188186646\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012372396886348724\n",
      "          model: {}\n",
      "          policy_loss: -0.0411723256111145\n",
      "          total_loss: 10.337221145629883\n",
      "          vf_explained_var: 0.23358729481697083\n",
      "          vf_loss: 10.365865707397461\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.974999999999994\n",
      "    ram_util_percent: 54.75\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 1.5\n",
      "    pol2: 21.80000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -16.86\n",
      "    pol2: -13.091999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -52.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3919718370485684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21400007818149205\n",
      "    mean_inference_ms: 1.6614986312066475\n",
      "    mean_raw_obs_processing_ms: 1.6281655566585516\n",
      "  time_since_restore: 242.72782826423645\n",
      "  time_this_iter_s: 5.330521106719971\n",
      "  time_total_s: 242.72782826423645\n",
      "  timers:\n",
      "    learn_throughput: 883.005\n",
      "    learn_time_ms: 4529.985\n",
      "    sample_throughput: 5335.274\n",
      "    sample_time_ms: 749.727\n",
      "    update_time_ms: 2.905\n",
      "  timestamp: 1619703864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         238.335</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\"> -34.848</td><td style=\"text-align: right;\">               -16.8</td><td style=\"text-align: right;\">               -57  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         242.728</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\"> -29.952</td><td style=\"text-align: right;\">               -15.9</td><td style=\"text-align: right;\">               -57.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         237.969</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\"> -33.564</td><td style=\"text-align: right;\">                -8.1</td><td style=\"text-align: right;\">               -60.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         237.994</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\"> -39.144</td><td style=\"text-align: right;\">               -21.3</td><td style=\"text-align: right;\">               -58.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.100000000000005\n",
      "  episode_reward_mean: -30.992999999999988\n",
      "  episode_reward_min: -48.30000000000004\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1600\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7552467584609985\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014542650431394577\n",
      "          model: {}\n",
      "          policy_loss: -0.05497484654188156\n",
      "          total_loss: 33.39323425292969\n",
      "          vf_explained_var: 0.1488848328590393\n",
      "          vf_loss: 33.43348693847656\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7421549558639526\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015557962469756603\n",
      "          model: {}\n",
      "          policy_loss: -0.05535295233130455\n",
      "          total_loss: 8.615297317504883\n",
      "          vf_explained_var: 0.3691687285900116\n",
      "          vf_loss: 8.654897689819336\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.47142857142857\n",
      "    ram_util_percent: 54.74285714285714\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 6.0\n",
      "    pol2: 55.89999999999987\n",
      "  policy_reward_mean:\n",
      "    pol1: -17.395\n",
      "    pol2: -13.597999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -71.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3774979260944575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20748621313030305\n",
      "    mean_inference_ms: 1.5949069911491605\n",
      "    mean_raw_obs_processing_ms: 1.5633324287070587\n",
      "  time_since_restore: 243.24371457099915\n",
      "  time_this_iter_s: 5.274590015411377\n",
      "  time_total_s: 243.24371457099915\n",
      "  timers:\n",
      "    learn_throughput: 870.922\n",
      "    learn_time_ms: 4592.835\n",
      "    sample_throughput: 5721.063\n",
      "    sample_time_ms: 699.171\n",
      "    update_time_ms: 3.109\n",
      "  timestamp: 1619703864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16.79999999999998\n",
      "  episode_reward_mean: -33.168\n",
      "  episode_reward_min: -57.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1600\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6914464235305786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011058717966079712\n",
      "          model: {}\n",
      "          policy_loss: -0.030441217124462128\n",
      "          total_loss: 56.38155746459961\n",
      "          vf_explained_var: 0.12871107459068298\n",
      "          vf_loss: 56.40080261230469\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6245630979537964\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012300310656428337\n",
      "          model: {}\n",
      "          policy_loss: -0.042175181210041046\n",
      "          total_loss: 25.137798309326172\n",
      "          vf_explained_var: 0.2323729693889618\n",
      "          vf_loss: 25.167518615722656\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.7625\n",
      "    ram_util_percent: 54.7375\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -5.5\n",
      "    pol2: 28.400000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -27.16\n",
      "    pol2: -6.007999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -69.5\n",
      "    pol2: -18.899999999999963\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.40055446678635975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22290791833489387\n",
      "    mean_inference_ms: 1.6951948167586681\n",
      "    mean_raw_obs_processing_ms: 1.6648474808815836\n",
      "  time_since_restore: 243.79453945159912\n",
      "  time_this_iter_s: 5.459249973297119\n",
      "  time_total_s: 243.79453945159912\n",
      "  timers:\n",
      "    learn_throughput: 874.745\n",
      "    learn_time_ms: 4572.763\n",
      "    sample_throughput: 5219.405\n",
      "    sample_time_ms: 766.371\n",
      "    update_time_ms: 3.651\n",
      "  timestamp: 1619703864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -21.899999999999977\n",
      "  episode_reward_mean: -38.33100000000001\n",
      "  episode_reward_min: -64.50000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1600\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7088476419448853\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012602792121469975\n",
      "          model: {}\n",
      "          policy_loss: -0.03998521715402603\n",
      "          total_loss: 56.46704864501953\n",
      "          vf_explained_var: 0.14569872617721558\n",
      "          vf_loss: 56.4942741394043\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.709307849407196\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013395233079791069\n",
      "          model: {}\n",
      "          policy_loss: -0.04359015077352524\n",
      "          total_loss: 13.206748962402344\n",
      "          vf_explained_var: 0.280917763710022\n",
      "          vf_loss: 13.236776351928711\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.475\n",
      "    ram_util_percent: 54.75\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -3.0\n",
      "    pol2: 11.900000000000025\n",
      "  policy_reward_mean:\n",
      "    pol1: -25.8\n",
      "    pol2: -12.530999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -57.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3911418943188042\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21106186177798528\n",
      "    mean_inference_ms: 1.6413363356371464\n",
      "    mean_raw_obs_processing_ms: 1.6256011851065484\n",
      "  time_since_restore: 243.46238899230957\n",
      "  time_this_iter_s: 5.468670845031738\n",
      "  time_total_s: 243.46238899230957\n",
      "  timers:\n",
      "    learn_throughput: 867.934\n",
      "    learn_time_ms: 4608.647\n",
      "    sample_throughput: 5666.702\n",
      "    sample_time_ms: 705.878\n",
      "    update_time_ms: 3.231\n",
      "  timestamp: 1619703864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16.799999999999997\n",
      "  episode_reward_mean: -30.15299999999999\n",
      "  episode_reward_min: -58.200000000000074\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1640\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7169259190559387\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013321880251169205\n",
      "          model: {}\n",
      "          policy_loss: -0.03742083162069321\n",
      "          total_loss: 50.27531051635742\n",
      "          vf_explained_var: 0.2023952603340149\n",
      "          vf_loss: 50.29924011230469\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6979228258132935\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013934411108493805\n",
      "          model: {}\n",
      "          policy_loss: -0.0479261577129364\n",
      "          total_loss: 11.045158386230469\n",
      "          vf_explained_var: 0.29550135135650635\n",
      "          vf_loss: 11.078975677490234\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.05\n",
      "    ram_util_percent: 55.2625\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 1.5\n",
      "    pol2: 4.20000000000003\n",
      "  policy_reward_mean:\n",
      "    pol1: -16.39\n",
      "    pol2: -13.762999999999971\n",
      "  policy_reward_min:\n",
      "    pol1: -41.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39097930234497325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21357497551454208\n",
      "    mean_inference_ms: 1.65623662492196\n",
      "    mean_raw_obs_processing_ms: 1.6247086691876205\n",
      "  time_since_restore: 248.28787112236023\n",
      "  time_this_iter_s: 5.560042858123779\n",
      "  time_total_s: 248.28787112236023\n",
      "  timers:\n",
      "    learn_throughput: 885.317\n",
      "    learn_time_ms: 4518.158\n",
      "    sample_throughput: 5292.725\n",
      "    sample_time_ms: 755.754\n",
      "    update_time_ms: 3.188\n",
      "  timestamp: 1619703869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         243.795</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\"> -33.168</td><td style=\"text-align: right;\">               -16.8</td><td style=\"text-align: right;\">               -57  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         248.288</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\"> -30.153</td><td style=\"text-align: right;\">               -16.8</td><td style=\"text-align: right;\">               -58.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         243.244</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\"> -30.993</td><td style=\"text-align: right;\">                -8.1</td><td style=\"text-align: right;\">               -48.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         243.462</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\"> -38.331</td><td style=\"text-align: right;\">               -21.9</td><td style=\"text-align: right;\">               -64.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.100000000000005\n",
      "  episode_reward_mean: -31.046999999999993\n",
      "  episode_reward_min: -57.00000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1640\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7417598962783813\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012348907068371773\n",
      "          model: {}\n",
      "          policy_loss: -0.04774465784430504\n",
      "          total_loss: 52.86546325683594\n",
      "          vf_explained_var: 0.15233348309993744\n",
      "          vf_loss: 52.90070343017578\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.7275129556655884\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013823186978697777\n",
      "          model: {}\n",
      "          policy_loss: -0.05079159885644913\n",
      "          total_loss: 23.994470596313477\n",
      "          vf_explained_var: 0.2704513967037201\n",
      "          vf_loss: 24.031265258789062\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.1\n",
      "    ram_util_percent: 55.2625\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 6.0\n",
      "    pol2: 55.89999999999987\n",
      "  policy_reward_mean:\n",
      "    pol1: -17.295\n",
      "    pol2: -13.75199999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -71.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.37601031139026003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20677344079213833\n",
      "    mean_inference_ms: 1.5884195125715124\n",
      "    mean_raw_obs_processing_ms: 1.5573076402315404\n",
      "  time_since_restore: 248.65386533737183\n",
      "  time_this_iter_s: 5.410150766372681\n",
      "  time_total_s: 248.65386533737183\n",
      "  timers:\n",
      "    learn_throughput: 873.266\n",
      "    learn_time_ms: 4580.504\n",
      "    sample_throughput: 5751.3\n",
      "    sample_time_ms: 695.495\n",
      "    update_time_ms: 3.156\n",
      "  timestamp: 1619703869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1000000000000068\n",
      "  episode_reward_mean: -33.785999999999994\n",
      "  episode_reward_min: -56.10000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1640\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6776335835456848\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011684381403028965\n",
      "          model: {}\n",
      "          policy_loss: -0.03093407303094864\n",
      "          total_loss: 92.39463806152344\n",
      "          vf_explained_var: 0.12180263549089432\n",
      "          vf_loss: 92.41374206542969\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6088876724243164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009054379537701607\n",
      "          model: {}\n",
      "          policy_loss: -0.025099605321884155\n",
      "          total_loss: 57.0578498840332\n",
      "          vf_explained_var: 0.2552034258842468\n",
      "          vf_loss: 57.07378387451172\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.9875\n",
      "    ram_util_percent: 55.325\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: 0.5\n",
      "    pol2: 49.29999999999986\n",
      "  policy_reward_mean:\n",
      "    pol1: -28.35\n",
      "    pol2: -5.4359999999999875\n",
      "  policy_reward_min:\n",
      "    pol1: -73.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3999344156691962\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2225727425433264\n",
      "    mean_inference_ms: 1.6912359784440347\n",
      "    mean_raw_obs_processing_ms: 1.6631729047755324\n",
      "  time_since_restore: 249.39314150810242\n",
      "  time_this_iter_s: 5.598602056503296\n",
      "  time_total_s: 249.39314150810242\n",
      "  timers:\n",
      "    learn_throughput: 874.953\n",
      "    learn_time_ms: 4571.676\n",
      "    sample_throughput: 5234.476\n",
      "    sample_time_ms: 764.164\n",
      "    update_time_ms: 3.549\n",
      "  timestamp: 1619703870\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -9.299999999999994\n",
      "  episode_reward_mean: -39.00600000000001\n",
      "  episode_reward_min: -76.20000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1640\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7122434377670288\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011789594776928425\n",
      "          model: {}\n",
      "          policy_loss: -0.038876477628946304\n",
      "          total_loss: 69.16931915283203\n",
      "          vf_explained_var: 0.15019327402114868\n",
      "          vf_loss: 69.19625854492188\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6951428651809692\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011243836022913456\n",
      "          model: {}\n",
      "          policy_loss: -0.033737748861312866\n",
      "          total_loss: 21.696693420410156\n",
      "          vf_explained_var: 0.22553376853466034\n",
      "          vf_loss: 21.719043731689453\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.8375\n",
      "    ram_util_percent: 55.337500000000006\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -6.5\n",
      "    pol2: 20.70000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -26.365\n",
      "    pol2: -12.640999999999977\n",
      "  policy_reward_min:\n",
      "    pol1: -59.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39112176904890555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21099580958162\n",
      "    mean_inference_ms: 1.6400704192563638\n",
      "    mean_raw_obs_processing_ms: 1.6257167275805755\n",
      "  time_since_restore: 249.0686719417572\n",
      "  time_this_iter_s: 5.606282949447632\n",
      "  time_total_s: 249.0686719417572\n",
      "  timers:\n",
      "    learn_throughput: 869.377\n",
      "    learn_time_ms: 4600.996\n",
      "    sample_throughput: 5546.235\n",
      "    sample_time_ms: 721.21\n",
      "    update_time_ms: 3.186\n",
      "  timestamp: 1619703870\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -14.399999999999997\n",
      "  episode_reward_mean: -30.255\n",
      "  episode_reward_min: -58.200000000000074\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1680\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.695355236530304\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011845503002405167\n",
      "          model: {}\n",
      "          policy_loss: -0.037286385893821716\n",
      "          total_loss: 62.379337310791016\n",
      "          vf_explained_var: 0.18025225400924683\n",
      "          vf_loss: 62.404632568359375\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6623541712760925\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011171335354447365\n",
      "          model: {}\n",
      "          policy_loss: -0.03861244395375252\n",
      "          total_loss: 25.054882049560547\n",
      "          vf_explained_var: 0.3392680585384369\n",
      "          vf_loss: 25.08218002319336\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.012499999999996\n",
      "    ram_util_percent: 55.5125\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 1.0\n",
      "    pol2: 20.700000000000024\n",
      "  policy_reward_mean:\n",
      "    pol1: -17.405\n",
      "    pol2: -12.849999999999977\n",
      "  policy_reward_min:\n",
      "    pol1: -60.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3904076704203595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2133604836958301\n",
      "    mean_inference_ms: 1.6532273087932403\n",
      "    mean_raw_obs_processing_ms: 1.6227019077913312\n",
      "  time_since_restore: 253.79624128341675\n",
      "  time_this_iter_s: 5.5083701610565186\n",
      "  time_total_s: 253.79624128341675\n",
      "  timers:\n",
      "    learn_throughput: 884.976\n",
      "    learn_time_ms: 4519.895\n",
      "    sample_throughput: 5330.177\n",
      "    sample_time_ms: 750.444\n",
      "    update_time_ms: 3.407\n",
      "  timestamp: 1619703875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         249.393</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\"> -33.786</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">               -56.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         253.796</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\"> -30.255</td><td style=\"text-align: right;\">               -14.4</td><td style=\"text-align: right;\">               -58.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         248.654</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\"> -31.047</td><td style=\"text-align: right;\">                -8.1</td><td style=\"text-align: right;\">               -57  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         249.069</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\"> -39.006</td><td style=\"text-align: right;\">                -9.3</td><td style=\"text-align: right;\">               -76.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.700000000000003\n",
      "  episode_reward_mean: -31.139999999999997\n",
      "  episode_reward_min: -57.00000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1680\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7316937446594238\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012332964688539505\n",
      "          model: {}\n",
      "          policy_loss: -0.03773343563079834\n",
      "          total_loss: 63.31257247924805\n",
      "          vf_explained_var: 0.15398120880126953\n",
      "          vf_loss: 63.33781814575195\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6999694108963013\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010501755401492119\n",
      "          model: {}\n",
      "          policy_loss: -0.04951058700680733\n",
      "          total_loss: 42.32758331298828\n",
      "          vf_explained_var: 0.30174705386161804\n",
      "          vf_loss: 42.366458892822266\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.16250000000001\n",
      "    ram_util_percent: 55.5125\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 2.0\n",
      "    pol2: 65.79999999999995\n",
      "  policy_reward_mean:\n",
      "    pol1: -16.915\n",
      "    pol2: -14.224999999999971\n",
      "  policy_reward_min:\n",
      "    pol1: -74.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.37517166402270663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20636059932902856\n",
      "    mean_inference_ms: 1.5839928016994358\n",
      "    mean_raw_obs_processing_ms: 1.5538409452912676\n",
      "  time_since_restore: 254.20389437675476\n",
      "  time_this_iter_s: 5.550029039382935\n",
      "  time_total_s: 254.20389437675476\n",
      "  timers:\n",
      "    learn_throughput: 873.619\n",
      "    learn_time_ms: 4578.652\n",
      "    sample_throughput: 5713.843\n",
      "    sample_time_ms: 700.054\n",
      "    update_time_ms: 3.237\n",
      "  timestamp: 1619703875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1000000000000068\n",
      "  episode_reward_mean: -33.404999999999994\n",
      "  episode_reward_min: -73.50000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1680\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6744298934936523\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012057451531291008\n",
      "          model: {}\n",
      "          policy_loss: -0.0336676687002182\n",
      "          total_loss: 82.07420349121094\n",
      "          vf_explained_var: 0.15121473371982574\n",
      "          vf_loss: 82.09565734863281\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6107769012451172\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011677002534270287\n",
      "          model: {}\n",
      "          policy_loss: -0.03854057192802429\n",
      "          total_loss: 33.33670425415039\n",
      "          vf_explained_var: 0.278296560049057\n",
      "          vf_loss: 33.363426208496094\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.3375\n",
      "    ram_util_percent: 55.5375\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: 0.5\n",
      "    pol2: 49.29999999999986\n",
      "  policy_reward_mean:\n",
      "    pol1: -28.86\n",
      "    pol2: -4.54499999999999\n",
      "  policy_reward_min:\n",
      "    pol1: -73.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3990384564294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22204659551150996\n",
      "    mean_inference_ms: 1.686859509921299\n",
      "    mean_raw_obs_processing_ms: 1.6598984599018234\n",
      "  time_since_restore: 254.75708556175232\n",
      "  time_this_iter_s: 5.363944053649902\n",
      "  time_total_s: 254.75708556175232\n",
      "  timers:\n",
      "    learn_throughput: 872.989\n",
      "    learn_time_ms: 4581.958\n",
      "    sample_throughput: 5325.675\n",
      "    sample_time_ms: 751.079\n",
      "    update_time_ms: 3.433\n",
      "  timestamp: 1619703875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -9.299999999999994\n",
      "  episode_reward_mean: -38.628000000000014\n",
      "  episode_reward_min: -76.20000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1680\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.705325722694397\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013329140841960907\n",
      "          model: {}\n",
      "          policy_loss: -0.04505608230829239\n",
      "          total_loss: 70.34996795654297\n",
      "          vf_explained_var: 0.18168342113494873\n",
      "          vf_loss: 70.38153076171875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6674498319625854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011615170165896416\n",
      "          model: {}\n",
      "          policy_loss: -0.03998580202460289\n",
      "          total_loss: 16.293678283691406\n",
      "          vf_explained_var: 0.27831798791885376\n",
      "          vf_loss: 16.321903228759766\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.24285714285715\n",
      "    ram_util_percent: 55.52857142857143\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -1.5\n",
      "    pol2: 21.800000000000022\n",
      "  policy_reward_mean:\n",
      "    pol1: -26.515\n",
      "    pol2: -12.112999999999975\n",
      "  policy_reward_min:\n",
      "    pol1: -59.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3907216857273332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2107363880864017\n",
      "    mean_inference_ms: 1.637859604371152\n",
      "    mean_raw_obs_processing_ms: 1.6237329886996796\n",
      "  time_since_restore: 254.4098298549652\n",
      "  time_this_iter_s: 5.341157913208008\n",
      "  time_total_s: 254.4098298549652\n",
      "  timers:\n",
      "    learn_throughput: 869.895\n",
      "    learn_time_ms: 4598.255\n",
      "    sample_throughput: 5592.523\n",
      "    sample_time_ms: 715.241\n",
      "    update_time_ms: 3.07\n",
      "  timestamp: 1619703875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -14.399999999999997\n",
      "  episode_reward_mean: -29.154\n",
      "  episode_reward_min: -58.200000000000074\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1720\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7060133218765259\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013949190266430378\n",
      "          model: {}\n",
      "          policy_loss: -0.04744083061814308\n",
      "          total_loss: 32.4549446105957\n",
      "          vf_explained_var: 0.1927400827407837\n",
      "          vf_loss: 32.48826599121094\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6581153869628906\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013684004545211792\n",
      "          model: {}\n",
      "          policy_loss: -0.049962032586336136\n",
      "          total_loss: 12.130102157592773\n",
      "          vf_explained_var: 0.25113344192504883\n",
      "          vf_loss: 12.166208267211914\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.83749999999999\n",
      "    ram_util_percent: 55.7875\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -1.0\n",
      "    pol2: 20.700000000000024\n",
      "  policy_reward_mean:\n",
      "    pol1: -17.195\n",
      "    pol2: -11.958999999999978\n",
      "  policy_reward_min:\n",
      "    pol1: -60.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.389662864614894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21301846810853958\n",
      "    mean_inference_ms: 1.6498336937784106\n",
      "    mean_raw_obs_processing_ms: 1.6191877082577424\n",
      "  time_since_restore: 259.34171533584595\n",
      "  time_this_iter_s: 5.545474052429199\n",
      "  time_total_s: 259.34171533584595\n",
      "  timers:\n",
      "    learn_throughput: 880.599\n",
      "    learn_time_ms: 4542.363\n",
      "    sample_throughput: 5618.782\n",
      "    sample_time_ms: 711.898\n",
      "    update_time_ms: 3.532\n",
      "  timestamp: 1619703880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         254.757</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\"> -33.405</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">               -73.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         259.342</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\"> -29.154</td><td style=\"text-align: right;\">               -14.4</td><td style=\"text-align: right;\">               -58.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         254.204</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\"> -31.14 </td><td style=\"text-align: right;\">                -8.7</td><td style=\"text-align: right;\">               -57  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         254.41 </td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\"> -38.628</td><td style=\"text-align: right;\">                -9.3</td><td style=\"text-align: right;\">               -76.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.69999999999999\n",
      "  episode_reward_mean: -32.04899999999999\n",
      "  episode_reward_min: -54.30000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1720\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.7055241465568542\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013003507629036903\n",
      "          model: {}\n",
      "          policy_loss: -0.04498252645134926\n",
      "          total_loss: 71.33078002929688\n",
      "          vf_explained_var: 0.20857921242713928\n",
      "          vf_loss: 71.36259460449219\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.686924159526825\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010292747989296913\n",
      "          model: {}\n",
      "          policy_loss: -0.046301838010549545\n",
      "          total_loss: 49.31321716308594\n",
      "          vf_explained_var: 0.25417810678482056\n",
      "          vf_loss: 49.349098205566406\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.85\n",
      "    ram_util_percent: 55.7875\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 2.0\n",
      "    pol2: 65.79999999999995\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.77\n",
      "    pol2: -13.278999999999971\n",
      "  policy_reward_min:\n",
      "    pol1: -74.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.37458521125405453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20601776904780217\n",
      "    mean_inference_ms: 1.580678111108963\n",
      "    mean_raw_obs_processing_ms: 1.5506448722638049\n",
      "  time_since_restore: 259.82322430610657\n",
      "  time_this_iter_s: 5.619329929351807\n",
      "  time_total_s: 259.82322430610657\n",
      "  timers:\n",
      "    learn_throughput: 869.602\n",
      "    learn_time_ms: 4599.808\n",
      "    sample_throughput: 5796.878\n",
      "    sample_time_ms: 690.027\n",
      "    update_time_ms: 3.169\n",
      "  timestamp: 1619703881\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1000000000000068\n",
      "  episode_reward_mean: -33.86399999999999\n",
      "  episode_reward_min: -73.50000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1720\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6775262355804443\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01095800194889307\n",
      "          model: {}\n",
      "          policy_loss: -0.03882015123963356\n",
      "          total_loss: 110.65919494628906\n",
      "          vf_explained_var: 0.1164325550198555\n",
      "          vf_loss: 110.68692016601562\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5882312059402466\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010590569116175175\n",
      "          model: {}\n",
      "          policy_loss: -0.036307334899902344\n",
      "          total_loss: 79.71906280517578\n",
      "          vf_explained_var: 0.2523486018180847\n",
      "          vf_loss: 79.7446517944336\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.65\n",
      "    ram_util_percent: 55.8\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: 0.5\n",
      "    pol2: 51.49999999999998\n",
      "  policy_reward_mean:\n",
      "    pol1: -30.32\n",
      "    pol2: -3.5439999999999894\n",
      "  policy_reward_min:\n",
      "    pol1: -74.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39766316572376753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.22127242232290265\n",
      "    mean_inference_ms: 1.681496460831952\n",
      "    mean_raw_obs_processing_ms: 1.6542688659869924\n",
      "  time_since_restore: 260.4175937175751\n",
      "  time_this_iter_s: 5.660508155822754\n",
      "  time_total_s: 260.4175937175751\n",
      "  timers:\n",
      "    learn_throughput: 870.583\n",
      "    learn_time_ms: 4594.621\n",
      "    sample_throughput: 5405.928\n",
      "    sample_time_ms: 739.928\n",
      "    update_time_ms: 3.392\n",
      "  timestamp: 1619703881\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -9.299999999999994\n",
      "  episode_reward_mean: -39.19500000000002\n",
      "  episode_reward_min: -67.80000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1720\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6890767812728882\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012651791796088219\n",
      "          model: {}\n",
      "          policy_loss: -0.04100362956523895\n",
      "          total_loss: 77.30097961425781\n",
      "          vf_explained_var: 0.12290734052658081\n",
      "          vf_loss: 77.32917785644531\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6684705018997192\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01096426136791706\n",
      "          model: {}\n",
      "          policy_loss: -0.03749369829893112\n",
      "          total_loss: 29.798274993896484\n",
      "          vf_explained_var: 0.2951197028160095\n",
      "          vf_loss: 29.82466697692871\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.85\n",
      "    ram_util_percent: 55.7875\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 3.0\n",
      "    pol2: 30.600000000000016\n",
      "  policy_reward_mean:\n",
      "    pol1: -27.665\n",
      "    pol2: -11.529999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -61.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.389787521398658\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21023649463205907\n",
      "    mean_inference_ms: 1.634271571275242\n",
      "    mean_raw_obs_processing_ms: 1.6194400976946453\n",
      "  time_since_restore: 260.08202171325684\n",
      "  time_this_iter_s: 5.672191858291626\n",
      "  time_total_s: 260.08202171325684\n",
      "  timers:\n",
      "    learn_throughput: 866.754\n",
      "    learn_time_ms: 4614.922\n",
      "    sample_throughput: 5529.003\n",
      "    sample_time_ms: 723.458\n",
      "    update_time_ms: 2.95\n",
      "  timestamp: 1619703881\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -11.10000000000001\n",
      "  episode_reward_mean: -27.335999999999995\n",
      "  episode_reward_min: -42.89999999999999\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1760\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6834224462509155\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0116733368486166\n",
      "          model: {}\n",
      "          policy_loss: -0.033881232142448425\n",
      "          total_loss: 43.07594299316406\n",
      "          vf_explained_var: 0.198744535446167\n",
      "          vf_loss: 43.0980110168457\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.635840892791748\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011600321158766747\n",
      "          model: {}\n",
      "          policy_loss: -0.0471639484167099\n",
      "          total_loss: 19.504837036132812\n",
      "          vf_explained_var: 0.24595104157924652\n",
      "          vf_loss: 19.54025650024414\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.15\n",
      "    ram_util_percent: 56.0125\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 0.5\n",
      "    pol2: 20.700000000000024\n",
      "  policy_reward_mean:\n",
      "    pol1: -15.795\n",
      "    pol2: -11.540999999999976\n",
      "  policy_reward_min:\n",
      "    pol1: -47.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38853971045380686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2124311044963509\n",
      "    mean_inference_ms: 1.6453442669185256\n",
      "    mean_raw_obs_processing_ms: 1.6134472392264039\n",
      "  time_since_restore: 264.7081561088562\n",
      "  time_this_iter_s: 5.366440773010254\n",
      "  time_total_s: 264.7081561088562\n",
      "  timers:\n",
      "    learn_throughput: 881.073\n",
      "    learn_time_ms: 4539.919\n",
      "    sample_throughput: 5693.477\n",
      "    sample_time_ms: 702.558\n",
      "    update_time_ms: 3.693\n",
      "  timestamp: 1619703886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         260.418</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\"> -33.864</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">               -73.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         264.708</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\"> -27.336</td><td style=\"text-align: right;\">               -11.1</td><td style=\"text-align: right;\">               -42.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         259.823</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\"> -32.049</td><td style=\"text-align: right;\">                -8.7</td><td style=\"text-align: right;\">               -54.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         260.082</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\"> -39.195</td><td style=\"text-align: right;\">                -9.3</td><td style=\"text-align: right;\">               -67.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.69999999999999\n",
      "  episode_reward_mean: -30.948\n",
      "  episode_reward_min: -54.30000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1760\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6968677043914795\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013342528603971004\n",
      "          model: {}\n",
      "          policy_loss: -0.04849805310368538\n",
      "          total_loss: 43.174903869628906\n",
      "          vf_explained_var: 0.2104724496603012\n",
      "          vf_loss: 43.20989227294922\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6853911876678467\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013547487556934357\n",
      "          model: {}\n",
      "          policy_loss: -0.05078338086605072\n",
      "          total_loss: 20.450559616088867\n",
      "          vf_explained_var: 0.3073679804801941\n",
      "          vf_loss: 20.487628936767578\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.6\n",
      "    ram_util_percent: 56.0125\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 4.5\n",
      "    pol2: 47.099999999999945\n",
      "  policy_reward_mean:\n",
      "    pol1: -17.845\n",
      "    pol2: -13.102999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -67.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.37375369104600836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20553674157769905\n",
      "    mean_inference_ms: 1.5770114785319596\n",
      "    mean_raw_obs_processing_ms: 1.5461259635620435\n",
      "  time_since_restore: 265.12169432640076\n",
      "  time_this_iter_s: 5.2984700202941895\n",
      "  time_total_s: 265.12169432640076\n",
      "  timers:\n",
      "    learn_throughput: 871.886\n",
      "    learn_time_ms: 4587.754\n",
      "    sample_throughput: 5836.53\n",
      "    sample_time_ms: 685.339\n",
      "    update_time_ms: 3.246\n",
      "  timestamp: 1619703886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -14.7\n",
      "  episode_reward_mean: -33.495\n",
      "  episode_reward_min: -54.60000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1760\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6622254252433777\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011662262491881847\n",
      "          model: {}\n",
      "          policy_loss: -0.0308777317404747\n",
      "          total_loss: 52.184120178222656\n",
      "          vf_explained_var: 0.0876905620098114\n",
      "          vf_loss: 52.203189849853516\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6074295043945312\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011855097487568855\n",
      "          model: {}\n",
      "          policy_loss: -0.04606470465660095\n",
      "          total_loss: 15.554903984069824\n",
      "          vf_explained_var: 0.19805438816547394\n",
      "          vf_loss: 15.588964462280273\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.65714285714286\n",
      "    ram_util_percent: 55.98571428571429\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -6.5\n",
      "    pol2: 51.49999999999998\n",
      "  policy_reward_mean:\n",
      "    pol1: -28.73\n",
      "    pol2: -4.764999999999989\n",
      "  policy_reward_min:\n",
      "    pol1: -74.0\n",
      "    pol2: -18.899999999999967\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39608282831025293\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.220379275151096\n",
      "    mean_inference_ms: 1.675728789963091\n",
      "    mean_raw_obs_processing_ms: 1.6475297760824745\n",
      "  time_since_restore: 265.7122678756714\n",
      "  time_this_iter_s: 5.2946741580963135\n",
      "  time_total_s: 265.7122678756714\n",
      "  timers:\n",
      "    learn_throughput: 871.317\n",
      "    learn_time_ms: 4590.751\n",
      "    sample_throughput: 5406.445\n",
      "    sample_time_ms: 739.858\n",
      "    update_time_ms: 3.195\n",
      "  timestamp: 1619703886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -14.699999999999989\n",
      "  episode_reward_mean: -37.92600000000001\n",
      "  episode_reward_min: -67.80000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1760\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6738306283950806\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011546596884727478\n",
      "          model: {}\n",
      "          policy_loss: -0.03391214460134506\n",
      "          total_loss: 58.910404205322266\n",
      "          vf_explained_var: 0.14127686619758606\n",
      "          vf_loss: 58.93262481689453\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.638172447681427\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011891892179846764\n",
      "          model: {}\n",
      "          policy_loss: -0.04416986554861069\n",
      "          total_loss: 19.837596893310547\n",
      "          vf_explained_var: 0.24686585366725922\n",
      "          vf_loss: 19.869728088378906\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.6125\n",
      "    ram_util_percent: 56.0125\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 3.0\n",
      "    pol2: 30.600000000000016\n",
      "  policy_reward_mean:\n",
      "    pol1: -27.045\n",
      "    pol2: -10.880999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -61.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3886814064624399\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20964414532146478\n",
      "    mean_inference_ms: 1.6302918785829308\n",
      "    mean_raw_obs_processing_ms: 1.614139048335618\n",
      "  time_since_restore: 265.37337470054626\n",
      "  time_this_iter_s: 5.291352987289429\n",
      "  time_total_s: 265.37337470054626\n",
      "  timers:\n",
      "    learn_throughput: 869.338\n",
      "    learn_time_ms: 4601.203\n",
      "    sample_throughput: 5516.899\n",
      "    sample_time_ms: 725.045\n",
      "    update_time_ms: 2.918\n",
      "  timestamp: 1619703886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -11.10000000000001\n",
      "  episode_reward_mean: -26.624999999999986\n",
      "  episode_reward_min: -44.700000000000024\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1800\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6720934510231018\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01024271734058857\n",
      "          model: {}\n",
      "          policy_loss: -0.03408787399530411\n",
      "          total_loss: 104.3261489868164\n",
      "          vf_explained_var: 0.15992629528045654\n",
      "          vf_loss: 104.34986877441406\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6199432611465454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011501101776957512\n",
      "          model: {}\n",
      "          policy_loss: -0.043013639748096466\n",
      "          total_loss: 67.54017639160156\n",
      "          vf_explained_var: 0.22758319973945618\n",
      "          vf_loss: 67.5715560913086\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.46666666666667\n",
      "    ram_util_percent: 59.86666666666666\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 0.5\n",
      "    pol2: 44.899999999999984\n",
      "  policy_reward_mean:\n",
      "    pol1: -17.955\n",
      "    pol2: -8.669999999999982\n",
      "  policy_reward_min:\n",
      "    pol1: -77.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3873862923915544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21184405474332565\n",
      "    mean_inference_ms: 1.6406424974419176\n",
      "    mean_raw_obs_processing_ms: 1.607062718258563\n",
      "  time_since_restore: 270.92808413505554\n",
      "  time_this_iter_s: 6.219928026199341\n",
      "  time_total_s: 270.92808413505554\n",
      "  timers:\n",
      "    learn_throughput: 868.34\n",
      "    learn_time_ms: 4606.491\n",
      "    sample_throughput: 5851.722\n",
      "    sample_time_ms: 683.559\n",
      "    update_time_ms: 3.82\n",
      "  timestamp: 1619703892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         265.712</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\"> -33.495</td><td style=\"text-align: right;\">               -14.7</td><td style=\"text-align: right;\">               -54.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         270.928</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\"> -26.625</td><td style=\"text-align: right;\">               -11.1</td><td style=\"text-align: right;\">               -44.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         265.122</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\"> -30.948</td><td style=\"text-align: right;\">                -8.7</td><td style=\"text-align: right;\">               -54.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         265.373</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\"> -37.926</td><td style=\"text-align: right;\">               -14.7</td><td style=\"text-align: right;\">               -67.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.69999999999999\n",
      "  episode_reward_mean: -28.934999999999995\n",
      "  episode_reward_min: -48.90000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1800\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6995198726654053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011494915001094341\n",
      "          model: {}\n",
      "          policy_loss: -0.037927594035863876\n",
      "          total_loss: 36.368751525878906\n",
      "          vf_explained_var: 0.16457778215408325\n",
      "          vf_loss: 36.39503479003906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6877318620681763\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015462695620954037\n",
      "          model: {}\n",
      "          policy_loss: -0.06311605125665665\n",
      "          total_loss: 10.960050582885742\n",
      "          vf_explained_var: 0.30648213624954224\n",
      "          vf_loss: 11.007511138916016\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.0888888888889\n",
      "    ram_util_percent: 59.833333333333336\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 4.5\n",
      "    pol2: 47.099999999999945\n",
      "  policy_reward_mean:\n",
      "    pol1: -15.865\n",
      "    pol2: -13.06999999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -67.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3727771971343786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2049827149317543\n",
      "    mean_inference_ms: 1.5728373322962446\n",
      "    mean_raw_obs_processing_ms: 1.5406040378967805\n",
      "  time_since_restore: 271.28210616111755\n",
      "  time_this_iter_s: 6.160411834716797\n",
      "  time_total_s: 271.28210616111755\n",
      "  timers:\n",
      "    learn_throughput: 860.325\n",
      "    learn_time_ms: 4649.404\n",
      "    sample_throughput: 5912.296\n",
      "    sample_time_ms: 676.556\n",
      "    update_time_ms: 3.246\n",
      "  timestamp: 1619703892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -14.099999999999993\n",
      "  episode_reward_mean: -32.73\n",
      "  episode_reward_min: -54.60000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1800\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6462393999099731\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015841903164982796\n",
      "          model: {}\n",
      "          policy_loss: -0.04058879241347313\n",
      "          total_loss: 87.28571319580078\n",
      "          vf_explained_var: 0.14687323570251465\n",
      "          vf_loss: 87.31027221679688\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5935428142547607\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008698733523488045\n",
      "          model: {}\n",
      "          policy_loss: -0.02427171915769577\n",
      "          total_loss: 55.705230712890625\n",
      "          vf_explained_var: 0.25941187143325806\n",
      "          vf_loss: 55.72069549560547\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.4\n",
      "    ram_util_percent: 59.43333333333333\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -4.0\n",
      "    pol2: 63.59999999999992\n",
      "  policy_reward_mean:\n",
      "    pol1: -27.58\n",
      "    pol2: -5.149999999999989\n",
      "  policy_reward_min:\n",
      "    pol1: -81.0\n",
      "    pol2: -17.799999999999965\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3945605151563177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21952532551567353\n",
      "    mean_inference_ms: 1.6703135916285277\n",
      "    mean_raw_obs_processing_ms: 1.6421238991008311\n",
      "  time_since_restore: 271.8178496360779\n",
      "  time_this_iter_s: 6.105581760406494\n",
      "  time_total_s: 271.8178496360779\n",
      "  timers:\n",
      "    learn_throughput: 863.799\n",
      "    learn_time_ms: 4630.704\n",
      "    sample_throughput: 5453.723\n",
      "    sample_time_ms: 733.444\n",
      "    update_time_ms: 3.125\n",
      "  timestamp: 1619703892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -14.699999999999989\n",
      "  episode_reward_mean: -37.95000000000001\n",
      "  episode_reward_min: -63.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1800\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.678398847579956\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010917885228991508\n",
      "          model: {}\n",
      "          policy_loss: -0.03275182843208313\n",
      "          total_loss: 67.53396606445312\n",
      "          vf_explained_var: 0.14835891127586365\n",
      "          vf_loss: 67.5556640625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6320211291313171\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009308943524956703\n",
      "          model: {}\n",
      "          policy_loss: -0.04110102728009224\n",
      "          total_loss: 27.474626541137695\n",
      "          vf_explained_var: 0.31018149852752686\n",
      "          vf_loss: 27.506301879882812\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.07777777777777\n",
      "    ram_util_percent: 59.833333333333336\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 2.0\n",
      "    pol2: 49.29999999999991\n",
      "  policy_reward_mean:\n",
      "    pol1: -27.19\n",
      "    pol2: -10.759999999999977\n",
      "  policy_reward_min:\n",
      "    pol1: -65.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3877068840838612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20910896176712296\n",
      "    mean_inference_ms: 1.6268145619637744\n",
      "    mean_raw_obs_processing_ms: 1.6095307324116168\n",
      "  time_since_restore: 271.4501826763153\n",
      "  time_this_iter_s: 6.076807975769043\n",
      "  time_total_s: 271.4501826763153\n",
      "  timers:\n",
      "    learn_throughput: 862.163\n",
      "    learn_time_ms: 4639.492\n",
      "    sample_throughput: 5545.028\n",
      "    sample_time_ms: 721.367\n",
      "    update_time_ms: 2.937\n",
      "  timestamp: 1619703892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -13.799999999999997\n",
      "  episode_reward_mean: -26.88899999999999\n",
      "  episode_reward_min: -48.000000000000036\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1840\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6812177896499634\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012020491063594818\n",
      "          model: {}\n",
      "          policy_loss: -0.04155341163277626\n",
      "          total_loss: 41.824554443359375\n",
      "          vf_explained_var: 0.18725429475307465\n",
      "          vf_loss: 41.85393524169922\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6227909326553345\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011961381882429123\n",
      "          model: {}\n",
      "          policy_loss: -0.043551474809646606\n",
      "          total_loss: 18.948802947998047\n",
      "          vf_explained_var: 0.239116370677948\n",
      "          vf_loss: 18.980241775512695\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.54285714285714\n",
      "    ram_util_percent: 63.185714285714276\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -0.5\n",
      "    pol2: 44.899999999999984\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.175\n",
      "    pol2: -8.71399999999998\n",
      "  policy_reward_min:\n",
      "    pol1: -77.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3862124388936734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21123571217239479\n",
      "    mean_inference_ms: 1.6360050698047934\n",
      "    mean_raw_obs_processing_ms: 1.600609653348479\n",
      "  time_since_restore: 276.337336063385\n",
      "  time_this_iter_s: 5.409251928329468\n",
      "  time_total_s: 276.337336063385\n",
      "  timers:\n",
      "    learn_throughput: 869.916\n",
      "    learn_time_ms: 4598.145\n",
      "    sample_throughput: 5920.083\n",
      "    sample_time_ms: 675.666\n",
      "    update_time_ms: 3.822\n",
      "  timestamp: 1619703898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         271.818</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\"> -32.73 </td><td style=\"text-align: right;\">               -14.1</td><td style=\"text-align: right;\">               -54.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         276.337</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\"> -26.889</td><td style=\"text-align: right;\">               -13.8</td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         271.282</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\"> -28.935</td><td style=\"text-align: right;\">                -8.7</td><td style=\"text-align: right;\">               -48.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         271.45 </td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\"> -37.95 </td><td style=\"text-align: right;\">               -14.7</td><td style=\"text-align: right;\">               -63  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -13.799999999999999\n",
      "  episode_reward_mean: -28.241999999999997\n",
      "  episode_reward_min: -55.20000000000004\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1840\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.672947883605957\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01259644515812397\n",
      "          model: {}\n",
      "          policy_loss: -0.04462946206331253\n",
      "          total_loss: 39.78437423706055\n",
      "          vf_explained_var: 0.18982207775115967\n",
      "          vf_loss: 39.81624984741211\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6695921421051025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015067320317029953\n",
      "          model: {}\n",
      "          policy_loss: -0.06637437641620636\n",
      "          total_loss: 12.65978717803955\n",
      "          vf_explained_var: 0.266470342874527\n",
      "          vf_loss: 12.710906028747559\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.57142857142857\n",
      "    ram_util_percent: 63.185714285714276\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 4.5\n",
      "    pol2: 18.50000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -14.71\n",
      "    pol2: -13.531999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -42.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3718079097796567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20443768184235117\n",
      "    mean_inference_ms: 1.5688331059154728\n",
      "    mean_raw_obs_processing_ms: 1.5353551588336893\n",
      "  time_since_restore: 276.7101380825043\n",
      "  time_this_iter_s: 5.428031921386719\n",
      "  time_total_s: 276.7101380825043\n",
      "  timers:\n",
      "    learn_throughput: 863.357\n",
      "    learn_time_ms: 4633.076\n",
      "    sample_throughput: 5899.406\n",
      "    sample_time_ms: 678.034\n",
      "    update_time_ms: 3.285\n",
      "  timestamp: 1619703898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.800000000000004\n",
      "  episode_reward_mean: -32.05199999999999\n",
      "  episode_reward_min: -73.2000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1840\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6672954559326172\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011938964948058128\n",
      "          model: {}\n",
      "          policy_loss: -0.042346417903900146\n",
      "          total_loss: 79.57119750976562\n",
      "          vf_explained_var: 0.10467943549156189\n",
      "          vf_loss: 79.60145568847656\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5800645351409912\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011251993477344513\n",
      "          model: {}\n",
      "          policy_loss: -0.03256342560052872\n",
      "          total_loss: 34.429168701171875\n",
      "          vf_explained_var: 0.23494304716587067\n",
      "          vf_loss: 34.45033645629883\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.25\n",
      "    ram_util_percent: 63.2375\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -4.0\n",
      "    pol2: 63.59999999999992\n",
      "  policy_reward_mean:\n",
      "    pol1: -27.21\n",
      "    pol2: -4.841999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -81.0\n",
      "    pol2: -18.899999999999967\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39312369972233513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21871811188596157\n",
      "    mean_inference_ms: 1.6650710337063435\n",
      "    mean_raw_obs_processing_ms: 1.6369303327585458\n",
      "  time_since_restore: 277.27478981018066\n",
      "  time_this_iter_s: 5.456940174102783\n",
      "  time_total_s: 277.27478981018066\n",
      "  timers:\n",
      "    learn_throughput: 866.231\n",
      "    learn_time_ms: 4617.704\n",
      "    sample_throughput: 5562.683\n",
      "    sample_time_ms: 719.077\n",
      "    update_time_ms: 3.323\n",
      "  timestamp: 1619703898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-44-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16.199999999999974\n",
      "  episode_reward_mean: -37.236\n",
      "  episode_reward_min: -63.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1840\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6700292825698853\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01108509860932827\n",
      "          model: {}\n",
      "          policy_loss: -0.031266625970602036\n",
      "          total_loss: 55.78887939453125\n",
      "          vf_explained_var: 0.10826052725315094\n",
      "          vf_loss: 55.808921813964844\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6388744711875916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012215890921652317\n",
      "          model: {}\n",
      "          policy_loss: -0.04841001331806183\n",
      "          total_loss: 12.405403137207031\n",
      "          vf_explained_var: 0.24489355087280273\n",
      "          vf_loss: 12.441444396972656\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.4125\n",
      "    ram_util_percent: 63.1875\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -1.0\n",
      "    pol2: 49.29999999999991\n",
      "  policy_reward_mean:\n",
      "    pol1: -25.475\n",
      "    pol2: -11.760999999999981\n",
      "  policy_reward_min:\n",
      "    pol1: -65.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38673592185629013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2085365400097352\n",
      "    mean_inference_ms: 1.6233920670557866\n",
      "    mean_raw_obs_processing_ms: 1.6059104335351804\n",
      "  time_since_restore: 276.9006857872009\n",
      "  time_this_iter_s: 5.45050311088562\n",
      "  time_total_s: 276.9006857872009\n",
      "  timers:\n",
      "    learn_throughput: 865.821\n",
      "    learn_time_ms: 4619.895\n",
      "    sample_throughput: 5556.854\n",
      "    sample_time_ms: 719.832\n",
      "    update_time_ms: 2.749\n",
      "  timestamp: 1619703898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -13.799999999999997\n",
      "  episode_reward_mean: -27.929999999999996\n",
      "  episode_reward_min: -48.000000000000036\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1880\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.64996737241745\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012634797021746635\n",
      "          model: {}\n",
      "          policy_loss: -0.04036514833569527\n",
      "          total_loss: 65.52072143554688\n",
      "          vf_explained_var: 0.17133665084838867\n",
      "          vf_loss: 65.54829406738281\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.597643256187439\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009937920607626438\n",
      "          model: {}\n",
      "          policy_loss: -0.034784477204084396\n",
      "          total_loss: 26.984676361083984\n",
      "          vf_explained_var: 0.2565515339374542\n",
      "          vf_loss: 27.0093994140625\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.15555555555555\n",
      "    ram_util_percent: 63.17777777777778\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 1.5\n",
      "    pol2: 44.899999999999984\n",
      "  policy_reward_mean:\n",
      "    pol1: -19.04\n",
      "    pol2: -8.889999999999981\n",
      "  policy_reward_min:\n",
      "    pol1: -64.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3853392356316118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21076978132134144\n",
      "    mean_inference_ms: 1.6321013065464343\n",
      "    mean_raw_obs_processing_ms: 1.5956897792125806\n",
      "  time_since_restore: 282.02981877326965\n",
      "  time_this_iter_s: 5.6924827098846436\n",
      "  time_total_s: 282.02981877326965\n",
      "  timers:\n",
      "    learn_throughput: 871.056\n",
      "    learn_time_ms: 4592.128\n",
      "    sample_throughput: 5847.019\n",
      "    sample_time_ms: 684.109\n",
      "    update_time_ms: 3.676\n",
      "  timestamp: 1619703903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         277.275</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\"> -32.052</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               -73.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         282.03 </td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\"> -27.93 </td><td style=\"text-align: right;\">               -13.8</td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         276.71 </td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\"> -28.242</td><td style=\"text-align: right;\">               -13.8</td><td style=\"text-align: right;\">               -55.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         276.901</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\"> -37.236</td><td style=\"text-align: right;\">               -16.2</td><td style=\"text-align: right;\">               -63  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -13.799999999999999\n",
      "  episode_reward_mean: -29.306999999999995\n",
      "  episode_reward_min: -55.20000000000004\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1880\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6598195433616638\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011603888124227524\n",
      "          model: {}\n",
      "          policy_loss: -0.037345319986343384\n",
      "          total_loss: 37.23706817626953\n",
      "          vf_explained_var: 0.2108994871377945\n",
      "          vf_loss: 37.262664794921875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6632430553436279\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014625046402215958\n",
      "          model: {}\n",
      "          policy_loss: -0.06405121088027954\n",
      "          total_loss: 12.398653030395508\n",
      "          vf_explained_var: 0.3048197031021118\n",
      "          vf_loss: 12.447896957397461\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.42222222222222\n",
      "    ram_util_percent: 63.17777777777778\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 4.0\n",
      "    pol2: 10.800000000000036\n",
      "  policy_reward_mean:\n",
      "    pol1: -14.895\n",
      "    pol2: -14.411999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -46.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3708227939143209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20387650352197453\n",
      "    mean_inference_ms: 1.5649122778196676\n",
      "    mean_raw_obs_processing_ms: 1.5301766136266068\n",
      "  time_since_restore: 282.3280420303345\n",
      "  time_this_iter_s: 5.6179039478302\n",
      "  time_total_s: 282.3280420303345\n",
      "  timers:\n",
      "    learn_throughput: 864.277\n",
      "    learn_time_ms: 4628.148\n",
      "    sample_throughput: 6084.143\n",
      "    sample_time_ms: 657.447\n",
      "    update_time_ms: 3.451\n",
      "  timestamp: 1619703903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.800000000000004\n",
      "  episode_reward_mean: -31.616999999999997\n",
      "  episode_reward_min: -73.2000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1880\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6678687334060669\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011408423073589802\n",
      "          model: {}\n",
      "          policy_loss: -0.04326135292649269\n",
      "          total_loss: 59.70896911621094\n",
      "          vf_explained_var: 0.1127510666847229\n",
      "          vf_loss: 59.74067687988281\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.582347571849823\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011779790744185448\n",
      "          model: {}\n",
      "          policy_loss: -0.049467455595731735\n",
      "          total_loss: 18.98721694946289\n",
      "          vf_explained_var: 0.2140904664993286\n",
      "          vf_loss: 19.024757385253906\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.7875\n",
      "    ram_util_percent: 63.1875\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -1.0\n",
      "    pol2: 32.80000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -25.675\n",
      "    pol2: -5.941999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -56.5\n",
      "    pol2: -18.899999999999967\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.391685283794664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21790666160433012\n",
      "    mean_inference_ms: 1.6597873986384253\n",
      "    mean_raw_obs_processing_ms: 1.6312497740319905\n",
      "  time_since_restore: 282.8977165222168\n",
      "  time_this_iter_s: 5.622926712036133\n",
      "  time_total_s: 282.8977165222168\n",
      "  timers:\n",
      "    learn_throughput: 866.425\n",
      "    learn_time_ms: 4616.672\n",
      "    sample_throughput: 5616.027\n",
      "    sample_time_ms: 712.247\n",
      "    update_time_ms: 3.462\n",
      "  timestamp: 1619703903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16.200000000000003\n",
      "  episode_reward_mean: -36.78600000000001\n",
      "  episode_reward_min: -61.200000000000045\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1880\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6425372362136841\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012012340128421783\n",
      "          model: {}\n",
      "          policy_loss: -0.03778254613280296\n",
      "          total_loss: 64.1619873046875\n",
      "          vf_explained_var: 0.16902796924114227\n",
      "          vf_loss: 64.18761444091797\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6232355833053589\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009614062495529652\n",
      "          model: {}\n",
      "          policy_loss: -0.03587380051612854\n",
      "          total_loss: 25.593120574951172\n",
      "          vf_explained_var: 0.2596535086631775\n",
      "          vf_loss: 25.619258880615234\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.825\n",
      "    ram_util_percent: 63.1875\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 0.5\n",
      "    pol2: 43.79999999999989\n",
      "  policy_reward_mean:\n",
      "    pol1: -24.42\n",
      "    pol2: -12.365999999999978\n",
      "  policy_reward_min:\n",
      "    pol1: -66.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3856936209749398\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2079673065026612\n",
      "    mean_inference_ms: 1.6196744683611823\n",
      "    mean_raw_obs_processing_ms: 1.602150983181079\n",
      "  time_since_restore: 282.501993894577\n",
      "  time_this_iter_s: 5.601308107376099\n",
      "  time_total_s: 282.501993894577\n",
      "  timers:\n",
      "    learn_throughput: 867.375\n",
      "    learn_time_ms: 4611.614\n",
      "    sample_throughput: 5568.398\n",
      "    sample_time_ms: 718.34\n",
      "    update_time_ms: 2.757\n",
      "  timestamp: 1619703903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -9.300000000000004\n",
      "  episode_reward_mean: -27.767999999999986\n",
      "  episode_reward_min: -48.000000000000036\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1920\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6712192296981812\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011886105872690678\n",
      "          model: {}\n",
      "          policy_loss: -0.03234665095806122\n",
      "          total_loss: 62.48846435546875\n",
      "          vf_explained_var: 0.18650870025157928\n",
      "          vf_loss: 62.50877380371094\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5843812823295593\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011250725015997887\n",
      "          model: {}\n",
      "          policy_loss: -0.045455679297447205\n",
      "          total_loss: 39.981475830078125\n",
      "          vf_explained_var: 0.2464936375617981\n",
      "          vf_loss: 40.015541076660156\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.44285714285714\n",
      "    ram_util_percent: 63.71428571428572\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 1.5\n",
      "    pol2: 37.2\n",
      "  policy_reward_mean:\n",
      "    pol1: -17.855\n",
      "    pol2: -9.912999999999979\n",
      "  policy_reward_min:\n",
      "    pol1: -64.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3845700655496159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2103203990926495\n",
      "    mean_inference_ms: 1.628468998947438\n",
      "    mean_raw_obs_processing_ms: 1.5909130606318032\n",
      "  time_since_restore: 287.4483277797699\n",
      "  time_this_iter_s: 5.418509006500244\n",
      "  time_total_s: 287.4483277797699\n",
      "  timers:\n",
      "    learn_throughput: 868.405\n",
      "    learn_time_ms: 4606.143\n",
      "    sample_throughput: 5785.144\n",
      "    sample_time_ms: 691.426\n",
      "    update_time_ms: 3.722\n",
      "  timestamp: 1619703909\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: 6768d_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         282.898</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\"> -31.617</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               -73.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         287.448</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\"> -27.768</td><td style=\"text-align: right;\">                -9.3</td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         282.328</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\"> -29.307</td><td style=\"text-align: right;\">               -13.8</td><td style=\"text-align: right;\">               -55.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         282.502</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\"> -36.786</td><td style=\"text-align: right;\">               -16.2</td><td style=\"text-align: right;\">               -61.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12.90000000000001\n",
      "  episode_reward_mean: -28.856999999999992\n",
      "  episode_reward_min: -51.30000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1920\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6441656351089478\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013493036851286888\n",
      "          model: {}\n",
      "          policy_loss: -0.043827593326568604\n",
      "          total_loss: 38.107139587402344\n",
      "          vf_explained_var: 0.23160181939601898\n",
      "          vf_loss: 38.137306213378906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6551792025566101\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015967056155204773\n",
      "          model: {}\n",
      "          policy_loss: -0.05962385982275009\n",
      "          total_loss: 7.02272367477417\n",
      "          vf_explained_var: 0.3768867552280426\n",
      "          vf_loss: 7.066181182861328\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.74285714285715\n",
      "    ram_util_percent: 63.71428571428572\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 6.0\n",
      "    pol2: 10.800000000000036\n",
      "  policy_reward_mean:\n",
      "    pol1: -14.17\n",
      "    pol2: -14.68699999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -46.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.36998176233116736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20338726446678762\n",
      "    mean_inference_ms: 1.5615444681676331\n",
      "    mean_raw_obs_processing_ms: 1.5255719261831873\n",
      "  time_since_restore: 287.7994441986084\n",
      "  time_this_iter_s: 5.471402168273926\n",
      "  time_total_s: 287.7994441986084\n",
      "  timers:\n",
      "    learn_throughput: 863.321\n",
      "    learn_time_ms: 4633.273\n",
      "    sample_throughput: 6014.906\n",
      "    sample_time_ms: 665.015\n",
      "    update_time_ms: 3.395\n",
      "  timestamp: 1619703909\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -14.399999999999977\n",
      "  episode_reward_mean: -30.653999999999993\n",
      "  episode_reward_min: -51.90000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1920\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6361811757087708\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011293550953269005\n",
      "          model: {}\n",
      "          policy_loss: -0.03735695034265518\n",
      "          total_loss: 60.13932800292969\n",
      "          vf_explained_var: 0.09665850549936295\n",
      "          vf_loss: 60.165245056152344\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5680421590805054\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01138952188193798\n",
      "          model: {}\n",
      "          policy_loss: -0.04012333229184151\n",
      "          total_loss: 24.5834903717041\n",
      "          vf_explained_var: 0.22144033014774323\n",
      "          vf_loss: 24.612079620361328\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.9\n",
      "    ram_util_percent: 63.7\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -1.0\n",
      "    pol2: 32.80000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -23.975\n",
      "    pol2: -6.678999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -59.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.39044492056917846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21720011222719904\n",
      "    mean_inference_ms: 1.6551585987571276\n",
      "    mean_raw_obs_processing_ms: 1.6256438828482487\n",
      "  time_since_restore: 288.35697841644287\n",
      "  time_this_iter_s: 5.459261894226074\n",
      "  time_total_s: 288.35697841644287\n",
      "  timers:\n",
      "    learn_throughput: 864.669\n",
      "    learn_time_ms: 4626.05\n",
      "    sample_throughput: 5734.619\n",
      "    sample_time_ms: 697.518\n",
      "    update_time_ms: 3.637\n",
      "  timestamp: 1619703909\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16.200000000000003\n",
      "  episode_reward_mean: -37.30500000000001\n",
      "  episode_reward_min: -68.10000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1920\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6575058698654175\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010791724547743797\n",
      "          model: {}\n",
      "          policy_loss: -0.03114771656692028\n",
      "          total_loss: 63.242820739746094\n",
      "          vf_explained_var: 0.15285170078277588\n",
      "          vf_loss: 63.263038635253906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6208012700080872\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012493779882788658\n",
      "          model: {}\n",
      "          policy_loss: -0.05149893835186958\n",
      "          total_loss: 12.571687698364258\n",
      "          vf_explained_var: 0.253430038690567\n",
      "          vf_loss: 12.610536575317383\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.95\n",
      "    ram_util_percent: 63.725\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 0.5\n",
      "    pol2: 43.79999999999989\n",
      "  policy_reward_mean:\n",
      "    pol1: -24.51\n",
      "    pol2: -12.794999999999977\n",
      "  policy_reward_min:\n",
      "    pol1: -66.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38479872329718956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20745409294313968\n",
      "    mean_inference_ms: 1.616524082002846\n",
      "    mean_raw_obs_processing_ms: 1.5983980051635402\n",
      "  time_since_restore: 288.039434671402\n",
      "  time_this_iter_s: 5.537440776824951\n",
      "  time_total_s: 288.039434671402\n",
      "  timers:\n",
      "    learn_throughput: 864.403\n",
      "    learn_time_ms: 4627.474\n",
      "    sample_throughput: 5569.993\n",
      "    sample_time_ms: 718.134\n",
      "    update_time_ms: 2.797\n",
      "  timestamp: 1619703909\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -9.0\n",
      "  episode_reward_mean: -27.90299999999999\n",
      "  episode_reward_min: -51.900000000000055\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1960\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6484858989715576\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011386638507246971\n",
      "          model: {}\n",
      "          policy_loss: -0.041767336428165436\n",
      "          total_loss: 39.11225128173828\n",
      "          vf_explained_var: 0.21687160432338715\n",
      "          vf_loss: 39.142486572265625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6607301831245422\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013691751286387444\n",
      "          model: {}\n",
      "          policy_loss: -0.04903566092252731\n",
      "          total_loss: 13.841753005981445\n",
      "          vf_explained_var: 0.26557934284210205\n",
      "          vf_loss: 13.87692642211914\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.14444444444445\n",
      "    ram_util_percent: 63.77777777777778\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 11.0\n",
      "    pol2: 10.800000000000036\n",
      "  policy_reward_mean:\n",
      "    pol1: -13.095\n",
      "    pol2: -14.807999999999971\n",
      "  policy_reward_min:\n",
      "    pol1: -46.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.36905585482206243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2028584531631521\n",
      "    mean_inference_ms: 1.5577352633485662\n",
      "    mean_raw_obs_processing_ms: 1.5206777274947194\n",
      "  time_since_restore: 293.54738330841064\n",
      "  time_this_iter_s: 5.747939109802246\n",
      "  time_total_s: 293.54738330841064\n",
      "  timers:\n",
      "    learn_throughput: 854.726\n",
      "    learn_time_ms: 4679.862\n",
      "    sample_throughput: 6063.786\n",
      "    sample_time_ms: 659.654\n",
      "    update_time_ms: 3.414\n",
      "  timestamp: 1619703915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 6768d_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         288.357</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\"> -30.654</td><td style=\"text-align: right;\">               -14.4</td><td style=\"text-align: right;\">               -51.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         287.448</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\"> -27.768</td><td style=\"text-align: right;\">                -9.3</td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         293.547</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\"> -27.903</td><td style=\"text-align: right;\">                -9  </td><td style=\"text-align: right;\">               -51.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         288.039</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\"> -37.305</td><td style=\"text-align: right;\">               -16.2</td><td style=\"text-align: right;\">               -68.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -13.49999999999999\n",
      "  episode_reward_mean: -29.543999999999983\n",
      "  episode_reward_min: -51.90000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1960\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6254383325576782\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01105943787842989\n",
      "          model: {}\n",
      "          policy_loss: -0.039970993995666504\n",
      "          total_loss: 47.576210021972656\n",
      "          vf_explained_var: 0.09534189105033875\n",
      "          vf_loss: 47.604984283447266\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5783809423446655\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012861847877502441\n",
      "          model: {}\n",
      "          policy_loss: -0.04896652698516846\n",
      "          total_loss: 15.755123138427734\n",
      "          vf_explained_var: 0.23832976818084717\n",
      "          vf_loss: 15.791067123413086\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.8\n",
      "    ram_util_percent: 63.787499999999994\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -1.0\n",
      "    pol2: 18.500000000000004\n",
      "  policy_reward_mean:\n",
      "    pol1: -21.655\n",
      "    pol2: -7.888999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -59.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38906846176834287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21642292484113249\n",
      "    mean_inference_ms: 1.6499879484068272\n",
      "    mean_raw_obs_processing_ms: 1.6194264185505372\n",
      "  time_since_restore: 294.0962104797363\n",
      "  time_this_iter_s: 5.739232063293457\n",
      "  time_total_s: 294.0962104797363\n",
      "  timers:\n",
      "    learn_throughput: 854.768\n",
      "    learn_time_ms: 4679.632\n",
      "    sample_throughput: 5768.212\n",
      "    sample_time_ms: 693.456\n",
      "    update_time_ms: 3.639\n",
      "  timestamp: 1619703915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -9.300000000000004\n",
      "  episode_reward_mean: -27.46799999999999\n",
      "  episode_reward_min: -60.00000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1960\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6457604169845581\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012572159990668297\n",
      "          model: {}\n",
      "          policy_loss: -0.04330635815858841\n",
      "          total_loss: 53.317466735839844\n",
      "          vf_explained_var: 0.16978973150253296\n",
      "          vf_loss: 53.348045349121094\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5768857002258301\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01276122871786356\n",
      "          model: {}\n",
      "          policy_loss: -0.05179356411099434\n",
      "          total_loss: 23.86664581298828\n",
      "          vf_explained_var: 0.24264076352119446\n",
      "          vf_loss: 23.905517578125\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.44444444444444\n",
      "    ram_util_percent: 63.77777777777778\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 2.0\n",
      "    pol2: 37.2\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.38\n",
      "    pol2: -9.087999999999983\n",
      "  policy_reward_min:\n",
      "    pol1: -64.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3840637045761022\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.210064688588584\n",
      "    mean_inference_ms: 1.625535240017008\n",
      "    mean_raw_obs_processing_ms: 1.5879605135902823\n",
      "  time_since_restore: 293.31864762306213\n",
      "  time_this_iter_s: 5.870319843292236\n",
      "  time_total_s: 293.31864762306213\n",
      "  timers:\n",
      "    learn_throughput: 857.781\n",
      "    learn_time_ms: 4663.193\n",
      "    sample_throughput: 5731.295\n",
      "    sample_time_ms: 697.923\n",
      "    update_time_ms: 3.792\n",
      "  timestamp: 1619703915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12.600000000000001\n",
      "  episode_reward_mean: -37.43400000000001\n",
      "  episode_reward_min: -68.10000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1960\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6393343210220337\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010171346366405487\n",
      "          model: {}\n",
      "          policy_loss: -0.030877521261572838\n",
      "          total_loss: 73.49972534179688\n",
      "          vf_explained_var: 0.12069399654865265\n",
      "          vf_loss: 73.52029418945312\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6127287149429321\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012438730336725712\n",
      "          model: {}\n",
      "          policy_loss: -0.04911718890070915\n",
      "          total_loss: 12.107179641723633\n",
      "          vf_explained_var: 0.31958144903182983\n",
      "          vf_loss: 12.14370346069336\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.8375\n",
      "    ram_util_percent: 63.775\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 3.0\n",
      "    pol2: 19.6\n",
      "  policy_reward_mean:\n",
      "    pol1: -23.55\n",
      "    pol2: -13.883999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -59.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3838465463989846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20692022815551542\n",
      "    mean_inference_ms: 1.6131847213212194\n",
      "    mean_raw_obs_processing_ms: 1.5938216651914998\n",
      "  time_since_restore: 293.74397373199463\n",
      "  time_this_iter_s: 5.704539060592651\n",
      "  time_total_s: 293.74397373199463\n",
      "  timers:\n",
      "    learn_throughput: 856.15\n",
      "    learn_time_ms: 4672.077\n",
      "    sample_throughput: 5610.654\n",
      "    sample_time_ms: 712.929\n",
      "    update_time_ms: 3.141\n",
      "  timestamp: 1619703915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.799999999999994\n",
      "  episode_reward_mean: -30.05399999999999\n",
      "  episode_reward_min: -55.80000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2000\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6359100937843323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011404462158679962\n",
      "          model: {}\n",
      "          policy_loss: -0.026970934122800827\n",
      "          total_loss: 104.57138061523438\n",
      "          vf_explained_var: 0.14470019936561584\n",
      "          vf_loss: 104.58680725097656\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5662419199943542\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007868364453315735\n",
      "          model: {}\n",
      "          policy_loss: -0.034459568560123444\n",
      "          total_loss: 65.92350006103516\n",
      "          vf_explained_var: 0.2826274633407593\n",
      "          vf_loss: 65.94999694824219\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.73333333333333\n",
      "    ram_util_percent: 65.7888888888889\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -3.5\n",
      "    pol2: 103.19999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: -23.155\n",
      "    pol2: -6.898999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -114.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3878507141217186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2157642307733724\n",
      "    mean_inference_ms: 1.6449647443038828\n",
      "    mean_raw_obs_processing_ms: 1.6141258782973529\n",
      "  time_since_restore: 300.4652352333069\n",
      "  time_this_iter_s: 6.369024753570557\n",
      "  time_total_s: 300.4652352333069\n",
      "  timers:\n",
      "    learn_throughput: 837.692\n",
      "    learn_time_ms: 4775.026\n",
      "    sample_throughput: 5822.542\n",
      "    sample_time_ms: 686.985\n",
      "    update_time_ms: 3.695\n",
      "  timestamp: 1619703921\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         300.465</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\"> -30.054</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               -55.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         293.319</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\"> -27.468</td><td style=\"text-align: right;\">                -9.3</td><td style=\"text-align: right;\">               -60  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         293.547</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\"> -27.903</td><td style=\"text-align: right;\">                -9  </td><td style=\"text-align: right;\">               -51.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         293.744</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\"> -37.434</td><td style=\"text-align: right;\">               -12.6</td><td style=\"text-align: right;\">               -68.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12.600000000000001\n",
      "  episode_reward_mean: -36.62100000000001\n",
      "  episode_reward_min: -66.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2000\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6448127031326294\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009580668993294239\n",
      "          model: {}\n",
      "          policy_loss: -0.019873110577464104\n",
      "          total_loss: 74.49268341064453\n",
      "          vf_explained_var: 0.1424368917942047\n",
      "          vf_loss: 74.50285339355469\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6137830018997192\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011508302763104439\n",
      "          model: {}\n",
      "          policy_loss: -0.04544908553361893\n",
      "          total_loss: 11.672922134399414\n",
      "          vf_explained_var: 0.27061474323272705\n",
      "          vf_loss: 11.706720352172852\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.144444444444446\n",
      "    ram_util_percent: 65.8777777777778\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 3.0\n",
      "    pol2: 16.300000000000036\n",
      "  policy_reward_mean:\n",
      "    pol1: -23.155\n",
      "    pol2: -13.465999999999976\n",
      "  policy_reward_min:\n",
      "    pol1: -62.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38266514915562344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20630114523509555\n",
      "    mean_inference_ms: 1.60871924327497\n",
      "    mean_raw_obs_processing_ms: 1.5879418584105411\n",
      "  time_since_restore: 300.0749306678772\n",
      "  time_this_iter_s: 6.330956935882568\n",
      "  time_total_s: 300.0749306678772\n",
      "  timers:\n",
      "    learn_throughput: 838.235\n",
      "    learn_time_ms: 4771.933\n",
      "    sample_throughput: 5738.619\n",
      "    sample_time_ms: 697.032\n",
      "    update_time_ms: 3.287\n",
      "  timestamp: 1619703921\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.699999999999998\n",
      "  episode_reward_mean: -27.392999999999994\n",
      "  episode_reward_min: -51.900000000000055\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2000\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6488144397735596\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013307156041264534\n",
      "          model: {}\n",
      "          policy_loss: -0.043658070266246796\n",
      "          total_loss: 77.4985122680664\n",
      "          vf_explained_var: 0.20279738306999207\n",
      "          vf_loss: 77.52869415283203\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6409832239151001\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010174576193094254\n",
      "          model: {}\n",
      "          policy_loss: -0.04108678549528122\n",
      "          total_loss: 62.15512466430664\n",
      "          vf_explained_var: 0.3140895962715149\n",
      "          vf_loss: 62.1859130859375\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.13333333333333\n",
      "    ram_util_percent: 65.86666666666667\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 11.0\n",
      "    pol2: 71.3\n",
      "  policy_reward_mean:\n",
      "    pol1: -14.565\n",
      "    pol2: -12.827999999999976\n",
      "  policy_reward_min:\n",
      "    pol1: -80.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3684626206780985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20259680740003566\n",
      "    mean_inference_ms: 1.5547053842279246\n",
      "    mean_raw_obs_processing_ms: 1.517900046427349\n",
      "  time_since_restore: 299.99585127830505\n",
      "  time_this_iter_s: 6.448467969894409\n",
      "  time_total_s: 299.99585127830505\n",
      "  timers:\n",
      "    learn_throughput: 836.06\n",
      "    learn_time_ms: 4784.343\n",
      "    sample_throughput: 5934.978\n",
      "    sample_time_ms: 673.97\n",
      "    update_time_ms: 3.335\n",
      "  timestamp: 1619703921\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.800000000000002\n",
      "  episode_reward_mean: -27.464999999999996\n",
      "  episode_reward_min: -60.00000000000013\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2000\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.659134566783905\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011652106419205666\n",
      "          model: {}\n",
      "          policy_loss: -0.030813638120889664\n",
      "          total_loss: 45.34729766845703\n",
      "          vf_explained_var: 0.14997592568397522\n",
      "          vf_loss: 45.366310119628906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5872794389724731\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011498374864459038\n",
      "          model: {}\n",
      "          policy_loss: -0.045096609741449356\n",
      "          total_loss: 22.790489196777344\n",
      "          vf_explained_var: 0.2196878045797348\n",
      "          vf_loss: 22.823944091796875\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.08888888888889\n",
      "    ram_util_percent: 65.88888888888889\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 2.0\n",
      "    pol2: 26.200000000000017\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.135\n",
      "    pol2: -9.32999999999998\n",
      "  policy_reward_min:\n",
      "    pol1: -48.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38358153750530094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2098461953317589\n",
      "    mean_inference_ms: 1.6222862061618566\n",
      "    mean_raw_obs_processing_ms: 1.5855308202117993\n",
      "  time_since_restore: 299.68634557724\n",
      "  time_this_iter_s: 6.3676979541778564\n",
      "  time_total_s: 299.68634557724\n",
      "  timers:\n",
      "    learn_throughput: 839.475\n",
      "    learn_time_ms: 4764.885\n",
      "    sample_throughput: 5682.345\n",
      "    sample_time_ms: 703.935\n",
      "    update_time_ms: 3.909\n",
      "  timestamp: 1619703921\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12.600000000000001\n",
      "  episode_reward_mean: -37.05000000000001\n",
      "  episode_reward_min: -66.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2040\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6373250484466553\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01242567878216505\n",
      "          model: {}\n",
      "          policy_loss: -0.045919738709926605\n",
      "          total_loss: 54.797203063964844\n",
      "          vf_explained_var: 0.19603177905082703\n",
      "          vf_loss: 54.83053970336914\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6026635766029358\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01204860582947731\n",
      "          model: {}\n",
      "          policy_loss: -0.03963842988014221\n",
      "          total_loss: 11.548612594604492\n",
      "          vf_explained_var: 0.27917855978012085\n",
      "          vf_loss: 11.576053619384766\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.7375\n",
      "    ram_util_percent: 66.95\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 3.0\n",
      "    pol2: 16.300000000000036\n",
      "  policy_reward_mean:\n",
      "    pol1: -23.98\n",
      "    pol2: -13.069999999999975\n",
      "  policy_reward_min:\n",
      "    pol1: -62.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3816473451088996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20578201869629248\n",
      "    mean_inference_ms: 1.6046217608551916\n",
      "    mean_raw_obs_processing_ms: 1.5825276938153927\n",
      "  time_since_restore: 305.55579566955566\n",
      "  time_this_iter_s: 5.480865001678467\n",
      "  time_total_s: 305.55579566955566\n",
      "  timers:\n",
      "    learn_throughput: 838.948\n",
      "    learn_time_ms: 4767.877\n",
      "    sample_throughput: 5834.567\n",
      "    sample_time_ms: 685.569\n",
      "    update_time_ms: 3.436\n",
      "  timestamp: 1619703927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 6768d_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         300.465</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\"> -30.054</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               -55.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         299.686</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\"> -27.465</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               -60  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         299.996</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\"> -27.393</td><td style=\"text-align: right;\">                -8.7</td><td style=\"text-align: right;\">               -51.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         305.556</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\"> -37.05 </td><td style=\"text-align: right;\">               -12.6</td><td style=\"text-align: right;\">               -66  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.799999999999994\n",
      "  episode_reward_mean: -29.74499999999999\n",
      "  episode_reward_min: -55.80000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2040\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6240818500518799\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011015006341040134\n",
      "          model: {}\n",
      "          policy_loss: -0.036762431263923645\n",
      "          total_loss: 70.20338439941406\n",
      "          vf_explained_var: 0.11177567392587662\n",
      "          vf_loss: 70.22899627685547\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5550153255462646\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011979649774730206\n",
      "          model: {}\n",
      "          policy_loss: -0.04351962357759476\n",
      "          total_loss: 31.317611694335938\n",
      "          vf_explained_var: 0.1966986060142517\n",
      "          vf_loss: 31.349002838134766\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.5625\n",
      "    ram_util_percent: 66.92500000000001\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -3.5\n",
      "    pol2: 103.19999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: -25.31\n",
      "    pol2: -4.434999999999989\n",
      "  policy_reward_min:\n",
      "    pol1: -114.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.387218257416096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21537647208865024\n",
      "    mean_inference_ms: 1.641736974037209\n",
      "    mean_raw_obs_processing_ms: 1.6110771805104376\n",
      "  time_since_restore: 305.9917302131653\n",
      "  time_this_iter_s: 5.526494979858398\n",
      "  time_total_s: 305.9917302131653\n",
      "  timers:\n",
      "    learn_throughput: 838.79\n",
      "    learn_time_ms: 4768.775\n",
      "    sample_throughput: 5850.479\n",
      "    sample_time_ms: 683.705\n",
      "    update_time_ms: 3.799\n",
      "  timestamp: 1619703927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.699999999999998\n",
      "  episode_reward_mean: -26.81099999999999\n",
      "  episode_reward_min: -51.900000000000055\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2040\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.642392635345459\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012344234623014927\n",
      "          model: {}\n",
      "          policy_loss: -0.0346146859228611\n",
      "          total_loss: 38.196746826171875\n",
      "          vf_explained_var: 0.17568999528884888\n",
      "          vf_loss: 38.21886444091797\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6439501047134399\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015005717054009438\n",
      "          model: {}\n",
      "          policy_loss: -0.05603744834661484\n",
      "          total_loss: 13.0316162109375\n",
      "          vf_explained_var: 0.283747136592865\n",
      "          vf_loss: 13.072460174560547\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.4875\n",
      "    ram_util_percent: 66.95\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 11.0\n",
      "    pol2: 71.3\n",
      "  policy_reward_mean:\n",
      "    pol1: -14.555\n",
      "    pol2: -12.255999999999979\n",
      "  policy_reward_min:\n",
      "    pol1: -80.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3681936212569845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20247760278079135\n",
      "    mean_inference_ms: 1.552729138921341\n",
      "    mean_raw_obs_processing_ms: 1.5162410439040426\n",
      "  time_since_restore: 305.5086944103241\n",
      "  time_this_iter_s: 5.512843132019043\n",
      "  time_total_s: 305.5086944103241\n",
      "  timers:\n",
      "    learn_throughput: 835.627\n",
      "    learn_time_ms: 4786.825\n",
      "    sample_throughput: 5884.009\n",
      "    sample_time_ms: 679.809\n",
      "    update_time_ms: 3.283\n",
      "  timestamp: 1619703927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.800000000000002\n",
      "  episode_reward_mean: -27.21599999999999\n",
      "  episode_reward_min: -54.60000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2040\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6503458023071289\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01183727290481329\n",
      "          model: {}\n",
      "          policy_loss: -0.04034401476383209\n",
      "          total_loss: 45.43401336669922\n",
      "          vf_explained_var: 0.17967428267002106\n",
      "          vf_loss: 45.462371826171875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5790516138076782\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0116652250289917\n",
      "          model: {}\n",
      "          policy_loss: -0.05834266170859337\n",
      "          total_loss: 17.03138542175293\n",
      "          vf_explained_var: 0.3339172303676605\n",
      "          vf_loss: 17.07791519165039\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.4875\n",
      "    ram_util_percent: 66.95\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -0.5\n",
      "    pol2: 26.200000000000017\n",
      "  policy_reward_mean:\n",
      "    pol1: -17.16\n",
      "    pol2: -10.055999999999981\n",
      "  policy_reward_min:\n",
      "    pol1: -46.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38311272656023587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20963825893103347\n",
      "    mean_inference_ms: 1.6198371024022993\n",
      "    mean_raw_obs_processing_ms: 1.5835524518666375\n",
      "  time_since_restore: 305.1483817100525\n",
      "  time_this_iter_s: 5.4620361328125\n",
      "  time_total_s: 305.1483817100525\n",
      "  timers:\n",
      "    learn_throughput: 840.056\n",
      "    learn_time_ms: 4761.585\n",
      "    sample_throughput: 5725.659\n",
      "    sample_time_ms: 698.61\n",
      "    update_time_ms: 3.827\n",
      "  timestamp: 1619703927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16.199999999999996\n",
      "  episode_reward_mean: -36.46200000000001\n",
      "  episode_reward_min: -60.000000000000085\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2080\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6089454889297485\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010790562257170677\n",
      "          model: {}\n",
      "          policy_loss: -0.03264700621366501\n",
      "          total_loss: 53.75783920288086\n",
      "          vf_explained_var: 0.14241129159927368\n",
      "          vf_loss: 53.779563903808594\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5907964706420898\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01036483608186245\n",
      "          model: {}\n",
      "          policy_loss: -0.053033486008644104\n",
      "          total_loss: 21.382709503173828\n",
      "          vf_explained_var: 0.25696322321891785\n",
      "          vf_loss: 21.425249099731445\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.84\n",
      "    ram_util_percent: 68.71999999999998\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -2.0\n",
      "    pol2: 37.19999999999986\n",
      "  policy_reward_mean:\n",
      "    pol1: -23.425\n",
      "    pol2: -13.036999999999978\n",
      "  policy_reward_min:\n",
      "    pol1: -58.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38062341393974436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20526987718842477\n",
      "    mean_inference_ms: 1.600106593657232\n",
      "    mean_raw_obs_processing_ms: 1.5774966394823815\n",
      "  time_since_restore: 312.9149646759033\n",
      "  time_this_iter_s: 7.359169006347656\n",
      "  time_total_s: 312.9149646759033\n",
      "  timers:\n",
      "    learn_throughput: 806.168\n",
      "    learn_time_ms: 4961.742\n",
      "    sample_throughput: 5867.76\n",
      "    sample_time_ms: 681.691\n",
      "    update_time_ms: 3.661\n",
      "  timestamp: 1619703934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: 6768d_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         305.992</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\"> -29.745</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               -55.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         305.148</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\"> -27.216</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               -54.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         305.509</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\"> -26.811</td><td style=\"text-align: right;\">                -8.7</td><td style=\"text-align: right;\">               -51.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         312.915</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\"> -36.462</td><td style=\"text-align: right;\">               -16.2</td><td style=\"text-align: right;\">               -60  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12.000000000000004\n",
      "  episode_reward_mean: -29.318999999999996\n",
      "  episode_reward_min: -58.20000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2080\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6388430595397949\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009823620319366455\n",
      "          model: {}\n",
      "          policy_loss: -0.025797009468078613\n",
      "          total_loss: 102.05728149414062\n",
      "          vf_explained_var: 0.1473698914051056\n",
      "          vf_loss: 102.07312774658203\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.537311315536499\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009795920923352242\n",
      "          model: {}\n",
      "          policy_loss: -0.03356475010514259\n",
      "          total_loss: 60.658775329589844\n",
      "          vf_explained_var: 0.25377142429351807\n",
      "          vf_loss: 60.68242263793945\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.12727272727273\n",
      "    ram_util_percent: 68.7090909090909\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -4.0\n",
      "    pol2: 59.199999999999946\n",
      "  policy_reward_mean:\n",
      "    pol1: -26.215\n",
      "    pol2: -3.1039999999999917\n",
      "  policy_reward_min:\n",
      "    pol1: -80.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38654843185944004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21496075099417744\n",
      "    mean_inference_ms: 1.6379609955497392\n",
      "    mean_raw_obs_processing_ms: 1.6080701338592887\n",
      "  time_since_restore: 313.3653492927551\n",
      "  time_this_iter_s: 7.373619079589844\n",
      "  time_total_s: 313.3653492927551\n",
      "  timers:\n",
      "    learn_throughput: 806.232\n",
      "    learn_time_ms: 4961.351\n",
      "    sample_throughput: 5863.967\n",
      "    sample_time_ms: 682.132\n",
      "    update_time_ms: 3.992\n",
      "  timestamp: 1619703934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.200000000000002\n",
      "  episode_reward_mean: -26.888999999999992\n",
      "  episode_reward_min: -50.700000000000024\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2080\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6153789758682251\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010999426245689392\n",
      "          model: {}\n",
      "          policy_loss: -0.04006454348564148\n",
      "          total_loss: 51.425819396972656\n",
      "          vf_explained_var: 0.2018316686153412\n",
      "          vf_loss: 51.45474624633789\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.632022500038147\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01006331481039524\n",
      "          model: {}\n",
      "          policy_loss: -0.0352877601981163\n",
      "          total_loss: 35.668418884277344\n",
      "          vf_explained_var: 0.3276510536670685\n",
      "          vf_loss: 35.693519592285156\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.85\n",
      "    ram_util_percent: 68.71999999999998\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 6.5\n",
      "    pol2: 54.80000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: -13.83\n",
      "    pol2: -13.058999999999978\n",
      "  policy_reward_min:\n",
      "    pol1: -56.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.36822766014067243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20249080847325845\n",
      "    mean_inference_ms: 1.5515903026753333\n",
      "    mean_raw_obs_processing_ms: 1.5164434160819313\n",
      "  time_since_restore: 312.87410140037537\n",
      "  time_this_iter_s: 7.3654069900512695\n",
      "  time_total_s: 312.87410140037537\n",
      "  timers:\n",
      "    learn_throughput: 805.321\n",
      "    learn_time_ms: 4966.966\n",
      "    sample_throughput: 5868.251\n",
      "    sample_time_ms: 681.634\n",
      "    update_time_ms: 3.208\n",
      "  timestamp: 1619703934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.09999999999999\n",
      "  episode_reward_mean: -28.38299999999999\n",
      "  episode_reward_min: -54.60000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2080\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.637983500957489\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011143351905047894\n",
      "          model: {}\n",
      "          policy_loss: -0.03244907408952713\n",
      "          total_loss: 52.460670471191406\n",
      "          vf_explained_var: 0.16627678275108337\n",
      "          vf_loss: 52.48183822631836\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5424714088439941\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011822257190942764\n",
      "          model: {}\n",
      "          policy_loss: -0.04884714260697365\n",
      "          total_loss: 19.25909996032715\n",
      "          vf_explained_var: 0.29882389307022095\n",
      "          vf_loss: 19.295978546142578\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.21818181818183\n",
      "    ram_util_percent: 68.89999999999999\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -0.5\n",
      "    pol2: 16.300000000000026\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.36\n",
      "    pol2: -10.022999999999982\n",
      "  policy_reward_min:\n",
      "    pol1: -46.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3827888825296641\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20949182581043424\n",
      "    mean_inference_ms: 1.6180712014285406\n",
      "    mean_raw_obs_processing_ms: 1.5826139356300637\n",
      "  time_since_restore: 312.54877161979675\n",
      "  time_this_iter_s: 7.400389909744263\n",
      "  time_total_s: 312.54877161979675\n",
      "  timers:\n",
      "    learn_throughput: 808.185\n",
      "    learn_time_ms: 4949.365\n",
      "    sample_throughput: 5698.871\n",
      "    sample_time_ms: 701.893\n",
      "    update_time_ms: 3.709\n",
      "  timestamp: 1619703934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12.9\n",
      "  episode_reward_mean: -29.258999999999986\n",
      "  episode_reward_min: -58.20000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2120\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.627043604850769\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010747330263257027\n",
      "          model: {}\n",
      "          policy_loss: -0.04175534099340439\n",
      "          total_loss: 81.57264709472656\n",
      "          vf_explained_var: 0.13583019375801086\n",
      "          vf_loss: 81.603515625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5304436087608337\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013523338362574577\n",
      "          model: {}\n",
      "          policy_loss: -0.03755393251776695\n",
      "          total_loss: 36.99059295654297\n",
      "          vf_explained_var: 0.26363369822502136\n",
      "          vf_loss: 37.01445388793945\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.96666666666667\n",
      "    ram_util_percent: 72.9111111111111\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -4.5\n",
      "    pol2: 59.199999999999946\n",
      "  policy_reward_mean:\n",
      "    pol1: -26.265\n",
      "    pol2: -2.993999999999989\n",
      "  policy_reward_min:\n",
      "    pol1: -80.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38634304459560587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21480199714953496\n",
      "    mean_inference_ms: 1.6368900868453489\n",
      "    mean_raw_obs_processing_ms: 1.607131743419963\n",
      "  time_since_restore: 319.88654613494873\n",
      "  time_this_iter_s: 6.5211968421936035\n",
      "  time_total_s: 319.88654613494873\n",
      "  timers:\n",
      "    learn_throughput: 797.047\n",
      "    learn_time_ms: 5018.527\n",
      "    sample_throughput: 5702.684\n",
      "    sample_time_ms: 701.424\n",
      "    update_time_ms: 3.906\n",
      "  timestamp: 1619703941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         319.887</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\"> -29.259</td><td style=\"text-align: right;\">               -12.9</td><td style=\"text-align: right;\">               -58.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         312.549</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\"> -28.383</td><td style=\"text-align: right;\">                -8.1</td><td style=\"text-align: right;\">               -54.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         312.874</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\"> -26.889</td><td style=\"text-align: right;\">                -1.2</td><td style=\"text-align: right;\">               -50.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         312.915</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\"> -36.462</td><td style=\"text-align: right;\">               -16.2</td><td style=\"text-align: right;\">               -60  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -7.50000000000001\n",
      "  episode_reward_mean: -27.317999999999987\n",
      "  episode_reward_min: -49.50000000000003\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2120\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6354122757911682\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010645512491464615\n",
      "          model: {}\n",
      "          policy_loss: -0.039844147861003876\n",
      "          total_loss: 46.61997985839844\n",
      "          vf_explained_var: 0.1894303858280182\n",
      "          vf_loss: 46.6490478515625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5471245050430298\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012437622994184494\n",
      "          model: {}\n",
      "          policy_loss: -0.04199999198317528\n",
      "          total_loss: 15.195737838745117\n",
      "          vf_explained_var: 0.2616478204727173\n",
      "          vf_loss: 15.225143432617188\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.54444444444445\n",
      "    ram_util_percent: 73.22222222222223\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 1.5\n",
      "    pol2: 20.700000000000024\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.01\n",
      "    pol2: -9.307999999999984\n",
      "  policy_reward_min:\n",
      "    pol1: -49.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3829181689154093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20955600398848204\n",
      "    mean_inference_ms: 1.6192365896522032\n",
      "    mean_raw_obs_processing_ms: 1.5837110344771288\n",
      "  time_since_restore: 319.12047147750854\n",
      "  time_this_iter_s: 6.571699857711792\n",
      "  time_total_s: 319.12047147750854\n",
      "  timers:\n",
      "    learn_throughput: 796.913\n",
      "    learn_time_ms: 5019.369\n",
      "    sample_throughput: 5540.784\n",
      "    sample_time_ms: 721.919\n",
      "    update_time_ms: 3.932\n",
      "  timestamp: 1619703941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12.600000000000005\n",
      "  episode_reward_mean: -35.247\n",
      "  episode_reward_min: -57.00000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2120\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6008073687553406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009820935316383839\n",
      "          model: {}\n",
      "          policy_loss: -0.029432158917188644\n",
      "          total_loss: 67.16081237792969\n",
      "          vf_explained_var: 0.16666781902313232\n",
      "          vf_loss: 67.1802978515625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5852469205856323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009855126030743122\n",
      "          model: {}\n",
      "          policy_loss: -0.03073504939675331\n",
      "          total_loss: 23.585430145263672\n",
      "          vf_explained_var: 0.23841021955013275\n",
      "          vf_loss: 23.60618782043457\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.22\n",
      "    ram_util_percent: 72.92999999999999\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 3.0\n",
      "    pol2: 37.19999999999986\n",
      "  policy_reward_mean:\n",
      "    pol1: -22.76\n",
      "    pol2: -12.486999999999977\n",
      "  policy_reward_min:\n",
      "    pol1: -65.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3807564706866131\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2053179158370603\n",
      "    mean_inference_ms: 1.6002483422374956\n",
      "    mean_raw_obs_processing_ms: 1.5772132338962472\n",
      "  time_since_restore: 319.66112065315247\n",
      "  time_this_iter_s: 6.7461559772491455\n",
      "  time_total_s: 319.66112065315247\n",
      "  timers:\n",
      "    learn_throughput: 794.428\n",
      "    learn_time_ms: 5035.07\n",
      "    sample_throughput: 5658.766\n",
      "    sample_time_ms: 706.868\n",
      "    update_time_ms: 4.058\n",
      "  timestamp: 1619703941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.200000000000002\n",
      "  episode_reward_mean: -25.95299999999999\n",
      "  episode_reward_min: -47.400000000000034\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2120\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6274850964546204\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012271980755031109\n",
      "          model: {}\n",
      "          policy_loss: -0.04467841610312462\n",
      "          total_loss: 34.738426208496094\n",
      "          vf_explained_var: 0.2237255573272705\n",
      "          vf_loss: 34.77067947387695\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6163890361785889\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01604531705379486\n",
      "          model: {}\n",
      "          policy_loss: -0.06829476356506348\n",
      "          total_loss: 12.171480178833008\n",
      "          vf_explained_var: 0.27845191955566406\n",
      "          vf_loss: 12.223529815673828\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.22999999999999\n",
      "    ram_util_percent: 72.92999999999999\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 6.5\n",
      "    pol2: 54.80000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: -12.355\n",
      "    pol2: -13.597999999999978\n",
      "  policy_reward_min:\n",
      "    pol1: -56.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.36869529005717383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20271072179293723\n",
      "    mean_inference_ms: 1.5528930056459729\n",
      "    mean_raw_obs_processing_ms: 1.5183401177314102\n",
      "  time_since_restore: 319.4972174167633\n",
      "  time_this_iter_s: 6.6231160163879395\n",
      "  time_total_s: 319.4972174167633\n",
      "  timers:\n",
      "    learn_throughput: 793.725\n",
      "    learn_time_ms: 5039.53\n",
      "    sample_throughput: 5728.315\n",
      "    sample_time_ms: 698.286\n",
      "    update_time_ms: 3.323\n",
      "  timestamp: 1619703941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -6.899999999999997\n",
      "  episode_reward_mean: -28.772999999999993\n",
      "  episode_reward_min: -58.20000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2160\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6270093321800232\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010215768590569496\n",
      "          model: {}\n",
      "          policy_loss: -0.03466777503490448\n",
      "          total_loss: 106.47935485839844\n",
      "          vf_explained_var: 0.14695322513580322\n",
      "          vf_loss: 106.50367736816406\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4985229969024658\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009653253480792046\n",
      "          model: {}\n",
      "          policy_loss: -0.029008900746703148\n",
      "          total_loss: 81.1611328125\n",
      "          vf_explained_var: 0.2333565503358841\n",
      "          vf_loss: 81.18037414550781\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.15\n",
      "    ram_util_percent: 75.325\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -4.5\n",
      "    pol2: 63.60000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -27.0\n",
      "    pol2: -1.772999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -85.5\n",
      "    pol2: -18.899999999999967\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38622476296300046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2147096340728254\n",
      "    mean_inference_ms: 1.6369136931972357\n",
      "    mean_raw_obs_processing_ms: 1.606369887764581\n",
      "  time_since_restore: 325.4911901950836\n",
      "  time_this_iter_s: 5.604644060134888\n",
      "  time_total_s: 325.4911901950836\n",
      "  timers:\n",
      "    learn_throughput: 793.614\n",
      "    learn_time_ms: 5040.236\n",
      "    sample_throughput: 5654.926\n",
      "    sample_time_ms: 707.348\n",
      "    update_time_ms: 3.976\n",
      "  timestamp: 1619703946\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         325.491</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\"> -28.773</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -58.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         319.12 </td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\"> -27.318</td><td style=\"text-align: right;\">                -7.5</td><td style=\"text-align: right;\">               -49.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         319.497</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\"> -25.953</td><td style=\"text-align: right;\">                -1.2</td><td style=\"text-align: right;\">               -47.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         319.661</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\"> -35.247</td><td style=\"text-align: right;\">               -12.6</td><td style=\"text-align: right;\">               -57  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -7.50000000000001\n",
      "  episode_reward_mean: -26.412\n",
      "  episode_reward_min: -56.10000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2160\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6164888739585876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011558332480490208\n",
      "          model: {}\n",
      "          policy_loss: -0.040928248316049576\n",
      "          total_loss: 59.5357551574707\n",
      "          vf_explained_var: 0.1788903772830963\n",
      "          vf_loss: 59.564979553222656\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5370966196060181\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011656714603304863\n",
      "          model: {}\n",
      "          policy_loss: -0.03753155097365379\n",
      "          total_loss: 33.103389739990234\n",
      "          vf_explained_var: 0.24819210171699524\n",
      "          vf_loss: 33.129119873046875\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.1375\n",
      "    ram_util_percent: 75.26249999999999\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 6.0\n",
      "    pol2: 20.700000000000024\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.38\n",
      "    pol2: -8.031999999999984\n",
      "  policy_reward_min:\n",
      "    pol1: -49.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38308146827626294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20958530787134486\n",
      "    mean_inference_ms: 1.6203486182392748\n",
      "    mean_raw_obs_processing_ms: 1.5840705485279625\n",
      "  time_since_restore: 324.74981141090393\n",
      "  time_this_iter_s: 5.629339933395386\n",
      "  time_total_s: 324.74981141090393\n",
      "  timers:\n",
      "    learn_throughput: 792.572\n",
      "    learn_time_ms: 5046.859\n",
      "    sample_throughput: 5492.844\n",
      "    sample_time_ms: 728.22\n",
      "    update_time_ms: 3.826\n",
      "  timestamp: 1619703947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.200000000000002\n",
      "  episode_reward_mean: -26.288999999999998\n",
      "  episode_reward_min: -59.70000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2160\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6248937249183655\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012780293822288513\n",
      "          model: {}\n",
      "          policy_loss: -0.034291282296180725\n",
      "          total_loss: 40.08734893798828\n",
      "          vf_explained_var: 0.25602781772613525\n",
      "          vf_loss: 40.10869598388672\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6195626854896545\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01515164040029049\n",
      "          model: {}\n",
      "          policy_loss: -0.0629461258649826\n",
      "          total_loss: 7.776988983154297\n",
      "          vf_explained_var: 0.3335457444190979\n",
      "          vf_loss: 7.824594497680664\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.15\n",
      "    ram_util_percent: 75.26249999999999\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 8.5\n",
      "    pol2: 54.80000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: -12.02\n",
      "    pol2: -14.268999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -56.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.36916429291155145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20292775666477034\n",
      "    mean_inference_ms: 1.5546105798691587\n",
      "    mean_raw_obs_processing_ms: 1.5204545050154843\n",
      "  time_since_restore: 325.0761933326721\n",
      "  time_this_iter_s: 5.5789759159088135\n",
      "  time_total_s: 325.0761933326721\n",
      "  timers:\n",
      "    learn_throughput: 791.667\n",
      "    learn_time_ms: 5052.63\n",
      "    sample_throughput: 5632.291\n",
      "    sample_time_ms: 710.191\n",
      "    update_time_ms: 3.326\n",
      "  timestamp: 1619703947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12.600000000000005\n",
      "  episode_reward_mean: -35.343\n",
      "  episode_reward_min: -57.30000000000011\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2160\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5941200256347656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010995535179972649\n",
      "          model: {}\n",
      "          policy_loss: -0.027769990265369415\n",
      "          total_loss: 57.04133605957031\n",
      "          vf_explained_var: 0.1627502143383026\n",
      "          vf_loss: 57.05797576904297\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5805580019950867\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012703681364655495\n",
      "          model: {}\n",
      "          policy_loss: -0.04432668536901474\n",
      "          total_loss: 20.010276794433594\n",
      "          vf_explained_var: 0.26597362756729126\n",
      "          vf_loss: 20.04174041748047\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.65\n",
      "    ram_util_percent: 75.275\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 3.0\n",
      "    pol2: 37.19999999999986\n",
      "  policy_reward_mean:\n",
      "    pol1: -23.505\n",
      "    pol2: -11.837999999999981\n",
      "  policy_reward_min:\n",
      "    pol1: -65.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38117867412697093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2054937581502826\n",
      "    mean_inference_ms: 1.601895508093705\n",
      "    mean_raw_obs_processing_ms: 1.5776624807262025\n",
      "  time_since_restore: 325.32586550712585\n",
      "  time_this_iter_s: 5.664744853973389\n",
      "  time_total_s: 325.32586550712585\n",
      "  timers:\n",
      "    learn_throughput: 790.064\n",
      "    learn_time_ms: 5062.882\n",
      "    sample_throughput: 5625.425\n",
      "    sample_time_ms: 711.057\n",
      "    update_time_ms: 4.064\n",
      "  timestamp: 1619703947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -6.899999999999997\n",
      "  episode_reward_mean: -26.94899999999999\n",
      "  episode_reward_min: -48.60000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2200\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6097315549850464\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010541540570557117\n",
      "          model: {}\n",
      "          policy_loss: -0.04358895868062973\n",
      "          total_loss: 80.21873474121094\n",
      "          vf_explained_var: 0.11352120339870453\n",
      "          vf_loss: 80.25164794921875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5047568082809448\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011252976953983307\n",
      "          model: {}\n",
      "          policy_loss: -0.031824465841054916\n",
      "          total_loss: 63.98290252685547\n",
      "          vf_explained_var: 0.184182807803154\n",
      "          vf_loss: 64.00333404541016\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.0888888888889\n",
      "    ram_util_percent: 74.95555555555556\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -0.5\n",
      "    pol2: 63.60000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -26.54\n",
      "    pol2: -0.408999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -85.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3859181769698731\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21454878742318684\n",
      "    mean_inference_ms: 1.6365567866886754\n",
      "    mean_raw_obs_processing_ms: 1.604808284721996\n",
      "  time_since_restore: 331.2426643371582\n",
      "  time_this_iter_s: 5.751474142074585\n",
      "  time_total_s: 331.2426643371582\n",
      "  timers:\n",
      "    learn_throughput: 800.506\n",
      "    learn_time_ms: 4996.841\n",
      "    sample_throughput: 5644.74\n",
      "    sample_time_ms: 708.624\n",
      "    update_time_ms: 4.006\n",
      "  timestamp: 1619703952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         331.243</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\"> -26.949</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -48.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         324.75 </td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\"> -26.412</td><td style=\"text-align: right;\">                -7.5</td><td style=\"text-align: right;\">               -56.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         325.076</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\"> -26.289</td><td style=\"text-align: right;\">                -1.2</td><td style=\"text-align: right;\">               -59.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         325.326</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\"> -35.343</td><td style=\"text-align: right;\">               -12.6</td><td style=\"text-align: right;\">               -57.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12.600000000000005\n",
      "  episode_reward_mean: -34.164\n",
      "  episode_reward_min: -65.40000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2200\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.598090648651123\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012055113911628723\n",
      "          model: {}\n",
      "          policy_loss: -0.04696172848343849\n",
      "          total_loss: 73.78448486328125\n",
      "          vf_explained_var: 0.17110657691955566\n",
      "          vf_loss: 73.81924438476562\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5841679573059082\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010458055883646011\n",
      "          model: {}\n",
      "          policy_loss: -0.03424356132745743\n",
      "          total_loss: 37.59673309326172\n",
      "          vf_explained_var: 0.3096037805080414\n",
      "          vf_loss: 37.620391845703125\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.65\n",
      "    ram_util_percent: 74.95\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 3.0\n",
      "    pol2: 40.49999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: -21.82\n",
      "    pol2: -12.343999999999978\n",
      "  policy_reward_min:\n",
      "    pol1: -67.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38119837820955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20550329916038815\n",
      "    mean_inference_ms: 1.6021832809509475\n",
      "    mean_raw_obs_processing_ms: 1.5764176676943498\n",
      "  time_since_restore: 331.0707633495331\n",
      "  time_this_iter_s: 5.744897842407227\n",
      "  time_total_s: 331.0707633495331\n",
      "  timers:\n",
      "    learn_throughput: 795.964\n",
      "    learn_time_ms: 5025.352\n",
      "    sample_throughput: 5633.437\n",
      "    sample_time_ms: 710.046\n",
      "    update_time_ms: 4.036\n",
      "  timestamp: 1619703952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.100000000000009\n",
      "  episode_reward_mean: -26.177999999999997\n",
      "  episode_reward_min: -59.70000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2200\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6014417409896851\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010991140268743038\n",
      "          model: {}\n",
      "          policy_loss: -0.03092411905527115\n",
      "          total_loss: 35.82010269165039\n",
      "          vf_explained_var: 0.28742608428001404\n",
      "          vf_loss: 35.83989715576172\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6032537817955017\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011537209153175354\n",
      "          model: {}\n",
      "          policy_loss: -0.050977349281311035\n",
      "          total_loss: 20.7459774017334\n",
      "          vf_explained_var: 0.2878778576850891\n",
      "          vf_loss: 20.7852725982666\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.46666666666667\n",
      "    ram_util_percent: 74.94444444444444\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 8.5\n",
      "    pol2: 24.000000000000018\n",
      "  policy_reward_mean:\n",
      "    pol1: -11.48\n",
      "    pol2: -14.697999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -43.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.36958550345517266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20318625171921945\n",
      "    mean_inference_ms: 1.5563254519927852\n",
      "    mean_raw_obs_processing_ms: 1.5227573291591776\n",
      "  time_since_restore: 330.94378542900085\n",
      "  time_this_iter_s: 5.867592096328735\n",
      "  time_total_s: 330.94378542900085\n",
      "  timers:\n",
      "    learn_throughput: 799.173\n",
      "    learn_time_ms: 5005.175\n",
      "    sample_throughput: 5508.037\n",
      "    sample_time_ms: 726.211\n",
      "    update_time_ms: 3.447\n",
      "  timestamp: 1619703953\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -7.50000000000001\n",
      "  episode_reward_mean: -25.409999999999982\n",
      "  episode_reward_min: -56.10000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2200\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6121828556060791\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011142855510115623\n",
      "          model: {}\n",
      "          policy_loss: -0.043401673436164856\n",
      "          total_loss: 44.194091796875\n",
      "          vf_explained_var: 0.1669783741235733\n",
      "          vf_loss: 44.22621154785156\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5345891714096069\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013642743229866028\n",
      "          model: {}\n",
      "          policy_loss: -0.0545530803501606\n",
      "          total_loss: 16.972518920898438\n",
      "          vf_explained_var: 0.257757306098938\n",
      "          vf_loss: 17.013259887695312\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.65555555555555\n",
      "    ram_util_percent: 74.94444444444444\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 6.0\n",
      "    pol2: 20.70000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -17.125\n",
      "    pol2: -8.284999999999982\n",
      "  policy_reward_min:\n",
      "    pol1: -46.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38345369357287834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2097237191092743\n",
      "    mean_inference_ms: 1.6217097826379048\n",
      "    mean_raw_obs_processing_ms: 1.5854689840638096\n",
      "  time_since_restore: 330.67403650283813\n",
      "  time_this_iter_s: 5.924225091934204\n",
      "  time_total_s: 330.67403650283813\n",
      "  timers:\n",
      "    learn_throughput: 800.164\n",
      "    learn_time_ms: 4998.977\n",
      "    sample_throughput: 5354.476\n",
      "    sample_time_ms: 747.039\n",
      "    update_time_ms: 3.684\n",
      "  timestamp: 1619703953\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -7.800000000000006\n",
      "  episode_reward_mean: -26.417999999999996\n",
      "  episode_reward_min: -51.60000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2240\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6269472241401672\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010407738387584686\n",
      "          model: {}\n",
      "          policy_loss: -0.03511255234479904\n",
      "          total_loss: 58.97712326049805\n",
      "          vf_explained_var: 0.12036674469709396\n",
      "          vf_loss: 59.00170135498047\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4907259941101074\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010730649344623089\n",
      "          model: {}\n",
      "          policy_loss: -0.0352652408182621\n",
      "          total_loss: 31.188987731933594\n",
      "          vf_explained_var: 0.20316678285598755\n",
      "          vf_loss: 31.213388442993164\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.575\n",
      "    ram_util_percent: 74.2375\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -0.5\n",
      "    pol2: 49.29999999999988\n",
      "  policy_reward_mean:\n",
      "    pol1: -23.985\n",
      "    pol2: -2.4329999999999874\n",
      "  policy_reward_min:\n",
      "    pol1: -65.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3855511318641432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2143494495539033\n",
      "    mean_inference_ms: 1.6353790001834465\n",
      "    mean_raw_obs_processing_ms: 1.6025962262608042\n",
      "  time_since_restore: 337.002569437027\n",
      "  time_this_iter_s: 5.759905099868774\n",
      "  time_total_s: 337.002569437027\n",
      "  timers:\n",
      "    learn_throughput: 797.403\n",
      "    learn_time_ms: 5016.282\n",
      "    sample_throughput: 5576.192\n",
      "    sample_time_ms: 717.335\n",
      "    update_time_ms: 3.922\n",
      "  timestamp: 1619703958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         337.003</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\"> -26.418</td><td style=\"text-align: right;\">                -7.8</td><td style=\"text-align: right;\">               -51.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         330.674</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\"> -25.41 </td><td style=\"text-align: right;\">                -7.5</td><td style=\"text-align: right;\">               -56.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         330.944</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\"> -26.178</td><td style=\"text-align: right;\">                -8.1</td><td style=\"text-align: right;\">               -59.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         331.071</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\"> -34.164</td><td style=\"text-align: right;\">               -12.6</td><td style=\"text-align: right;\">               -65.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -17.099999999999966\n",
      "  episode_reward_mean: -34.782000000000004\n",
      "  episode_reward_min: -65.40000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2240\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5774433612823486\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01109391637146473\n",
      "          model: {}\n",
      "          policy_loss: -0.025817465037107468\n",
      "          total_loss: 60.31378173828125\n",
      "          vf_explained_var: 0.10948525369167328\n",
      "          vf_loss: 60.32836151123047\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5700975060462952\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011718341149389744\n",
      "          model: {}\n",
      "          policy_loss: -0.04230620712041855\n",
      "          total_loss: 9.820497512817383\n",
      "          vf_explained_var: 0.28162816166877747\n",
      "          vf_loss: 9.85093879699707\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.94444444444444\n",
      "    ram_util_percent: 74.33333333333333\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -4.0\n",
      "    pol2: 40.49999999999997\n",
      "  policy_reward_mean:\n",
      "    pol1: -22.46\n",
      "    pol2: -12.321999999999976\n",
      "  policy_reward_min:\n",
      "    pol1: -67.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3805463358800597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20518770213977697\n",
      "    mean_inference_ms: 1.599920763068248\n",
      "    mean_raw_obs_processing_ms: 1.5727929701715961\n",
      "  time_since_restore: 336.7719666957855\n",
      "  time_this_iter_s: 5.701203346252441\n",
      "  time_total_s: 336.7719666957855\n",
      "  timers:\n",
      "    learn_throughput: 791.254\n",
      "    learn_time_ms: 5055.268\n",
      "    sample_throughput: 5682.213\n",
      "    sample_time_ms: 703.951\n",
      "    update_time_ms: 4.125\n",
      "  timestamp: 1619703958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.100000000000009\n",
      "  episode_reward_mean: -26.297999999999988\n",
      "  episode_reward_min: -59.70000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2240\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5896005630493164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010698526166379452\n",
      "          model: {}\n",
      "          policy_loss: -0.042076870799064636\n",
      "          total_loss: 38.6236457824707\n",
      "          vf_explained_var: 0.31140050292015076\n",
      "          vf_loss: 38.65488815307617\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.6049503684043884\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013560905121266842\n",
      "          model: {}\n",
      "          policy_loss: -0.051271937787532806\n",
      "          total_loss: 17.49078369140625\n",
      "          vf_explained_var: 0.38928741216659546\n",
      "          vf_loss: 17.528324127197266\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.55\n",
      "    ram_util_percent: 74.23750000000001\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 3.5\n",
      "    pol2: 25.10000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -11.655\n",
      "    pol2: -14.642999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -43.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.369634055939634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20323519519052063\n",
      "    mean_inference_ms: 1.556507761848721\n",
      "    mean_raw_obs_processing_ms: 1.523510713951889\n",
      "  time_since_restore: 336.6777904033661\n",
      "  time_this_iter_s: 5.734004974365234\n",
      "  time_total_s: 336.6777904033661\n",
      "  timers:\n",
      "    learn_throughput: 795.159\n",
      "    learn_time_ms: 5030.439\n",
      "    sample_throughput: 5473.505\n",
      "    sample_time_ms: 730.793\n",
      "    update_time_ms: 3.432\n",
      "  timestamp: 1619703958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-45-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -6.899999999999996\n",
      "  episode_reward_mean: -25.151999999999994\n",
      "  episode_reward_min: -43.500000000000014\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2240\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5948911309242249\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010956710204482079\n",
      "          model: {}\n",
      "          policy_loss: -0.03121960535645485\n",
      "          total_loss: 68.66421508789062\n",
      "          vf_explained_var: 0.17913353443145752\n",
      "          vf_loss: 68.68434143066406\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5193319916725159\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01158746238797903\n",
      "          model: {}\n",
      "          policy_loss: -0.04442364349961281\n",
      "          total_loss: 45.82522201538086\n",
      "          vf_explained_var: 0.23378318548202515\n",
      "          vf_loss: 45.85791015625\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.5125\n",
      "    ram_util_percent: 74.25\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: 3.0\n",
      "    pol2: 33.9\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.605\n",
      "    pol2: -6.546999999999984\n",
      "  policy_reward_min:\n",
      "    pol1: -48.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38359219074175416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2097352340396797\n",
      "    mean_inference_ms: 1.621918189592658\n",
      "    mean_raw_obs_processing_ms: 1.5861348579534391\n",
      "  time_since_restore: 336.4107196331024\n",
      "  time_this_iter_s: 5.736683130264282\n",
      "  time_total_s: 336.4107196331024\n",
      "  timers:\n",
      "    learn_throughput: 795.856\n",
      "    learn_time_ms: 5026.032\n",
      "    sample_throughput: 5299.37\n",
      "    sample_time_ms: 754.807\n",
      "    update_time_ms: 3.654\n",
      "  timestamp: 1619703958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -6.000000000000008\n",
      "  episode_reward_mean: -27.329999999999995\n",
      "  episode_reward_min: -51.60000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2280\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6113284826278687\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010052917525172234\n",
      "          model: {}\n",
      "          policy_loss: -0.03581884875893593\n",
      "          total_loss: 63.41443634033203\n",
      "          vf_explained_var: 0.1167600154876709\n",
      "          vf_loss: 63.4400749206543\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4939013123512268\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010692236945033073\n",
      "          model: {}\n",
      "          policy_loss: -0.035508740693330765\n",
      "          total_loss: 24.922561645507812\n",
      "          vf_explained_var: 0.2119852751493454\n",
      "          vf_loss: 24.947242736816406\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.6125\n",
      "    ram_util_percent: 74.4375\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: 0.0\n",
      "    pol2: 42.70000000000003\n",
      "  policy_reward_mean:\n",
      "    pol1: -21.52\n",
      "    pol2: -5.809999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -52.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3856914101625422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2144067751496366\n",
      "    mean_inference_ms: 1.6352622614515553\n",
      "    mean_raw_obs_processing_ms: 1.6027404558499114\n",
      "  time_since_restore: 342.54918241500854\n",
      "  time_this_iter_s: 5.546612977981567\n",
      "  time_total_s: 342.54918241500854\n",
      "  timers:\n",
      "    learn_throughput: 802.658\n",
      "    learn_time_ms: 4983.442\n",
      "    sample_throughput: 5439.508\n",
      "    sample_time_ms: 735.361\n",
      "    update_time_ms: 3.94\n",
      "  timestamp: 1619703964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         342.549</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -27.33 </td><td style=\"text-align: right;\">                -6  </td><td style=\"text-align: right;\">               -51.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>RUNNING </td><td>192.168.0.100:27761</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         336.411</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\"> -25.152</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -43.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         336.678</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\"> -26.298</td><td style=\"text-align: right;\">                -8.1</td><td style=\"text-align: right;\">               -59.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         336.772</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\"> -34.782</td><td style=\"text-align: right;\">               -17.1</td><td style=\"text-align: right;\">               -65.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -15.299999999999983\n",
      "  episode_reward_mean: -34.01699999999999\n",
      "  episode_reward_min: -65.40000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2280\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5810614228248596\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009506484493613243\n",
      "          model: {}\n",
      "          policy_loss: -0.02447396144270897\n",
      "          total_loss: 74.84034729003906\n",
      "          vf_explained_var: 0.2173941731452942\n",
      "          vf_loss: 74.85519409179688\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5795292258262634\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010057330131530762\n",
      "          model: {}\n",
      "          policy_loss: -0.03551209717988968\n",
      "          total_loss: 27.2907657623291\n",
      "          vf_explained_var: 0.3637845516204834\n",
      "          vf_loss: 27.31609344482422\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.714285714285715\n",
      "    ram_util_percent: 74.42857142857143\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -3.0\n",
      "    pol2: 40.499999999999936\n",
      "  policy_reward_mean:\n",
      "    pol1: -21.915\n",
      "    pol2: -12.10199999999998\n",
      "  policy_reward_min:\n",
      "    pol1: -90.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.37958402972023414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2047167455019625\n",
      "    mean_inference_ms: 1.5965380898457477\n",
      "    mean_raw_obs_processing_ms: 1.568205376724484\n",
      "  time_since_restore: 342.12432885169983\n",
      "  time_this_iter_s: 5.352362155914307\n",
      "  time_total_s: 342.12432885169983\n",
      "  timers:\n",
      "    learn_throughput: 795.272\n",
      "    learn_time_ms: 5029.725\n",
      "    sample_throughput: 5734.397\n",
      "    sample_time_ms: 697.545\n",
      "    update_time_ms: 4.376\n",
      "  timestamp: 1619703964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.100000000000009\n",
      "  episode_reward_mean: -25.697999999999993\n",
      "  episode_reward_min: -50.400000000000034\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2280\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6032168865203857\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011352697387337685\n",
      "          model: {}\n",
      "          policy_loss: -0.03550933301448822\n",
      "          total_loss: 43.2633056640625\n",
      "          vf_explained_var: 0.23753269016742706\n",
      "          vf_loss: 43.28731918334961\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5931344032287598\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014523142948746681\n",
      "          model: {}\n",
      "          policy_loss: -0.058172233402729034\n",
      "          total_loss: 20.25910758972168\n",
      "          vf_explained_var: 0.29379522800445557\n",
      "          vf_loss: 20.302574157714844\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.65714285714286\n",
      "    ram_util_percent: 74.42857142857143\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 7.5\n",
      "    pol2: 25.10000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -11.22\n",
      "    pol2: -14.477999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -44.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3692834976083801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2030067673349999\n",
      "    mean_inference_ms: 1.5551911141810155\n",
      "    mean_raw_obs_processing_ms: 1.5220191958591789\n",
      "  time_since_restore: 342.0156705379486\n",
      "  time_this_iter_s: 5.3378801345825195\n",
      "  time_total_s: 342.0156705379486\n",
      "  timers:\n",
      "    learn_throughput: 799.851\n",
      "    learn_time_ms: 5000.929\n",
      "    sample_throughput: 5454.974\n",
      "    sample_time_ms: 733.276\n",
      "    update_time_ms: 3.499\n",
      "  timestamp: 1619703964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00001:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-04\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -6.899999999999996\n",
      "  episode_reward_mean: -24.66299999999999\n",
      "  episode_reward_min: -47.10000000000002\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2280\n",
      "  experiment_id: 88ed0d232176421b9f10b011ac47f80b\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.6000813245773315\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01111726276576519\n",
      "          model: {}\n",
      "          policy_loss: -0.03467992693185806\n",
      "          total_loss: 36.65880584716797\n",
      "          vf_explained_var: 0.2695101201534271\n",
      "          vf_loss: 36.682228088378906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5290122032165527\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013238245621323586\n",
      "          model: {}\n",
      "          policy_loss: -0.046762898564338684\n",
      "          total_loss: 15.508162498474121\n",
      "          vf_explained_var: 0.2641872763633728\n",
      "          vf_loss: 15.541522026062012\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.685714285714276\n",
      "    ram_util_percent: 74.42857142857143\n",
      "  pid: 27761\n",
      "  policy_reward_max:\n",
      "    pol1: -0.5\n",
      "    pol2: 33.9\n",
      "  policy_reward_mean:\n",
      "    pol1: -17.555\n",
      "    pol2: -7.107999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -48.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38329385167046326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2095245373306334\n",
      "    mean_inference_ms: 1.6207554465673166\n",
      "    mean_raw_obs_processing_ms: 1.5849195410787758\n",
      "  time_since_restore: 341.7373378276825\n",
      "  time_this_iter_s: 5.326618194580078\n",
      "  time_total_s: 341.7373378276825\n",
      "  timers:\n",
      "    learn_throughput: 800.643\n",
      "    learn_time_ms: 4995.983\n",
      "    sample_throughput: 5325.001\n",
      "    sample_time_ms: 751.174\n",
      "    update_time_ms: 3.675\n",
      "  timestamp: 1619703964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 6768d_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -6.000000000000008\n",
      "  episode_reward_mean: -27.50699999999999\n",
      "  episode_reward_min: -48.90000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2320\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5968334674835205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011222739703953266\n",
      "          model: {}\n",
      "          policy_loss: -0.0405098982155323\n",
      "          total_loss: 54.838653564453125\n",
      "          vf_explained_var: 0.13172343373298645\n",
      "          vf_loss: 54.867801666259766\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4889272451400757\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009361948817968369\n",
      "          model: {}\n",
      "          policy_loss: -0.03282172977924347\n",
      "          total_loss: 23.11496353149414\n",
      "          vf_explained_var: 0.21071375906467438\n",
      "          vf_loss: 23.1383056640625\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.05714285714285\n",
      "    ram_util_percent: 72.22857142857143\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: 0.0\n",
      "    pol2: 40.5\n",
      "  policy_reward_mean:\n",
      "    pol1: -21.015\n",
      "    pol2: -6.491999999999989\n",
      "  policy_reward_min:\n",
      "    pol1: -55.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3859375841481578\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21451486978085513\n",
      "    mean_inference_ms: 1.6354688331285672\n",
      "    mean_raw_obs_processing_ms: 1.603805694775052\n",
      "  time_since_restore: 347.5315716266632\n",
      "  time_this_iter_s: 4.982389211654663\n",
      "  time_total_s: 347.5315716266632\n",
      "  timers:\n",
      "    learn_throughput: 811.292\n",
      "    learn_time_ms: 4930.409\n",
      "    sample_throughput: 5379.165\n",
      "    sample_time_ms: 743.61\n",
      "    update_time_ms: 3.761\n",
      "  timestamp: 1619703969\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (3 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING   </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         347.532</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\"> -27.507</td><td style=\"text-align: right;\">                -6  </td><td style=\"text-align: right;\">               -48.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING   </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         342.016</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -25.698</td><td style=\"text-align: right;\">                -8.1</td><td style=\"text-align: right;\">               -50.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         342.124</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -34.017</td><td style=\"text-align: right;\">               -15.3</td><td style=\"text-align: right;\">               -65.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.699999999999996\n",
      "  episode_reward_mean: -33.74699999999999\n",
      "  episode_reward_min: -49.50000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2320\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5718386173248291\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00854343269020319\n",
      "          model: {}\n",
      "          policy_loss: -0.02570328116416931\n",
      "          total_loss: 82.67941284179688\n",
      "          vf_explained_var: 0.2271953821182251\n",
      "          vf_loss: 82.69647216796875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5592383146286011\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0077451905235648155\n",
      "          model: {}\n",
      "          policy_loss: -0.03318657726049423\n",
      "          total_loss: 92.18989562988281\n",
      "          vf_explained_var: 0.31211015582084656\n",
      "          vf_loss: 92.21524047851562\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.975\n",
      "    ram_util_percent: 72.58749999999999\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: -0.5\n",
      "    pol2: 114.2\n",
      "  policy_reward_mean:\n",
      "    pol1: -22.03\n",
      "    pol2: -11.71699999999998\n",
      "  policy_reward_min:\n",
      "    pol1: -108.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3789853168643282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2044571618942722\n",
      "    mean_inference_ms: 1.5943241147710285\n",
      "    mean_raw_obs_processing_ms: 1.565417394438512\n",
      "  time_since_restore: 347.13143658638\n",
      "  time_this_iter_s: 5.007107734680176\n",
      "  time_total_s: 347.13143658638\n",
      "  timers:\n",
      "    learn_throughput: 804.494\n",
      "    learn_time_ms: 4972.072\n",
      "    sample_throughput: 5673.026\n",
      "    sample_time_ms: 705.091\n",
      "    update_time_ms: 4.266\n",
      "  timestamp: 1619703969\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.10000000000001\n",
      "  episode_reward_mean: -26.480999999999995\n",
      "  episode_reward_min: -50.400000000000034\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2320\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5934070348739624\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010885019786655903\n",
      "          model: {}\n",
      "          policy_loss: -0.03375307470560074\n",
      "          total_loss: 30.141647338867188\n",
      "          vf_explained_var: 0.2632247805595398\n",
      "          vf_loss: 30.16438102722168\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5941411256790161\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016464948654174805\n",
      "          model: {}\n",
      "          policy_loss: -0.062319401651620865\n",
      "          total_loss: 7.724163055419922\n",
      "          vf_explained_var: 0.33217155933380127\n",
      "          vf_loss: 7.769811630249023\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.0875\n",
      "    ram_util_percent: 72.58749999999999\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 7.5\n",
      "    pol2: 25.10000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -12.025\n",
      "    pol2: -14.455999999999971\n",
      "  policy_reward_min:\n",
      "    pol1: -44.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.36957322725309055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20310146517690555\n",
      "    mean_inference_ms: 1.55600193132137\n",
      "    mean_raw_obs_processing_ms: 1.5229311633447673\n",
      "  time_since_restore: 347.075567483902\n",
      "  time_this_iter_s: 5.059896945953369\n",
      "  time_total_s: 347.075567483902\n",
      "  timers:\n",
      "    learn_throughput: 809.817\n",
      "    learn_time_ms: 4939.389\n",
      "    sample_throughput: 5281.634\n",
      "    sample_time_ms: 757.341\n",
      "    update_time_ms: 3.485\n",
      "  timestamp: 1619703969\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -9.599999999999998\n",
      "  episode_reward_mean: -27.491999999999994\n",
      "  episode_reward_min: -48.90000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2360\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5725498199462891\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010709594935178757\n",
      "          model: {}\n",
      "          policy_loss: -0.04108542576432228\n",
      "          total_loss: 48.73853302001953\n",
      "          vf_explained_var: 0.133002370595932\n",
      "          vf_loss: 48.768775939941406\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4746076166629791\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010660652071237564\n",
      "          model: {}\n",
      "          policy_loss: -0.034140486270189285\n",
      "          total_loss: 23.92502784729004\n",
      "          vf_explained_var: 0.21016362309455872\n",
      "          vf_loss: 23.948375701904297\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.699999999999996\n",
      "    ram_util_percent: 72.22857142857143\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -2.0\n",
      "    pol2: 37.19999999999986\n",
      "  policy_reward_mean:\n",
      "    pol1: -21.22\n",
      "    pol2: -6.271999999999992\n",
      "  policy_reward_min:\n",
      "    pol1: -55.5\n",
      "    pol2: -18.899999999999967\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38526334806657153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2141694438789315\n",
      "    mean_inference_ms: 1.6326036355306628\n",
      "    mean_raw_obs_processing_ms: 1.601479103218008\n",
      "  time_since_restore: 352.7561984062195\n",
      "  time_this_iter_s: 5.224626779556274\n",
      "  time_total_s: 352.7561984062195\n",
      "  timers:\n",
      "    learn_throughput: 817.325\n",
      "    learn_time_ms: 4894.014\n",
      "    sample_throughput: 5415.387\n",
      "    sample_time_ms: 738.636\n",
      "    update_time_ms: 3.625\n",
      "  timestamp: 1619703974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (3 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING   </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         352.756</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\"> -27.492</td><td style=\"text-align: right;\">                -9.6</td><td style=\"text-align: right;\">               -48.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING   </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         347.076</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\"> -26.481</td><td style=\"text-align: right;\">                -8.1</td><td style=\"text-align: right;\">               -50.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         347.131</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\"> -33.747</td><td style=\"text-align: right;\">                 5.7</td><td style=\"text-align: right;\">               -49.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.699999999999996\n",
      "  episode_reward_mean: -33.974999999999994\n",
      "  episode_reward_min: -63.000000000000135\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2360\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5892821550369263\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010701831430196762\n",
      "          model: {}\n",
      "          policy_loss: -0.03261489421129227\n",
      "          total_loss: 60.32833480834961\n",
      "          vf_explained_var: 0.15444166958332062\n",
      "          vf_loss: 60.35011672973633\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5509440898895264\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010953143239021301\n",
      "          model: {}\n",
      "          policy_loss: -0.042206160724163055\n",
      "          total_loss: 36.47643280029297\n",
      "          vf_explained_var: 0.32116955518722534\n",
      "          vf_loss: 36.507545471191406\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.271428571428565\n",
      "    ram_util_percent: 72.3142857142857\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 1.5\n",
      "    pol2: 114.2\n",
      "  policy_reward_mean:\n",
      "    pol1: -21.895\n",
      "    pol2: -12.079999999999975\n",
      "  policy_reward_min:\n",
      "    pol1: -108.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3781679449817389\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.204093060948899\n",
      "    mean_inference_ms: 1.5913428799321576\n",
      "    mean_raw_obs_processing_ms: 1.5622160130510623\n",
      "  time_since_restore: 352.43413162231445\n",
      "  time_this_iter_s: 5.302695035934448\n",
      "  time_total_s: 352.43413162231445\n",
      "  timers:\n",
      "    learn_throughput: 809.291\n",
      "    learn_time_ms: 4942.598\n",
      "    sample_throughput: 5725.286\n",
      "    sample_time_ms: 698.655\n",
      "    update_time_ms: 3.881\n",
      "  timestamp: 1619703974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.10000000000001\n",
      "  episode_reward_mean: -26.204999999999988\n",
      "  episode_reward_min: -42.90000000000002\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2360\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5865382552146912\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012386666610836983\n",
      "          model: {}\n",
      "          policy_loss: -0.04129365459084511\n",
      "          total_loss: 31.238059997558594\n",
      "          vf_explained_var: 0.3001548945903778\n",
      "          vf_loss: 31.266813278198242\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5755102038383484\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01402880996465683\n",
      "          model: {}\n",
      "          policy_loss: -0.05456278473138809\n",
      "          total_loss: 8.846919059753418\n",
      "          vf_explained_var: 0.40421542525291443\n",
      "          vf_loss: 8.887277603149414\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.285714285714285\n",
      "    ram_util_percent: 72.3142857142857\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 7.5\n",
      "    pol2: 9.700000000000026\n",
      "  policy_reward_mean:\n",
      "    pol1: -10.935\n",
      "    pol2: -15.269999999999971\n",
      "  policy_reward_min:\n",
      "    pol1: -50.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.36973489595168857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20316983196069713\n",
      "    mean_inference_ms: 1.5566027090118668\n",
      "    mean_raw_obs_processing_ms: 1.5235986075223116\n",
      "  time_since_restore: 352.3151514530182\n",
      "  time_this_iter_s: 5.239583969116211\n",
      "  time_total_s: 352.3151514530182\n",
      "  timers:\n",
      "    learn_throughput: 816.678\n",
      "    learn_time_ms: 4897.894\n",
      "    sample_throughput: 5251.64\n",
      "    sample_time_ms: 761.667\n",
      "    update_time_ms: 3.345\n",
      "  timestamp: 1619703974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.199999999999992\n",
      "  episode_reward_mean: -27.119999999999994\n",
      "  episode_reward_min: -48.90000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2400\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5773633122444153\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010148127563297749\n",
      "          model: {}\n",
      "          policy_loss: -0.04041573405265808\n",
      "          total_loss: 60.38362121582031\n",
      "          vf_explained_var: 0.13912849128246307\n",
      "          vf_loss: 60.41376495361328\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.46680253744125366\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01010581012815237\n",
      "          model: {}\n",
      "          policy_loss: -0.04245680198073387\n",
      "          total_loss: 27.31644058227539\n",
      "          vf_explained_var: 0.23185566067695618\n",
      "          vf_loss: 27.348665237426758\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.9125\n",
      "    ram_util_percent: 74.425\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -2.0\n",
      "    pol2: 32.79999999999988\n",
      "  policy_reward_mean:\n",
      "    pol1: -20.43\n",
      "    pol2: -6.68999999999999\n",
      "  policy_reward_min:\n",
      "    pol1: -62.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3836724111046135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2133669884006811\n",
      "    mean_inference_ms: 1.6267660163454587\n",
      "    mean_raw_obs_processing_ms: 1.595532852700727\n",
      "  time_since_restore: 358.1380934715271\n",
      "  time_this_iter_s: 5.381895065307617\n",
      "  time_total_s: 358.1380934715271\n",
      "  timers:\n",
      "    learn_throughput: 831.342\n",
      "    learn_time_ms: 4811.497\n",
      "    sample_throughput: 5517.51\n",
      "    sample_time_ms: 724.965\n",
      "    update_time_ms: 3.57\n",
      "  timestamp: 1619703979\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (3 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING   </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         358.138</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\"> -27.12 </td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">               -48.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING   </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         352.315</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\"> -26.205</td><td style=\"text-align: right;\">                -8.1</td><td style=\"text-align: right;\">               -42.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         352.434</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\"> -33.975</td><td style=\"text-align: right;\">                 5.7</td><td style=\"text-align: right;\">               -63  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.699999999999996\n",
      "  episode_reward_mean: -33.195\n",
      "  episode_reward_min: -63.000000000000135\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2400\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5813271999359131\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010782662779092789\n",
      "          model: {}\n",
      "          policy_loss: -0.03663226589560509\n",
      "          total_loss: 56.77137756347656\n",
      "          vf_explained_var: 0.15052932500839233\n",
      "          vf_loss: 56.797088623046875\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5331763029098511\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012116593308746815\n",
      "          model: {}\n",
      "          policy_loss: -0.038724690675735474\n",
      "          total_loss: 24.390350341796875\n",
      "          vf_explained_var: 0.2028500735759735\n",
      "          vf_loss: 24.41680908203125\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.425\n",
      "    ram_util_percent: 74.5625\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 1.5\n",
      "    pol2: 114.2\n",
      "  policy_reward_mean:\n",
      "    pol1: -21.995\n",
      "    pol2: -11.199999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -108.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.37693602048943087\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20353025476447353\n",
      "    mean_inference_ms: 1.5866176764796085\n",
      "    mean_raw_obs_processing_ms: 1.5576921080215862\n",
      "  time_since_restore: 357.86361956596375\n",
      "  time_this_iter_s: 5.429487943649292\n",
      "  time_total_s: 357.86361956596375\n",
      "  timers:\n",
      "    learn_throughput: 822.983\n",
      "    learn_time_ms: 4860.366\n",
      "    sample_throughput: 5750.683\n",
      "    sample_time_ms: 695.57\n",
      "    update_time_ms: 3.655\n",
      "  timestamp: 1619703980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.200000000000003\n",
      "  episode_reward_mean: -26.801999999999992\n",
      "  episode_reward_min: -45.30000000000004\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2400\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5969222784042358\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01491577923297882\n",
      "          model: {}\n",
      "          policy_loss: -0.04434003680944443\n",
      "          total_loss: 51.614131927490234\n",
      "          vf_explained_var: 0.27213621139526367\n",
      "          vf_loss: 51.64337158203125\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5681596994400024\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013960188254714012\n",
      "          model: {}\n",
      "          policy_loss: -0.0578688308596611\n",
      "          total_loss: 17.189884185791016\n",
      "          vf_explained_var: 0.3700685203075409\n",
      "          vf_loss: 17.233617782592773\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.4375\n",
      "    ram_util_percent: 74.5625\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 7.0\n",
      "    pol2: 15.200000000000006\n",
      "  policy_reward_mean:\n",
      "    pol1: -11.785\n",
      "    pol2: -15.01699999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -59.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.36915777478321127\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2028914797527014\n",
      "    mean_inference_ms: 1.5545565844451585\n",
      "    mean_raw_obs_processing_ms: 1.5220276775788966\n",
      "  time_since_restore: 357.7857723236084\n",
      "  time_this_iter_s: 5.47062087059021\n",
      "  time_total_s: 357.7857723236084\n",
      "  timers:\n",
      "    learn_throughput: 829.75\n",
      "    learn_time_ms: 4820.731\n",
      "    sample_throughput: 5365.304\n",
      "    sample_time_ms: 745.531\n",
      "    update_time_ms: 3.313\n",
      "  timestamp: 1619703980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -11.699999999999998\n",
      "  episode_reward_mean: -27.66599999999999\n",
      "  episode_reward_min: -51.60000000000004\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2480\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5351572036743164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009702826850116253\n",
      "          model: {}\n",
      "          policy_loss: -0.03419099748134613\n",
      "          total_loss: 46.9598274230957\n",
      "          vf_explained_var: 0.16057908535003662\n",
      "          vf_loss: 46.984195709228516\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.47412917017936707\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01233283057808876\n",
      "          model: {}\n",
      "          policy_loss: -0.04200618714094162\n",
      "          total_loss: 15.560928344726562\n",
      "          vf_explained_var: 0.27073049545288086\n",
      "          vf_loss: 15.590448379516602\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.01428571428572\n",
      "    ram_util_percent: 75.2\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: 1.5\n",
      "    pol2: 32.79999999999988\n",
      "  policy_reward_mean:\n",
      "    pol1: -20.415\n",
      "    pol2: -7.250999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -62.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.38047330278661917\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21172947649683507\n",
      "    mean_inference_ms: 1.6157106441915738\n",
      "    mean_raw_obs_processing_ms: 1.5821685561409884\n",
      "  time_since_restore: 367.5053734779358\n",
      "  time_this_iter_s: 4.617004156112671\n",
      "  time_total_s: 367.5053734779358\n",
      "  timers:\n",
      "    learn_throughput: 892.474\n",
      "    learn_time_ms: 4481.922\n",
      "    sample_throughput: 5691.822\n",
      "    sample_time_ms: 702.763\n",
      "    update_time_ms: 3.309\n",
      "  timestamp: 1619703989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (3 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING   </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         367.505</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\"> -27.666</td><td style=\"text-align: right;\">               -11.7</td><td style=\"text-align: right;\">               -51.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING   </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         362.439</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\"> -26.562</td><td style=\"text-align: right;\">               -10.2</td><td style=\"text-align: right;\">               -45.3</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         362.536</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\"> -33.246</td><td style=\"text-align: right;\">                -3.3</td><td style=\"text-align: right;\">               -63  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.3\n",
      "  episode_reward_mean: -33.00299999999999\n",
      "  episode_reward_min: -57.00000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2480\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5644306540489197\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010993688367307186\n",
      "          model: {}\n",
      "          policy_loss: -0.036069903522729874\n",
      "          total_loss: 51.7763786315918\n",
      "          vf_explained_var: 0.15618523955345154\n",
      "          vf_loss: 51.80132293701172\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.554719090461731\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01676006242632866\n",
      "          model: {}\n",
      "          policy_loss: -0.06298714876174927\n",
      "          total_loss: 9.123902320861816\n",
      "          vf_explained_var: 0.3224030137062073\n",
      "          vf_loss: 9.169920921325684\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.99999999999999\n",
      "    ram_util_percent: 75.13333333333334\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 0.0\n",
      "    pol2: 92.19999999999996\n",
      "  policy_reward_mean:\n",
      "    pol1: -20.945\n",
      "    pol2: -12.057999999999975\n",
      "  policy_reward_min:\n",
      "    pol1: -95.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.37423442384730665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20229067787522476\n",
      "    mean_inference_ms: 1.5769693391748438\n",
      "    mean_raw_obs_processing_ms: 1.5476383659214117\n",
      "  time_since_restore: 367.17046546936035\n",
      "  time_this_iter_s: 4.634051084518433\n",
      "  time_total_s: 367.17046546936035\n",
      "  timers:\n",
      "    learn_throughput: 882.609\n",
      "    learn_time_ms: 4532.019\n",
      "    sample_throughput: 5847.038\n",
      "    sample_time_ms: 684.107\n",
      "    update_time_ms: 3.402\n",
      "  timestamp: 1619703989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.799999999999994\n",
      "  episode_reward_mean: -26.27999999999999\n",
      "  episode_reward_min: -45.00000000000002\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2480\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5646636486053467\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011518510058522224\n",
      "          model: {}\n",
      "          policy_loss: -0.044277094304561615\n",
      "          total_loss: 38.46916580200195\n",
      "          vf_explained_var: 0.2802698016166687\n",
      "          vf_loss: 38.50178146362305\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5572385787963867\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014473799616098404\n",
      "          model: {}\n",
      "          policy_loss: -0.056720711290836334\n",
      "          total_loss: 12.297057151794434\n",
      "          vf_explained_var: 0.368375301361084\n",
      "          vf_loss: 12.339122772216797\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.0\n",
      "    ram_util_percent: 75.13333333333334\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 4.5\n",
      "    pol2: 15.200000000000006\n",
      "  policy_reward_mean:\n",
      "    pol1: -11.67\n",
      "    pol2: -14.609999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -59.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3669635091998283\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20184602372963809\n",
      "    mean_inference_ms: 1.547167087529862\n",
      "    mean_raw_obs_processing_ms: 1.5149301613207626\n",
      "  time_since_restore: 367.0279860496521\n",
      "  time_this_iter_s: 4.588871955871582\n",
      "  time_total_s: 367.0279860496521\n",
      "  timers:\n",
      "    learn_throughput: 890.412\n",
      "    learn_time_ms: 4492.303\n",
      "    sample_throughput: 5534.05\n",
      "    sample_time_ms: 722.798\n",
      "    update_time_ms: 3.121\n",
      "  timestamp: 1619703989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -9.899999999999999\n",
      "  episode_reward_mean: -26.08499999999999\n",
      "  episode_reward_min: -61.20000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2560\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.54987633228302\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009495170786976814\n",
      "          model: {}\n",
      "          policy_loss: -0.029398320242762566\n",
      "          total_loss: 39.45379638671875\n",
      "          vf_explained_var: 0.15561643242835999\n",
      "          vf_loss: 39.47357940673828\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4572833776473999\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011258333921432495\n",
      "          model: {}\n",
      "          policy_loss: -0.038090627640485764\n",
      "          total_loss: 16.657325744628906\n",
      "          vf_explained_var: 0.2350296676158905\n",
      "          vf_loss: 16.684019088745117\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.24285714285713\n",
      "    ram_util_percent: 75.48571428571428\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: 1.5\n",
      "    pol2: 12.999999999999998\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.295\n",
      "    pol2: -7.789999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -44.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3775895930408437\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2102781987398371\n",
      "    mean_inference_ms: 1.6048926204698089\n",
      "    mean_raw_obs_processing_ms: 1.5700641893809677\n",
      "  time_since_restore: 376.98303174972534\n",
      "  time_this_iter_s: 4.830999135971069\n",
      "  time_total_s: 376.98303174972534\n",
      "  timers:\n",
      "    learn_throughput: 937.968\n",
      "    learn_time_ms: 4264.536\n",
      "    sample_throughput: 6043.589\n",
      "    sample_time_ms: 661.858\n",
      "    update_time_ms: 3.307\n",
      "  timestamp: 1619703998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (3 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING   </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         376.983</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\"> -26.085</td><td style=\"text-align: right;\">                -9.9</td><td style=\"text-align: right;\">               -61.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>RUNNING   </td><td>192.168.0.100:27752</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         371.652</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\"> -25.809</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               -57.9</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         371.803</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\"> -31.482</td><td style=\"text-align: right;\">                 1.5</td><td style=\"text-align: right;\">               -57  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.499999999999996\n",
      "  episode_reward_mean: -30.881999999999998\n",
      "  episode_reward_min: -60.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2560\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5531268119812012\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009923124685883522\n",
      "          model: {}\n",
      "          policy_loss: -0.028609829023480415\n",
      "          total_loss: 54.256229400634766\n",
      "          vf_explained_var: 0.13008983433246613\n",
      "          vf_loss: 54.2747917175293\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5331672430038452\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01156301237642765\n",
      "          model: {}\n",
      "          policy_loss: -0.039815161377191544\n",
      "          total_loss: 32.16468048095703\n",
      "          vf_explained_var: 0.286316454410553\n",
      "          vf_loss: 32.192787170410156\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.32857142857143\n",
      "    ram_util_percent: 75.71428571428571\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 2.5\n",
      "    pol2: 73.5\n",
      "  policy_reward_mean:\n",
      "    pol1: -19.715\n",
      "    pol2: -11.166999999999975\n",
      "  policy_reward_min:\n",
      "    pol1: -72.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.37162967807570446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.201091397027076\n",
      "    mean_inference_ms: 1.5670942031345547\n",
      "    mean_raw_obs_processing_ms: 1.5370626482719831\n",
      "  time_since_restore: 376.66589522361755\n",
      "  time_this_iter_s: 4.862748861312866\n",
      "  time_total_s: 376.66589522361755\n",
      "  timers:\n",
      "    learn_throughput: 930.462\n",
      "    learn_time_ms: 4298.942\n",
      "    sample_throughput: 6286.138\n",
      "    sample_time_ms: 636.321\n",
      "    update_time_ms: 3.157\n",
      "  timestamp: 1619703998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.499999999999988\n",
      "  episode_reward_mean: -25.769999999999992\n",
      "  episode_reward_min: -57.90000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2560\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5465443134307861\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011936025694012642\n",
      "          model: {}\n",
      "          policy_loss: -0.047641072422266006\n",
      "          total_loss: 30.858238220214844\n",
      "          vf_explained_var: 0.3304274082183838\n",
      "          vf_loss: 30.893795013427734\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5426801443099976\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014496563002467155\n",
      "          model: {}\n",
      "          policy_loss: -0.05256060138344765\n",
      "          total_loss: 13.71214485168457\n",
      "          vf_explained_var: 0.3531215786933899\n",
      "          vf_loss: 13.750028610229492\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.185714285714276\n",
      "    ram_util_percent: 75.71428571428571\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 5.0\n",
      "    pol2: 13.00000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -10.445\n",
      "    pol2: -15.32499999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -39.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.36467170080141725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20079326144835258\n",
      "    mean_inference_ms: 1.5387843229261646\n",
      "    mean_raw_obs_processing_ms: 1.5060636047367442\n",
      "  time_since_restore: 376.5172529220581\n",
      "  time_this_iter_s: 4.864873886108398\n",
      "  time_total_s: 376.5172529220581\n",
      "  timers:\n",
      "    learn_throughput: 935.81\n",
      "    learn_time_ms: 4274.373\n",
      "    sample_throughput: 5838.591\n",
      "    sample_time_ms: 685.097\n",
      "    update_time_ms: 2.882\n",
      "  timestamp: 1619703999\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00002:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-43\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.499999999999988\n",
      "  episode_reward_mean: -23.76899999999999\n",
      "  episode_reward_min: -42.00000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2600\n",
      "  experiment_id: e1a3447d40ef4fcb931f851e938075bc\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5403254628181458\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011607391759753227\n",
      "          model: {}\n",
      "          policy_loss: -0.03179967403411865\n",
      "          total_loss: 28.02826499938965\n",
      "          vf_explained_var: 0.3113558292388916\n",
      "          vf_loss: 28.04831314086914\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5573756098747253\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01633259654045105\n",
      "          model: {}\n",
      "          policy_loss: -0.058498285710811615\n",
      "          total_loss: 8.710810661315918\n",
      "          vf_explained_var: 0.3404330611228943\n",
      "          vf_loss: 8.752771377563477\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.81428571428571\n",
      "    ram_util_percent: 76.92857142857143\n",
      "  pid: 27752\n",
      "  policy_reward_max:\n",
      "    pol1: 5.5\n",
      "    pol2: 13.00000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -8.455\n",
      "    pol2: -15.31399999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -27.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.36351969772654086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20025039794374705\n",
      "    mean_inference_ms: 1.5342007014668797\n",
      "    mean_raw_obs_processing_ms: 1.5014870975067551\n",
      "  time_since_restore: 381.1233777999878\n",
      "  time_this_iter_s: 4.6061248779296875\n",
      "  time_total_s: 381.1233777999878\n",
      "  timers:\n",
      "    learn_throughput: 957.988\n",
      "    learn_time_ms: 4175.418\n",
      "    sample_throughput: 6031.21\n",
      "    sample_time_ms: 663.217\n",
      "    update_time_ms: 2.737\n",
      "  timestamp: 1619704003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: 6768d_00002\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.699999999999978\n",
      "  episode_reward_mean: -27.40199999999999\n",
      "  episode_reward_min: -61.20000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2640\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5401418209075928\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010674595832824707\n",
      "          model: {}\n",
      "          policy_loss: -0.03496692702174187\n",
      "          total_loss: 62.507328033447266\n",
      "          vf_explained_var: 0.14913532137870789\n",
      "          vf_loss: 62.53148651123047\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4488639235496521\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009517638012766838\n",
      "          model: {}\n",
      "          policy_loss: -0.03609462082386017\n",
      "          total_loss: 35.30914306640625\n",
      "          vf_explained_var: 0.24116191267967224\n",
      "          vf_loss: 35.33559799194336\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.583333333333336\n",
      "    ram_util_percent: 74.53333333333335\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -3.5\n",
      "    pol2: 34.99999999999993\n",
      "  policy_reward_mean:\n",
      "    pol1: -19.535\n",
      "    pol2: -7.866999999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -50.0\n",
      "    pol2: -18.899999999999963\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3745673350336805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2087729660678554\n",
      "    mean_inference_ms: 1.5931030653641374\n",
      "    mean_raw_obs_processing_ms: 1.558112442893312\n",
      "  time_since_restore: 385.6462917327881\n",
      "  time_this_iter_s: 3.974997043609619\n",
      "  time_total_s: 385.6462917327881\n",
      "  timers:\n",
      "    learn_throughput: 992.922\n",
      "    learn_time_ms: 4028.514\n",
      "    sample_throughput: 6380.244\n",
      "    sample_time_ms: 626.935\n",
      "    update_time_ms: 3.036\n",
      "  timestamp: 1619704007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING   </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         385.646</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\"> -27.402</td><td style=\"text-align: right;\">                -8.7</td><td style=\"text-align: right;\">               -61.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.288</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -31.209</td><td style=\"text-align: right;\">                 1.5</td><td style=\"text-align: right;\">               -60  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.123</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -23.769</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               -42  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -6.000000000000007\n",
      "  episode_reward_mean: -30.94199999999999\n",
      "  episode_reward_min: -60.00000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2640\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5442802309989929\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009576226584613323\n",
      "          model: {}\n",
      "          policy_loss: -0.04068347066640854\n",
      "          total_loss: 72.03518676757812\n",
      "          vf_explained_var: 0.18012464046478271\n",
      "          vf_loss: 72.06617736816406\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.504706084728241\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009034616872668266\n",
      "          model: {}\n",
      "          policy_loss: -0.025731975212693214\n",
      "          total_loss: 51.201087951660156\n",
      "          vf_explained_var: 0.32170405983924866\n",
      "          vf_loss: 51.217674255371094\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.82000000000001\n",
      "    ram_util_percent: 74.58\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 2.5\n",
      "    pol2: 42.7\n",
      "  policy_reward_mean:\n",
      "    pol1: -19.39\n",
      "    pol2: -11.551999999999978\n",
      "  policy_reward_min:\n",
      "    pol1: -59.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.36893743137598195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19981418839963375\n",
      "    mean_inference_ms: 1.555769761645128\n",
      "    mean_raw_obs_processing_ms: 1.5260289881193165\n",
      "  time_since_restore: 385.25390791893005\n",
      "  time_this_iter_s: 3.965569019317627\n",
      "  time_total_s: 385.25390791893005\n",
      "  timers:\n",
      "    learn_throughput: 987.199\n",
      "    learn_time_ms: 4051.869\n",
      "    sample_throughput: 6538.838\n",
      "    sample_time_ms: 611.729\n",
      "    update_time_ms: 3.024\n",
      "  timestamp: 1619704007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -9.599999999999987\n",
      "  episode_reward_mean: -26.08199999999999\n",
      "  episode_reward_min: -56.100000000000044\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2720\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5387532711029053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010056430473923683\n",
      "          model: {}\n",
      "          policy_loss: -0.040574412792921066\n",
      "          total_loss: 40.158905029296875\n",
      "          vf_explained_var: 0.1583099514245987\n",
      "          vf_loss: 40.18929672241211\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.43534067273139954\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01065453514456749\n",
      "          model: {}\n",
      "          policy_loss: -0.03657342121005058\n",
      "          total_loss: 15.601179122924805\n",
      "          vf_explained_var: 0.2523953914642334\n",
      "          vf_loss: 15.626964569091797\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.35\n",
      "    ram_util_percent: 74.96666666666667\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -2.5\n",
      "    pol2: 34.99999999999993\n",
      "  policy_reward_mean:\n",
      "    pol1: -17.665\n",
      "    pol2: -8.416999999999991\n",
      "  policy_reward_min:\n",
      "    pol1: -63.5\n",
      "    pol2: -18.899999999999963\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3708846297157687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20685055102392746\n",
      "    mean_inference_ms: 1.5772805489034523\n",
      "    mean_raw_obs_processing_ms: 1.5434829275379351\n",
      "  time_since_restore: 393.7086868286133\n",
      "  time_this_iter_s: 4.3527679443359375\n",
      "  time_total_s: 393.7086868286133\n",
      "  timers:\n",
      "    learn_throughput: 1038.546\n",
      "    learn_time_ms: 3851.539\n",
      "    sample_throughput: 7100.513\n",
      "    sample_time_ms: 563.34\n",
      "    update_time_ms: 2.717\n",
      "  timestamp: 1619704015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: 6768d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING   </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         393.709</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\"> -26.082</td><td style=\"text-align: right;\">                -9.6</td><td style=\"text-align: right;\">               -56.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         388.96 </td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\"> -31.242</td><td style=\"text-align: right;\">                -9.3</td><td style=\"text-align: right;\">               -64.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.123</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -23.769</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               -42  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-46-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -6.300000000000007\n",
      "  episode_reward_mean: -32.523\n",
      "  episode_reward_min: -64.50000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2720\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5339787602424622\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0100470669567585\n",
      "          model: {}\n",
      "          policy_loss: -0.02859780751168728\n",
      "          total_loss: 74.75786590576172\n",
      "          vf_explained_var: 0.14006027579307556\n",
      "          vf_loss: 74.77629089355469\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5012737512588501\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008438019081950188\n",
      "          model: {}\n",
      "          policy_loss: -0.02743980661034584\n",
      "          total_loss: 45.50981140136719\n",
      "          vf_explained_var: 0.23828566074371338\n",
      "          vf_loss: 45.528709411621094\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.61666666666667\n",
      "    ram_util_percent: 75.43333333333332\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 2.5\n",
      "    pol2: 64.69999999999995\n",
      "  policy_reward_mean:\n",
      "    pol1: -20.685\n",
      "    pol2: -11.837999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -71.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.36559940906669575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1981384739857181\n",
      "    mean_inference_ms: 1.5408837144860654\n",
      "    mean_raw_obs_processing_ms: 1.5124012126981523\n",
      "  time_since_restore: 393.40787410736084\n",
      "  time_this_iter_s: 4.447784185409546\n",
      "  time_total_s: 393.40787410736084\n",
      "  timers:\n",
      "    learn_throughput: 1030.573\n",
      "    learn_time_ms: 3881.337\n",
      "    sample_throughput: 7028.121\n",
      "    sample_time_ms: 569.142\n",
      "    update_time_ms: 2.736\n",
      "  timestamp: 1619704015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: 6768d_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING   </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         398.646</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\"> -25.575</td><td style=\"text-align: right;\">                -9.6</td><td style=\"text-align: right;\">               -56.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         398.253</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\"> -31.248</td><td style=\"text-align: right;\">                -6.3</td><td style=\"text-align: right;\">               -64.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.123</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -23.769</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               -42  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-47-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.199999999999989\n",
      "  episode_reward_mean: -26.249999999999986\n",
      "  episode_reward_min: -45.00000000000004\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2800\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5122224688529968\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00915178470313549\n",
      "          model: {}\n",
      "          policy_loss: -0.02914438769221306\n",
      "          total_loss: 49.6755485534668\n",
      "          vf_explained_var: 0.19187286496162415\n",
      "          vf_loss: 49.69542694091797\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.41618776321411133\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011289779096841812\n",
      "          model: {}\n",
      "          policy_loss: -0.048736706376075745\n",
      "          total_loss: 11.527568817138672\n",
      "          vf_explained_var: 0.2759339213371277\n",
      "          vf_loss: 11.564874649047852\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03333333333333\n",
      "    ram_util_percent: 47.48333333333333\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: 1.5\n",
      "    pol2: 26.20000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -17.085\n",
      "    pol2: -9.164999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -44.5\n",
      "    pol2: -18.899999999999967\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3690496975043327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20584975218784685\n",
      "    mean_inference_ms: 1.5713951882016237\n",
      "    mean_raw_obs_processing_ms: 1.5358116615102049\n",
      "  time_since_restore: 402.4117500782013\n",
      "  time_this_iter_s: 3.7654290199279785\n",
      "  time_total_s: 402.4117500782013\n",
      "  timers:\n",
      "    learn_throughput: 1097.595\n",
      "    learn_time_ms: 3644.333\n",
      "    sample_throughput: 6815.117\n",
      "    sample_time_ms: 586.93\n",
      "    update_time_ms: 2.536\n",
      "  timestamp: 1619704024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-47-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -6.300000000000007\n",
      "  episode_reward_mean: -31.067999999999998\n",
      "  episode_reward_min: -59.70000000000007\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2800\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5314479470252991\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010413099080324173\n",
      "          model: {}\n",
      "          policy_loss: -0.04564874246716499\n",
      "          total_loss: 44.451622009277344\n",
      "          vf_explained_var: 0.267170786857605\n",
      "          vf_loss: 44.48672866821289\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.5042107701301575\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011790778487920761\n",
      "          model: {}\n",
      "          policy_loss: -0.05499402433633804\n",
      "          total_loss: 16.503759384155273\n",
      "          vf_explained_var: 0.5044594407081604\n",
      "          vf_loss: 16.546815872192383\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.339999999999996\n",
      "    ram_util_percent: 47.120000000000005\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 6.5\n",
      "    pol2: 64.69999999999995\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.405\n",
      "    pol2: -12.662999999999977\n",
      "  policy_reward_min:\n",
      "    pol1: -71.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3639536566170484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19739111304277124\n",
      "    mean_inference_ms: 1.5359007225132475\n",
      "    mean_raw_obs_processing_ms: 1.5057246999318898\n",
      "  time_since_restore: 402.01405906677246\n",
      "  time_this_iter_s: 3.7607879638671875\n",
      "  time_total_s: 402.01405906677246\n",
      "  timers:\n",
      "    learn_throughput: 1094.089\n",
      "    learn_time_ms: 3656.009\n",
      "    sample_throughput: 6797.343\n",
      "    sample_time_ms: 588.465\n",
      "    update_time_ms: 2.656\n",
      "  timestamp: 1619704024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: 6768d_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING   </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         406.252</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\"> -27.183</td><td style=\"text-align: right;\">               -10.2</td><td style=\"text-align: right;\">               -51.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         402.014</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\"> -31.068</td><td style=\"text-align: right;\">                -6.3</td><td style=\"text-align: right;\">               -59.7</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.123</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -23.769</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               -42  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-47-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.500000000000004\n",
      "  episode_reward_mean: -26.132999999999985\n",
      "  episode_reward_min: -51.60000000000004\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2880\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5092606544494629\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010658515617251396\n",
      "          model: {}\n",
      "          policy_loss: -0.04323691874742508\n",
      "          total_loss: 49.281822204589844\n",
      "          vf_explained_var: 0.1927822232246399\n",
      "          vf_loss: 49.31426239013672\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.41845494508743286\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011739704757928848\n",
      "          model: {}\n",
      "          policy_loss: -0.03603582829236984\n",
      "          total_loss: 19.968786239624023\n",
      "          vf_explained_var: 0.21143785119056702\n",
      "          vf_loss: 19.992935180664062\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_agent_steps_trained: 576000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.18333333333333\n",
      "    ram_util_percent: 47.400000000000006\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: -1.5\n",
      "    pol2: 20.700000000000035\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.145\n",
      "    pol2: -7.987999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -43.0\n",
      "    pol2: -18.899999999999967\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3661065002752176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20426221144655443\n",
      "    mean_inference_ms: 1.5604870344733215\n",
      "    mean_raw_obs_processing_ms: 1.5232729294195777\n",
      "  time_since_restore: 410.0435872077942\n",
      "  time_this_iter_s: 3.791840076446533\n",
      "  time_total_s: 410.0435872077942\n",
      "  timers:\n",
      "    learn_throughput: 1142.799\n",
      "    learn_time_ms: 3500.18\n",
      "    sample_throughput: 7033.789\n",
      "    sample_time_ms: 568.683\n",
      "    update_time_ms: 2.512\n",
      "  timestamp: 1619704032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-47-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.799999999999997\n",
      "  episode_reward_mean: -30.117000000000004\n",
      "  episode_reward_min: -65.40000000000008\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2880\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5064994096755981\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011699149385094643\n",
      "          model: {}\n",
      "          policy_loss: -0.033681586384773254\n",
      "          total_loss: 55.422142028808594\n",
      "          vf_explained_var: 0.15486429631710052\n",
      "          vf_loss: 55.44397735595703\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.48042356967926025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011283906176686287\n",
      "          model: {}\n",
      "          policy_loss: -0.0330381914973259\n",
      "          total_loss: 21.0638484954834\n",
      "          vf_explained_var: 0.2914576530456543\n",
      "          vf_loss: 21.085460662841797\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_agent_steps_trained: 576000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.1\n",
      "    ram_util_percent: 47.379999999999995\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 7.0\n",
      "    pol2: 40.49999999999986\n",
      "  policy_reward_mean:\n",
      "    pol1: -15.705\n",
      "    pol2: -14.41199999999997\n",
      "  policy_reward_min:\n",
      "    pol1: -60.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3615228176967113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1962045269083079\n",
      "    mean_inference_ms: 1.5260630366382315\n",
      "    mean_raw_obs_processing_ms: 1.4950017353031841\n",
      "  time_since_restore: 409.6731939315796\n",
      "  time_this_iter_s: 3.807436943054199\n",
      "  time_total_s: 409.6731939315796\n",
      "  timers:\n",
      "    learn_throughput: 1137.439\n",
      "    learn_time_ms: 3516.671\n",
      "    sample_throughput: 7000.855\n",
      "    sample_time_ms: 571.359\n",
      "    update_time_ms: 2.467\n",
      "  timestamp: 1619704032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: 6768d_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>RUNNING   </td><td>192.168.0.100:27751</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         414.094</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\"> -25.521</td><td style=\"text-align: right;\">               -10.2</td><td style=\"text-align: right;\">               -51.6</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         409.673</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\"> -30.117</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               -65.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.123</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -23.769</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               -42  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00000:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-47-20\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.400000000000002\n",
      "  episode_reward_mean: -24.722999999999992\n",
      "  episode_reward_min: -47.400000000000006\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2960\n",
      "  experiment_id: ca7d5fe006ec4c509dd419cd353bfbf5\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.4993453323841095\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010554897598922253\n",
      "          model: {}\n",
      "          policy_loss: -0.03376566618680954\n",
      "          total_loss: 45.99901580810547\n",
      "          vf_explained_var: 0.15946485102176666\n",
      "          vf_loss: 46.0220947265625\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4197559058666229\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009408500045537949\n",
      "          model: {}\n",
      "          policy_loss: -0.0292044710367918\n",
      "          total_loss: 15.324899673461914\n",
      "          vf_explained_var: 0.28923729062080383\n",
      "          vf_loss: 15.34457778930664\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_agent_steps_trained: 592000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.51428571428572\n",
      "    ram_util_percent: 51.01428571428572\n",
      "  pid: 27751\n",
      "  policy_reward_max:\n",
      "    pol1: 4.5\n",
      "    pol2: 23.999999999999993\n",
      "  policy_reward_mean:\n",
      "    pol1: -16.075\n",
      "    pol2: -8.647999999999985\n",
      "  policy_reward_min:\n",
      "    pol1: -47.5\n",
      "    pol2: -18.899999999999963\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3629200910739799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20256538738926225\n",
      "    mean_inference_ms: 1.5482046315293028\n",
      "    mean_raw_obs_processing_ms: 1.509826056509432\n",
      "  time_since_restore: 418.4309129714966\n",
      "  time_this_iter_s: 4.336885929107666\n",
      "  time_total_s: 418.4309129714966\n",
      "  timers:\n",
      "    learn_throughput: 1171.845\n",
      "    learn_time_ms: 3413.42\n",
      "    sample_throughput: 7195.139\n",
      "    sample_time_ms: 555.931\n",
      "    update_time_ms: 2.338\n",
      "  timestamp: 1619704040\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: 6768d_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-47-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.499999999999997\n",
      "  episode_reward_mean: -28.69499999999999\n",
      "  episode_reward_min: -48.000000000000014\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 2960\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.5167256593704224\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006487528793513775\n",
      "          model: {}\n",
      "          policy_loss: -0.027374206110835075\n",
      "          total_loss: 42.41520690917969\n",
      "          vf_explained_var: 0.176407590508461\n",
      "          vf_loss: 42.43272399902344\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4870011806488037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013832150027155876\n",
      "          model: {}\n",
      "          policy_loss: -0.05571892112493515\n",
      "          total_loss: 12.31396484375\n",
      "          vf_explained_var: 0.24438877403736115\n",
      "          vf_loss: 12.35567855834961\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_agent_steps_trained: 592000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.050000000000004\n",
      "    ram_util_percent: 51.166666666666664\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 7.0\n",
      "    pol2: 157.10000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -18.21\n",
      "    pol2: -10.484999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -153.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.35886314392401575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19489009484689013\n",
      "    mean_inference_ms: 1.5151436066714856\n",
      "    mean_raw_obs_processing_ms: 1.483635890865922\n",
      "  time_since_restore: 418.134948015213\n",
      "  time_this_iter_s: 4.373969078063965\n",
      "  time_total_s: 418.134948015213\n",
      "  timers:\n",
      "    learn_throughput: 1166.782\n",
      "    learn_time_ms: 3428.233\n",
      "    sample_throughput: 7111.57\n",
      "    sample_time_ms: 562.464\n",
      "    update_time_ms: 2.255\n",
      "  timestamp: 1619704040\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: 6768d_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         421.671</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\"> -27.96 </td><td style=\"text-align: right;\">                 4.5</td><td style=\"text-align: right;\">               -48  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         418.431</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> -24.723</td><td style=\"text-align: right;\">                -8.4</td><td style=\"text-align: right;\">               -47.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.123</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -23.769</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               -42  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-47-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -6.600000000000004\n",
      "  episode_reward_mean: -27.716999999999988\n",
      "  episode_reward_min: -48.00000000000014\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 3040\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.4986693561077118\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0081246979534626\n",
      "          model: {}\n",
      "          policy_loss: -0.04725240170955658\n",
      "          total_loss: 45.27167510986328\n",
      "          vf_explained_var: 0.2570512890815735\n",
      "          vf_loss: 45.30658721923828\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.47247081995010376\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009746932424604893\n",
      "          model: {}\n",
      "          policy_loss: -0.03410228341817856\n",
      "          total_loss: 17.01066017150879\n",
      "          vf_explained_var: 0.4309450387954712\n",
      "          vf_loss: 17.034893035888672\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_agent_steps_trained: 608000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.72\n",
      "    ram_util_percent: 51.82000000000001\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 8.5\n",
      "    pol2: 39.399999999999864\n",
      "  policy_reward_mean:\n",
      "    pol1: -13.14\n",
      "    pol2: -14.576999999999973\n",
      "  policy_reward_min:\n",
      "    pol1: -46.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.356285223257033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19360330866462364\n",
      "    mean_inference_ms: 1.5055546981977768\n",
      "    mean_raw_obs_processing_ms: 1.4727219828848268\n",
      "  time_since_restore: 425.11490511894226\n",
      "  time_this_iter_s: 3.444211959838867\n",
      "  time_total_s: 425.11490511894226\n",
      "  timers:\n",
      "    learn_throughput: 1222.216\n",
      "    learn_time_ms: 3272.743\n",
      "    sample_throughput: 7247.089\n",
      "    sample_time_ms: 551.946\n",
      "    update_time_ms: 2.109\n",
      "  timestamp: 1619704047\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: 6768d_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         428.551</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\"> -27.498</td><td style=\"text-align: right;\">                -6.6</td><td style=\"text-align: right;\">               -49.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         418.431</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> -24.723</td><td style=\"text-align: right;\">                -8.4</td><td style=\"text-align: right;\">               -47.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.123</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -23.769</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               -42  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-47-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.799999999999994\n",
      "  episode_reward_mean: -27.311999999999994\n",
      "  episode_reward_min: -49.800000000000075\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 3120\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.4930303692817688\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007112347986549139\n",
      "          model: {}\n",
      "          policy_loss: -0.03969614952802658\n",
      "          total_loss: 48.887725830078125\n",
      "          vf_explained_var: 0.20681846141815186\n",
      "          vf_loss: 48.91661834716797\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4797918498516083\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011160362511873245\n",
      "          model: {}\n",
      "          policy_loss: -0.029196079820394516\n",
      "          total_loss: 12.854145050048828\n",
      "          vf_explained_var: 0.2986953854560852\n",
      "          vf_loss: 12.872041702270508\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_agent_steps_trained: 624000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.76\n",
      "    ram_util_percent: 57.779999999999994\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 7.0\n",
      "    pol2: 6.399999999999993\n",
      "  policy_reward_mean:\n",
      "    pol1: -12.57\n",
      "    pol2: -14.741999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -39.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.35368741286917454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1923068405282315\n",
      "    mean_inference_ms: 1.4951691013269244\n",
      "    mean_raw_obs_processing_ms: 1.4614827631819205\n",
      "  time_since_restore: 431.70752334594727\n",
      "  time_this_iter_s: 3.156316041946411\n",
      "  time_total_s: 431.70752334594727\n",
      "  timers:\n",
      "    learn_throughput: 1280.42\n",
      "    learn_time_ms: 3123.976\n",
      "    sample_throughput: 7195.237\n",
      "    sample_time_ms: 555.923\n",
      "    update_time_ms: 1.959\n",
      "  timestamp: 1619704054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: 6768d_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         434.823</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\"> -26.469</td><td style=\"text-align: right;\">                -8.7</td><td style=\"text-align: right;\">               -49.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         418.431</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> -24.723</td><td style=\"text-align: right;\">                -8.4</td><td style=\"text-align: right;\">               -47.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.123</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -23.769</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               -42  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-47-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.700000000000005\n",
      "  episode_reward_mean: -25.40999999999999\n",
      "  episode_reward_min: -45.60000000000002\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 3200\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.4889751076698303\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007557917386293411\n",
      "          model: {}\n",
      "          policy_loss: -0.04218336194753647\n",
      "          total_loss: 49.497039794921875\n",
      "          vf_explained_var: 0.2600170373916626\n",
      "          vf_loss: 49.527748107910156\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4711330533027649\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011126899160444736\n",
      "          model: {}\n",
      "          policy_loss: -0.047896549105644226\n",
      "          total_loss: 17.5264892578125\n",
      "          vf_explained_var: 0.339138388633728\n",
      "          vf_loss: 17.563119888305664\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_agent_steps_trained: 640000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.025\n",
      "    ram_util_percent: 57.1\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 8.5\n",
      "    pol2: 22.90000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: -11.735\n",
      "    pol2: -13.674999999999976\n",
      "  policy_reward_min:\n",
      "    pol1: -59.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.35056016836495035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19067484059731998\n",
      "    mean_inference_ms: 1.4825996059061957\n",
      "    mean_raw_obs_processing_ms: 1.4482518680245755\n",
      "  time_since_restore: 438.01373314857483\n",
      "  time_this_iter_s: 3.190643072128296\n",
      "  time_total_s: 438.01373314857483\n",
      "  timers:\n",
      "    learn_throughput: 1357.716\n",
      "    learn_time_ms: 2946.125\n",
      "    sample_throughput: 7861.421\n",
      "    sample_time_ms: 508.814\n",
      "    update_time_ms: 1.908\n",
      "  timestamp: 1619704060\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: 6768d_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         441.235</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\"> -25.314</td><td style=\"text-align: right;\">                -8.4</td><td style=\"text-align: right;\">               -49.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         418.431</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> -24.723</td><td style=\"text-align: right;\">                -8.4</td><td style=\"text-align: right;\">               -47.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.123</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -23.769</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               -42  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 656000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-47-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.399999999999997\n",
      "  episode_reward_mean: -25.712999999999997\n",
      "  episode_reward_min: -49.500000000000085\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 3280\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.4842988848686218\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0069571854546666145\n",
      "          model: {}\n",
      "          policy_loss: -0.033499155193567276\n",
      "          total_loss: 45.17038345336914\n",
      "          vf_explained_var: 0.20376478135585785\n",
      "          vf_loss: 45.19331359863281\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4602969288825989\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011538746766746044\n",
      "          model: {}\n",
      "          policy_loss: -0.050382815301418304\n",
      "          total_loss: 19.074901580810547\n",
      "          vf_explained_var: 0.26259511709213257\n",
      "          vf_loss: 19.113601684570312\n",
      "    num_agent_steps_sampled: 656000\n",
      "    num_agent_steps_trained: 656000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.675\n",
      "    ram_util_percent: 56.9\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 8.5\n",
      "    pol2: 25.099999999999998\n",
      "  policy_reward_mean:\n",
      "    pol1: -12.445\n",
      "    pol2: -13.267999999999974\n",
      "  policy_reward_min:\n",
      "    pol1: -49.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3475271048887357\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18896376841971446\n",
      "    mean_inference_ms: 1.4726161586371642\n",
      "    mean_raw_obs_processing_ms: 1.4339918462516936\n",
      "  time_since_restore: 444.412011384964\n",
      "  time_this_iter_s: 3.1770670413970947\n",
      "  time_total_s: 444.412011384964\n",
      "  timers:\n",
      "    learn_throughput: 1411.042\n",
      "    learn_time_ms: 2834.784\n",
      "    sample_throughput: 8024.865\n",
      "    sample_time_ms: 498.451\n",
      "    update_time_ms: 1.887\n",
      "  timestamp: 1619704067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: 6768d_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         447.581</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\"> -26.382</td><td style=\"text-align: right;\">                -8.4</td><td style=\"text-align: right;\">               -49.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         418.431</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> -24.723</td><td style=\"text-align: right;\">                -8.4</td><td style=\"text-align: right;\">               -47.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.123</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -23.769</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               -42  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 672000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-47-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -9.300000000000004\n",
      "  episode_reward_mean: -26.906999999999993\n",
      "  episode_reward_min: -64.50000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 3360\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.47307562828063965\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006755154579877853\n",
      "          model: {}\n",
      "          policy_loss: -0.03495004028081894\n",
      "          total_loss: 61.36048126220703\n",
      "          vf_explained_var: 0.20576593279838562\n",
      "          vf_loss: 61.38517379760742\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4345189929008484\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01025080680847168\n",
      "          model: {}\n",
      "          policy_loss: -0.03447359427809715\n",
      "          total_loss: 33.81359100341797\n",
      "          vf_explained_var: 0.2938723564147949\n",
      "          vf_loss: 33.837684631347656\n",
      "    num_agent_steps_sampled: 672000\n",
      "    num_agent_steps_trained: 672000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.025\n",
      "    ram_util_percent: 57.0\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 7.5\n",
      "    pol2: 37.199999999999854\n",
      "  policy_reward_mean:\n",
      "    pol1: -14.805\n",
      "    pol2: -12.101999999999975\n",
      "  policy_reward_min:\n",
      "    pol1: -50.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.34446647676752107\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18724218126257525\n",
      "    mean_inference_ms: 1.463045378642816\n",
      "    mean_raw_obs_processing_ms: 1.4194861220818025\n",
      "  time_since_restore: 450.75842928886414\n",
      "  time_this_iter_s: 3.1778149604797363\n",
      "  time_total_s: 450.75842928886414\n",
      "  timers:\n",
      "    learn_throughput: 1507.537\n",
      "    learn_time_ms: 2653.335\n",
      "    sample_throughput: 8367.303\n",
      "    sample_time_ms: 478.051\n",
      "    update_time_ms: 1.787\n",
      "  timestamp: 1619704073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: 6768d_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         453.952</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\"> -25.599</td><td style=\"text-align: right;\">                -3.3</td><td style=\"text-align: right;\">               -64.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         418.431</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> -24.723</td><td style=\"text-align: right;\">                -8.4</td><td style=\"text-align: right;\">               -47.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.123</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -23.769</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               -42  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 688000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-48-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.300000000000008\n",
      "  episode_reward_mean: -25.967999999999996\n",
      "  episode_reward_min: -64.50000000000009\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 3440\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.48155075311660767\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007136037107557058\n",
      "          model: {}\n",
      "          policy_loss: -0.03913995623588562\n",
      "          total_loss: 64.23258209228516\n",
      "          vf_explained_var: 0.20871126651763916\n",
      "          vf_loss: 64.26087951660156\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4466527998447418\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008977638557553291\n",
      "          model: {}\n",
      "          policy_loss: -0.03188735991716385\n",
      "          total_loss: 45.495323181152344\n",
      "          vf_explained_var: 0.2859770953655243\n",
      "          vf_loss: 45.51812744140625\n",
      "    num_agent_steps_sampled: 688000\n",
      "    num_agent_steps_trained: 688000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.440000000000005\n",
      "    ram_util_percent: 57.36\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 8.0\n",
      "    pol2: 90.00000000000001\n",
      "  policy_reward_mean:\n",
      "    pol1: -16.275\n",
      "    pol2: -9.692999999999977\n",
      "  policy_reward_min:\n",
      "    pol1: -105.0\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3416715911189091\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18555333518739495\n",
      "    mean_inference_ms: 1.4543220518641256\n",
      "    mean_raw_obs_processing_ms: 1.4054316450394548\n",
      "  time_since_restore: 457.1579113006592\n",
      "  time_this_iter_s: 3.206395149230957\n",
      "  time_total_s: 457.1579113006592\n",
      "  timers:\n",
      "    learn_throughput: 1529.426\n",
      "    learn_time_ms: 2615.36\n",
      "    sample_throughput: 8526.772\n",
      "    sample_time_ms: 469.111\n",
      "    update_time_ms: 1.795\n",
      "  timestamp: 1619704080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: 6768d_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>RUNNING   </td><td>192.168.0.100:27831</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         460.319</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\"> -26.067</td><td style=\"text-align: right;\">                -0.3</td><td style=\"text-align: right;\">               -59.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         418.431</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> -24.723</td><td style=\"text-align: right;\">                -8.4</td><td style=\"text-align: right;\">               -47.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.123</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -23.769</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               -42  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 704000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-48-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.29999999999999544\n",
      "  episode_reward_mean: -25.682999999999993\n",
      "  episode_reward_min: -59.10000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 3520\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.4750180244445801\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007025967352092266\n",
      "          model: {}\n",
      "          policy_loss: -0.026259679347276688\n",
      "          total_loss: 41.6136589050293\n",
      "          vf_explained_var: 0.23172453045845032\n",
      "          vf_loss: 41.629249572753906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.4473591446876526\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011299842968583107\n",
      "          model: {}\n",
      "          policy_loss: -0.03907177969813347\n",
      "          total_loss: 12.29029655456543\n",
      "          vf_explained_var: 0.2799800634384155\n",
      "          vf_loss: 12.317927360534668\n",
      "    num_agent_steps_sampled: 704000\n",
      "    num_agent_steps_trained: 704000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.1\n",
      "    ram_util_percent: 57.760000000000005\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 8.0\n",
      "    pol2: 41.59999999999998\n",
      "  policy_reward_mean:\n",
      "    pol1: -12.745\n",
      "    pol2: -12.937999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -65.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.33879016195782563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18395666569155075\n",
      "    mean_inference_ms: 1.445428472870825\n",
      "    mean_raw_obs_processing_ms: 1.3923802084705736\n",
      "  time_since_restore: 463.6607689857483\n",
      "  time_this_iter_s: 3.342007875442505\n",
      "  time_total_s: 463.6607689857483\n",
      "  timers:\n",
      "    learn_throughput: 1529.597\n",
      "    learn_time_ms: 2615.067\n",
      "    sample_throughput: 8727.316\n",
      "    sample_time_ms: 458.331\n",
      "    update_time_ms: 1.859\n",
      "  timestamp: 1619704086\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: 6768d_00003\n",
      "  \n",
      "Result for PPO_MultiAgentArena_6768d_00003:\n",
      "  agent_timesteps_total: 712000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-29_15-48-10\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.29999999999999544\n",
      "  episode_reward_mean: -24.77999999999999\n",
      "  episode_reward_min: -55.200000000000045\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 3560\n",
      "  experiment_id: 57a3943dc3314ef292b59a6778e27a63\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.47325414419174194\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006966812536120415\n",
      "          model: {}\n",
      "          policy_loss: -0.032126251608133316\n",
      "          total_loss: 36.81022644042969\n",
      "          vf_explained_var: 0.25519198179244995\n",
      "          vf_loss: 36.83177185058594\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.43417155742645264\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013051265850663185\n",
      "          model: {}\n",
      "          policy_loss: -0.04770105332136154\n",
      "          total_loss: 14.553186416625977\n",
      "          vf_explained_var: 0.2584596276283264\n",
      "          vf_loss: 14.58767318725586\n",
      "    num_agent_steps_sampled: 712000\n",
      "    num_agent_steps_trained: 712000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.919999999999998\n",
      "    ram_util_percent: 58.1\n",
      "  pid: 27831\n",
      "  policy_reward_max:\n",
      "    pol1: 8.5\n",
      "    pol2: 26.200000000000024\n",
      "  policy_reward_mean:\n",
      "    pol1: -11.105\n",
      "    pol2: -13.674999999999972\n",
      "  policy_reward_min:\n",
      "    pol1: -65.5\n",
      "    pol2: -19.99999999999996\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.33752416516652173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18331974439138043\n",
      "    mean_inference_ms: 1.4407267513663902\n",
      "    mean_raw_obs_processing_ms: 1.3870134390682016\n",
      "  time_since_restore: 466.94366097450256\n",
      "  time_this_iter_s: 3.2828919887542725\n",
      "  time_total_s: 466.94366097450256\n",
      "  timers:\n",
      "    learn_throughput: 1522.061\n",
      "    learn_time_ms: 2628.016\n",
      "    sample_throughput: 8661.459\n",
      "    sample_time_ms: 461.816\n",
      "    update_time_ms: 1.857\n",
      "  timestamp: 1619704090\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: 6768d_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         418.431</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> -24.723</td><td style=\"text-align: right;\">                -8.4</td><td style=\"text-align: right;\">               -47.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.123</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -23.769</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               -42  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         466.944</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\"> -24.78 </td><td style=\"text-align: right;\">                -0.3</td><td style=\"text-align: right;\">               -55.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.54 GiB heap, 0.0/2.27 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         418.431</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> -24.723</td><td style=\"text-align: right;\">                -8.4</td><td style=\"text-align: right;\">               -47.4</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         341.737</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -24.663</td><td style=\"text-align: right;\">                -6.9</td><td style=\"text-align: right;\">               -47.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         381.123</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -23.769</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">               -42  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_6768d_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         466.944</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\"> -24.78 </td><td style=\"text-align: right;\">                -0.3</td><td style=\"text-align: right;\">               -55.2</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Exception ignored in: <function Connection.__del__ at 0x7fe49c83e940>\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/redis/connection.py\", line 543, in __del__\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     try:\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1069, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     if worker.core_worker.current_actor_is_asyncio():\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"python/ray/_raylet.pyx\", line 556, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/utils.py\", line 102, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/utils.py\", line 102, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Exception ignored in: 'ray._raylet.task_execution_handler'\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/utils.py\", line 102, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Exception ignored in: <function Connection.__del__ at 0x7fe49c83e940>\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/redis/connection.py\", line 543, in __del__\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     try:\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1069, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     if worker.core_worker.current_actor_is_asyncio():\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"python/ray/_raylet.pyx\", line 556, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/utils.py\", line 102, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/utils.py\", line 102, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Exception ignored in: 'ray._raylet.task_execution_handler'\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/utils.py\", line 102, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=27831)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m 2021-04-29 15:48:10,692\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1077, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/traceback.py\", line 120, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m     return list(TracebackException(\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m 2021-04-29 15:48:10,692\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1077, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/traceback.py\", line 120, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m     return list(TracebackException(\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=27840)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m 2021-04-29 15:48:10,689\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1062, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m     ray.worker.disconnect()\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 1373, in disconnect\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m     ray_actor.ActorClassMethodMetadata.reset_cache()\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 196, in reset_cache\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m     cls._cache.clear()\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m 2021-04-29 15:48:10,689\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 1062, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m     ray.worker.disconnect()\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 1373, in disconnect\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m     ray_actor.ActorClassMethodMetadata.reset_cache()\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/actor.py\", line 196, in reset_cache\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m     cls._cache.clear()\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/ray_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=27838)\u001b[0m SystemExit: 1\n",
      "2021-04-29 15:48:10,797\tINFO tune.py:549 -- Total run time: 486.25 seconds (485.58 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7ffc08b49280>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution to Exercise #2:\n",
    "\n",
    "# Update our config and set it up for 2x tune grid-searches (leading to 4 parallel trials in total).\n",
    "config.update({\n",
    "    \"lr\": tune.grid_search([0.0001, 0.0005]),\n",
    "    \"train_batch_size\": tune.grid_search([2000, 3000]),\n",
    "    \"num_envs_per_worker\": 10,\n",
    "    # Change our model to be simpler.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [128, 128],\n",
    "    },\n",
    "})\n",
    "\n",
    "# Run the experiment.\n",
    "tune.run(\"PPO\", config=config, stop={\"episode_reward_mean\": -25.0, \"training_iteration\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Infinite laptop:\n",
    "\n",
    "# NOTE: The following cell will only work if you are already on-boarded to our Anyscale Inc. \"Infinite Laptop\".\n",
    "# To get more information, see https://www.anyscale.com/product\n",
    "\n",
    "# Let's quickly divert from our MultiAgentArena and move to something much heavier in terms of environment/simulator complexity.\n",
    "# We will now demonstrate, how you can use Anyscale's infinite laptop to launch an RLlib experiment on a cloud 4 GPU + 32 CPU machine\n",
    "# all from within this Jupyter cell here.\n",
    "# Start an experiment in the cloud using Anyscale's product, RLlib, and a more complex multi-agent env.\n",
    "\n",
    "# NOTE \n",
    "import anyscale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Custom Neural Network Models.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                input_space,\n",
    "                action_space,\n",
    "                num_outputs,\n",
    "                name=\"\",\n",
    "                *,\n",
    "                layers = (256, 256)):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense_layers = []\n",
    "        for i, layer_size in enumerate(layers):\n",
    "            self.dense_layers.append(tf.keras.layers.Dense(\n",
    "                layer_size, activation=tf.nn.relu, name=f\"dense_{i}\"))\n",
    "\n",
    "        self.logits = tf.keras.layers.Dense(\n",
    "            num_outputs,\n",
    "            activation=tf.keras.activations.linear,\n",
    "            name=\"logits\")\n",
    "        self.values = tf.keras.layers.Dense(\n",
    "            1, activation=None, name=\"values\")\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # Standardized input args:\n",
    "        # - input_dict (RLlib `SampleBatch` object, which is basically a dict with numpy arrays\n",
    "        # in it)\n",
    "        out = inputs[\"obs\"]\n",
    "        for l in self.dense_layers:\n",
    "            out = l(out)\n",
    "        logits = self.logits(out)\n",
    "        values = self.values(out)\n",
    "\n",
    "        # Standardized output:\n",
    "        # - \"normal\" model output tensor (e.g. action logits).\n",
    "        # - list of internal state outputs (only needed for RNN-/memory enhanced models).\n",
    "        # - \"extra outs\", such as model's side branches, e.g. value function outputs.\n",
    "        return logits, [], {\"vf_preds\": tf.reshape(values, [-1])}\n",
    "\n",
    "# Do a quick test.\n",
    "from gym.spaces import Box\n",
    "test_model = MyModel(\n",
    "    input_space=Box(-1.0, 1.0, (2, )),\n",
    "    action_space=None,\n",
    "    num_outputs=2,\n",
    ")\n",
    "test_model({\"obs\": np.array([[0.5, 0.5]])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Hacking in\": How do we customize our RL loop?\n",
    "# RLlib offers a callbacks API that allows you to add custom behavior at\n",
    "# all major events during the environment sampling and learning process.\n",
    "\n",
    "# Our problem: So far, we can only see the total reward (sum for both agents).\n",
    "# This does not give us enough insights into the question of which agent\n",
    "# learns what (maybe agent2 doesn't learn anything and the reward we are observing\n",
    "# is mostly due to agent1's progress in covering the map!).\n",
    "# The following custom callbacks class allows us to add each agents single reward to\n",
    "# the returned metrics, which will then be displayed in tensorboard.\n",
    "\n",
    "# We will override RLlib's DefaultCallbacks class and implement the\n",
    "# `on_episode_step` and `on_episode_end` methods therein.\n",
    "\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(self, *, worker, base_env,\n",
    "                         policies, episode,\n",
    "                         env_index, **kwargs):\n",
    "        episode.user_data[\"agent1_rewards\"] = []\n",
    "        episode.user_data[\"agent2_rewards\"] = []\n",
    "\n",
    "    def on_episode_step(self, *, worker, base_env,\n",
    "                        episode, env_index, **kwargs):\n",
    "        # Make sure this episode is ongoing.\n",
    "        assert episode.length > 0, \\\n",
    "            \"ERROR: `on_episode_step()` callback should not be called right \" \\\n",
    "            \"after env reset!\"\n",
    "        ag1_r = episode.prev_reward_for(\"agent1\")\n",
    "        ag2_r = episode.prev_reward_for(\"agent2\")\n",
    "        #print(\"ag1_r={} ag2_r={}\".format(ag1_r, ag2_r))\n",
    "        episode.user_data[\"agent1_rewards\"].append(ag1_r)\n",
    "        episode.user_data[\"agent2_rewards\"].append(ag2_r)\n",
    "\n",
    "    def on_episode_end(self, *, worker, base_env,\n",
    "                       policies, episode,\n",
    "                       env_index, **kwargs):\n",
    "        episode.custom_metrics[\"ag1_R\"] = sum(episode.user_data[\"agent1_rewards\"])\n",
    "        episode.custom_metrics[\"ag2_R\"] = sum(episode.user_data[\"agent2_rewards\"])\n",
    "        episode.hist_data[\"agent1_rewards\"] = episode.user_data[\"agent1_rewards\"]\n",
    "        episode.hist_data[\"agent2_rewards\"] = episode.user_data[\"agent2_rewards\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2fe8eb-c52f-4a26-9067-96ad9fe160a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our config to point to our new custom callbacks class:\n",
    "config.update({\n",
    "    \"env\": MultiAgentArena,  # force \"reload\"\n",
    "    \"callbacks\": MyCallbacks,  # by default, this would point to `rllib.agents.callbacks.DefaultCallbacks`, which does nothing.\n",
    "    # Revert these to single trials.\n",
    "    \"lr\": 0.0001,\n",
    "    \"train_batch_size\": 4000,\n",
    "})\n",
    "\n",
    "tune.run(\"PPO\", config=config, stop={\"training_iteration\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b387c93-219e-4a9b-9e90-729198e56b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise #3:\n",
    "# ============\n",
    "# The episode mean rewards reported to us thus far were always the sum\n",
    "# of both agents, which doesn't seem to make too much sense given that\n",
    "# the agents are adversarial.\n",
    "# Instead, we would like to know, what the individual agents' rewards are in\n",
    "# our environment.\n",
    "# Write your own custom callback class (sub-class\n",
    "# ray.rllib.agents.callback::DefaultCallbacks) and override one or more methods\n",
    "# therein to manipulate and collect the following data:\n",
    "\n",
    "#TODO\n",
    "\n",
    "# a) Extract each agent's individual rewards from ...\n",
    "# b) Store each agents reward under the new \"reward_agent1\" and\n",
    "#    \"reward_agent2\" keys in the custom metrics.\n",
    "# c) Run a simple experiment and confirm that you are seeing these two new stats\n",
    "#    in the tensorboard output.\n",
    "# Good luck! :)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
