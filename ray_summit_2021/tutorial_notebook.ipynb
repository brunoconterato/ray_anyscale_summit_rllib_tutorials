{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "# Hands-on RL with Ray’s RLlib\n",
    "## A beginner’s tutorial for working with multi-agent environments, models, and algorithms\n",
    "\n",
    "<img src=\"images/pitfall.jpg\" width=250> <img src=\"images/tesla.jpg\" width=254> <img src=\"images/forklifts.jpg\" width=169> <img src=\"images/robots.jpg\" width=252> <img src=\"images/dota2.jpg\" width=213>\n",
    "\n",
    "### Overview\n",
    "“Hands-on RL with Ray’s RLlib” is a beginners tutorial for working with reinforcement learning (RL) environments, models, and algorithms using Ray’s RLlib library. RLlib offers high scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL) before proceeding to RLlib (multi- and single-agent) environments, neural network models, hyperparameter tuning, debugging, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo.\n",
    "\n",
    "### Intended Audience\n",
    "* Python programmers who want to get started with reinforcement learning and RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac and Linux):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install ray[rllib]\n",
    "$ pip install [tensorflow|torch]  # <- either one works!\n",
    "$ pip install jupyterlab\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib python=3.8\n",
    "$ conda activate rllib\n",
    "$ pip install ray[rllib]\n",
    "$ pip install [tensorflow|torch]  # <- either one works!\n",
    "$ pip install jupyterlab\n",
    "$ conda install pywin32\n",
    "```\n",
    "\n",
    "Also, for Win10 Atari support, we have to install atari_py from a different source (gym does not support Atari envs on Windows).\n",
    "\n",
    "```\n",
    "$ pip install git+https://github.com/Kojoley/atari-py.git\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and why RLlib?\n",
    "* Core concepts of RLlib: Environments, Trainers, Policies, and Models.\n",
    "* How to configure, hyperparameter-tune, and parallelize RLlib.\n",
    "* RLlib debugging best practices.\n",
    "\n",
    "### Tutorial Outline\n",
    "1. RL and RLlib in a nutshell.\n",
    "1. Defining an RL-solvable problem: Our first environment.\n",
    "1. **Exercise No.1**: Environment loop.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. Picking an algorithm and training our first RLlib Trainer.\n",
    "1. Configurations and hyperparameters - Easy tuning with Ray Tune.\n",
    "1. Fixing our experiment's config - Going multi-agent.\n",
    "1. The \"infinite laptop\": Quick intro into how to use RLlib with Anyscale's product.\n",
    "1. **Exercise No.2**: Run your own Ray RLlib+Tune experiment)\n",
    "1. Neural network models - Provide your custom models using tf.keras or torch.nn.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. Deeper dive into RLlib's parallelization architecture.\n",
    "1. Specifying different compute resources and parallelization options through our config.\n",
    "1. \"Hacking in\": Using callbacks to customize the RL loop and generate our own metrics.\n",
    "1. **Exercise No.3**: Write your own custom callback.\n",
    "1. \"Hacking in (part II)\" - Debugging with RLlib and PyCharm.\n",
    "1. Checking on the \"infinite laptop\" - Did RLlib learn to solve the problem?\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-yorkshire",
   "metadata": {},
   "source": [
    "<img src=\"images/rl-cycle.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62744730",
   "metadata": {},
   "source": [
    "### Coding/defining our \"problem\" via an RL environment.\n",
    "\n",
    "We will use the following (adversarial) multi-agent environment\n",
    "throughout this tutorial to demonstrate a large fraction of RLlib's\n",
    "APIs, features, and customization options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb35116-efda-4799-8bae-e96d7775a0d1",
   "metadata": {},
   "source": [
    "<img src=\"images/environment.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe753-d7e0-4de1-b937-160507f75ed8",
   "metadata": {},
   "source": [
    "### A word or two on Spaces:\n",
    "\n",
    "Spaces are used in ML to describe what possible/valid values inputs and outputs of a neural network can have.\n",
    "\n",
    "RL environments also use them to describe what their valid observations and actions are.\n",
    "\n",
    "Spaces are usually defined by their shape (e.g. 84x84x3 RGB images) and datatype (e.g. uint8 for RGB values between 0 and 255).\n",
    "However, spaces could also be composed of other spaces (see Tuple or Dict spaces) or could be simply discrete with n fixed possible values\n",
    "(represented by integers). For example, in our game, where each agent can only go up/down/left/right, the action space would be \"Discrete(4)\"\n",
    "(no datatype, no shape needs to be defined here)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e4135-98ed-4e65-9e26-66f340747529",
   "metadata": {},
   "source": [
    "<img src=\"images/spaces.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "871a3661-2d74-4a50-b4ef-a89c27d978f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MultiAgentArena at 0x7f9375787670>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's code (parts of) our multi-agent environment.\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "class MultiAgentArena(MultiAgentEnv):\n",
    "    def __init__(self, config=None):\n",
    "        # !LIVE CODING!\n",
    "        #from environment import _init\n",
    "        #_init(self, config)\n",
    "        \n",
    "        config = config or {}\n",
    "        self.height = config.get(\"height\", 10)\n",
    "        self.width = config.get(\"width\", 10)\n",
    "\n",
    "        self.observation_space = gym.spaces.MultiDiscrete([self.height*self.width, self.height*self.width])\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        \n",
    "        self.max_timesteps = config.get(\"max_timesteps\", 100)\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):  # returns initial observation of next(!) episode\n",
    "        # !LIVE CODING!\n",
    "        #from environment import _reset\n",
    "        #return _reset(self)\n",
    "        self.ts = 0\n",
    "        self.agent1_pos = [0, 0]\n",
    "        # Row major.\n",
    "        self.agent2_pos = [self.height - 1, self.width - 1]  # [9, 9]\n",
    "        \n",
    "        self.agent1_visited_states = set()\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: dict):  # returns obs, rewards, dones, infos.\n",
    "        # !LIVE CODING!\n",
    "        #from environment import _step\n",
    "        #return _step(action)\n",
    "        self.ts += 1\n",
    "        \n",
    "        # Determine, which agent moves first.\n",
    "        if random.random() > 0.5:\n",
    "            # Possible events: new_field|collision\n",
    "            events = self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "            events |= self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        else:\n",
    "            events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "            events |= self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "\n",
    "        # Reward function:\n",
    "        if \"collision\" in events:\n",
    "            r1 = -1.0\n",
    "            r2 = 1.0\n",
    "        elif \"new_field\" in events:\n",
    "            r1 = 1.0\n",
    "            r2 = -0.1\n",
    "        else:\n",
    "            r1 = -0.5\n",
    "            r2 = -0.1\n",
    "            \n",
    "        done = self.ts >= self.max_timesteps\n",
    "        \n",
    "        rewards = {\"agent1\": r1, \"agent2\": r2}\n",
    "        dones = {\"agent1\": done, \"agent2\": done, \"__all__\": done}\n",
    "        \n",
    "        return self._get_obs(), rewards, dones, {}  # <- info dict (not used here)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns obs dict (agent name to discrete-pos tuple) using each\n",
    "        agent's current x/y-positions.\n",
    "        \"\"\"\n",
    "        ag1_discrete_pos = self.agent1_pos[0] * self.width + \\\n",
    "            (self.agent1_pos[1] % self.width)\n",
    "        ag2_discrete_pos = self.agent2_pos[0] * self.width + \\\n",
    "            (self.agent2_pos[1] % self.width)\n",
    "        return {\n",
    "            \"agent1\": np.array([ag1_discrete_pos, ag2_discrete_pos]),\n",
    "            \"agent2\": np.array([ag2_discrete_pos, ag1_discrete_pos]),\n",
    "        }\n",
    "\n",
    "    def _move(self, coords, action, is_agent1):\n",
    "        \"\"\"\n",
    "        Moves an agent (agent1 iff is_agent1=True, else agent2) from `coords` (x/y) using the\n",
    "        given action (0=up, 1=right, etc..) and returns a resulting events dict:\n",
    "        Agent1: \"new\" when entering a new field. \"bumped\" when having been bumped into by agent2.\n",
    "        Agent2: \"bumped\" when bumping into agent1 (agent1 then gets -1.0).\n",
    "        \"\"\"\n",
    "        orig_coords = coords[:]\n",
    "        # Change the row: 0=up (-1), 2=down (+1)\n",
    "        coords[0] += -1 if action == 0 else 1 if action == 2 else 0\n",
    "        # Change the column: 1=right (+1), 3=left (-1)\n",
    "        coords[1] += 1 if action == 1 else -1 if action == 3 else 0\n",
    "\n",
    "        # Solve collisions.\n",
    "        # Make sure, we don't end up on the other agent's position.\n",
    "        # If yes, don't move (we are blocked).\n",
    "        if (is_agent1 and coords == self.agent2_pos) or (not is_agent1 and coords == self.agent1_pos):\n",
    "            coords[0], coords[1] = orig_coords\n",
    "            # Agent2 blocked agent1 (agent1 tried to run into agent2)\n",
    "            # OR Agent2 bumped into agent1 (agent2 tried to run into agent1)\n",
    "            return {\"collision\"}\n",
    "\n",
    "        # No agent blocking -> check walls.\n",
    "        if coords[0] < 0:\n",
    "            coords[0] = 0\n",
    "        elif coords[0] >= self.height:\n",
    "            coords[0] = self.height - 1\n",
    "        if coords[1] < 0:\n",
    "            coords[1] = 0\n",
    "        elif coords[1] >= self.width:\n",
    "            coords[1] = self.width - 1\n",
    "\n",
    "        # If agent1 -> \"new\" if new tile covered.\n",
    "        if is_agent1 and not tuple(coords) in self.agent1_visited_states:\n",
    "            self.agent1_visited_states.add(tuple(coords))\n",
    "            return {\"new_field\"}\n",
    "        # No new tile for agent1.\n",
    "        return set()\n",
    "\n",
    "    # Optionally: Add `render` method returning some img.\n",
    "    def render(self, mode=None):\n",
    "        field_size = 40\n",
    "\n",
    "        if not hasattr(self, \"viewer\"):\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(400, 400)\n",
    "            self.fields = {}\n",
    "            # Add our grid, and the two agents to the viewer.\n",
    "            for i in range(self.width):\n",
    "                l = i * field_size\n",
    "                r = l + field_size\n",
    "                for j in range(self.height):\n",
    "                    b = 400 - j * field_size - field_size\n",
    "                    t = b + field_size\n",
    "                    field = rendering.PolyLine([(l, b), (l, t), (r, t), (r, b)], close=True)\n",
    "                    field.set_color(.0, .0, .0)\n",
    "                    field.set_linewidth(1.0)\n",
    "                    self.fields[(j, i)] = field\n",
    "                    self.viewer.add_geom(field)\n",
    "            \n",
    "            agent1 = rendering.make_circle(radius=field_size // 2 - 4)\n",
    "            agent1.set_color(.0, 0.8, 0.1)\n",
    "            self.agent1_trans = rendering.Transform()\n",
    "            agent1.add_attr(self.agent1_trans)\n",
    "            agent2 = rendering.make_circle(radius=field_size // 2 - 4)\n",
    "            agent2.set_color(.5, 0.1, 0.1)\n",
    "            self.agent2_trans = rendering.Transform()\n",
    "            agent2.add_attr(self.agent2_trans)\n",
    "            self.viewer.add_geom(agent1)\n",
    "            self.viewer.add_geom(agent2)\n",
    "\n",
    "        # Mark those fields green that have been covered by agent1,\n",
    "        # all others black.\n",
    "        for i in range(self.width):\n",
    "            for j in range(self.height):\n",
    "                self.fields[(j, i)].set_color(.0, .0, .0)\n",
    "                self.fields[(j, i)].set_linewidth(1.0)\n",
    "        for (j, i) in self.agent1_visited_states:\n",
    "            self.fields[(j, i)].set_color(.1, .5, .1)\n",
    "            self.fields[(j, i)].set_linewidth(5.0)\n",
    "        \n",
    "        # Edit the pole polygon vertex\n",
    "        self.agent1_trans.set_translation(self.agent1_pos[1] * field_size + field_size / 2, 400 - (self.agent1_pos[0] * field_size + field_size / 2))\n",
    "        self.agent2_trans.set_translation(self.agent2_pos[1] * field_size + field_size / 2, 400 - (self.agent2_pos[0] * field_size + field_size / 2))\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
    "\n",
    "dummy_env = MultiAgentArena()\n",
    "dummy_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-sussex",
   "metadata": {},
   "source": [
    "## Exercise No 1\n",
    "\n",
    "<hr />\n",
    "\n",
    "Write an \"environment loop\" using our `MultiAgentArena` class.\n",
    "\n",
    "1. Create an env object.\n",
    "1. `reset` your environment to get the first (initial) observation.\n",
    "1. `step` through the environment using a provided\n",
    "   \"DummyTrainer.compute_action([obs])\" method to compute action dicts (see cell below, in which you can create a DummyTrainer object and query it for random actions).\n",
    "1. When an episode is done, remember to `reset()` your environment before the next call to `step()`.\n",
    "1. If you feel, this is way too easy for you ;) , try to extract each agent's reward, sum it up over one episode and - at the end of an episode (when done=True) - print out each agent's accumulated reward (also called \"return\").\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action=3\n",
      "action=0\n",
      "action=2\n",
      "action=0\n",
      "action=0\n"
     ]
    }
   ],
   "source": [
    "class DummyTrainer:\n",
    "    \"\"\"Dummy Trainer class used in Exercise #1.\n",
    "\n",
    "    Use its `compute_action` method to get a new action, given some environment\n",
    "    observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_action(self, obs=None):\n",
    "        # Returns a random action for a single agent.\n",
    "        return np.random.randint(4)  # Discrete(4) -> return rand int between 0 and 3 (incl. 3).\n",
    "\n",
    "dummy_trainer = DummyTrainer()\n",
    "# Check, whether it's working.\n",
    "for _ in range(5):\n",
    "    print(\"action={}\".format(dummy_trainer.compute_action(np.array([0, 10]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "liable-district",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.999999999999975\n",
      "4.500000000000026\n",
      "-4.5\n",
      "-22.500000000000025\n",
      "-11.999999999999984\n"
     ]
    }
   ],
   "source": [
    "# Solution to Exercise #1\n",
    "# !LIVE CODING!\n",
    "\n",
    "import time\n",
    "\n",
    "env = MultiAgentArena()\n",
    "num_episodes = 0\n",
    "episode_return = 0.0\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "while num_episodes < 5:\n",
    "    action1 = dummy_trainer.compute_action(obs[\"agent1\"])\n",
    "    action2 = dummy_trainer.compute_action(obs[\"agent2\"])\n",
    "\n",
    "    obs, reward, done, _ = env.step({\"agent1\": action1, \"agent2\": action2})\n",
    "    episode_return += reward[\"agent1\"] + reward[\"agent2\"]\n",
    "\n",
    "    if done[\"agent1\"] and done[\"agent2\"]:\n",
    "        num_episodes += 1\n",
    "        print(episode_return)\n",
    "        episode_return = 0.0\n",
    "        obs = env.reset()\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.02)\n",
    "\n",
    "env.viewer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49af8d95-6501-4a6d-b6fa-d823fce28921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-25 15:01:13,930\tINFO services.py:1262 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.102',\n",
       " 'raylet_ip_address': '192.168.0.102',\n",
       " 'redis_address': '192.168.0.102:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-05-25_15-01-11_922474_33329/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-05-25_15-01-11_922474_33329/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-05-25_15-01-11_922474_33329',\n",
       " 'metrics_export_port': 65503,\n",
       " 'node_id': '15b74b842c7a56a3a4e799801a9ffd44b5bdf841fede0758bd57afdb'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now for something completely different:\n",
    "# Plugging in RLlib!\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "# Start a new instance of Ray or connect to an already running one.\n",
    "ray.init()  # Hear the engine humming? ;)\n",
    "\n",
    "# In case you encounter the following error during our tutorial:\n",
    "# RuntimeError: Maybe you called ray.init twice by accident?\n",
    "# Try: ray.shutdown() or ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_algos.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bcc1116-a14c-4479-87c0-6ece58ab0464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-25 15:01:15,742\tINFO trainer.py:666 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-05-25 15:01:15,744\tINFO trainer.py:691 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-05-25 15:01:24,507\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "# We use the PPO algorithm here b/c its very flexible wrt its supported\n",
    "# action spaces and model types and b/c it learns well almost any problem.\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "# Specify a very simple config, defining our environment and some environment\n",
    "# options (see environment.py).\n",
    "config = {\n",
    "    \"env\": MultiAgentArena,  # \"my_env\" <- if we previously have registered the env with `tune.register_env(\"[name]\", lambda config: [returns env object])`.\n",
    "    \"env_config\": {\n",
    "        \"config\": {\n",
    "            \"width\": 10,\n",
    "            \"height\": 10,\n",
    "        },\n",
    "    },\n",
    "    # \"framework\": \"torch\",  # If users have chosen to install torch instead of tf.\n",
    "    \"create_env_on_driver\": True,\n",
    "}\n",
    "# Instantiate the Trainer object using above config.\n",
    "rllib_trainer = PPOTrainer(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spectacular-guard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'episode_reward_max': 14.100000000000016, 'episode_reward_min': -31.500000000000036, 'episode_reward_mean': -6.209999999999995, 'episode_len_mean': 100.0, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [14.100000000000016, -19.500000000000004, 12.00000000000003, -31.500000000000036, -14.999999999999993, -24.0, 0.9000000000000111, 4.200000000000031, -4.499999999999989, -5.999999999999979, -5.999999999999993, 6.000000000000012, -31.50000000000003, 12.600000000000017, 3.00000000000001, 4.500000000000007, -5.999999999999979, 4.5000000000000036, -27.000000000000036, -14.99999999999999], 'episode_lengths': [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.13066910125397063, 'mean_inference_ms': 0.5442456646518157, 'mean_action_processing_ms': 0.04946363793981897, 'mean_env_wait_ms': 0.0270069181383192, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_healthy_workers': 2, 'timesteps_total': 4000, 'agent_timesteps_total': 4000, 'timers': {'sample_time_ms': 778.726, 'sample_throughput': 5136.593, 'load_time_ms': 34.228, 'load_throughput': 116862.277, 'learn_time_ms': 2241.368, 'learn_throughput': 1784.624, 'update_time_ms': 1.614}, 'info': {'learner': {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': 25.042053, 'policy_loss': -0.05027485, 'vf_loss': 25.088587, 'vf_explained_var': 0.13388777, 'kl': 0.018707331, 'entropy': 1.3679172, 'entropy_coeff': 0.0, 'model': {}}}}, 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'episodes_total': 20, 'training_iteration': 1, 'experiment_id': '94b7c28fc2a14a629f5f4c8d7b0665fb', 'date': '2021-05-25_13-08-09', 'timestamp': 1621940889, 'time_this_iter_s': 3.0592501163482666, 'time_total_s': 3.0592501163482666, 'pid': 31529, 'hostname': 'Svens-MacBook-Pro.local', 'node_ip': '192.168.0.102', 'config': {'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': True, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'MultiAgentArena', 'env_config': {'config': {'width': 10, 'height': 10}}, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 5e-05, 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'simple_optimizer': False, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'time_since_restore': 3.0592501163482666, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': 25.8, 'ram_util_percent': 58.120000000000005}}\n"
     ]
    }
   ],
   "source": [
    "# That's it, we are ready to train.\n",
    "# Calling `train` once runs a single \"training iteration\". One iteration\n",
    "# for most algos contains a) sampling from the environment(s) + b) using the\n",
    "# sampled data (observations, actions taken, rewards) to update the policy\n",
    "# model (neural network), such that it would pick better actions in the future,\n",
    "# leading to higher rewards.\n",
    "# !LIVE CODING! (call and print out `trainer.train()`)\n",
    "results = rllib_trainer.train()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3381a70-4b44-44e7-a9b3-852340c94b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=17: R=8.98199999999999\n",
      "Iteration=18: R=9.434999999999985\n",
      "Iteration=19: R=10.349999999999982\n",
      "Iteration=20: R=10.742999999999984\n",
      "Iteration=21: R=11.993999999999973\n",
      "Iteration=22: R=12.644999999999971\n",
      "Iteration=23: R=13.499999999999968\n",
      "Iteration=24: R=13.454999999999966\n",
      "Iteration=25: R=13.736999999999961\n",
      "Iteration=26: R=14.516999999999964\n",
      "Iteration=27: R=14.627999999999961\n",
      "Iteration=28: R=14.906999999999956\n",
      "Iteration=29: R=16.64999999999995\n",
      "Iteration=30: R=18.350999999999946\n",
      "Iteration=31: R=19.118999999999936\n"
     ]
    }
   ],
   "source": [
    "# Run `train()` n times. Try to repeatedly call this to see rewards increase.\n",
    "# Move on once you see episode rewards of 15.0 or more.\n",
    "for _ in range(15):\n",
    "    results = rllib_trainer.train()\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "736144ef-66fd-43bd-843d-9f0b29da721e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPO"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "# Use the above solution of Exercise #1 and replace our `dummy_trainer`\n",
    "# with the already trained `rllib_trainer`.\n",
    "rllib_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa7e0d09-e4fa-4657-b602-aa9d6750a33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy=<ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fabeeb2fac0>\n",
      "observation-space=Box(-1.0, 1.0, (200,), float32)\n",
      "action-space=Discrete(4)\n",
      "model=<ray.rllib.models.tf.fcnet.FullyConnectedNetwork object at 0x7fabeeb2faf0>\n",
      "logits=[[-4.9493103  7.095663   1.2463962 -5.00922  ]]\n",
      "action sample=[1]\n"
     ]
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "# Let's actually \"look inside\" our Trainer to see what's in there.\n",
    "import numpy as np\n",
    "policy = rllib_trainer.get_policy()\n",
    "sess = policy.get_session()\n",
    "print(f\"policy={policy}\")\n",
    "print(f\"observation-space={policy.observation_space}\")\n",
    "print(f\"action-space={policy.action_space}\")\n",
    "\n",
    "model = policy.model\n",
    "print(f\"model={model}\")\n",
    "\n",
    "obs_sample = np.expand_dims(policy.observation_space.sample(), 0)\n",
    "action_logits, _ = model({\"obs\": obs_sample})\n",
    "\n",
    "print(\"logits={}\".format(sess.run(action_logits)))\n",
    "\n",
    "action_distribution = policy.dist_class(action_logits)\n",
    "action = action_distribution.sample()\n",
    "print(\"action sample={}\".format(sess.run(action)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "saved-equilibrium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer (at iteration 31 was saved in '/Users/sven/ray_results/PPO_MultiAgentArena_2021-05-25_13-07-562s553d7d/checkpoint_000031/checkpoint-31'!\n",
      "The checkpoint directory contains the following files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['checkpoint-31.tune_metadata', '.is_checkpoint', 'checkpoint-31']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Currently, `rllib_trainer` is in an already trained state.\n",
    "# It holds optimized weights in its Policy's model that allow it to act\n",
    "# already somewhat smart in our environment when given an action.\n",
    "\n",
    "# If we closed this notebook, all the effort would have been for nothing.\n",
    "# Let's save the state of our trainer to disk for later!\n",
    "checkpoint_path = rllib_trainer.save()\n",
    "print(f\"Trainer (at iteration {rllib_trainer.iteration} was saved in '{checkpoint_path}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "import os\n",
    "os.listdir(os.path.dirname(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "global-canon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:10:37,433\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "2021-05-25 13:10:39,086\tINFO trainable.py:377 -- Restored on 192.168.0.102 from checkpoint: /Users/sven/ray_results/PPO_MultiAgentArena_2021-05-25_13-07-562s553d7d/checkpoint_000031/checkpoint-31\n",
      "2021-05-25 13:10:39,086\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 31, '_timesteps_total': None, '_time_total': 86.76738381385803, '_episodes_total': 620}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating new trainer: R=-10.934999999999999\n",
      "Evaluating restored trainer: R=15.959999999999946\n"
     ]
    }
   ],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_trainer = PPOTrainer(config=config)\n",
    "# Evaluate the new trainer (this should yield random results).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "new_trainer.restore(checkpoint_path)\n",
    "\n",
    "# Evaluate again (this should yield results we saw after having trained our saved agent).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating restored trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "continent-architecture",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO's default config is:\n",
      "{'_fake_gpus': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': True,\n",
      " 'clip_param': 0.3,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': 180,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'eager_tracing': False,\n",
      " 'entropy_coeff': 0.0,\n",
      " 'entropy_coeff_schedule': None,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'env_task_fn': None,\n",
      " 'evaluation_config': {},\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': 10,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'evaluation_parallel_to_training': False,\n",
      " 'exploration_config': {'type': 'StochasticSampling'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'tf',\n",
      " 'gamma': 0.99,\n",
      " 'grad_clip': None,\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'kl_coeff': 0.2,\n",
      " 'kl_target': 0.01,\n",
      " 'lambda': 1.0,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'lr': 5e-05,\n",
      " 'lr_schedule': None,\n",
      " 'metrics_smoothing_episodes': 100,\n",
      " 'min_iter_time_s': 0,\n",
      " 'model': {'_time_major': False,\n",
      "           '_use_default_native_models': False,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'num_framestacks': 'auto',\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': False,\n",
      "           'zero_mean': True},\n",
      " 'monitor': -1,\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_mapping_fn': None,\n",
      "                'replay_mode': 'independent'},\n",
      " 'no_done_at_end': False,\n",
      " 'normalize_actions': False,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_sgd_iter': 30,\n",
      " 'num_workers': 2,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'record_env': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'rollout_fragment_length': 200,\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'seed': None,\n",
      " 'sgd_minibatch_size': 128,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'shuffle_sequences': True,\n",
      " 'simple_optimizer': -1,\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 0,\n",
      " 'train_batch_size': 4000,\n",
      " 'use_critic': True,\n",
      " 'use_gae': True,\n",
      " 'vf_clip_param': 10.0,\n",
      " 'vf_loss_coeff': 1.0,\n",
      " 'vf_share_layers': -1}\n"
     ]
    }
   ],
   "source": [
    "# 5) Configuration dicts and Ray Tune.\n",
    "# Where are the default configuration dicts stored?\n",
    "import pprint\n",
    "\n",
    "# PPO algorithm:\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as PPO_DEFAULT_CONFIG\n",
    "print(f\"PPO's default config is:\")\n",
    "pprint.pprint(PPO_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint.pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint.pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "latest-feature",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_55818_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m 2021-05-25 15:01:40,826\tINFO trainer.py:666 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m 2021-05-25 15:01:40,826\tINFO trainer.py:691 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-05-25 15:01:45,831\tWARNING tune.py:506 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m 2021-05-25 15:01:49,678\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_55818_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-25_15-01-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.000000000000028\n",
      "  episode_reward_mean: -7.664999999999994\n",
      "  episode_reward_min: -33.00000000000007\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 2163d328c3014ebd9bd67eb7a55dbbe4\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3665188550949097\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020290959626436234\n",
      "          model: {}\n",
      "          policy_loss: -0.05183352530002594\n",
      "          total_loss: 17.472394943237305\n",
      "          vf_explained_var: 0.12051534652709961\n",
      "          vf_loss: 17.52016830444336\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.499999999999996\n",
      "    ram_util_percent: 72.08333333333334\n",
      "  pid: 33378\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05951354077288677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.030337037382783243\n",
      "    mean_inference_ms: 0.6454797176928905\n",
      "    mean_raw_obs_processing_ms: 0.14413665462802577\n",
      "  time_since_restore: 4.130733013153076\n",
      "  time_this_iter_s: 4.130733013153076\n",
      "  time_total_s: 4.130733013153076\n",
      "  timers:\n",
      "    learn_throughput: 1305.958\n",
      "    learn_time_ms: 3062.885\n",
      "    load_throughput: 98465.93\n",
      "    load_time_ms: 40.623\n",
      "    sample_throughput: 4393.663\n",
      "    sample_time_ms: 910.402\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1621947713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '55818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_55818_00000</td><td>RUNNING </td><td>192.168.0.102:33378</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.13073</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  -7.665</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -33</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_55818_00000</td><td>RUNNING </td><td>192.168.0.102:33378</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.13073</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  -7.665</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -33</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m 2021-05-25 15:01:53,891\tERROR worker.py:396 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"python/ray/_raylet.pyx\", line 594, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"python/ray/_raylet.pyx\", line 489, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"python/ray/_raylet.pyx\", line 496, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"python/ray/_raylet.pyx\", line 500, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 566, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 589, in train\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     result = Trainable.train(self)\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\", line 172, in step\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     res = next(self.train_exec_impl)\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     return next(self.built_iterator)\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   [Previous line repeated 1 more time]\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 876, in apply_flatten\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 828, in add_wait_hooks\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     item = next(it)\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   [Previous line repeated 1 more time]\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 471, in base_iterator\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     yield ray.get(futures, timeout=timeout)\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 62, in wrapper\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 1468, in get\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     values, debugger_breakpoint = worker.get_objects(\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 315, in get_objects\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     data_metadata_pairs = self.core_worker.get_objects(\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 393, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=33378)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m 2021-05-25 15:01:53,880\tERROR worker.py:396 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/policy/sample_batch.py\", line 432, in get\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m     return self.__getitem__(key)\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/policy/sample_batch.py\", line 458, in __getitem__\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m     value = dict.__getitem__(self, key)\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m KeyError: 'seq_lens'\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"python/ray/_raylet.pyx\", line 594, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"python/ray/_raylet.pyx\", line 489, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"python/ray/_raylet.pyx\", line 496, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"python/ray/_raylet.pyx\", line 500, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 566, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 333, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 707, in sample\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 99, in next\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 226, in get_data\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 625, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m     eval_results = _do_policy_eval(\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 1009, in _do_policy_eval\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m     input_dict = sample_collector.get_inference_input_dict(policy_id)\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py\", line 603, in get_inference_input_dict\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m     return SampleBatch(input_dict)\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/policy/sample_batch.py\", line 107, in __init__\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m     if self.get(\"seq_lens\") is not None and \\\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/policy/sample_batch.py\", line 432, in get\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m     return self.__getitem__(key)\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 393, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=33386)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m 2021-05-25 15:01:53,880\tERROR worker.py:396 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"python/ray/_raylet.pyx\", line 594, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"python/ray/_raylet.pyx\", line 489, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"python/ray/_raylet.pyx\", line 496, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"python/ray/_raylet.pyx\", line 500, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 566, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 333, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 707, in sample\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 99, in next\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 226, in get_data\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 599, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m     _process_observations(\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 810, in _process_observations\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m     prep_obs: EnvObsType = _get_or_raise(preprocessors,\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py\", line 166, in transform\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m     arr = np.zeros(self._init_shape(self._obs_space, {}), dtype=np.float32)\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 393, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=33389)\u001b[0m SystemExit: 1\n",
      "2021-05-25 15:01:54,086\tERROR tune.py:545 -- Trials did not complete: [PPO_MultiAgentArena_55818_00000]\n",
      "2021-05-25 15:01:54,087\tINFO tune.py:549 -- Total run time: 19.17 seconds (18.92 seconds for the tuning loop).\n",
      "2021-05-25 15:01:54,088\tWARNING tune.py:553 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f9363f3d730>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plugging in Ray Tune.\n",
    "# Note that this is the recommended way to run any experiments with RLlib.\n",
    "# Reasons:\n",
    "# - Tune allows you to do hyperparameter tuning in a user-friendly way\n",
    "#   and at large scale!\n",
    "# - Tune automatically allocates needed resources for the different\n",
    "#   hyperparam trials and experiment runs.\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Now that we will run things \"automatically\" through tune, we have to\n",
    "# define one or more stopping criteria.\n",
    "stop = {\n",
    "    # explain that keys here can be anything present in the above print(trainer.train())\n",
    "    \"training_iteration\": 3,\n",
    "    \"episode_reward_mean\": 9999.9,\n",
    "}\n",
    "\n",
    "# \"PPO\" is a registered name that points to RLlib's PPOTrainer.\n",
    "# See `ray/rllib/agents/registry.py`\n",
    "# Run our simple experiment until one of the stop criteria is met.\n",
    "tune.run(\"PPO\", config=config, stop=stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "discrete-quilt",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_60f15_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_60f15_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.5   </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=33382)\u001b[0m 2021-05-25 15:02:01,498\tINFO trainer.py:666 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=33382)\u001b[0m 2021-05-25 15:02:01,498\tINFO trainer.py:691 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=33380)\u001b[0m 2021-05-25 15:02:01,496\tINFO trainer.py:666 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=33380)\u001b[0m 2021-05-25 15:02:01,496\tINFO trainer.py:691 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=33380)\u001b[0m 2021-05-25 15:02:11,207\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=33382)\u001b[0m 2021-05-25 15:02:11,215\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_60f15_00001:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-25_15-02-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.50000000000003\n",
      "  episode_reward_mean: -8.594999999999988\n",
      "  episode_reward_min: -28.50000000000005\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: bd6f019bf13048638270272320b576ee\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.08564744144678116\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 29.168235778808594\n",
      "          model: {}\n",
      "          policy_loss: 0.4312191903591156\n",
      "          total_loss: 41.855220794677734\n",
      "          vf_explained_var: -0.00611991249024868\n",
      "          vf_loss: 35.59035873413086\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.037499999999994\n",
      "    ram_util_percent: 64.725\n",
      "  pid: 33380\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06849317998438328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03636466873275651\n",
      "    mean_inference_ms: 0.778569922699676\n",
      "    mean_raw_obs_processing_ms: 0.1681623878059806\n",
      "  time_since_restore: 5.407656192779541\n",
      "  time_this_iter_s: 5.407656192779541\n",
      "  time_total_s: 5.407656192779541\n",
      "  timers:\n",
      "    learn_throughput: 964.322\n",
      "    learn_time_ms: 4147.991\n",
      "    load_throughput: 101685.643\n",
      "    load_time_ms: 39.337\n",
      "    sample_throughput: 3688.733\n",
      "    sample_time_ms: 1084.383\n",
      "    update_time_ms: 3.291\n",
      "  timestamp: 1621947736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 60f15_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_60f15_00000</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_60f15_00001</td><td>RUNNING </td><td>192.168.0.102:33380</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.40766</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  -8.595</td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               -28.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_60f15_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-25_15-02-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.300000000000027\n",
      "  episode_reward_mean: -14.580000000000009\n",
      "  episode_reward_min: -36.000000000000064\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 0ea7872cd21243c9b688a6f66fb28c88\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3483853340148926\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03889012709259987\n",
      "          model: {}\n",
      "          policy_loss: -0.07277801632881165\n",
      "          total_loss: 30.49079704284668\n",
      "          vf_explained_var: 0.12279991805553436\n",
      "          vf_loss: 30.555797576904297\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.125\n",
      "    ram_util_percent: 64.76249999999999\n",
      "  pid: 33382\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06872350042992899\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03553044188629974\n",
      "    mean_inference_ms: 0.783509426898175\n",
      "    mean_raw_obs_processing_ms: 0.16683739024799663\n",
      "  time_since_restore: 5.407294988632202\n",
      "  time_this_iter_s: 5.407294988632202\n",
      "  time_total_s: 5.407294988632202\n",
      "  timers:\n",
      "    learn_throughput: 963.474\n",
      "    learn_time_ms: 4151.641\n",
      "    load_throughput: 109570.501\n",
      "    load_time_ms: 36.506\n",
      "    sample_throughput: 3687.962\n",
      "    sample_time_ms: 1084.61\n",
      "    update_time_ms: 3.294\n",
      "  timestamp: 1621947736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 60f15_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_60f15_00000</td><td>RUNNING </td><td>192.168.0.102:33382</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         10.302 </td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> -6.7425</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -36  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_60f15_00001</td><td>RUNNING </td><td>192.168.0.102:33380</td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         10.3092</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-26.91  </td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_60f15_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-25_15-02-26\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.50000000000003\n",
      "  episode_reward_mean: -33.44000000000003\n",
      "  episode_reward_min: -46.500000000000064\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: bd6f019bf13048638270272320b576ee\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.5\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          model: {}\n",
      "          policy_loss: 0.0016429759562015533\n",
      "          total_loss: 111.94747161865234\n",
      "          vf_explained_var: -0.0012123853666707873\n",
      "          vf_loss: 111.94583129882812\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.05714285714286\n",
      "    ram_util_percent: 65.67142857142856\n",
      "  pid: 33380\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06739702141369838\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0358349049801565\n",
      "    mean_inference_ms: 0.7553273682974136\n",
      "    mean_raw_obs_processing_ms: 0.1664742831804949\n",
      "  time_since_restore: 15.004008293151855\n",
      "  time_this_iter_s: 4.69484806060791\n",
      "  time_total_s: 15.004008293151855\n",
      "  timers:\n",
      "    learn_throughput: 1047.208\n",
      "    learn_time_ms: 3819.679\n",
      "    load_throughput: 267154.539\n",
      "    load_time_ms: 14.973\n",
      "    sample_throughput: 3873.528\n",
      "    sample_time_ms: 1032.65\n",
      "    update_time_ms: 3.426\n",
      "  timestamp: 1621947746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 60f15_00001\n",
      "  \n",
      "Result for PPO_MultiAgentArena_60f15_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-25_15-02-26\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.59999999999995\n",
      "  episode_reward_mean: -4.4300000000000015\n",
      "  episode_reward_min: -36.000000000000064\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: 0ea7872cd21243c9b688a6f66fb28c88\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2851088047027588\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029188670217990875\n",
      "          model: {}\n",
      "          policy_loss: -0.06842983514070511\n",
      "          total_loss: 16.655841827392578\n",
      "          vf_explained_var: 0.3319612741470337\n",
      "          vf_loss: 16.711137771606445\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.08571428571428\n",
      "    ram_util_percent: 65.68571428571428\n",
      "  pid: 33382\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0686160967623853\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03539260236692134\n",
      "    mean_inference_ms: 0.7635725739144091\n",
      "    mean_raw_obs_processing_ms: 0.16649130851254323\n",
      "  time_since_restore: 15.000564813613892\n",
      "  time_this_iter_s: 4.698555946350098\n",
      "  time_total_s: 15.000564813613892\n",
      "  timers:\n",
      "    learn_throughput: 1046.499\n",
      "    learn_time_ms: 3822.268\n",
      "    load_throughput: 279646.678\n",
      "    load_time_ms: 14.304\n",
      "    sample_throughput: 3827.286\n",
      "    sample_time_ms: 1045.127\n",
      "    update_time_ms: 2.357\n",
      "  timestamp: 1621947746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 60f15_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_60f15_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         15.0006</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   -4.43</td><td style=\"text-align: right;\">                21.6</td><td style=\"text-align: right;\">               -36  </td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_60f15_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.5   </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         15.004 </td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -33.44</td><td style=\"text-align: right;\">                 7.5</td><td style=\"text-align: right;\">               -46.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-25 15:02:26,788\tINFO tune.py:549 -- Total run time: 32.69 seconds (32.31 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis object at 0x7f93755ab790>\n"
     ]
    }
   ],
   "source": [
    "# Updating an algo's default config dict and adding hyperparameter tuning\n",
    "# options to it.\n",
    "# Note: Hyperparameter tuning options (e.g. grid_search) will only work,\n",
    "# if we run these configs via `tune.run`.\n",
    "config.update(\n",
    "    {\n",
    "        # Try 2 different learning rates.\n",
    "        \"lr\": tune.grid_search([0.0001, 0.5]),\n",
    "        # NN model config to tweak the default model\n",
    "        # that'll be created by RLlib for the policy.\n",
    "        \"model\": {\n",
    "            # e.g. change the dense layer stack.\n",
    "            \"fcnet_hiddens\": [256, 256, 256],\n",
    "            # Alternatively, you can specify a custom model here\n",
    "            # (we'll cover that later).\n",
    "            # \"custom_model\": ...\n",
    "            # Pass kwargs to your custom model.\n",
    "            # \"custom_model_config\": {}\n",
    "        },\n",
    "    }\n",
    ")\n",
    "# Repeat our experiment using tune's grid-search feature.\n",
    "results = tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,\n",
    "    stop=stop,\n",
    "    checkpoint_at_end=True,  # create a checkpoint when done.\n",
    "    checkpoint_freq=1,  # create a checkpoint on every iteration.\n",
    ")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e587e514-05a5-4b73-be84-7b88f60249c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going multi-policy:\n",
    "\n",
    "# Our experiment is ill-configured b/c both\n",
    "# agents, which should behave differently due to their different\n",
    "# tasks and reward functions, learn the same policy (the \"default_policy\",\n",
    "# which RLlib always provides if you don't configure anything else; Remember\n",
    "# that RLlib does not know at Trainer setup time, how many and which agents\n",
    "# the environment will \"produce\").\n",
    "# Let's fix this and introduce the \"multiagent\" API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13900163-f520-40f1-87be-d759760bd3a5",
   "metadata": {},
   "source": [
    "<img src=\"images/from_single_agent_to_multi_agent.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "649bb179-3266-4eed-bb3b-51d98c0bdbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an agent->policy mapping function.\n",
    "# Which agents (defined by the environment) use which policies\n",
    "# (defined by us)? Mapping is M (agents) -> N (policies), where M >= N.\n",
    "def policy_mapping_fn(agent: str):\n",
    "    assert agent in [\"agent1\", \"agent2\"], f\"ERROR: invalid agent {agent}!\"\n",
    "    return \"pol1\" if agent == \"agent1\" else \"pol2\"\n",
    "    \n",
    "# Define details for our two policies.\n",
    "#TODO: coding Sven: Make it possible to not need obs/action spaces\n",
    "#  if they are the default anyways.\n",
    "observation_space = rllib_trainer.workers.local_worker().env.observation_space\n",
    "action_space = rllib_trainer.workers.local_worker().env.action_space\n",
    "# Btw, the above is equivalent to saying:\n",
    "# >>> rllib_trainer.get_policy(\"default_policy\").obs/action_space\n",
    "policies = {\n",
    "    \"pol1\": (None, observation_space, action_space, {\"lr\": 0.0003}),\n",
    "    \"pol2\": (None, observation_space, action_space, {\"lr\": 0.0004}),\n",
    "}\n",
    "\n",
    "# policies_to_train = [\"pol1\", \"pol2\"]\n",
    "\n",
    "# Adding the above to our config.\n",
    "config.update({\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        # \"policies_to_train\": policies_to_train,\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a74ec7-a6c1-431d-83aa-35df56d93185",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise No 2\n",
    "\n",
    "<hr />\n",
    "\n",
    "Using the `config` that we have built so far (the one we just updated to include a multi-agent setup),\n",
    "try learning our environment using Ray tune.run and a simple hyperparameter grid_search over:\n",
    "- 2 different learning rates (pick your own values).\n",
    "- AND 2 different `train_batch_size` settings (use 2000 and 3000).\n",
    "\n",
    "Also, make RLlib use a [128,128] dense layer stack as the NN model.\n",
    "\n",
    "Also, use the config setting of `num_envs_per_worker=10` to increase the sampling throughput.\n",
    "\n",
    "In case your local machine has less than 12 CPUs, try setting `num_workers=1` to make all tune trials run at the same time.\n",
    "Background: PPO by default uses 2 workers, which makes 1 trial use 3 CPUs (2 workers + \"driver\" (\"local-worker\")),\n",
    "which makes the entire experiment use 12 CPUs. Tune will run trials in sequence in case it cannot allocate enough CPUs at once\n",
    "(which is also fine, but then takes longer).\n",
    "\n",
    "Try to reach a total reward (sum of agent1 and agent2) of 15.0.\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0077e6-16b1-428b-80b7-2516e1302030",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_74737_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_74737_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_74737_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_74737_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-25 15:02:30,281\tWARNING tune.py:506 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m 2021-05-25 15:02:34,584\tINFO trainer.py:666 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m 2021-05-25 15:02:34,584\tINFO trainer.py:691 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m 2021-05-25 15:02:34,584\tINFO trainer.py:666 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m 2021-05-25 15:02:34,584\tINFO trainer.py:691 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m 2021-05-25 15:02:34,584\tINFO trainer.py:666 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m 2021-05-25 15:02:34,584\tINFO trainer.py:691 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m 2021-05-25 15:02:34,584\tINFO trainer.py:666 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m 2021-05-25 15:02:34,584\tINFO trainer.py:691 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m 2021-05-25 15:02:46,091\tINFO trainable.py:101 -- Trainable.setup took 11.508 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m 2021-05-25 15:02:46,091\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m 2021-05-25 15:02:46,499\tINFO trainable.py:101 -- Trainable.setup took 11.916 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m 2021-05-25 15:02:46,500\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m 2021-05-25 15:02:46,500\tINFO trainable.py:101 -- Trainable.setup took 11.917 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m 2021-05-25 15:02:46,500\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m 2021-05-25 15:02:46,505\tINFO trainable.py:101 -- Trainable.setup took 11.922 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m 2021-05-25 15:02:46,506\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_74737_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-25_15-02-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.499999999999986\n",
      "  episode_reward_mean: -8.722500000000002\n",
      "  episode_reward_min: -36.00000000000005\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 2d48b8f2f76245db9aa0e0888af44ced\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.3459675312042236\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.041345931589603424\n",
      "          model: {}\n",
      "          policy_loss: -0.08170092850923538\n",
      "          total_loss: 45.23959732055664\n",
      "          vf_explained_var: 0.1294073462486267\n",
      "          vf_loss: 45.31303024291992\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.3435704708099365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04444278031587601\n",
      "          model: {}\n",
      "          policy_loss: -0.07135061919689178\n",
      "          total_loss: 2.3808655738830566\n",
      "          vf_explained_var: 0.37073028087615967\n",
      "          vf_loss: 2.4433276653289795\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.21999999999999\n",
      "    ram_util_percent: 63.529999999999994\n",
      "  pid: 33377\n",
      "  policy_reward_max:\n",
      "    pol1: 26.5\n",
      "    pol2: 1.0000000000000018\n",
      "  policy_reward_mean:\n",
      "    pol1: -0.125\n",
      "    pol2: -8.597499999999982\n",
      "  policy_reward_min:\n",
      "    pol1: -26.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29074730564705764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1676883270491415\n",
      "    mean_inference_ms: 1.7751354483229602\n",
      "    mean_raw_obs_processing_ms: 1.2787135679330397\n",
      "  time_since_restore: 6.487511157989502\n",
      "  time_this_iter_s: 6.487511157989502\n",
      "  time_total_s: 6.487511157989502\n",
      "  timers:\n",
      "    learn_throughput: 731.07\n",
      "    learn_time_ms: 5471.432\n",
      "    sample_throughput: 5077.898\n",
      "    sample_time_ms: 787.728\n",
      "    update_time_ms: 4.146\n",
      "  timestamp: 1621947772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '74737_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_74737_00000</td><td>RUNNING </td><td>192.168.0.102:33377</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.48751</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -8.7225</td><td style=\"text-align: right;\">                16.5</td><td style=\"text-align: right;\">                 -36</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_74737_00001</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_74737_00002</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_74737_00003</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 4/4 (4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_74737_00000</td><td>RUNNING </td><td>192.168.0.102:33377</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.48751</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -8.7225</td><td style=\"text-align: right;\">                16.5</td><td style=\"text-align: right;\">                 -36</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_74737_00001</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              2000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_74737_00002</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MultiAgentArena_74737_00003</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">              3000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m 2021-05-25 15:02:52,750\tERROR worker.py:396 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"python/ray/_raylet.pyx\", line 594, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"python/ray/_raylet.pyx\", line 489, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"python/ray/_raylet.pyx\", line 496, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"python/ray/_raylet.pyx\", line 500, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 566, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 589, in train\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     result = Trainable.train(self)\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\", line 172, in step\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     res = next(self.train_exec_impl)\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     return next(self.built_iterator)\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 791, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     result = fn(item)\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/execution/train_ops.py\", line 63, in __call__\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     info = do_minibatch_sgd(\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/utils/sgd.py\", line 111, in do_minibatch_sgd\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     batch_fetches = (local_worker.learn_on_batch(\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 903, in learn_on_batch\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     info_out.update({k: builder.get(v) for k, v in to_fetch.items()})\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 903, in <dictcomp>\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     info_out.update({k: builder.get(v) for k, v in to_fetch.items()})\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/utils/tf_run_builder.py\", line 42, in get\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     self._executed = run_timeline(\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/utils/tf_run_builder.py\", line 89, in run_timeline\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 967, in run\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1163, in _run\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py\", line 1144, in is_compatible_with\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     if not x_dim.is_compatible_with(y_dim):\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py\", line 273, in is_compatible_with\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     other = as_dimension(other)\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 393, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=33375)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m 2021-05-25 15:02:52,743\tERROR worker.py:396 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"python/ray/_raylet.pyx\", line 594, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"python/ray/_raylet.pyx\", line 489, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"python/ray/_raylet.pyx\", line 496, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"python/ray/_raylet.pyx\", line 500, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 566, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 589, in train\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     result = Trainable.train(self)\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\", line 172, in step\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     res = next(self.train_exec_impl)\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     return next(self.built_iterator)\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 791, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     result = fn(item)\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/execution/train_ops.py\", line 63, in __call__\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     info = do_minibatch_sgd(\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/utils/sgd.py\", line 110, in do_minibatch_sgd\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     for minibatch in minibatches(batch, sgd_minibatch_size):\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/utils/sgd.py\", line 71, in minibatches\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     samples.shuffle()\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/policy/sample_batch.py\", line 260, in shuffle\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     self[key] = val[permutation]\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 393, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=33379)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m 2021-05-25 15:02:52,754\tERROR worker.py:396 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"python/ray/_raylet.pyx\", line 594, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"python/ray/_raylet.pyx\", line 489, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"python/ray/_raylet.pyx\", line 496, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"python/ray/_raylet.pyx\", line 500, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 566, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 589, in train\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     result = Trainable.train(self)\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\", line 172, in step\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     res = next(self.train_exec_impl)\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     return next(self.built_iterator)\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   [Previous line repeated 1 more time]\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 876, in apply_flatten\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 828, in add_wait_hooks\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     item = next(it)\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   [Previous line repeated 1 more time]\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 471, in base_iterator\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     yield ray.get(futures, timeout=timeout)\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 62, in wrapper\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 1468, in get\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     values, debugger_breakpoint = worker.get_objects(\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 315, in get_objects\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     data_metadata_pairs = self.core_worker.get_objects(\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 393, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=33377)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m 2021-05-25 15:02:52,746\tERROR worker.py:396 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"python/ray/_raylet.pyx\", line 594, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"python/ray/_raylet.pyx\", line 489, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"python/ray/_raylet.pyx\", line 496, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"python/ray/_raylet.pyx\", line 500, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 566, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 589, in train\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     result = Trainable.train(self)\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\", line 172, in step\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     res = next(self.train_exec_impl)\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     return next(self.built_iterator)\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 791, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     result = fn(item)\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/execution/train_ops.py\", line 63, in __call__\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     info = do_minibatch_sgd(\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/utils/sgd.py\", line 111, in do_minibatch_sgd\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     batch_fetches = (local_worker.learn_on_batch(\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 903, in learn_on_batch\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     info_out.update({k: builder.get(v) for k, v in to_fetch.items()})\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 903, in <dictcomp>\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     info_out.update({k: builder.get(v) for k, v in to_fetch.items()})\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/utils/tf_run_builder.py\", line 42, in get\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     self._executed = run_timeline(\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/utils/tf_run_builder.py\", line 89, in run_timeline\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 967, in run\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1175, in _run\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     fetch_handler = _FetchHandler(\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 487, in __init__\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     self._fetch_mapper = _FetchMapper.for_fetch(fetches)\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 268, in for_fetch\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     return _ListFetchMapper(fetch)\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 380, in __init__\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 380, in <listcomp>\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 270, in for_fetch\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     return _DictFetchMapper(fetch)\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 418, in __init__\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     self._mappers = [\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 419, in <listcomp>\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     _FetchMapper.for_fetch(fetch) for fetch in fetches.values()\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 270, in for_fetch\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     return _DictFetchMapper(fetch)\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 418, in __init__\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     self._mappers = [\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 419, in <listcomp>\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     _FetchMapper.for_fetch(fetch) for fetch in fetches.values()\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 276, in for_fetch\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     if isinstance(fetch, tensor_type):\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/abc.py\", line 98, in __instancecheck__\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     return _abc_instancecheck(cls, instance)\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 393, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=33376)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m 2021-05-25 15:02:52,746\tERROR worker.py:396 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"python/ray/_raylet.pyx\", line 594, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"python/ray/_raylet.pyx\", line 489, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"python/ray/_raylet.pyx\", line 496, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"python/ray/_raylet.pyx\", line 500, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 566, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 333, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 707, in sample\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 99, in next\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 226, in get_data\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 625, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     eval_results = _do_policy_eval(\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 1011, in _do_policy_eval\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     policy.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py\", line 388, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     fetched = builder.get(to_fetch)\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/utils/tf_run_builder.py\", line 42, in get\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     self._executed = run_timeline(\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/utils/tf_run_builder.py\", line 89, in run_timeline\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 967, in run\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1175, in _run\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     fetch_handler = _FetchHandler(\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 487, in __init__\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     self._fetch_mapper = _FetchMapper.for_fetch(fetches)\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 268, in for_fetch\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     return _ListFetchMapper(fetch)\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 380, in __init__\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 380, in <listcomp>\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 270, in for_fetch\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     return _DictFetchMapper(fetch)\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 418, in __init__\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     self._mappers = [\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 419, in <listcomp>\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     _FetchMapper.for_fetch(fetch) for fetch in fetches.values()\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 278, in for_fetch\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     return _ElementFetchMapper(fetches, contraction_fn)\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 306, in __init__\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3754, in as_graph_element\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     with self._lock:\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 393, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=33374)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m 2021-05-25 15:02:52,746\tERROR worker.py:396 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"python/ray/_raylet.pyx\", line 594, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"python/ray/_raylet.pyx\", line 489, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"python/ray/_raylet.pyx\", line 496, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"python/ray/_raylet.pyx\", line 500, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 566, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 333, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 707, in sample\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 99, in next\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 226, in get_data\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 625, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m     eval_results = _do_policy_eval(\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 1011, in _do_policy_eval\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m     policy.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py\", line 384, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m     to_fetch = self._build_compute_actions(\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py\", line 788, in _build_compute_actions\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m     builder.add_feed_dict({self._input_dict[key]: value})\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 842, in __hash__\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m     g = getattr(self, \"graph\", None)\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 419, in graph\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m     @property\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 393, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=33441)\u001b[0m SystemExit: 1\n",
      "2021-05-25 15:02:52,954\tERROR tune.py:545 -- Trials did not complete: [PPO_MultiAgentArena_74737_00000, PPO_MultiAgentArena_74737_00001, PPO_MultiAgentArena_74737_00002, PPO_MultiAgentArena_74737_00003]\n",
      "2021-05-25 15:02:52,955\tINFO tune.py:549 -- Total run time: 26.13 seconds (25.86 seconds for the tuning loop).\n",
      "2021-05-25 15:02:52,955\tWARNING tune.py:553 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    }
   ],
   "source": [
    "# Solution to Exercise #2\n",
    "# !LIVE CODING!\n",
    "# Solution to Exercise #2:\n",
    "\n",
    "# Update our config and set it up for 2x tune grid-searches (leading to 4 parallel trials in total).\n",
    "config.update({\n",
    "    \"lr\": tune.grid_search([0.0001, 0.0005]),\n",
    "    \"train_batch_size\": tune.grid_search([2000, 3000]),\n",
    "    \"num_envs_per_worker\": 10,\n",
    "    # Change our model to be simpler.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [128, 128],\n",
    "    },\n",
    "})\n",
    "\n",
    "# Run the experiment.\n",
    "experiment_analysis = tune.run(\"PPO\", config=config, stop={\"episode_reward_mean\": 15.0, \"training_iteration\": 100})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fluid-seeking",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:08:19,854\tINFO services.py:1272 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.101',\n",
       " 'raylet_ip_address': '192.168.0.101',\n",
       " 'redis_address': '192.168.0.101:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-05-25_14-08-18_732159_31529/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-05-25_14-08-18_732159_31529/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-05-25_14-08-18_732159_31529',\n",
       " 'metrics_export_port': 64172,\n",
       " 'node_id': '9c5f474ac4dfdc490dd4df5141be1c58807009b827a263430032ccca'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anyscale's Infinite laptop:\n",
    "\n",
    "# NOTE: The following cell will only work if you are already on-boarded to our Anyscale Inc. \"Infinite Laptop\".\n",
    "# To get more information, see https://www.anyscale.com/product\n",
    "\n",
    "# Let's quickly divert from our MultiAgentArena and move to something much heavier in terms of environment/simulator complexity.\n",
    "# We will now demonstrate, how you can use Anyscale's infinite laptop to launch an RLlib experiment on a cloud 4 GPU + 32 CPU machine\n",
    "# all from within this Jupyter cell here.\n",
    "# Start an experiment in the cloud using Anyscale's product, RLlib, and a more complex multi-agent env.\n",
    "\n",
    "# NOTE \n",
    "import ray\n",
    "ray.shutdown()\n",
    "ray.client().connect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5516d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Neural Network Models.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                input_space,\n",
    "                action_space,\n",
    "                num_outputs,\n",
    "                name=\"\",\n",
    "                *,\n",
    "                layers = (256, 256)):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense_layers = []\n",
    "        for i, layer_size in enumerate(layers):\n",
    "            self.dense_layers.append(tf.keras.layers.Dense(\n",
    "                layer_size, activation=tf.nn.relu, name=f\"dense_{i}\"))\n",
    "\n",
    "        self.logits = tf.keras.layers.Dense(\n",
    "            num_outputs,\n",
    "            activation=tf.keras.activations.linear,\n",
    "            name=\"logits\")\n",
    "        self.values = tf.keras.layers.Dense(\n",
    "            1, activation=None, name=\"values\")\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # Standardized input args:\n",
    "        # - input_dict (RLlib `SampleBatch` object, which is basically a dict with numpy arrays\n",
    "        # in it)\n",
    "        out = inputs[\"obs\"]\n",
    "        for l in self.dense_layers:\n",
    "            out = l(out)\n",
    "        logits = self.logits(out)\n",
    "        values = self.values(out)\n",
    "\n",
    "        # Standardized output:\n",
    "        # - \"normal\" model output tensor (e.g. action logits).\n",
    "        # - list of internal state outputs (only needed for RNN-/memory enhanced models).\n",
    "        # - \"extra outs\", such as model's side branches, e.g. value function outputs.\n",
    "        return logits, [], {\"vf_preds\": tf.reshape(values, [-1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "controversial-repair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'my_model/logits/BiasAdd:0' shape=(1, 2) dtype=float64>,\n",
       " [],\n",
       " {'vf_preds': <tf.Tensor 'my_model/Reshape:0' shape=(1,) dtype=float64>})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do a quick test on the custom model class.\n",
    "from gym.spaces import Box\n",
    "test_model = MyModel(\n",
    "    input_space=Box(-1.0, 1.0, (2, )),\n",
    "    action_space=None,\n",
    "    num_outputs=2,\n",
    ")\n",
    "test_model({\"obs\": np.array([[0.5, 0.5]])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2237526a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_841c3_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m 2021-05-25 15:02:58,026\tINFO trainer.py:666 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m 2021-05-25 15:02:58,026\tINFO trainer.py:691 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m 2021-05-25 15:03:02,523\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <bound method MyModel.call of <__main__.MyModel object at 0x7fecb7958b50>> and will run it as-is.\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m Cause: Unable to locate the source code of <bound method MyModel.call of <__main__.MyModel object at 0x7fecb7958b50>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <bound method MyModel.call of <__main__.MyModel object at 0x7fc6fa816be0>> and will run it as-is.\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m Cause: Unable to locate the source code of <bound method MyModel.call of <__main__.MyModel object at 0x7fc6fa816be0>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m 2021-05-25 15:03:03,868\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <bound method MyModel.call of <__main__.MyModel object at 0x7fe381d66160>> and will run it as-is.\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m Cause: Unable to locate the source code of <bound method MyModel.call of <__main__.MyModel object at 0x7fe381d66160>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m 2021-05-25 15:03:05,181\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_841c3_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-25_15-03-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.600000000000035\n",
      "  episode_reward_mean: -9.202499999999995\n",
      "  episode_reward_min: -31.50000000000004\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 7a06020468884649b96d5538baad0125\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.3631614446640015\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025275129824876785\n",
      "          policy_loss: -0.03527597337961197\n",
      "          total_loss: 26.51007843017578\n",
      "          vf_explained_var: 0.27398768067359924\n",
      "          vf_loss: 26.540298461914062\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.3708057403564453\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01341917458921671\n",
      "          policy_loss: -0.0413493849337101\n",
      "          total_loss: 1.4435670375823975\n",
      "          vf_explained_var: 0.5503895878791809\n",
      "          vf_loss: 1.4822325706481934\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.914285714285715\n",
      "    ram_util_percent: 60.87142857142857\n",
      "  pid: 33454\n",
      "  policy_reward_max:\n",
      "    pol1: 18.5\n",
      "    pol2: -5.599999999999992\n",
      "  policy_reward_mean:\n",
      "    pol1: 0.1375\n",
      "    pol2: -9.33999999999998\n",
      "  policy_reward_min:\n",
      "    pol1: -21.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22760493245290875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13343611759925955\n",
      "    mean_inference_ms: 1.423236149460522\n",
      "    mean_raw_obs_processing_ms: 1.0063132243369943\n",
      "  time_since_restore: 4.400547027587891\n",
      "  time_this_iter_s: 4.400547027587891\n",
      "  time_total_s: 4.400547027587891\n",
      "  timers:\n",
      "    learn_throughput: 1092.424\n",
      "    learn_time_ms: 3661.581\n",
      "    sample_throughput: 6307.282\n",
      "    sample_time_ms: 634.188\n",
      "    update_time_ms: 1.773\n",
      "  timestamp: 1621947789\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 841c3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_841c3_00000</td><td>RUNNING </td><td>192.168.0.102:33454</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.40055</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -9.2025</td><td style=\"text-align: right;\">                 9.6</td><td style=\"text-align: right;\">               -31.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-25 15:03:13,215\tWARNING tune.py:506 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_841c3_00000</td><td>RUNNING </td><td>192.168.0.102:33454</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">          8.3951</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> -5.8725</td><td style=\"text-align: right;\">                17.1</td><td style=\"text-align: right;\">               -31.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m 2021-05-25 15:03:13,691\tERROR worker.py:396 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"python/ray/_raylet.pyx\", line 594, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"python/ray/_raylet.pyx\", line 489, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"python/ray/_raylet.pyx\", line 496, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"python/ray/_raylet.pyx\", line 500, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 566, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 589, in train\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     result = Trainable.train(self)\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\", line 172, in step\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     res = next(self.train_exec_impl)\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     return next(self.built_iterator)\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   [Previous line repeated 1 more time]\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 876, in apply_flatten\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 828, in add_wait_hooks\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     item = next(it)\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   [Previous line repeated 1 more time]\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 471, in base_iterator\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     yield ray.get(futures, timeout=timeout)\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 62, in wrapper\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 1468, in get\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     values, debugger_breakpoint = worker.get_objects(\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 315, in get_objects\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     data_metadata_pairs = self.core_worker.get_objects(\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 393, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=33454)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m 2021-05-25 15:03:13,686\tERROR worker.py:396 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"python/ray/_raylet.pyx\", line 594, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"python/ray/_raylet.pyx\", line 489, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"python/ray/_raylet.pyx\", line 496, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"python/ray/_raylet.pyx\", line 500, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 566, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 333, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 707, in sample\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 99, in next\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 226, in get_data\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 599, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m     _process_observations(\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 814, in _process_observations\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m     filtered_obs: EnvObsType = _get_or_raise(obs_filters,\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 393, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=33457)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m 2021-05-25 15:03:13,686\tERROR worker.py:396 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"python/ray/_raylet.pyx\", line 594, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"python/ray/_raylet.pyx\", line 489, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"python/ray/_raylet.pyx\", line 496, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"python/ray/_raylet.pyx\", line 500, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 566, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 333, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 707, in sample\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 99, in next\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 226, in get_data\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 599, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m     _process_observations(\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 866, in _process_observations\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m     rewards[env_id][agent_id] or 0.0)\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m   File \"/Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/worker.py\", line 393, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=33464)\u001b[0m SystemExit: 1\n",
      "2021-05-25 15:03:13,898\tERROR tune.py:545 -- Trials did not complete: [PPO_MultiAgentArena_841c3_00000]\n",
      "2021-05-25 15:03:13,899\tINFO tune.py:549 -- Total run time: 20.80 seconds (20.54 seconds for the tuning loop).\n",
      "2021-05-25 15:03:13,899\tWARNING tune.py:553 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f93668d2220>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up our custom model and re-run the experiment.\n",
    "\n",
    "config.update({\n",
    "    \"model\": {\n",
    "        \"custom_model\": MyModel,\n",
    "        \"custom_model_config\": {\n",
    "            \"layers\": [128, 128],\n",
    "        },\n",
    "    },\n",
    "    # Revert these to single trials (and use those hyperparams that performed well in our Exercise #2).\n",
    "    \"lr\": 0.0005,\n",
    "    \"train_batch_size\": 2000,\n",
    "})\n",
    "\n",
    "tune.run(\"PPO\", config=config, stop=stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "working-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Hacking in\": How do we customize our RL loop?\n",
    "# RLlib offers a callbacks API that allows you to add custom behavior to\n",
    "# all major events during the environment sampling- and learning process.\n",
    "\n",
    "# Our problem: So far, we can only see the total reward (sum for both agents).\n",
    "# This does not give us enough insights into the question of which agent\n",
    "# learns what (maybe agent2 doesn't learn anything and the reward we are observing\n",
    "# is mostly due to agent1's progress in covering the map!).\n",
    "\n",
    "# The following custom callbacks class allows us to add each agents single reward to\n",
    "# the returned metrics, which will then be displayed in tensorboard.\n",
    "\n",
    "# We will override RLlib's DefaultCallbacks class and implement the\n",
    "# `on_episode_start`, `on_episode_step`, and `on_episode_end` methods therein.\n",
    "\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(self, *, worker, base_env,\n",
    "                         policies, episode,\n",
    "                         env_index, **kwargs):\n",
    "        # We will use the `MultiAgentEpisode` object being passed into\n",
    "        # all episode-related callbacks. It comes with a user_data property (dict),\n",
    "        # which we can write arbitrary data into.\n",
    "\n",
    "        # At the end of an episode, we'll transfer that data into the `hist_data`, and `custom_metrics`\n",
    "        # properties to make sure our custom data is displayed in TensorBoard.\n",
    "\n",
    "        # The episode is starting:\n",
    "        # Wipe out the rewards-lists for individual agents 1 and 2.\n",
    "        episode.user_data[\"agent1_rewards\"] = []\n",
    "        episode.user_data[\"agent2_rewards\"] = []\n",
    "\n",
    "    def on_episode_step(self, *, worker, base_env,\n",
    "                        episode, env_index, **kwargs):\n",
    "        # Get the last rewards for individual agents 1 and 2\n",
    "        # from the MultiAgentEpisode object (`episode`).\n",
    "        ag1_r = episode.prev_reward_for(\"agent1\")\n",
    "        ag2_r = episode.prev_reward_for(\"agent2\")\n",
    "        #print(\"ag1_r={} ag2_r={}\".format(ag1_r, ag2_r))\n",
    "\n",
    "        # Add individual rewards to our lists.\n",
    "        episode.user_data[\"agent1_rewards\"].append(ag1_r)\n",
    "        episode.user_data[\"agent2_rewards\"].append(ag2_r)\n",
    "\n",
    "    def on_episode_end(self, *, worker, base_env,\n",
    "                       policies, episode,\n",
    "                       env_index, **kwargs):\n",
    "        # Episode is done:\n",
    "        # Write scalar values (sum over rewards) to `custom_metrics` and\n",
    "        # time-series data (rewards per time step) to `hist_data`.\n",
    "        # Both will be visible then in TensorBoard.\n",
    "\n",
    "        # Put scalar values (one per episode) under `custom_metrics`.\n",
    "        episode.custom_metrics[\"ag1_R\"] = sum(episode.user_data[\"agent1_rewards\"])\n",
    "        episode.custom_metrics[\"ag2_R\"] = sum(episode.user_data[\"agent2_rewards\"])\n",
    "        # Time series data goes into `hist_data`.\n",
    "        episode.hist_data[\"agent1_rewards\"] = episode.user_data[\"agent1_rewards\"]\n",
    "        episode.hist_data[\"agent2_rewards\"] = episode.user_data[\"agent2_rewards\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd2fe8eb-c52f-4a26-9067-96ad9fe160a4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_a19a9_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=33644)\u001b[0m 2021-05-25 15:18:06,247\tINFO trainer.py:666 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=33644)\u001b[0m 2021-05-25 15:18:06,247\tINFO trainer.py:691 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=33644)\u001b[0m 2021-05-25 15:18:14,232\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_a19a9_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=33644)\u001b[0m 2021-05-25 15:18:14,374\tINFO trainable.py:377 -- Restored on 192.168.0.102 from checkpoint: /Users/sven/ray_results/PPO/PPO_MultiAgentArena_a19a9_00000_0_2021-05-25_15-18-01/tmpt2yot0_trestore_from_object/checkpoint-10\n",
      "\u001b[2m\u001b[36m(pid=33644)\u001b[0m 2021-05-25 15:18:14,375\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': None, '_time_total': 44.477219343185425, '_episodes_total': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_a19a9_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics:\n",
      "    ag1_R_max: 35.5\n",
      "    ag1_R_mean: 19.5125\n",
      "    ag1_R_min: -2.5\n",
      "    ag2_R_max: -1.1000000000000039\n",
      "    ag2_R_mean: -8.277499999999986\n",
      "    ag2_R_min: -9.89999999999998\n",
      "  date: 2021-05-25_15-18-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.099999999999923\n",
      "  episode_reward_mean: 10.77749999999998\n",
      "  episode_reward_min: -10.79999999999998\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 440\n",
      "  experiment_id: 88f4f9358b2746b4a433baa3528e946e\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0878950357437134\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04845314100384712\n",
      "          model: {}\n",
      "          policy_loss: -0.07339268177747726\n",
      "          total_loss: 23.10724639892578\n",
      "          vf_explained_var: 0.5684372186660767\n",
      "          vf_loss: 23.170948028564453\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.1285157203674316\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04549764096736908\n",
      "          model: {}\n",
      "          policy_loss: -0.07081794738769531\n",
      "          total_loss: 1.9658052921295166\n",
      "          vf_explained_var: 0.38572221994400024\n",
      "          vf_loss: 2.0275235176086426\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.500000000000004\n",
      "    ram_util_percent: 71.81428571428572\n",
      "  pid: 33644\n",
      "  policy_reward_max:\n",
      "    pol1: 35.0\n",
      "    pol2: -1.200000000000004\n",
      "  policy_reward_mean:\n",
      "    pol1: 19.1\n",
      "    pol2: -8.322499999999986\n",
      "  policy_reward_min:\n",
      "    pol1: -3.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.23935564714877758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1360249163499519\n",
      "    mean_inference_ms: 1.8088408370516191\n",
      "    mean_raw_obs_processing_ms: 1.0294599912652922\n",
      "  time_since_restore: 4.576020002365112\n",
      "  time_this_iter_s: 4.576020002365112\n",
      "  time_total_s: 49.05323934555054\n",
      "  timers:\n",
      "    learn_throughput: 1088.103\n",
      "    learn_time_ms: 3676.121\n",
      "    sample_throughput: 5277.328\n",
      "    sample_time_ms: 757.959\n",
      "    update_time_ms: 2.116\n",
      "  timestamp: 1621948698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: a19a9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_a19a9_00000</td><td>RUNNING </td><td>192.168.0.102:33644</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         53.0848</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> 11.6812</td><td style=\"text-align: right;\">                26.1</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_a19a9_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics:\n",
      "    ag1_R_max: 42.0\n",
      "    ag1_R_mean: 19.335\n",
      "    ag1_R_min: -7.5\n",
      "    ag2_R_max: 4.400000000000013\n",
      "    ag2_R_mean: -6.929999999999989\n",
      "    ag2_R_min: -9.89999999999998\n",
      "  date: 2021-05-25_15-18-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 31.499999999999908\n",
      "  episode_reward_mean: 12.233999999999982\n",
      "  episode_reward_min: -17.999999999999986\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 520\n",
      "  experiment_id: 88f4f9358b2746b4a433baa3528e946e\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 1.0211660861968994\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03176135569810867\n",
      "          model: {}\n",
      "          policy_loss: -0.06694947183132172\n",
      "          total_loss: 33.059776306152344\n",
      "          vf_explained_var: 0.48005741834640503\n",
      "          vf_loss: 33.11243438720703\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0782160758972168\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.030843839049339294\n",
      "          model: {}\n",
      "          policy_loss: -0.06135513260960579\n",
      "          total_loss: 3.1897270679473877\n",
      "          vf_explained_var: 0.35209575295448303\n",
      "          vf_loss: 3.2372024059295654\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.5\n",
      "    ram_util_percent: 71.56666666666666\n",
      "  pid: 33644\n",
      "  policy_reward_max:\n",
      "    pol1: 41.5\n",
      "    pol2: 4.300000000000013\n",
      "  policy_reward_mean:\n",
      "    pol1: 19.22\n",
      "    pol2: -6.9859999999999856\n",
      "  policy_reward_min:\n",
      "    pol1: -8.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.23064132964777584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.12965646254749552\n",
      "    mean_inference_ms: 1.6123252092941862\n",
      "    mean_raw_obs_processing_ms: 0.9828432963534945\n",
      "  time_since_restore: 12.66549038887024\n",
      "  time_this_iter_s: 4.057945251464844\n",
      "  time_total_s: 57.142709732055664\n",
      "  timers:\n",
      "    learn_throughput: 1147.231\n",
      "    learn_time_ms: 3486.655\n",
      "    sample_throughput: 6518.037\n",
      "    sample_time_ms: 613.682\n",
      "    update_time_ms: 2.045\n",
      "  timestamp: 1621948707\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: a19a9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_a19a9_00000</td><td>RUNNING </td><td>192.168.0.102:33644</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         61.1943</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  13.512</td><td style=\"text-align: right;\">                31.5</td><td style=\"text-align: right;\">                 -18</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_a19a9_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics:\n",
      "    ag1_R_max: 39.0\n",
      "    ag1_R_mean: 21.78\n",
      "    ag1_R_min: -9.0\n",
      "    ag2_R_max: 4.400000000000013\n",
      "    ag2_R_mean: -7.127999999999989\n",
      "    ag2_R_min: -9.89999999999998\n",
      "  date: 2021-05-25_15-18-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.899999999999913\n",
      "  episode_reward_mean: 14.477999999999968\n",
      "  episode_reward_min: -17.999999999999986\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 600\n",
      "  experiment_id: 88f4f9358b2746b4a433baa3528e946e\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9753550291061401\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01630435511469841\n",
      "          model: {}\n",
      "          policy_loss: -0.047961167991161346\n",
      "          total_loss: 35.39598846435547\n",
      "          vf_explained_var: 0.4754991829395294\n",
      "          vf_loss: 35.42744064331055\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 1.0423173904418945\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01642986200749874\n",
      "          model: {}\n",
      "          policy_loss: -0.051350511610507965\n",
      "          total_loss: 2.4898324012756348\n",
      "          vf_explained_var: 0.3065849244594574\n",
      "          vf_loss: 2.524547576904297\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.433333333333337\n",
      "    ram_util_percent: 71.23333333333333\n",
      "  pid: 33644\n",
      "  policy_reward_max:\n",
      "    pol1: 38.5\n",
      "    pol2: 4.300000000000013\n",
      "  policy_reward_mean:\n",
      "    pol1: 21.695\n",
      "    pol2: -7.216999999999987\n",
      "  policy_reward_min:\n",
      "    pol1: -9.5\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22772731451264847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.12768153552834297\n",
      "    mean_inference_ms: 1.4874418561418365\n",
      "    mean_raw_obs_processing_ms: 0.9436570661281698\n",
      "  time_since_restore: 20.773210048675537\n",
      "  time_this_iter_s: 4.056123971939087\n",
      "  time_total_s: 65.25042939186096\n",
      "  timers:\n",
      "    learn_throughput: 1158.718\n",
      "    learn_time_ms: 3452.09\n",
      "    sample_throughput: 6859.689\n",
      "    sample_time_ms: 583.117\n",
      "    update_time_ms: 2.363\n",
      "  timestamp: 1621948715\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: a19a9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_a19a9_00000</td><td>RUNNING </td><td>192.168.0.102:33644</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         69.3193</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  16.344</td><td style=\"text-align: right;\">                30.9</td><td style=\"text-align: right;\">               -10.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_a19a9_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics:\n",
      "    ag1_R_max: 39.5\n",
      "    ag1_R_mean: 25.98\n",
      "    ag1_R_min: 0.5\n",
      "    ag2_R_max: 2.200000000000002\n",
      "    ag2_R_mean: -7.160999999999988\n",
      "    ag2_R_min: -9.89999999999998\n",
      "  date: 2021-05-25_15-18-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.699999999999946\n",
      "  episode_reward_mean: 18.61799999999995\n",
      "  episode_reward_min: -7.799999999999985\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 680\n",
      "  experiment_id: 88f4f9358b2746b4a433baa3528e946e\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9469641447067261\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014672379940748215\n",
      "          model: {}\n",
      "          policy_loss: -0.047013480216264725\n",
      "          total_loss: 24.736125946044922\n",
      "          vf_explained_var: 0.6006038784980774\n",
      "          vf_loss: 24.76828384399414\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9911838173866272\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01539328321814537\n",
      "          model: {}\n",
      "          policy_loss: -0.052095670253038406\n",
      "          total_loss: 2.535853624343872\n",
      "          vf_explained_var: 0.3366394639015198\n",
      "          vf_loss: 2.57236385345459\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.900000000000002\n",
      "    ram_util_percent: 71.11666666666667\n",
      "  pid: 33644\n",
      "  policy_reward_max:\n",
      "    pol1: 40.5\n",
      "    pol2: 2.100000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: 25.835\n",
      "    pol2: -7.216999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: 0.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22801409089290253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.12746061975209022\n",
      "    mean_inference_ms: 1.4639268936371463\n",
      "    mean_raw_obs_processing_ms: 0.931025861510868\n",
      "  time_since_restore: 28.911161184310913\n",
      "  time_this_iter_s: 4.069036960601807\n",
      "  time_total_s: 73.38838052749634\n",
      "  timers:\n",
      "    learn_throughput: 1163.898\n",
      "    learn_time_ms: 3436.727\n",
      "    sample_throughput: 6956.626\n",
      "    sample_time_ms: 574.991\n",
      "    update_time_ms: 2.293\n",
      "  timestamp: 1621948723\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: a19a9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_a19a9_00000</td><td>RUNNING </td><td>192.168.0.102:33644</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         77.5612</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  19.494</td><td style=\"text-align: right;\">                35.1</td><td style=\"text-align: right;\">                -8.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_a19a9_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics:\n",
      "    ag1_R_max: 45.0\n",
      "    ag1_R_mean: 27.285\n",
      "    ag1_R_min: -7.0\n",
      "    ag2_R_max: 8.80000000000002\n",
      "    ag2_R_mean: -6.665999999999989\n",
      "    ag2_R_min: -9.89999999999998\n",
      "  date: 2021-05-25_15-18-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.399999999999906\n",
      "  episode_reward_mean: 20.483999999999945\n",
      "  episode_reward_min: -8.10000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 760\n",
      "  experiment_id: 88f4f9358b2746b4a433baa3528e946e\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.9091278314590454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016280846670269966\n",
      "          model: {}\n",
      "          policy_loss: -0.059767886996269226\n",
      "          total_loss: 34.662628173828125\n",
      "          vf_explained_var: 0.5539247989654541\n",
      "          vf_loss: 34.705909729003906\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9699933528900146\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017046764492988586\n",
      "          model: {}\n",
      "          policy_loss: -0.05387716367840767\n",
      "          total_loss: 3.4466116428375244\n",
      "          vf_explained_var: 0.3045375347137451\n",
      "          vf_loss: 3.483229160308838\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.950000000000003\n",
      "    ram_util_percent: 71.53333333333335\n",
      "  pid: 33644\n",
      "  policy_reward_max:\n",
      "    pol1: 44.5\n",
      "    pol2: 8.70000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: 27.195\n",
      "    pol2: -6.710999999999989\n",
      "  policy_reward_min:\n",
      "    pol1: -8.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22999710878737667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.12861032236153583\n",
      "    mean_inference_ms: 1.4542277054977353\n",
      "    mean_raw_obs_processing_ms: 0.9311387681565293\n",
      "  time_since_restore: 37.231261253356934\n",
      "  time_this_iter_s: 4.147294998168945\n",
      "  time_total_s: 81.70848059654236\n",
      "  timers:\n",
      "    learn_throughput: 1159.934\n",
      "    learn_time_ms: 3448.472\n",
      "    sample_throughput: 7032.031\n",
      "    sample_time_ms: 568.826\n",
      "    update_time_ms: 2.231\n",
      "  timestamp: 1621948732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: a19a9_00000\n",
      "  \n",
      "Result for PPO_MultiAgentArena_a19a9_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics:\n",
      "    ag1_R_max: 43.0\n",
      "    ag1_R_mean: 27.705\n",
      "    ag1_R_min: -7.0\n",
      "    ag2_R_max: 8.80000000000002\n",
      "    ag2_R_mean: -6.665999999999988\n",
      "    ag2_R_min: -9.89999999999998\n",
      "  date: 2021-05-25_15-18-56\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.399999999999906\n",
      "  episode_reward_mean: 20.91299999999994\n",
      "  episode_reward_min: -8.10000000000001\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 800\n",
      "  experiment_id: 88f4f9358b2746b4a433baa3528e946e\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      pol1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0003000000142492354\n",
      "          entropy: 0.8978679180145264\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014544561505317688\n",
      "          model: {}\n",
      "          policy_loss: -0.0439334474503994\n",
      "          total_loss: 45.39497375488281\n",
      "          vf_explained_var: 0.4268862009048462\n",
      "          vf_loss: 45.4241828918457\n",
      "      pol2:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.00039999998989515007\n",
      "          entropy: 0.9695947170257568\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016521675512194633\n",
      "          model: {}\n",
      "          policy_loss: -0.05822443962097168\n",
      "          total_loss: 2.8393807411193848\n",
      "          vf_explained_var: 0.28869420289993286\n",
      "          vf_loss: 2.8808770179748535\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.71666666666666\n",
      "    ram_util_percent: 72.25\n",
      "  pid: 33644\n",
      "  policy_reward_max:\n",
      "    pol1: 43.0\n",
      "    pol2: 8.70000000000002\n",
      "  policy_reward_mean:\n",
      "    pol1: 27.635\n",
      "    pol2: -6.721999999999988\n",
      "  policy_reward_min:\n",
      "    pol1: -8.0\n",
      "    pol2: -9.99999999999998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.23067843591450854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1290826917992055\n",
      "    mean_inference_ms: 1.4514925286101152\n",
      "    mean_raw_obs_processing_ms: 0.933843135702358\n",
      "  time_since_restore: 41.753196239471436\n",
      "  time_this_iter_s: 4.521934986114502\n",
      "  time_total_s: 86.23041558265686\n",
      "  timers:\n",
      "    learn_throughput: 1148.405\n",
      "    learn_time_ms: 3483.092\n",
      "    sample_throughput: 7010.602\n",
      "    sample_time_ms: 570.564\n",
      "    update_time_ms: 2.225\n",
      "  timestamp: 1621948736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: a19a9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_a19a9_00000</td><td>RUNNING </td><td>192.168.0.102:33644</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         86.2304</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  20.913</td><td style=\"text-align: right;\">                35.4</td><td style=\"text-align: right;\">                -8.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_a19a9_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         86.2304</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  20.913</td><td style=\"text-align: right;\">                35.4</td><td style=\"text-align: right;\">                -8.1</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-25 15:18:57,233\tINFO tune.py:549 -- Total run time: 55.65 seconds (55.46 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# Setting up our config to point to our new custom callbacks class:\n",
    "config.update({\n",
    "    \"env\": MultiAgentArena,  # force \"reload\"\n",
    "    \"callbacks\": MyCallbacks,  # by default, this would point to `rllib.agents.callbacks.DefaultCallbacks`, which does nothing.\n",
    "    #TODO: remove this once native keras models are supported!\n",
    "    \"model\": {\n",
    "        \"custom_model\": None,\n",
    "    },\n",
    "})\n",
    "\n",
    "results = tune.run(\"PPO\", config=config, stop={\"training_iteration\": 20}, checkpoint_at_end=True, restore=\"/Users/sven/ray_results/PPO/PPO_MultiAgentArena_fd451_00000_0_2021-05-25_15-13-26/checkpoint_000010/checkpoint-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa6a24",
   "metadata": {},
   "source": [
    "### Let's check tensorboard for the new custom metrics!\n",
    "\n",
    "1. Head over to ~/ray_results/PPO/PPO_MultiAgentArena_[some key]_00000_0_[date]_[time]/\n",
    "1. In that directory, you should see a `event.out....` file.\n",
    "1. Run `tensorboard --logdir .` and head to https://localhost:6006\n",
    "\n",
    "<img src=\"images/tensorboard.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd73713f-581a-493d-9198-3c9a98109176",
   "metadata": {},
   "source": [
    "## Exercise No 3\n",
    "\n",
    "<hr />\n",
    "\n",
    "Assume we would like to know exactly how much (new) ground agent1 \n",
    "covers on average in an episode.\n",
    "Write your own custom callback class (sub-class\n",
    "ray.rllib.agents.callback::DefaultCallbacks) and override one or more methods\n",
    "therein to collect the following data:\n",
    "- The number of (unique) fields agent1 has covered in an episode.\n",
    "- The number of times agent2 has blocked agent1.\n",
    "\n",
    "Remember that you can get the last rewards for both agents via the `MultiAgentEpisode` object (`episode`) and its\n",
    "`prev_reward_for([agent name])` method. From these, you should be able to infer, whether a collision happened or whether agent1\n",
    "discovered a new field in the grid.\n",
    "\n",
    "Run a simple experiment using tune.run (and your custom callbacks class)\n",
    "and confirm the new metric shows up in tensorboard.\n",
    "\n",
    "**Good luck! :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d904ad01-7d6c-43c4-aa89-2bf1ff58e7e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_f5b12_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=34739)\u001b[0m 2021-05-25 16:46:23,388\tINFO trainer.py:666 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=34739)\u001b[0m 2021-05-25 16:46:23,388\tINFO trainer.py:691 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/sven/opt/anaconda3/envs/rllib_tutorial/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=34739)\u001b[0m 2021-05-25 16:46:32,133\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_f5b12_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 46\n",
      "    new_fields_discovered_mean: 32.95\n",
      "    new_fields_discovered_min: 21\n",
      "    num_collisions_max: 3\n",
      "    num_collisions_mean: 0.5\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-05-25_16-46-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.000000000000023\n",
      "  episode_reward_mean: -10.019999999999994\n",
      "  episode_reward_min: -28.500000000000053\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 379eb3f5e10b4832a53f910be6d7c8ec\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3659894466400146\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020383195951581\n",
      "          model: {}\n",
      "          policy_loss: -0.057828232645988464\n",
      "          total_loss: 18.440322875976562\n",
      "          vf_explained_var: 0.14482884109020233\n",
      "          vf_loss: 18.49407386779785\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.279999999999998\n",
      "    ram_util_percent: 65.92\n",
      "  pid: 34739\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0530845754510992\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.028531629960615562\n",
      "    mean_inference_ms: 0.5941165910734162\n",
      "    mean_raw_obs_processing_ms: 0.1374685561859405\n",
      "  time_since_restore: 3.1865899562835693\n",
      "  time_this_iter_s: 3.1865899562835693\n",
      "  time_total_s: 3.1865899562835693\n",
      "  timers:\n",
      "    learn_throughput: 1825.079\n",
      "    learn_time_ms: 2191.686\n",
      "    load_throughput: 135057.244\n",
      "    load_time_ms: 29.617\n",
      "    sample_throughput: 4777.176\n",
      "    sample_time_ms: 837.315\n",
      "    update_time_ms: 1.358\n",
      "  timestamp: 1621953995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: f5b12_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_f5b12_00000</td><td>RUNNING </td><td>192.168.0.102:34739</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.18659</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  -10.02</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">               -28.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_f5b12_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 61\n",
      "    new_fields_discovered_mean: 38.36666666666667\n",
      "    new_fields_discovered_min: 21\n",
      "    num_collisions_max: 7\n",
      "    num_collisions_mean: 1.1333333333333333\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-05-25_16-46-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.09999999999991\n",
      "  episode_reward_mean: -1.3349999999999904\n",
      "  episode_reward_min: -28.500000000000053\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: 379eb3f5e10b4832a53f910be6d7c8ec\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2982711791992188\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017935695126652718\n",
      "          model: {}\n",
      "          policy_loss: -0.05370116978883743\n",
      "          total_loss: 14.267951965332031\n",
      "          vf_explained_var: 0.38284537196159363\n",
      "          vf_loss: 14.316275596618652\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.42\n",
      "    ram_util_percent: 66.44\n",
      "  pid: 34739\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05212265180543505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02824042128608617\n",
      "    mean_inference_ms: 0.570871827023131\n",
      "    mean_raw_obs_processing_ms: 0.13610183748790483\n",
      "  time_since_restore: 9.337365865707397\n",
      "  time_this_iter_s: 3.362617015838623\n",
      "  time_total_s: 9.337365865707397\n",
      "  timers:\n",
      "    learn_throughput: 1821.84\n",
      "    learn_time_ms: 2195.582\n",
      "    load_throughput: 349062.341\n",
      "    load_time_ms: 11.459\n",
      "    sample_throughput: 5002.946\n",
      "    sample_time_ms: 799.529\n",
      "    update_time_ms: 1.457\n",
      "  timestamp: 1621954001\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: f5b12_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_f5b12_00000</td><td>RUNNING </td><td>192.168.0.102:34739</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         9.33737</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  -1.335</td><td style=\"text-align: right;\">                32.1</td><td style=\"text-align: right;\">               -28.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_f5b12_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 61\n",
      "    new_fields_discovered_mean: 37.92\n",
      "    new_fields_discovered_min: 21\n",
      "    num_collisions_max: 11\n",
      "    num_collisions_mean: 1.8\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-05-25_16-46-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.09999999999991\n",
      "  episode_reward_mean: -1.6679999999999888\n",
      "  episode_reward_min: -28.500000000000053\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: 379eb3f5e10b4832a53f910be6d7c8ec\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2496124505996704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02063153311610222\n",
      "          model: {}\n",
      "          policy_loss: -0.055548422038555145\n",
      "          total_loss: 14.722307205200195\n",
      "          vf_explained_var: 0.24040167033672333\n",
      "          vf_loss: 14.7716646194458\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.424999999999997\n",
      "    ram_util_percent: 66.425\n",
      "  pid: 34739\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05149066661660861\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02795866090608069\n",
      "    mean_inference_ms: 0.5584262987123582\n",
      "    mean_raw_obs_processing_ms: 0.13494174216888166\n",
      "  time_since_restore: 14.961751699447632\n",
      "  time_this_iter_s: 2.8638648986816406\n",
      "  time_total_s: 14.961751699447632\n",
      "  timers:\n",
      "    learn_throughput: 1888.547\n",
      "    learn_time_ms: 2118.031\n",
      "    load_throughput: 513853.561\n",
      "    load_time_ms: 7.784\n",
      "    sample_throughput: 5230.757\n",
      "    sample_time_ms: 764.708\n",
      "    update_time_ms: 1.49\n",
      "  timestamp: 1621954007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: f5b12_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_f5b12_00000</td><td>RUNNING </td><td>192.168.0.102:34739</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         14.9618</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -1.668</td><td style=\"text-align: right;\">                32.1</td><td style=\"text-align: right;\">               -28.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_f5b12_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 54\n",
      "    new_fields_discovered_mean: 38.81\n",
      "    new_fields_discovered_min: 27\n",
      "    num_collisions_max: 11\n",
      "    num_collisions_mean: 2.54\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-05-25_16-46-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.99999999999995\n",
      "  episode_reward_mean: 0.10500000000001433\n",
      "  episode_reward_min: -19.500000000000014\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 140\n",
      "  experiment_id: 379eb3f5e10b4832a53f910be6d7c8ec\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1811387538909912\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019207734614610672\n",
      "          model: {}\n",
      "          policy_loss: -0.05731140077114105\n",
      "          total_loss: 18.218515396118164\n",
      "          vf_explained_var: 0.23964814841747284\n",
      "          vf_loss: 18.26718521118164\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.43333333333334\n",
      "    ram_util_percent: 67.96666666666665\n",
      "  pid: 34739\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05100703475633614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.027721188207784492\n",
      "    mean_inference_ms: 0.5464527359708125\n",
      "    mean_raw_obs_processing_ms: 0.13458709723993706\n",
      "  time_since_restore: 21.73984456062317\n",
      "  time_this_iter_s: 3.8425509929656982\n",
      "  time_total_s: 21.73984456062317\n",
      "  timers:\n",
      "    learn_throughput: 1812.656\n",
      "    learn_time_ms: 2206.707\n",
      "    load_throughput: 531282.428\n",
      "    load_time_ms: 7.529\n",
      "    sample_throughput: 5070.287\n",
      "    sample_time_ms: 788.91\n",
      "    update_time_ms: 1.544\n",
      "  timestamp: 1621954014\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: f5b12_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_f5b12_00000</td><td>RUNNING </td><td>192.168.0.102:34739</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         21.7398</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   0.105</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_f5b12_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 52\n",
      "    new_fields_discovered_mean: 39.05\n",
      "    new_fields_discovered_min: 27\n",
      "    num_collisions_max: 11\n",
      "    num_collisions_mean: 2.37\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-05-25_16-47-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.99999999999995\n",
      "  episode_reward_mean: 0.3570000000000138\n",
      "  episode_reward_min: -19.500000000000014\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 180\n",
      "  experiment_id: 379eb3f5e10b4832a53f910be6d7c8ec\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.140802264213562\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018799131736159325\n",
      "          model: {}\n",
      "          policy_loss: -0.053859543055295944\n",
      "          total_loss: 17.579652786254883\n",
      "          vf_explained_var: 0.2881280183792114\n",
      "          vf_loss: 17.62505340576172\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.68\n",
      "    ram_util_percent: 67.0\n",
      "  pid: 34739\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051595891051201256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02820052612022285\n",
      "    mean_inference_ms: 0.550001702239574\n",
      "    mean_raw_obs_processing_ms: 0.1385516700970405\n",
      "  time_since_restore: 28.7998309135437\n",
      "  time_this_iter_s: 3.554793119430542\n",
      "  time_total_s: 28.7998309135437\n",
      "  timers:\n",
      "    learn_throughput: 1768.764\n",
      "    learn_time_ms: 2261.466\n",
      "    load_throughput: 597453.197\n",
      "    load_time_ms: 6.695\n",
      "    sample_throughput: 4848.516\n",
      "    sample_time_ms: 824.995\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1621954021\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: f5b12_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_f5b12_00000</td><td>RUNNING </td><td>192.168.0.102:34739</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         28.7998</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   0.357</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">               -19.5</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentArena_f5b12_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics:\n",
      "    new_fields_discovered_max: 52\n",
      "    new_fields_discovered_mean: 39.68\n",
      "    new_fields_discovered_min: 22\n",
      "    num_collisions_max: 12\n",
      "    num_collisions_mean: 2.76\n",
      "    num_collisions_min: 0\n",
      "  date: 2021-05-25_16-47-04\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.99999999999995\n",
      "  episode_reward_mean: 1.5660000000000123\n",
      "  episode_reward_min: -27.000000000000014\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 200\n",
      "  experiment_id: 379eb3f5e10b4832a53f910be6d7c8ec\n",
      "  hostname: Svens-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.119084119796753\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018166445195674896\n",
      "          model: {}\n",
      "          policy_loss: -0.053664419800043106\n",
      "          total_loss: 23.5438232421875\n",
      "          vf_explained_var: 0.16238699853420258\n",
      "          vf_loss: 23.589313507080078\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.102\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.825\n",
      "    ram_util_percent: 67.3\n",
      "  pid: 34739\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05229137190576492\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.028600853128104734\n",
      "    mean_inference_ms: 0.5572003230874837\n",
      "    mean_raw_obs_processing_ms: 0.14094620647091421\n",
      "  time_since_restore: 31.7145357131958\n",
      "  time_this_iter_s: 2.9147047996520996\n",
      "  time_total_s: 31.7145357131958\n",
      "  timers:\n",
      "    learn_throughput: 1788.237\n",
      "    learn_time_ms: 2236.84\n",
      "    load_throughput: 638815.672\n",
      "    load_time_ms: 6.262\n",
      "    sample_throughput: 4871.086\n",
      "    sample_time_ms: 821.172\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1621954024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: f5b12_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects<br>Result logdir: /Users/sven/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MultiAgentArena_f5b12_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         31.7145</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">   1.566</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">                 -27</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-25 16:47:04,553\tINFO tune.py:549 -- Total run time: 47.96 seconds (47.51 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f93675849a0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution Exercise #3\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "class MyCallback(DefaultCallbacks):\n",
    "    def on_episode_start(self, *, worker, base_env,\n",
    "                         policies, episode,\n",
    "                         env_index, **kwargs):\n",
    "        # Set per-episode object to capture, which states (observations)\n",
    "        # have been visited by agent1.\n",
    "        episode.user_data[\"new_fields_discovered\"] = 0\n",
    "        # Set per-episode agent2-blocks counter (how many times has agent2 blocked agent1?).\n",
    "        episode.user_data[\"num_collisions\"] = 0\n",
    "\n",
    "    def on_episode_step(self, *, worker, base_env,\n",
    "                        episode, env_index, **kwargs):\n",
    "        # Get both rewards.\n",
    "        ag1_r = episode.prev_reward_for(\"agent1\")\n",
    "        ag2_r = episode.prev_reward_for(\"agent2\")\n",
    "\n",
    "        # Agent1 discovered a new field.\n",
    "        if ag1_r == 1.0:\n",
    "            episode.user_data[\"new_fields_discovered\"] += 1\n",
    "        # Collision.\n",
    "        elif ag2_r == 1.0:\n",
    "            episode.user_data[\"num_collisions\"] += 1\n",
    "\n",
    "    def on_episode_end(self, *, worker, base_env,\n",
    "                       policies, episode,\n",
    "                       env_index, **kwargs):\n",
    "        # Store everything in `episode.custom_metrics`:\n",
    "        episode.custom_metrics[\"new_fields_discovered\"] = episode.user_data[\"new_fields_discovered\"]\n",
    "        episode.custom_metrics[\"num_collisions\"] = episode.user_data[\"num_collisions\"]\n",
    "\n",
    "\n",
    "stop = {\"training_iteration\": 10}\n",
    "# Specify env and custom callbacks in our config (leave everything else\n",
    "# as-is (defaults)).\n",
    "config = {\n",
    "    \"env\": MultiAgentArena,\n",
    "    \"callbacks\": MyCallback,\n",
    "}\n",
    "\n",
    "# Run for a few iterations.\n",
    "tune.run(\"PPO\", stop=stop, config=config)\n",
    "\n",
    "# Check tensorboard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85ad95",
   "metadata": {},
   "source": [
    "### A closer look at RLlib's APIs and structure\n",
    "\n",
    "We already took a quick look inside an RLlib Trainer object and extracted its Policy(ies) and the Policy's model (neural network). Here is a much more detailed overview of what's inside a Trainer object.\n",
    "\n",
    "At the core is the so-called `WorkerSet` sitting under `Trainer.workers`. A WorkerSet is a group of `RolloutWorker` (`rllib.evaluation.rollout_worker.py`) objects that always consists of a \"local worker\" (`Trainer.workers.local_worker()`) and n \"remote workers\" (`Trainer.workers.remote_workers()`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f37549",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_structure.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d72883",
   "metadata": {},
   "source": [
    "### Scaling RLlib\n",
    "\n",
    "Scaling RLlib works by parallelizing the \"jobs\" that the remote `RolloutWorkers` do. In a vanilla RL algorithm, like PPO, DQN, and many others, the `@ray.remote` labeled RolloutWorkers in the figure above are responsible for interacting with one or more environments and thereby collecting experiences. Observations are produced by the environment, actions are then computed by the Policy(ies) copy located on the remote worker and sent to the environment in order to produce yet another observation. This cycle is repeated endlessly and only sometimes interrupted to send experience batches (\"train batches\") of a certain size to the \"local worker\". There these batches are used to call `Policy.learn_on_batch()`, which performs a loss calculation, followed by a model weights update, and a subsequent weights broadcast back to all the remote workers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29328f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
