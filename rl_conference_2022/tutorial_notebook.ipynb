{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa06051",
   "metadata": {},
   "source": [
    "# Industry-grade, hands-on RL with Ray RLlib\n",
    "## Recommender systems, offline RL, and policy serving\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"images/youtube.png\" style=\"width: 230px;\"/> </td>\n",
    "    <td> <img src=\"images/dota2.jpg\" style=\"width: 213px;\"/> </td>\n",
    "    <td> <img src=\"images/forklifts.jpg\" style=\"width: 169px;\"/> </td>\n",
    "    <td> <img src=\"images/spotify.jpg\" style=\"width: 254px;\"/> </td>\n",
    "    <td> <img src=\"images/robots.jpg\" style=\"width: 252px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Overview\n",
    "“Industry-grade, hands-on RL with Ray RLlib” is a tutorial for industry researchers, domain-experts, and ML-engineers, showcasing ...\n",
    "\n",
    "1) .. how you can use RLlib to build a recommender system simulator for your industry applications and run a slate-capable algorithm against this simulator.\n",
    "\n",
    "2) .. how RLlib's offline algorithms pose solutions in case you don't have a simulator of your problem environment at hand.\n",
    "\n",
    "We will further explore how to deploy one or more trained models to production using Ray Serve and how to use RLlib's bandit algorithms to select a best model from some set of candidates for that purpose.\n",
    "\n",
    "During the live-coding phases, we will build a recommender system simulating environment with RLlib and google's RecSim, choose, configure, and run an RLlib algorithm, and experiment and tune hyperparameters with Ray Tune.\n",
    "\n",
    "RLlib offers industry-grade scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL?) before proceeding to RLlib (recommender system) environments, neural network models, offline RL, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "### Intended Audience\n",
    "* Python programmers who are interested in using RL to solve their specific industry decision making problems and who want to get started with RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "To get this very notebook up and running on your local machine, you can follow these steps here:\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Linux):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab\n",
    "$ pip install tensorflow  # <- either one works!\n",
    "$ pip install torch  # <- either one works!\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac):\n",
    "```\n",
    "$ conda create -n rllib python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install cmake \"ray[rllib,serve]\" recsim jupyterlab\n",
    "$ pip install grpcio # <- extra install only on apple M1 mac\n",
    "$ pip install tensorflow  # <- either one works!\n",
    "$ pip install torch  # <- either one works!\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab pywin32\n",
    "$ pip install tensorflow  # <- either one works!\n",
    "$ pip install torch  # <- either one works!\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials/rl_conference_2022\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and why RLlib?\n",
    "* How do recommender systems work? How do we build our own?\n",
    "* How to configure and hyperparameter-tune an RLlib algorithm that learns how to best recommend items.\n",
    "* RLlib debugging best practices.\n",
    "\n",
    "\n",
    "### Tutorial Outline\n",
    "\n",
    "1. RL and RLlib in a nutshell.\n",
    "1. Defining an RLlib-ready recommender system emulator with google's RecSim.\n",
    "1. Picking an algorithm and training our first RLlib Trainer.\n",
    "1. Configurations and hyperparameters - Easy tuning with Ray Tune.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. Intro to Offline RL.\n",
    "1. What if we don't have an environment? Pretending the output of our previous experiments is historic data with which we can train an offline RL agent.\n",
    "1. BC and MARWIL: Quick how-to and setup instructions.\n",
    "1. Off policy evaluation (OPE) as a means to estimate how well an offline-RL trained policy will perform in production.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. Checkpointing and restoring a Trainer.\n",
    "1. Ray Serve example: How can we deploy a trained policy into our production environment?\n",
    "1. Quick Ray Serve example.\n",
    "1. TODO: Bandits for training how to select the best policy in production.\n",
    "\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n",
    "\n",
    "<img src=\"images/unity3d_blog_post.png\" width=400>\n",
    "\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-yorkshire",
   "metadata": {},
   "source": [
    "## The RL cycle\n",
    "\n",
    "<img src=\"images/rl-cycle.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62744730",
   "metadata": {},
   "source": [
    "### Coding/defining our \"problem\" via an RL environment.\n",
    "\n",
    "We will use the following recommender system simulating environment (based on google's RecSim package)\n",
    "throughout this tutorial to demonstrate a large fraction of RLlib's\n",
    "APIs, features, and customization options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb35116-efda-4799-8bae-e96d7775a0d1",
   "metadata": {},
   "source": [
    "<img src=\"images/environment.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe753-d7e0-4de1-b937-160507f75ed8",
   "metadata": {},
   "source": [
    "#### A word or two on Spaces:\n",
    "\n",
    "Spaces are used in ML to describe what possible/valid values inputs and outputs of a neural network can have.\n",
    "\n",
    "RL environments also use them to describe what their valid observations and actions are.\n",
    "\n",
    "Spaces are usually defined by their shape (e.g. 84x84x3 RGB images) and datatype (e.g. uint8 for RGB values between 0 and 255).\n",
    "However, spaces could also be composed of other spaces (see Tuple or Dict spaces) or could be simply discrete with n fixed possible values\n",
    "(represented by integers). For example, in our recommender system env, where our agent has to suggest a k-slate of items, the action space would be `MultiDiscrete([num-items] * k)`. Our observation space will be a more complex `Dict` space containing user, item (document) and response information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e4135-98ed-4e65-9e26-66f340747529",
   "metadata": {},
   "source": [
    "<img src=\"images/spaces.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7e21f-f3de-4bad-a3a7-4bbd0b015559",
   "metadata": {},
   "source": [
    "# Diving in - Let's start coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b71fcdc-3c95-4f9c-8e47-19e42dbc20ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get started with some basic imports.\n",
    "\n",
    "import ray  # .. of course\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from scipy.stats import sem  # standard error of the mean\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6925507-0210-49d2-9e68-5d1e1157ccd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RecommSys001 at 0x7f8584e677f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from ray.rllib.utils.numpy import softmax\n",
    "\n",
    "\n",
    "class RecommSys001(gym.Env):\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "\n",
    "        config = config or {}\n",
    "\n",
    "        # E (embedding size)\n",
    "        self.num_features = config[\"num_features\"]\n",
    "        # D\n",
    "        self.num_items_to_select_from = config[\"num_items_to_select_from\"]\n",
    "        # k\n",
    "        self.slate_size = config[\"slate_size\"]\n",
    "\n",
    "        self.num_items_in_db = config.get(\"num_items_in_db\")\n",
    "        self.items_db = None\n",
    "        # Generate an items-DB containing n items, once.\n",
    "        if self.num_items_in_db is not None:\n",
    "            self.items_db = [np.random.uniform(-1, 1, size=(self.num_features,))\n",
    "                            for _ in range(self.num_items_in_db)]\n",
    "\n",
    "        self.num_users_in_db = config.get(\"num_users_in_db\")\n",
    "        self.users_db = None\n",
    "        # Store the user that's currently undergoing the episode/session.\n",
    "        self.current_user = None\n",
    "\n",
    "        # How much time does the user have to consume \n",
    "        self.user_time_budget = config.get(\"user_time_budget\", 1.0)\n",
    "        self.current_user_budget = self.user_time_budget\n",
    "\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            # The D items our agent sees at each timestep. It has to select a k-slate\n",
    "            # out of these.\n",
    "            \"doc\": gym.spaces.Dict({\n",
    "                str(idx):\n",
    "                    gym.spaces.Box(-1.0, 1.0, shape=(self.num_features,), dtype=np.float32)\n",
    "                    for idx in range(self.num_items_to_select_from)\n",
    "            }),\n",
    "            # The user engaging in this timestep/episode.\n",
    "            \"user\": gym.spaces.Box(-1.0, 1.0, shape=(self.num_features,), dtype=np.float32),\n",
    "            # For each item in the previous slate, was it clicked? If yes, how\n",
    "            # long was it being engaged with (e.g. watched)?\n",
    "            \"response\": gym.spaces.Tuple([\n",
    "                gym.spaces.Dict({\n",
    "                    # Clicked or not?\n",
    "                    \"click\": gym.spaces.Discrete(2),\n",
    "                    # Engagement time (how many minutes watched?).\n",
    "                    \"engagement\": gym.spaces.Box(-np.inf, np.inf, shape=(), dtype=np.float32),\n",
    "                }) for _ in range(self.slate_size)\n",
    "            ]),\n",
    "        })\n",
    "        # Our action space is\n",
    "        self.action_space = gym.spaces.MultiDiscrete([\n",
    "            self.num_items_to_select_from for _ in range(self.slate_size)\n",
    "        ])\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the current user's time budget.\n",
    "        self.current_user_budget = self.user_time_budget\n",
    "\n",
    "        # Sample a user for the next episode/session.\n",
    "        # Pick from a only-once-sampled user DB.\n",
    "        if self.num_users_in_db is not None:\n",
    "            if self.users_db is None:\n",
    "                self.users_db = [np.random.uniform(-1, 1, size=(self.num_features,))\n",
    "                                 for _ in range(self.num_users_in_db)]\n",
    "            self.current_user = self.users_db[np.random.choice(self.num_users_in_db)]\n",
    "        # Pick from an infinite pool of users.\n",
    "        else:\n",
    "            self.current_user = np.random.uniform(-1, 1, size=(self.num_features,))\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Action is the suggested slate (indices of the items in the suggested ones).\n",
    "\n",
    "        scores = [np.dot(self.current_user, item)\n",
    "                  for item in self.currently_suggested_items]\n",
    "        best_reward = np.max(scores)\n",
    "\n",
    "        # User choice model: User picks an item stochastically,\n",
    "        # where probs are dot products between user- and item feature\n",
    "        # vectors.\n",
    "        # There is also a no-click item whose weight is 0.0.\n",
    "        user_item_overlaps = np.array([scores[a] for a in action] + [0.0])\n",
    "        which_clicked = np.random.choice(\n",
    "            np.arange(self.slate_size + 1), p=softmax(user_item_overlaps))\n",
    "\n",
    "        # Reward is the overlap, if clicked. 0.0 if nothing clicked.\n",
    "        reward = 0.0\n",
    "        # If anything clicked, deduct from the current user's time budget and compute\n",
    "        # reward.\n",
    "        if which_clicked < self.slate_size:\n",
    "            regret = best_reward - user_item_overlaps[which_clicked]\n",
    "            reward = 1.0 - regret\n",
    "            self.current_user_budget -= 1.0\n",
    "        done = self.current_user_budget <= 0.0\n",
    "\n",
    "        # Compile response.\n",
    "        response = tuple({\n",
    "            \"click\": int(idx == which_clicked),\n",
    "            \"engagement\": reward if idx == which_clicked else 0.0,\n",
    "        } for idx in range(len(user_item_overlaps) - 1))\n",
    "\n",
    "        # Return 4-tuple: Next-observation, reward, done (True if episode has terminated), info dict (empty; not used here).\n",
    "        return self._get_obs(response=response), reward, done, {}\n",
    "\n",
    "    def _get_obs(self, response=None):\n",
    "        # Sample D items from infinity or our pre-existing items.\n",
    "        # Pick from a only-once-sampled items DB.\n",
    "        if self.num_items_in_db is not None:\n",
    "            self.currently_suggested_items = [\n",
    "                self.items_db[item_idx].astype(np.float32)\n",
    "                for item_idx in np.random.choice(self.num_items_in_db,\n",
    "                                                size=(self.num_items_to_select_from,),\n",
    "                                                replace=False)\n",
    "            ]\n",
    "        # Pick from an infinite pool of itemsdocs.\n",
    "        else:\n",
    "            self.currently_suggested_items = [\n",
    "                np.random.uniform(-1, 1, size=(self.num_features,)).astype(np.float32)\n",
    "                for _ in range(self.num_items_to_select_from)\n",
    "            ]\n",
    "\n",
    "        return {\n",
    "            \"user\": self.current_user.astype(np.float32),\n",
    "            \"doc\": {\n",
    "                str(idx): item for idx, item in enumerate(self.currently_suggested_items)\n",
    "            },\n",
    "            \"response\": response if response else self.observation_space[\"response\"].sample()\n",
    "        }\n",
    "\n",
    "env = RecommSys001(config={\n",
    "    \"num_features\": 20,  # E (embedding size)\n",
    "\n",
    "    \"num_items_in_db\": 100,  # total number of items in our database\n",
    "    \"num_items_to_select_from\": 10,  # number of items to present to the agent to pick a k-slate from\n",
    "    \"slate_size\": 1,  # k\n",
    "    \"num_users_in_db\": 1,  # total number  of users in our database\n",
    "})\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-sussex",
   "metadata": {},
   "source": [
    "## Testing our environment\n",
    "\n",
    "In the cell above, we created a new environment instance. In order to start \"walking\" through a recommender system episode, we need to perform `reset()` and then several `step()` calls (with different actions) until the returned `done` flag is True.\n",
    "\n",
    "Let's follow these instructions here to get this done:\n",
    "\n",
    "1. `reset` the already created (variable `env`) environment to get the first (initial) observation.\n",
    "1. Enter an infinite while loop.\n",
    "1. Compute the next action for our agent by calling `env.action_space.sample()`.\n",
    "1. Pass this computed action into the env's `step()` method.\n",
    "1. Check the returned `done` for True (episode is terminated) and if True, break out of the loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. episode reward=-0.26+/-0.04\n"
     ]
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "\n",
    "def test_env(env):\n",
    "\n",
    "    # 1) Reset the env.\n",
    "    obs = env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "\n",
    "    # 2) Enter an infinite while loop (to step through the episode).\n",
    "    while num_episodes < 1000:\n",
    "        # 3) Calculate agent's action, using random sampling via the environment's action space.\n",
    "        action = env.action_space.sample()\n",
    "        # action = trainer.compute_single_action([obs])\n",
    "\n",
    "        # 4) Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # 5) Check, whether the episde is done, if yes, break out of the while loop.\n",
    "        if done:\n",
    "            #print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            num_episodes += 1\n",
    "            env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # 6) Print out mean episode reward!\n",
    "    env_mean_random_reward = np.mean(episode_rewards)\n",
    "    print(f\"Avg. episode reward={env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
    "\n",
    "    return env_mean_random_reward, sem(episode_rewards)\n",
    "\n",
    "env_mean_random_reward, env_sem_random_reward = test_env(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a3d658",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 10 min break :)\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
   "metadata": {},
   "source": [
    "# Plugging in RLlib!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd830b90-5762-4d22-8fa9-0abf0777a240",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start a new instance of Ray (when running this tutorial locally) or\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# connect to an already running one (when running this tutorial through Anyscale).\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorials_2/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorials_2/lib/python3.9/site-packages/ray/worker.py:996\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, _enable_object_reconstruction, _redis_max_memory, _plasma_directory, _node_ip_address, _driver_object_store_memory, _memory, _redis_password, _temp_dir, _metrics_export_port, _system_config, _tracing_startup_hook, **kwargs)\u001b[0m\n\u001b[1;32m    994\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RayContext(\u001b[38;5;28mdict\u001b[39m(_global_node\u001b[38;5;241m.\u001b[39maddress_info, node_id\u001b[38;5;241m=\u001b[39mnode_id\u001b[38;5;241m.\u001b[39mhex()))\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 996\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    997\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaybe you called ray.init twice by accident? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    998\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis error can be suppressed by passing in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore_reinit_error=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1000\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mray.shutdown()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m prior to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mray.init()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1001\u001b[0m         )\n\u001b[1;32m   1003\u001b[0m _system_config \u001b[38;5;241m=\u001b[39m _system_config \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_system_config, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'."
     ]
    }
   ],
   "source": [
    "# Start a new instance of Ray (when running this tutorial locally) or\n",
    "# connect to an already running one (when running this tutorial through Anyscale).\n",
    "\n",
    "ray.init()  # Hear the engine humming? ;)\n",
    "\n",
    "# In case you encounter the following error during our tutorial: `RuntimeError: Maybe you called ray.init twice by accident?`\n",
    "# Try: `ray.shutdown() + ray.init()` or `ray.init(ignore_reinit_error=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
   "metadata": {},
   "source": [
    "## Picking an RLlib algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_algorithms.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4656220",
   "metadata": {},
   "source": [
    "### Trying a contextual Bandit on our environment\n",
    "<img src=\"images/contextual_bandit.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d98622c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 14:18:53,900\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................................................................................................................................................................................................................................"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "# We start our endeavor with the Bandit algorithms here b/c they are specialized in solving\n",
    "# n-arm/recommendation problems.\n",
    "from ray.rllib.agents.bandit import BanditLinUCBTrainer\n",
    "\n",
    "# Environment wrapping tools for:\n",
    "# a) Converting MultiDiscrete action space (k-slate recommendations) down to Discrete action space (we only have k=1 for now anyways).\n",
    "# b) Making sure our google RecSim-style environment is understood by RLlib's Bandit Trainers.\n",
    "from ray.rllib.env.wrappers.recsim import MultiDiscreteToDiscreteActionWrapper, \\\n",
    "    RecSimObservationBanditWrapper\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "tune.register_env(\n",
    "    \"recomm-sys-001-for-bandits\",\n",
    "    lambda config: RecSimObservationBanditWrapper(MultiDiscreteToDiscreteActionWrapper(RecommSys001(config))))\n",
    "\n",
    "config = {\n",
    "    # Use our tune-registered \"RecommSys001\" class.\n",
    "    \"env\": \"recomm-sys-001-for-bandits\",\n",
    "    \"env_config\": {\n",
    "        \"num_features\": 20,  # E\n",
    "\n",
    "        \"num_items_in_db\": 100,\n",
    "        \"num_items_to_select_from\": 10,  # D\n",
    "        \"slate_size\": 1,  # k=1\n",
    "\n",
    "        \"num_users_in_db\": 1,\n",
    "    },\n",
    "    #\"evaluation_duration_unit\": \"episodes\",\n",
    "    \"timesteps_per_iteration\": 1,\n",
    "}\n",
    "\n",
    "# Create the RLlib Trainer using above config.\n",
    "bandit_trainer = BanditLinUCBTrainer(config=config)\n",
    "\n",
    "# Train for n iterations (timesteps) and collect n-arm rewards.\n",
    "rewards = []\n",
    "for _ in range(300):\n",
    "    result = bandit_trainer.train()\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(rewards)#x=[i for i in range(len(rewards))], y=rewards, xerr=None, yerr=[sem(rewards) for i in range(len(rewards))])\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236aafae-2660-4894-9193-6176f7cc6d03",
   "metadata": {},
   "source": [
    "## Trying Slate-Q on a harder version of the same environment.\n",
    "\n",
    "So far, we have dumbed down our environment via the following contraints:\n",
    "\n",
    "1. An episode was always only one timestep long (via the config.user_time_budget setting of the env).\n",
    "1. Our slate size (k) was 1 (the algo only had to recommend a single item from the list of suggested ones).\n",
    "1. We were only dealing with a single user (the underlying user vector never changes and is only sampled once upon environment startup).\n",
    "\n",
    "Let's try relaxing all of these restrictions and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5251d23-5be5-44de-a550-87ace027f990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 14:19:20,439\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. episode reward=-3.75+/-0.14\n",
      "............................................................................................................................................................................................................................................................................................................"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Update our env_config: Making things harder.\n",
    "config.update({\n",
    "    \"env_config\": {\n",
    "        \"num_features\": 20,  # E (no change)\n",
    "\n",
    "        \"num_items_in_db\": 100,  # (no change)\n",
    "        \"num_items_to_select_from\": 10,  # D (no change)\n",
    "        \"slate_size\": 2,  # k=2\n",
    "\n",
    "        \"num_users_in_db\": 100,  # More users!\n",
    "        \"user_time_budget\": 10.0,  # Longer episodes.\n",
    "    },\n",
    "})\n",
    "\n",
    "# Re-computing our random baseline.\n",
    "harder_env = RecommSys001(config=config[\"env_config\"])\n",
    "harder_env_mean_random_reward, _ = test_env(harder_env)\n",
    "\n",
    "\n",
    "# Create the RLlib Trainer using above config.\n",
    "bandit_trainer = BanditLinUCBTrainer(config=config)\n",
    "\n",
    "# Train for n iterations (timesteps) and collect n-arm rewards.\n",
    "rewards = []\n",
    "for _ in range(300):\n",
    "    result = bandit_trainer.train()\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(rewards)#x=[i for i in range(len(rewards))], y=rewards, xerr=None, yerr=[sem(rewards) for i in range(len(rewards))])\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=harder_env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79746db-8abc-426d-b71f-f4f68eeb886b",
   "metadata": {},
   "source": [
    "#### Well, that doesn't look so well anymore.\n",
    "\n",
    "Bandits can learn in harder recommender-style envs, but are having a harder time when we increase the number of users, the slate size, or the episode/session length.\n",
    "\n",
    "Let's try the Slate-Q algorithm, which was designed for k-slate and long-time horizon (user journey) recommendations problems.\n",
    "\n",
    "### Switching to Slate-Q\n",
    "<img src=\"images/slateq.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5897c2ee-d0c8-4d06-b580-3c5da033c381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 14:23:03,779\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SlateQTrainer"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "# We use the SlateQ algorithm here b/c it is specialized in solving slate recommendation problems\n",
    "# and works well with RLlib's RecSim environment adapter.\n",
    "\n",
    "from ray.rllib.agents.slateq import SlateQTrainer\n",
    "\n",
    "tune.register_env(\n",
    "    \"recomm-sys-001-for-slateq\",\n",
    "    lambda config: RecommSys001(config))\n",
    "\n",
    "slateq_config = {\n",
    "    \"env\": \"recomm-sys-001-for-slateq\",\n",
    "    \"env_config\": {\n",
    "        \"num_features\": 20,  # E (no change)\n",
    "\n",
    "        \"num_items_in_db\": 100,  # (no change)\n",
    "        \"num_items_to_select_from\": 10,  # D (no change)\n",
    "        \"slate_size\": 2,  # k=2\n",
    "\n",
    "        \"num_users_in_db\": 100,  # More users!\n",
    "        \"user_time_budget\": 10.0,  # Longer episodes.\n",
    "    },\n",
    "}\n",
    "\n",
    "# Instantiate the Trainer object using the exact same config as in our last (harder-to-solve env) Bandit experiment above.\n",
    "slateq_trainer = SlateQTrainer(config=slateq_config)\n",
    "slateq_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ae150-c0a3-477f-8d78-d0a34f147958",
   "metadata": {},
   "source": [
    "### Ready to train with RLlib's SlateQ algorithm\n",
    "\n",
    "That's it, we are ready to train.\n",
    "Calling `Trainer.train()` will execute a single \"training iteration\".\n",
    "\n",
    "One iteration for most algos involves:\n",
    "\n",
    "1. Sampling from the environment(s)\n",
    "1. Using the sampled data (observations, actions taken, rewards) to update the policy model (neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0f6c94d4-6871-4d20-81af-3d4081f05f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sven/opt/anaconda3/envs/rllib_tutorials_2/lib/python3.9/site-packages/gym/spaces/box.py:142: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_timesteps_total': 1002,\n",
      " 'custom_metrics': {},\n",
      " 'date': '2022-02-16_14-21-44',\n",
      " 'done': False,\n",
      " 'episode_len_mean': 14.314285714285715,\n",
      " 'episode_media': {},\n",
      " 'episode_reward_max': 4.585414659546117,\n",
      " 'episode_reward_mean': -2.924722334901943,\n",
      " 'episode_reward_min': -17.265582475516716,\n",
      " 'episodes_this_iter': 70,\n",
      " 'episodes_total': 70,\n",
      " 'experiment_id': 'd0413ff28fa6462d8a348d1cac1ee193',\n",
      " 'hist_stats': {'episode_lengths': [13,\n",
      "                                    15,\n",
      "                                    14,\n",
      "                                    15,\n",
      "                                    15,\n",
      "                                    17,\n",
      "                                    12,\n",
      "                                    15,\n",
      "                                    16,\n",
      "                                    16,\n",
      "                                    14,\n",
      "                                    15,\n",
      "                                    13,\n",
      "                                    15,\n",
      "                                    14,\n",
      "                                    16,\n",
      "                                    18,\n",
      "                                    12,\n",
      "                                    17,\n",
      "                                    11,\n",
      "                                    13,\n",
      "                                    15,\n",
      "                                    13,\n",
      "                                    14,\n",
      "                                    19,\n",
      "                                    19,\n",
      "                                    13,\n",
      "                                    16,\n",
      "                                    15,\n",
      "                                    16,\n",
      "                                    12,\n",
      "                                    14,\n",
      "                                    13,\n",
      "                                    16,\n",
      "                                    12,\n",
      "                                    14,\n",
      "                                    14,\n",
      "                                    13,\n",
      "                                    13,\n",
      "                                    17,\n",
      "                                    12,\n",
      "                                    18,\n",
      "                                    15,\n",
      "                                    14,\n",
      "                                    12,\n",
      "                                    15,\n",
      "                                    14,\n",
      "                                    15,\n",
      "                                    15,\n",
      "                                    13,\n",
      "                                    11,\n",
      "                                    11,\n",
      "                                    16,\n",
      "                                    14,\n",
      "                                    18,\n",
      "                                    13,\n",
      "                                    11,\n",
      "                                    14,\n",
      "                                    15,\n",
      "                                    15,\n",
      "                                    17,\n",
      "                                    17,\n",
      "                                    13,\n",
      "                                    11,\n",
      "                                    16,\n",
      "                                    13,\n",
      "                                    15,\n",
      "                                    11,\n",
      "                                    14,\n",
      "                                    10],\n",
      "                'episode_reward': [2.5896854431762204,\n",
      "                                   -1.047208980716865,\n",
      "                                   -13.419414280757353,\n",
      "                                   -4.664059986786574,\n",
      "                                   -2.2709494817009985,\n",
      "                                   4.585414659546117,\n",
      "                                   -8.644499267001283,\n",
      "                                   -3.1411236985968403,\n",
      "                                   -2.752082544389871,\n",
      "                                   -1.7121791334040362,\n",
      "                                   -0.865204693544158,\n",
      "                                   -6.064178424285485,\n",
      "                                   -3.6204650908484917,\n",
      "                                   -2.6459714710709195,\n",
      "                                   -0.8489872510369731,\n",
      "                                   -3.3753347523054913,\n",
      "                                   -5.909350620464279,\n",
      "                                   -1.1984532995305015,\n",
      "                                   -1.0316027606948577,\n",
      "                                   1.826013430295092,\n",
      "                                   -0.4362490798022993,\n",
      "                                   -1.0626764020388864,\n",
      "                                   -1.4361890771272385,\n",
      "                                   2.6628633844965792,\n",
      "                                   1.398391037256614,\n",
      "                                   0.03332826217665552,\n",
      "                                   0.725685217592092,\n",
      "                                   -6.226116803892845,\n",
      "                                   -3.605017228573902,\n",
      "                                   -2.659024842090632,\n",
      "                                   -1.8166324520476662,\n",
      "                                   -0.5381930140099075,\n",
      "                                   0.9933334723481095,\n",
      "                                   3.700247014586473,\n",
      "                                   -2.61643408919132,\n",
      "                                   -4.1907963953196266,\n",
      "                                   0.21260347399041746,\n",
      "                                   -1.1504140435437409,\n",
      "                                   -7.976818237195051,\n",
      "                                   -5.610726805987547,\n",
      "                                   -7.285343189938271,\n",
      "                                   -4.991017471978443,\n",
      "                                   -3.242804075080139,\n",
      "                                   2.442587030823139,\n",
      "                                   -11.8930426665932,\n",
      "                                   -7.001031394157028,\n",
      "                                   -8.123471087802102,\n",
      "                                   -1.5696265496989503,\n",
      "                                   -0.14864991283708906,\n",
      "                                   -3.883091114216409,\n",
      "                                   0.0519868680138198,\n",
      "                                   -7.850196449281323,\n",
      "                                   -2.1622953979040274,\n",
      "                                   -11.188925966899186,\n",
      "                                   -6.779170387392881,\n",
      "                                   3.35992423583931,\n",
      "                                   0.9733520935792026,\n",
      "                                   0.27715922282163885,\n",
      "                                   -3.4296420305815536,\n",
      "                                   1.0382379486315803,\n",
      "                                   -7.4724688156361445,\n",
      "                                   -8.57308335934011,\n",
      "                                   -6.475060427532286,\n",
      "                                   -17.265582475516716,\n",
      "                                   -4.998370056522183,\n",
      "                                   -0.4899923153713681,\n",
      "                                   1.8550543378794166,\n",
      "                                   0.6603473498322172,\n",
      "                                   -1.2379932481090308,\n",
      "                                   -5.519565325676623]},\n",
      " 'hostname': 'Svens-MBP',\n",
      " 'info': {'last_target_update_ts': 1002,\n",
      "          'learner': {'default_policy': {'custom_metrics': {},\n",
      "                                         'learner_stats': {'allreduce_latency': 0.0,\n",
      "                                                           'choice_loss': 1.0986121892929077,\n",
      "                                                           'choice_model.beta': 0.0,\n",
      "                                                           'choice_model.score_no_click': 0.0,\n",
      "                                                           'grad_gnorm': array(0.13988364, dtype=float32),\n",
      "                                                           'next_q_minus_q': 0.019209474325180054,\n",
      "                                                           'next_q_values': -0.14001505076885223,\n",
      "                                                           'q_loss': 0.4339683949947357,\n",
      "                                                           'q_model.layers.0.bias': 0.007590076886117458,\n",
      "                                                           'q_model.layers.0.weight': -0.0011384141398593783,\n",
      "                                                           'q_model.layers.2.bias': 0.0039223977364599705,\n",
      "                                                           'q_model.layers.2.weight': 0.000167984573636204,\n",
      "                                                           'q_model.layers.4.bias': 0.009015259332954884,\n",
      "                                                           'q_model.layers.4.weight': -0.005302336998283863,\n",
      "                                                           'q_model.layers.6.bias': -0.23076164722442627,\n",
      "                                                           'q_model.layers.6.weight': -0.018669983372092247,\n",
      "                                                           'q_values': -0.1592245250940323,\n",
      "                                                           'raw_scores': 0.0,\n",
      "                                                           'target_q_values': -0.5412757992744446,\n",
      "                                                           'td_error': 0.7520093321800232},\n",
      "                                         'model': {},\n",
      "                                         'num_agent_steps_trained': 32}},\n",
      "          'num_agent_steps_sampled': 1002,\n",
      "          'num_agent_steps_trained': 32,\n",
      "          'num_steps_sampled': 1002,\n",
      "          'num_steps_trained': 32,\n",
      "          'num_steps_trained_this_iter': 32,\n",
      "          'num_target_updates': 1},\n",
      " 'iterations_since_restore': 1,\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_healthy_workers': 0,\n",
      " 'off_policy_estimator': {},\n",
      " 'perf': {'cpu_util_percent': 3.834545454545454,\n",
      "          'ram_util_percent': 64.53454545454545},\n",
      " 'pid': 11293,\n",
      " 'policy_reward_max': {},\n",
      " 'policy_reward_mean': {},\n",
      " 'policy_reward_min': {},\n",
      " 'sampler_perf': {'mean_action_processing_ms': 0.04438103612137221,\n",
      "                  'mean_env_render_ms': 0.0,\n",
      "                  'mean_env_wait_ms': 0.2075817625401384,\n",
      "                  'mean_inference_ms': 1.0413279680763616,\n",
      "                  'mean_raw_obs_processing_ms': 0.16913932199373563},\n",
      " 'time_since_restore': 1.5820789337158203,\n",
      " 'time_this_iter_s': 1.5820789337158203,\n",
      " 'time_total_s': 1.5820789337158203,\n",
      " 'timers': {'learn_throughput': 745.021, 'learn_time_ms': 42.952},\n",
      " 'timestamp': 1645017704,\n",
      " 'timesteps_since_restore': 32,\n",
      " 'timesteps_this_iter': 32,\n",
      " 'timesteps_total': 1002,\n",
      " 'training_iteration': 1,\n",
      " 'trial_id': 'default'}\n"
     ]
    }
   ],
   "source": [
    "results = slateq_trainer.train()\n",
    "\n",
    "# Delete the config from the results for clarity.\n",
    "# Only the stats will remain, then.\n",
    "del results[\"config\"]\n",
    "# Pretty print the stats.\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {},
   "source": [
    "Now that we have confirmed we have setup the Trainer correctly, let's call `train()` on it several times (what about 10 times?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "17ae724d-71cc-422b-96cb-3dc9faa2d111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=2: R(\"return\")=-4.073783967522302\n",
      "Iteration=3: R(\"return\")=-3.3724108400472845\n",
      "Iteration=4: R(\"return\")=-4.112727510589218\n",
      "Iteration=5: R(\"return\")=-3.10285891157491\n",
      "Iteration=6: R(\"return\")=-2.507387553738667\n",
      "Iteration=7: R(\"return\")=-4.253426039886509\n",
      "Iteration=8: R(\"return\")=-4.254775122550739\n",
      "Iteration=9: R(\"return\")=-4.553006182571148\n",
      "Iteration=10: R(\"return\")=-5.3015834325899505\n",
      "Iteration=11: R(\"return\")=-5.560551919857484\n"
     ]
    }
   ],
   "source": [
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "# Move on once you see episode rewards of 1050.0 or more.\n",
    "for _ in range(10):\n",
    "    results = slateq_trainer.train()\n",
    "    print(f\"Iteration={slateq_trainer.iteration}: R(\\\"return\\\")={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK!\n",
    "\n",
    "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using the SlateQ Policy and its NN models inside our Trainer object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aff679e8-74b4-4603-9d5c-4cc0c6ebe45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Policy right now is: SlateQTorchPolicy\n",
      "Our Policy's observation space is: Box([-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1.], (316,), float32)\n",
      "Our Policy's action space is: MultiDiscrete([30 30])\n",
      "tensor([[8.2595e-01, 1.7470e-01, 7.3145e-01, 3.5287e-01, 6.5554e-01, 1.1627e-01,\n",
      "         1.9538e-01, 2.9569e-01, 9.1216e-01, 7.1937e-01],\n",
      "        [2.7715e-01, 9.7778e-02, 4.3646e-01, 5.6098e-01, 4.8621e-01, 9.4148e-01,\n",
      "         2.8128e-01, 5.4114e-02, 2.6984e-01, 6.7640e-02],\n",
      "        [6.7142e-01, 9.0738e-01, 7.3706e-01, 3.4599e-01, 5.9760e-01, 6.5256e-01,\n",
      "         7.5734e-02, 5.1959e-01, 7.7910e-01, 4.0970e-01],\n",
      "        [6.2480e-01, 4.5129e-01, 8.2235e-01, 1.5214e-01, 1.5181e-01, 3.6052e-01,\n",
      "         3.6347e-01, 7.4918e-01, 5.0177e-01, 5.2725e-01],\n",
      "        [9.4736e-01, 9.6382e-01, 7.6199e-01, 2.3037e-01, 5.3119e-01, 2.8912e-01,\n",
      "         9.7314e-01, 5.5287e-01, 9.0727e-01, 5.8168e-02],\n",
      "        [2.0784e-01, 3.6301e-01, 3.0302e-01, 8.4939e-01, 8.3512e-01, 5.8265e-03,\n",
      "         4.9841e-01, 5.8831e-01, 1.8496e-01, 7.3397e-01],\n",
      "        [2.8238e-01, 3.8590e-01, 5.4743e-01, 5.9751e-01, 6.6261e-01, 6.5962e-01,\n",
      "         6.7830e-01, 5.1562e-02, 2.7760e-02, 1.6035e-01],\n",
      "        [3.0574e-01, 5.7392e-01, 3.7636e-02, 7.1081e-01, 6.7780e-01, 1.0396e-01,\n",
      "         8.6800e-01, 7.9141e-01, 3.4038e-02, 3.2254e-01],\n",
      "        [7.4668e-01, 8.6523e-01, 4.2979e-03, 3.7233e-01, 8.5583e-01, 9.7025e-01,\n",
      "         2.3564e-01, 3.5621e-01, 6.6130e-01, 3.1826e-01],\n",
      "        [6.4521e-01, 8.9560e-02, 9.0073e-01, 3.0372e-01, 2.4579e-01, 2.5068e-01,\n",
      "         6.3056e-01, 8.5475e-01, 6.4017e-01, 6.6661e-01],\n",
      "        [3.8823e-01, 5.2487e-01, 5.5200e-01, 5.6874e-01, 5.2224e-01, 3.6571e-01,\n",
      "         3.4889e-01, 2.8001e-01, 6.4296e-01, 9.3702e-01],\n",
      "        [1.9817e-01, 6.9506e-01, 4.7044e-01, 9.1216e-01, 8.2749e-01, 2.5531e-01,\n",
      "         3.2477e-01, 4.7254e-01, 2.6100e-01, 6.5995e-01],\n",
      "        [3.8318e-01, 1.0060e-01, 8.7112e-01, 7.2880e-01, 2.8704e-01, 7.0687e-01,\n",
      "         9.7571e-01, 9.6538e-01, 5.5599e-01, 8.7226e-01],\n",
      "        [4.9325e-01, 9.4688e-01, 1.7790e-01, 5.4299e-01, 5.8248e-01, 2.8587e-01,\n",
      "         4.1442e-01, 6.6975e-01, 6.9822e-01, 9.0139e-01],\n",
      "        [4.6419e-02, 1.7732e-01, 4.5413e-01, 8.8317e-01, 4.7311e-01, 5.9871e-01,\n",
      "         6.9290e-01, 6.4190e-02, 9.5755e-01, 8.5214e-01],\n",
      "        [4.9985e-01, 4.8116e-01, 4.2233e-01, 3.5511e-01, 8.0014e-01, 8.2693e-02,\n",
      "         4.6474e-01, 9.7115e-01, 5.1896e-01, 6.6145e-01],\n",
      "        [1.9029e-01, 6.7362e-01, 9.4070e-01, 3.4918e-01, 1.7775e-01, 8.3608e-02,\n",
      "         2.1935e-01, 3.1957e-01, 3.0736e-01, 3.3498e-01],\n",
      "        [4.2719e-01, 3.0642e-01, 6.9171e-01, 9.7563e-01, 6.5255e-01, 6.1530e-01,\n",
      "         4.6606e-01, 9.7936e-01, 4.0954e-01, 9.3630e-01],\n",
      "        [5.2361e-01, 5.5544e-01, 9.4363e-01, 6.9118e-01, 8.9117e-01, 6.3941e-01,\n",
      "         8.9934e-01, 8.5831e-01, 7.1882e-01, 6.0902e-01],\n",
      "        [8.3374e-01, 7.3830e-01, 6.3080e-01, 3.8877e-02, 2.6686e-01, 5.4578e-01,\n",
      "         8.3357e-01, 3.1755e-01, 2.0519e-01, 8.2784e-01],\n",
      "        [1.9808e-01, 4.2484e-01, 5.0244e-02, 5.9538e-01, 7.7702e-01, 8.6109e-01,\n",
      "         9.3176e-03, 4.4064e-01, 9.7721e-01, 5.1955e-02],\n",
      "        [3.4435e-01, 4.6911e-01, 8.2539e-01, 8.6543e-01, 9.1630e-01, 8.5066e-01,\n",
      "         8.0957e-01, 6.7817e-01, 3.5719e-01, 1.7915e-01],\n",
      "        [8.3800e-01, 1.0801e-01, 7.2825e-01, 5.7475e-01, 2.9694e-01, 2.9294e-01,\n",
      "         3.2274e-01, 2.7389e-01, 8.4201e-01, 7.8171e-01],\n",
      "        [4.6854e-01, 3.3313e-01, 2.9545e-01, 1.5283e-01, 5.2532e-02, 1.8615e-01,\n",
      "         4.5456e-01, 9.4791e-01, 3.2659e-01, 3.2962e-01],\n",
      "        [4.3334e-01, 1.8160e-01, 6.2855e-01, 6.8635e-01, 5.6040e-01, 6.1503e-01,\n",
      "         5.2718e-02, 9.0060e-01, 1.9062e-01, 1.3756e-01],\n",
      "        [8.8598e-01, 5.0523e-01, 2.8309e-01, 9.7989e-01, 6.7461e-02, 3.1464e-01,\n",
      "         6.6886e-01, 8.4451e-01, 9.3032e-01, 9.1704e-01],\n",
      "        [2.8753e-01, 8.8044e-02, 1.6935e-01, 1.3485e-04, 2.6764e-01, 8.5197e-01,\n",
      "         4.4163e-01, 1.5538e-01, 6.9641e-01, 4.9533e-01],\n",
      "        [1.8373e-01, 2.8194e-01, 6.3857e-01, 4.2281e-02, 4.0757e-01, 4.4247e-01,\n",
      "         1.4549e-01, 4.1030e-01, 1.1305e-01, 2.8227e-01],\n",
      "        [7.0610e-01, 4.3672e-01, 1.3129e-01, 6.4522e-01, 2.9132e-01, 2.5187e-01,\n",
      "         5.2524e-01, 2.4740e-01, 9.3095e-01, 8.0343e-01],\n",
      "        [8.1578e-01, 6.1946e-01, 3.5890e-01, 1.6776e-01, 3.8187e-01, 5.4450e-01,\n",
      "         7.2244e-01, 2.9826e-01, 4.1100e-01, 9.8848e-01]])\n",
      "per_slate_q_values=[[372.7125  394.28043 372.4248  407.0327  370.78564 372.00244 363.24164\n",
      "  383.90042 376.20673 374.5849  375.83932 416.4379  394.12863 375.58017\n",
      "  381.29205 372.7122  418.2554  441.94226 376.65524 366.2211  411.47504\n",
      "  373.4665  372.71503 372.03906 414.66272 372.71677 372.71695 371.64578\n",
      "  378.32535 394.3037  365.27396 407.03275 361.05417 344.01102 355.92926\n",
      "  383.91528 380.66898 376.8569  376.09402 416.47772 394.18246 377.5093\n",
      "  381.73257 330.14352 418.26602 441.94226 378.8786  355.18646 411.47516\n",
      "  375.0092  328.87906 357.62628 414.66513 327.7528  327.7634  371.0639\n",
      "  380.99463 394.30246 407.02808 394.2966  394.30234 394.25015 389.64865\n",
      "  394.29227 394.28833 394.0663  406.32916 394.2673  394.27692 394.0453\n",
      "  394.3037  413.9996  441.94144 394.27432 394.27887 411.41785 394.2937\n",
      "  394.30377 394.30188 413.6258  394.30374 394.30374 394.25748 394.27365\n",
      "  407.03275 361.81058 357.23026 356.22324 383.91428 379.90518 376.3105\n",
      "  376.05826 416.47577 394.17947 377.18256 381.69812 365.27054 418.26553\n",
      "  441.94226 378.57257 355.859   411.47513 374.25674 365.32617 361.27304\n",
      "  414.665   365.3607  365.3639  370.9414  380.69498 407.03278 407.03275\n",
      "  407.0327  407.02588 407.03275 407.03275 407.0326  407.03687 407.03073\n",
      "  407.03275 407.03253 407.03275 407.05176 440.4391  407.03275 407.03275\n",
      "  407.47137 407.03278 407.03275 407.03275 407.08447 407.03275 407.03275\n",
      "  407.0327  407.0327  359.13293 356.61487 383.90927 376.69733 373.78125\n",
      "  375.8543  416.46768 394.16592 375.56744 381.52383 361.0538  418.26337\n",
      "  441.94226 377.08212 356.67502 411.4751  370.95056 361.0639  360.4216\n",
      "  414.66452 361.0702  361.0708  370.09555 379.27322 355.7019  383.91394\n",
      "  379.51456 375.87076 376.02762 416.47598 394.17926 376.94504 381.68338\n",
      "  344.0198  418.26556 441.94226 378.38528 354.72827 411.47516 373.4616\n",
      "  344.05093 352.90616 414.66504 344.07153 344.07382 370.69775 380.55258\n",
      "  383.86716 365.23413 364.0421  374.16376 416.40634 394.05826 367.44235\n",
      "  380.12106 355.92926 418.24713 441.94226 369.1766  355.70013 411.47485\n",
      "  361.1115  355.9305  355.99323 414.6607  355.93134 355.9314  364.82764\n",
      "  371.42807 383.91193 383.90762 383.79007 403.2378  387.46658 383.90268\n",
      "  383.8603  383.91528 413.12817 441.94153 383.90347 383.89282 411.40042\n",
      "  383.9096  383.91528 383.91364 413.3783  383.91528 383.91528 383.88382\n",
      "  383.90717 378.72272 376.36914 416.45227 394.156   378.60437 381.69205\n",
      "  380.66843 418.25916 441.94226 379.43097 369.71896 411.47507 378.51453\n",
      "  380.6728  379.3644  414.66357 380.67545 380.6757  373.92026 380.91\n",
      "  376.14267 416.44815 394.14685 377.28113 381.5359  376.85648 418.2581\n",
      "  441.94226 378.24213 367.8041  411.47507 376.17953 376.8603  375.8184\n",
      "  414.66336 376.8626  376.86282 372.84756 379.83585 416.03247 393.64255\n",
      "  376.24753 379.55518 376.094   418.1457  441.94226 376.44638 375.1349\n",
      "  411.4736  376.0536  376.09427 376.02365 414.63773 376.0944  376.09442\n",
      "  375.43478 376.81207 410.548   416.42517 415.87286 416.47772 417.90076\n",
      "  441.94177 416.41724 416.44485 411.49493 416.45947 416.4777  416.47516\n",
      "  414.77374 416.4777  416.4777  416.40137 416.40997 394.12064 393.6021\n",
      "  394.18246 416.2168  441.94196 394.11478 394.12485 411.4503  394.15924\n",
      "  394.18246 394.17813 414.20276 394.1825  394.1825  394.0756  394.11328\n",
      "  381.4342  377.50906 418.25192 441.94226 378.25763 371.1637  411.47498\n",
      "  376.89978 377.5112  376.9001  414.66196 377.5125  377.51263 373.93448\n",
      "  379.55536 381.73257 418.10065 441.94223 381.49518 380.95294 411.47308\n",
      "  381.57028 381.73276 381.67462 414.62796 381.73288 381.73285 380.80728\n",
      "  381.6612  418.26602 441.94226 378.87842 355.1865  411.47516 375.0085\n",
      "  331.86176 357.62607 414.66513 331.94235 332.07648 371.0638  380.99448\n",
      "  441.9405  418.24973 418.2574  411.57855 418.26117 418.26602 418.26535\n",
      "  415.38147 418.26602 418.26602 418.24567 418.24777 441.94226 441.94226\n",
      "  441.79282 441.94226 441.94226 441.94226 441.9339  441.94226 441.94226\n",
      "  441.94226 441.94226 372.9645  411.47495 378.05338 378.88025 378.33026\n",
      "  414.6615  378.8814  378.8815  374.88818 380.02963 411.47504 364.12317\n",
      "  355.1892  355.37662 414.6631  355.19092 355.19107 367.2303  375.34323\n",
      "  411.47513 411.47516 411.47513 411.66214 411.47516 411.47516 411.47485\n",
      "  411.47495 375.0147  373.47433 414.66403 375.01855 375.01886 371.88486\n",
      "  379.87933 357.66336 414.66513 330.80017 331.05228 371.06525 380.9961\n",
      "  414.66495 357.6867  357.68896 370.73285 380.48413 414.66513 414.66513\n",
      "  414.66043 414.661   325.17477 371.0662  380.9971  371.06622 380.99716\n",
      "  376.34714]]\n"
     ]
    }
   ],
   "source": [
    "# To get the policy inside the Trainer, use `Trainer.get_policy([policy ID]=\"default_policy\")`:\n",
    "policy = rllib_trainer.get_policy()\n",
    "print(f\"Our Policy right now is: {policy}\")\n",
    "\n",
    "# To get to the model inside any policy, do:\n",
    "model = policy.model\n",
    "#print(f\"Our Policy's model is: {model}\")\n",
    "\n",
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\")\n",
    "\n",
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = env.observation_space.sample()\n",
    "print(torch.stack([torch.from_numpy(v) for k, v in obs[\"doc\"].items()]))\n",
    "\n",
    "# Get the action logits (as torch tensor).\n",
    "per_slate_q_values = model.get_per_slate_q_values(user=torch.from_numpy(obs[\"user\"]).unsqueeze(0), doc=torch.stack([torch.from_numpy(v) for k, v in obs[\"doc\"].items()]).unsqueeze(0))\n",
    "per_slate_q_values = per_slate_q_values.detach().cpu().numpy()\n",
    "print(f\"per_slate_q_values={per_slate_q_values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de603d14-f0cb-4363-a72b-8f147c094071",
   "metadata": {},
   "source": [
    "In order to release all resources from a Trainer, you can use a Trainer's `stop()` method.\n",
    "You should definitley run this cell as it frees resources that we'll need later in this tutorial, when we'll do parallel hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "737dca4f-942f-4fda-abcc-0052263a103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rllib_trainer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c1e4c-cb02-4719-ac5a-0106172a6c6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Moving stuff to the professional level: RLlib in connection w/ Ray Tune\n",
    "\n",
    "Running any experiments through Ray Tune is the recommended way of doing things with RLlib. If you look at our\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/rllib/examples\">examples scripts folder</a>, you will see that almost all of the scripts use Ray Tune to run the particular RLlib workload demonstrated in each script.\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdacebb-d27f-4174-9002-35c5657f146c",
   "metadata": {
    "tags": []
   },
   "source": [
    "When setting up hyperparameter sweeps for Tune, we'll do this in our already familiar config dict.\n",
    "\n",
    "So let's take a quick look at our SlateQ algo's default config to understand, which hyperparameters we may want to play around with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1b32582-52bd-4585-9009-2f877a0723a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlateQ's default config is:\n",
      "{'_disable_action_flattening': False,\n",
      " '_disable_execution_plan_api': False,\n",
      " '_disable_preprocessor_api': False,\n",
      " '_fake_gpus': False,\n",
      " '_tf_policy_handles_more_than_one_loss': False,\n",
      " 'action_space': None,\n",
      " 'actions_in_input_normalized': False,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'always_attach_evaluation_results': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'buffer_size': -1,\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': False,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': -1,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'double_q': True,\n",
      " 'eager_max_retraces': 20,\n",
      " 'eager_tracing': False,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'env_task_fn': None,\n",
      " 'evaluation_config': {'explore': False},\n",
      " 'evaluation_duration': 10,\n",
      " 'evaluation_duration_unit': 'episodes',\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': -1,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'evaluation_parallel_to_training': False,\n",
      " 'exploration_config': {'type': 'SlateSoftQ'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'torch',\n",
      " 'gamma': 0.99,\n",
      " 'grad_clip': 40,\n",
      " 'hiddens': [256, 64, 16],\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_config': {},\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'keep_per_episode_custom_metrics': False,\n",
      " 'learning_starts': 1000,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'lr': 0.0001,\n",
      " 'lr_choice_model': 0.001,\n",
      " 'lr_q_model': 0.001,\n",
      " 'metrics_episode_collection_timeout_s': 180,\n",
      " 'metrics_num_episodes_for_smoothing': 100,\n",
      " 'metrics_smoothing_episodes': -1,\n",
      " 'min_iter_time_s': -1,\n",
      " 'min_sample_timesteps_per_reporting': None,\n",
      " 'min_time_s_per_reporting': 1,\n",
      " 'min_train_timesteps_per_reporting': None,\n",
      " 'model': {'_disable_action_flattening': False,\n",
      "           '_disable_preprocessor_api': False,\n",
      "           '_time_major': False,\n",
      "           '_use_default_native_models': False,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': True,\n",
      "           'zero_mean': True},\n",
      " 'monitor': -1,\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_map_cache': None,\n",
      "                'policy_map_capacity': 100,\n",
      "                'policy_mapping_fn': None,\n",
      "                'replay_mode': 'independent'},\n",
      " 'no_done_at_end': False,\n",
      " 'normalize_actions': True,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_workers': 0,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'observation_space': None,\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_config': {},\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'record_env': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'replay_buffer_config': {'capacity': 100000, 'type': 'MultiAgentReplayBuffer'},\n",
      " 'replay_sequence_length': 1,\n",
      " 'rollout_fragment_length': 4,\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'seed': None,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'simple_optimizer': -1,\n",
      " 'slateq_strategy': 'QL',\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'target_network_update_freq': 1,\n",
      " 'tau': 0.005,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 1000,\n",
      " 'train_batch_size': 32,\n",
      " 'training_intensity': None,\n",
      " 'worker_side_prioritization': False}\n"
     ]
    }
   ],
   "source": [
    "# Configuration dicts and Ray Tune.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# SlateQ algorithm:\n",
    "from ray.rllib.agents.slateq import DEFAULT_CONFIG as SLATEQ_DEFAULT_CONFIG\n",
    "print(f\"SlateQ's default config is:\")\n",
    "pprint.pprint(SLATEQ_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint.pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint.pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded886cc-436e-46cd-8fea-d68af8b41236",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's do a very simple grid-search over two learning rates with tune.run().\n",
    "\n",
    "In particular, we will try the train_batch_sizes 32 and 64 using `tune.grid_search([...])`\n",
    "inside our config dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5063991e-173b-49be-a4e7-467e2e18321a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:25:42 (running for 00:00:00.12)<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (2 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                32</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72456)\u001b[0m 2022-02-13 18:25:51,048\tINFO trainer.py:861 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72458)\u001b[0m 2022-02-13 18:25:51,048\tINFO trainer.py:861 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:25:51 (running for 00:00:08.90)<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>RUNNING </td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72456)\u001b[0m 2022-02-13 18:25:51,155\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72458)\u001b[0m 2022-02-13 18:25:51,154\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72456)\u001b[0m /Users/sven/opt/anaconda3/envs/ray/lib/python3.8/site-packages/gym/spaces/box.py:142: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72456)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72458)\u001b[0m /Users/sven/opt/anaconda3/envs/ray/lib/python3.8/site-packages/gym/spaces/box.py:142: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72458)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:25:53 (running for 00:00:10.91)<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>RUNNING </td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SlateQ_my_env_f8560_00001:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-13_18-25-53\n",
      "  done: true\n",
      "  episode_len_mean: 87.54545454545455\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.277315709038163\n",
      "  episode_reward_mean: 9.182349776607046\n",
      "  episode_reward_min: 6.221774288301231\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 11\n",
      "  experiment_id: 2854ee45d5e04d1692c62dbbf2cd328a\n",
      "  hostname: Svens-MBP\n",
      "  info:\n",
      "    last_target_update_ts: 1000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          choice_beta: 10.0\n",
      "          choice_loss: 1.3261504173278809\n",
      "          choice_model.beta: 10.0\n",
      "          choice_model.score_no_click: 0.0\n",
      "          choice_score_no_click: 0.0\n",
      "          grad_gnorm: 0.10184044390916824\n",
      "          next_q_minus_q: 0.011721569113433361\n",
      "          next_q_values: 0.07172474265098572\n",
      "          q_loss: 0.01740223728120327\n",
      "          q_model.layers.0.bias: -0.005163680762052536\n",
      "          q_model.layers.0.weight: 0.0023021039087325335\n",
      "          q_model.layers.2.bias: -0.0011576698161661625\n",
      "          q_model.layers.2.weight: -0.00015765760326758027\n",
      "          q_model.layers.4.bias: -0.018162351101636887\n",
      "          q_model.layers.4.weight: 0.0009697921923361719\n",
      "          q_model.layers.6.bias: 0.08257558941841125\n",
      "          q_model.layers.6.weight: 0.01841333508491516\n",
      "          q_values: 0.06000317633152008\n",
      "          raw_scores: 0.6047173142433167\n",
      "          target_q_values: 0.17137761414051056\n",
      "          td_error: 0.11271998286247253\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 64\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 64\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 64\n",
      "    num_steps_trained_this_iter: 64\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.733333333333334\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 72458\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04264905855253145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20202461417976555\n",
      "    mean_inference_ms: 1.0549050349217433\n",
      "    mean_raw_obs_processing_ms: 0.2390135537375223\n",
      "  time_since_restore: 2.0409727096557617\n",
      "  time_this_iter_s: 2.0409727096557617\n",
      "  time_total_s: 2.0409727096557617\n",
      "  timers:\n",
      "    learn_throughput: 4559.413\n",
      "    learn_time_ms: 14.037\n",
      "  timestamp: 1644773153\n",
      "  timesteps_since_restore: 64\n",
      "  timesteps_this_iter: 64\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: f8560_00001\n",
      "  \n",
      "Result for SlateQ_my_env_f8560_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-13_18-25-53\n",
      "  done: false\n",
      "  episode_len_mean: 88.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.496796226342333\n",
      "  episode_reward_mean: 8.97666416090417\n",
      "  episode_reward_min: 6.2187911669965406\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 11\n",
      "  experiment_id: 7f16debe21064adeb50c9cea5966a4ab\n",
      "  hostname: Svens-MBP\n",
      "  info:\n",
      "    last_target_update_ts: 1000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          choice_beta: 10.0\n",
      "          choice_loss: 1.4960806369781494\n",
      "          choice_model.beta: 10.0\n",
      "          choice_model.score_no_click: 0.0\n",
      "          choice_score_no_click: 0.0\n",
      "          grad_gnorm: 0.10409796983003616\n",
      "          next_q_minus_q: 0.029564648866653442\n",
      "          next_q_values: 0.21553686261177063\n",
      "          q_loss: 0.009436151944100857\n",
      "          q_model.layers.0.bias: 0.001102454960346222\n",
      "          q_model.layers.0.weight: -0.0011577063705772161\n",
      "          q_model.layers.2.bias: -0.0010439902544021606\n",
      "          q_model.layers.2.weight: -0.00012651048018597066\n",
      "          q_model.layers.4.bias: 0.010950950905680656\n",
      "          q_model.layers.4.weight: -0.0013435980072245002\n",
      "          q_model.layers.6.bias: 0.18343889713287354\n",
      "          q_model.layers.6.weight: 0.02172801084816456\n",
      "          q_values: 0.185972198843956\n",
      "          raw_scores: 0.7186157703399658\n",
      "          target_q_values: 0.29751530289649963\n",
      "          td_error: 0.1116337850689888\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 32\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 32\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.733333333333334\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 72456\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04428273790723437\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20213608260635849\n",
      "    mean_inference_ms: 1.0659456491232155\n",
      "    mean_raw_obs_processing_ms: 0.23718313737349075\n",
      "  time_since_restore: 2.052877187728882\n",
      "  time_this_iter_s: 2.052877187728882\n",
      "  time_total_s: 2.052877187728882\n",
      "  timers:\n",
      "    learn_throughput: 2636.321\n",
      "    learn_time_ms: 12.138\n",
      "  timestamp: 1644773153\n",
      "  timesteps_since_restore: 32\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: f8560_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:25:59 (running for 00:00:16.88)<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING   </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         5.84984</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> 8.69381</td><td style=\"text-align: right;\">             13.4968</td><td style=\"text-align: right;\">             5.9909 </td><td style=\"text-align: right;\">           87.9091</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SlateQ_my_env_f8560_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-13_18-26-01\n",
      "  done: false\n",
      "  episode_len_mean: 87.8529411764706\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.496796226342333\n",
      "  episode_reward_mean: 8.542674939247744\n",
      "  episode_reward_min: 5.9908987127629265\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 34\n",
      "  experiment_id: 7f16debe21064adeb50c9cea5966a4ab\n",
      "  hostname: Svens-MBP\n",
      "  info:\n",
      "    last_target_update_ts: 3000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          choice_beta: 10.710765838623047\n",
      "          choice_loss: 2.6868820190429688\n",
      "          choice_model.beta: 10.710765838623047\n",
      "          choice_model.score_no_click: -0.7926892638206482\n",
      "          choice_score_no_click: -0.7926892638206482\n",
      "          grad_gnorm: 438.3589172363281\n",
      "          next_q_minus_q: -23.313079833984375\n",
      "          next_q_values: 9057.2119140625\n",
      "          q_loss: 521.5323486328125\n",
      "          q_model.layers.0.bias: 0.3147527575492859\n",
      "          q_model.layers.0.weight: 0.22903656959533691\n",
      "          q_model.layers.2.bias: 0.02270541898906231\n",
      "          q_model.layers.2.weight: 0.019072968512773514\n",
      "          q_model.layers.4.bias: 0.08097691833972931\n",
      "          q_model.layers.4.weight: -0.001361580565571785\n",
      "          q_model.layers.6.bias: 0.3352315127849579\n",
      "          q_model.layers.6.weight: 0.13700604438781738\n",
      "          q_values: 9080.5244140625\n",
      "          raw_scores: 1.4197367429733276\n",
      "          target_q_values: 8966.720703125\n",
      "          td_error: 522.0323486328125\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 16032\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 16032\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 501\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.933333333333332\n",
      "    ram_util_percent: 67.16666666666667\n",
      "  pid: 72456\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042279534494568366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19546526437953005\n",
      "    mean_inference_ms: 1.0207993432420064\n",
      "    mean_raw_obs_processing_ms: 0.22904998317152425\n",
      "  time_since_restore: 9.76020336151123\n",
      "  time_this_iter_s: 3.9103620052337646\n",
      "  time_total_s: 9.76020336151123\n",
      "  timers:\n",
      "    learn_throughput: 4097.238\n",
      "    learn_time_ms: 7.81\n",
      "  timestamp: 1644773161\n",
      "  timesteps_since_restore: 96\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: f8560_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:26:04 (running for 00:00:22.00)<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING   </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         9.7602 </td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\"> 8.54267</td><td style=\"text-align: right;\">             13.4968</td><td style=\"text-align: right;\">             5.9909 </td><td style=\"text-align: right;\">           87.8529</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:26:09 (running for 00:00:27.25)<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING   </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        14.0904 </td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 8.5577 </td><td style=\"text-align: right;\">             13.4968</td><td style=\"text-align: right;\">             5.9909 </td><td style=\"text-align: right;\">           87.9111</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SlateQ_my_env_f8560_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-13_18-26-09\n",
      "  done: false\n",
      "  episode_len_mean: 88.48214285714286\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.496796226342333\n",
      "  episode_reward_mean: 8.480204960215547\n",
      "  episode_reward_min: 5.9908987127629265\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 56\n",
      "  experiment_id: 7f16debe21064adeb50c9cea5966a4ab\n",
      "  hostname: Svens-MBP\n",
      "  info:\n",
      "    last_target_update_ts: 5000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          choice_beta: 10.850476264953613\n",
      "          choice_loss: 2.6736936569213867\n",
      "          choice_model.beta: 10.850476264953613\n",
      "          choice_model.score_no_click: -1.012082576751709\n",
      "          choice_score_no_click: -1.012082576751709\n",
      "          grad_gnorm: 5.253176689147949\n",
      "          next_q_minus_q: 1.8787908554077148\n",
      "          next_q_values: 130.59974670410156\n",
      "          q_loss: 1.602462649345398\n",
      "          q_model.layers.0.bias: 0.11360964179039001\n",
      "          q_model.layers.0.weight: 0.17807967960834503\n",
      "          q_model.layers.2.bias: 0.015433719381690025\n",
      "          q_model.layers.2.weight: 0.006932763382792473\n",
      "          q_model.layers.4.bias: 0.02636750414967537\n",
      "          q_model.layers.4.weight: -0.005369562655687332\n",
      "          q_model.layers.6.bias: 0.24597123265266418\n",
      "          q_model.layers.6.weight: 0.05647430568933487\n",
      "          q_values: 128.720947265625\n",
      "          raw_scores: 1.2273144721984863\n",
      "          target_q_values: 129.38209533691406\n",
      "          td_error: 2.032569646835327\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 32032\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 32032\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 1001\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.366666666666665\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 72456\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042281323353341616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19650953151085118\n",
      "    mean_inference_ms: 1.0213568236493216\n",
      "    mean_raw_obs_processing_ms: 0.2293473218779732\n",
      "  time_since_restore: 18.41587257385254\n",
      "  time_this_iter_s: 4.325501203536987\n",
      "  time_total_s: 18.41587257385254\n",
      "  timers:\n",
      "    learn_throughput: 4333.099\n",
      "    learn_time_ms: 7.385\n",
      "  timestamp: 1644773169\n",
      "  timesteps_since_restore: 160\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: f8560_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:26:15 (running for 00:00:33.14)<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING   </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">        22.7947 </td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\"> 8.51982</td><td style=\"text-align: right;\">             14.3484</td><td style=\"text-align: right;\">             5.34808</td><td style=\"text-align: right;\">           88.0882</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SlateQ_my_env_f8560_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-13_18-26-18\n",
      "  done: false\n",
      "  episode_len_mean: 88.31645569620254\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.348397316568608\n",
      "  episode_reward_mean: 8.471780915961952\n",
      "  episode_reward_min: 5.348079643947841\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 79\n",
      "  experiment_id: 7f16debe21064adeb50c9cea5966a4ab\n",
      "  hostname: Svens-MBP\n",
      "  info:\n",
      "    last_target_update_ts: 7000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          choice_beta: 10.843669891357422\n",
      "          choice_loss: 2.3360838890075684\n",
      "          choice_model.beta: 10.843669891357422\n",
      "          choice_model.score_no_click: -1.0141793489456177\n",
      "          choice_score_no_click: -1.0141793489456177\n",
      "          grad_gnorm: 2.5946853160858154\n",
      "          next_q_minus_q: 0.9468803405761719\n",
      "          next_q_values: 67.03594207763672\n",
      "          q_loss: 0.7521615624427795\n",
      "          q_model.layers.0.bias: 0.10560129582881927\n",
      "          q_model.layers.0.weight: 0.17562110722064972\n",
      "          q_model.layers.2.bias: 0.014242280274629593\n",
      "          q_model.layers.2.weight: 0.005855854135006666\n",
      "          q_model.layers.4.bias: 0.01642632856965065\n",
      "          q_model.layers.4.weight: -0.005849548615515232\n",
      "          q_model.layers.6.bias: 0.2236071228981018\n",
      "          q_model.layers.6.weight: 0.052395839244127274\n",
      "          q_values: 66.08906555175781\n",
      "          raw_scores: 1.2217755317687988\n",
      "          target_q_values: 66.4686050415039\n",
      "          td_error: 1.138209581375122\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 48032\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 48032\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 1501\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.785714285714286\n",
      "    ram_util_percent: 67.85714285714286\n",
      "  pid: 72456\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042495367338140584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19832518890269377\n",
      "    mean_inference_ms: 1.026992598488568\n",
      "    mean_raw_obs_processing_ms: 0.23065104757530014\n",
      "  time_since_restore: 27.20004153251648\n",
      "  time_this_iter_s: 4.405339956283569\n",
      "  time_total_s: 27.20004153251648\n",
      "  timers:\n",
      "    learn_throughput: 4418.342\n",
      "    learn_time_ms: 7.243\n",
      "  timestamp: 1644773178\n",
      "  timesteps_since_restore: 224\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: f8560_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:26:20 (running for 00:00:38.53)<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING   </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">        27.2    </td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\"> 8.47178</td><td style=\"text-align: right;\">             14.3484</td><td style=\"text-align: right;\">             5.34808</td><td style=\"text-align: right;\">           88.3165</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:26:26 (running for 00:00:43.94)<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING   </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        31.4886 </td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> 8.53312</td><td style=\"text-align: right;\">             14.3484</td><td style=\"text-align: right;\">             5.34808</td><td style=\"text-align: right;\">           88.5222</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SlateQ_my_env_f8560_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-13_18-26-27\n",
      "  done: false\n",
      "  episode_len_mean: 88.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.348397316568608\n",
      "  episode_reward_mean: 8.488203978658104\n",
      "  episode_reward_min: 5.348079643947841\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 101\n",
      "  experiment_id: 7f16debe21064adeb50c9cea5966a4ab\n",
      "  hostname: Svens-MBP\n",
      "  info:\n",
      "    last_target_update_ts: 9000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          choice_beta: 10.835955619812012\n",
      "          choice_loss: 2.959062337875366\n",
      "          choice_model.beta: 10.835955619812012\n",
      "          choice_model.score_no_click: -1.0179862976074219\n",
      "          choice_score_no_click: -1.0179862976074219\n",
      "          grad_gnorm: 11.886569023132324\n",
      "          next_q_minus_q: 1.3588831424713135\n",
      "          next_q_values: 89.31866455078125\n",
      "          q_loss: 0.9463427662849426\n",
      "          q_model.layers.0.bias: 0.10830353200435638\n",
      "          q_model.layers.0.weight: 0.1764192134141922\n",
      "          q_model.layers.2.bias: 0.0146125303581357\n",
      "          q_model.layers.2.weight: 0.006143838167190552\n",
      "          q_model.layers.4.bias: 0.020733069628477097\n",
      "          q_model.layers.4.weight: -0.00559161277487874\n",
      "          q_model.layers.6.bias: 0.23773539066314697\n",
      "          q_model.layers.6.weight: 0.05373510718345642\n",
      "          q_values: 87.95977783203125\n",
      "          raw_scores: 1.0845980644226074\n",
      "          target_q_values: 88.50926971435547\n",
      "          td_error: 1.3341495990753174\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 64032\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 64032\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 2001\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.75\n",
      "    ram_util_percent: 67.89999999999999\n",
      "  pid: 72456\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042658434715001727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19960969027865105\n",
      "    mean_inference_ms: 1.031034859101174\n",
      "    mean_raw_obs_processing_ms: 0.23157041855572197\n",
      "  time_since_restore: 35.668009757995605\n",
      "  time_this_iter_s: 4.179455041885376\n",
      "  time_total_s: 35.668009757995605\n",
      "  timers:\n",
      "    learn_throughput: 4756.475\n",
      "    learn_time_ms: 6.728\n",
      "  timestamp: 1644773187\n",
      "  timesteps_since_restore: 288\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: f8560_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:26:31 (running for 00:00:49.12)<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING   </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">        35.668  </td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\"> 8.4882 </td><td style=\"text-align: right;\">             14.3484</td><td style=\"text-align: right;\">             5.34808</td><td style=\"text-align: right;\">           88.61  </td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SlateQ_my_env_f8560_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-13_18-26-31\n",
      "  done: true\n",
      "  episode_len_mean: 88.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.348397316568608\n",
      "  episode_reward_mean: 8.43597573897418\n",
      "  episode_reward_min: 5.348079643947841\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 112\n",
      "  experiment_id: 7f16debe21064adeb50c9cea5966a4ab\n",
      "  hostname: Svens-MBP\n",
      "  info:\n",
      "    last_target_update_ts: 10000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          choice_beta: 10.830251693725586\n",
      "          choice_loss: 2.4941964149475098\n",
      "          choice_model.beta: 10.830251693725586\n",
      "          choice_model.score_no_click: -1.0198209285736084\n",
      "          choice_score_no_click: -1.0198209285736084\n",
      "          grad_gnorm: 13.080735206604004\n",
      "          next_q_minus_q: -1.5250234603881836\n",
      "          next_q_values: 76.68445587158203\n",
      "          q_loss: 3.1740760803222656\n",
      "          q_model.layers.0.bias: 0.1064179465174675\n",
      "          q_model.layers.0.weight: 0.1758025586605072\n",
      "          q_model.layers.2.bias: 0.014357289299368858\n",
      "          q_model.layers.2.weight: 0.005947076715528965\n",
      "          q_model.layers.4.bias: 0.01974213495850563\n",
      "          q_model.layers.4.weight: -0.005652374122291803\n",
      "          q_model.layers.6.bias: 0.23484385013580322\n",
      "          q_model.layers.6.weight: 0.05281691253185272\n",
      "          q_values: 78.20948028564453\n",
      "          raw_scores: 1.2586359977722168\n",
      "          target_q_values: 76.05487823486328\n",
      "          td_error: 3.538743495941162\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 72032\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 72032\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 2251\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.683333333333332\n",
      "    ram_util_percent: 67.86666666666666\n",
      "  pid: 72456\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042595305270069356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20004319682853997\n",
      "    mean_inference_ms: 1.0298484764294724\n",
      "    mean_raw_obs_processing_ms: 0.23155246404025653\n",
      "  time_since_restore: 40.178202867507935\n",
      "  time_this_iter_s: 4.510193109512329\n",
      "  time_total_s: 40.178202867507935\n",
      "  timers:\n",
      "    learn_throughput: 4886.028\n",
      "    learn_time_ms: 6.549\n",
      "  timestamp: 1644773191\n",
      "  timesteps_since_restore: 320\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: f8560_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:26:32 (running for 00:00:49.78)<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>TERMINATED</td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        40.1782 </td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\"> 8.43598</td><td style=\"text-align: right;\">             14.3484</td><td style=\"text-align: right;\">             5.34808</td><td style=\"text-align: right;\">           88.76  </td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\"> 1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 18:26:32,759\tINFO tune.py:636 -- Total run time: 50.46 seconds (49.70 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# Plugging in Ray Tune.\n",
    "# Note that this is the recommended way to run any experiments with RLlib.\n",
    "# Reasons:\n",
    "# - Tune allows you to do hyperparameter tuning in a user-friendly way\n",
    "#   and at large scale!\n",
    "# - Tune automatically allocates needed resources for the different\n",
    "#   hyperparam trials and experiment runs on a cluster.\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Running stuff with tune, we can re-use the exact\n",
    "# same config that we used when working with RLlib directly!\n",
    "tune_config = config.copy()\n",
    "\n",
    "# Let's add our first hyperparameter search via our config.\n",
    "tune_config[\"train_batch_size\"] = tune.grid_search([32, 64])\n",
    "\n",
    "# We will configure an \"output\" location here to make sure we record all environment interactions.\n",
    "# This for the second part of this tutorial, in which we will explore offline RL.\n",
    "tune_config[\"output\"] = \"logdir\"\n",
    "\n",
    "# Now that we will run things \"automatically\" through tune, we have to\n",
    "# define one or more stopping criteria.\n",
    "# Tune will stop the run, once any single one of the criteria is matched (not all of them!).\n",
    "stop = {\n",
    "    # Note that the keys used here can be anything present in the above `rllib_trainer.train()` output dict.\n",
    "    \"training_iteration\": 10,\n",
    "    \"episode_reward_mean\": 9.0,\n",
    "}\n",
    "\n",
    "# \"SlateQ\" is a registered name that points to RLlib's SlateQTrainer.\n",
    "# See `ray/rllib/agents/registry.py`\n",
    "\n",
    "# Run a simple experiment until one of the stopping criteria is met.\n",
    "results = tune.run(\n",
    "    \"SlateQ\",\n",
    "    config=tune_config,\n",
    "    stop=stop,\n",
    "\n",
    "    # Note that no trainers will be returned from this call here.\n",
    "    # Tune will create n Trainers internally, run them in parallel and destroy them at the end.\n",
    "    # However, you can ...\n",
    "    checkpoint_at_end=True,  # ... create a checkpoint when done.\n",
    "    checkpoint_freq=10,  # ... create a checkpoint every 10 training iterations.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b886fb8-6ccd-4be2-80bb-fc0936808d11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Why did we use 2 CPUs in the tune run above (1 CPU per trial)?\n",
    "\n",
    "PPO - by default - SlateQ uses 0 \"rollout\" workers (`num_workers=0`), meaning all sampling happens on the \"local worker\". SlateQ uses a replay buffer, which allows it to ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d282a-4ad1-4d5f-9dec-00afb8154048",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 15 min break :)\n",
    "------------------\n",
    "\n",
    "\n",
    "(while the above experiment is running (and hopefully learning))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82057-6b4c-4075-bd32-93c3426a1700",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction to Offline RL\n",
    "\n",
    "<img src=\"images/offline_rl.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5160e4d0-8feb-411d-a457-dfc10d50e909",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sven/ray_results/SlateQ/SlateQ_my_env_f8560_00001_1_train_batch_size=64_2022-02-13_18-25-42\n",
      "The logdir contains the following files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['result.json',\n",
       " 'params.pkl',\n",
       " 'params.json',\n",
       " 'output-2022-02-13_18-25-51_worker-0_0.json',\n",
       " 'checkpoint_000001',\n",
       " 'events.out.tfevents.1644773142.Svens-MBP',\n",
       " 'progress.csv']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The previous tune.run (the one we did before the break) produced \"historic data\" output.\n",
    "# We will use this output in the following as input to a newly initialized, untrained offline RL algorithm.\n",
    "\n",
    "# Let's take a look at the generated file(s) first:\n",
    "output_dir = results.get_best_logdir(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "print(output_dir)\n",
    "\n",
    "# Here is what the best log directory contains:\n",
    "print(\"The logdir contains the following files:\")\n",
    "os.listdir(os.path.dirname(output_dir + \"/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4230f",
   "metadata": {},
   "source": [
    "### Using an (offline) input file with an offline RL algorithm.\n",
    "\n",
    "We will now pretend that we don't have a simulator for our problem (same recommender system problem as above) available, however, let's assume we possess a lot of pre-recorded, historic data from some legacy (non-RL) system.\n",
    "\n",
    "Assuming that this legacy system wrote some data into a JSON file (we'll simply use the same JSON file that our SlateQ algo produced above), how can we use this historic data to do RL either way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98f129c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 18:58:50,981\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BCTrainer"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's configure a new RLlib Trainer, one that's capable of reading the JSON input described\n",
    "# above and able to learn from this input.\n",
    "\n",
    "# For simplicity, we'll start with a behavioral cloning (BC) trainer:\n",
    "from ray.rllib.agents.marwil import BCTrainer\n",
    "\n",
    "offline_rl_config = {\n",
    "    # Specify your offline RL algo's historic (JSON) input:\n",
    "    \"input\": output_dir + \"/output-2022-02-13_18-25-51_worker-0_0.json\",\n",
    "    # Note: For non-offline RL algos, this is set to \"sampler\" by default.\n",
    "    #\"input\": \"sampler\",\n",
    "    \"observation_space\": env.observation_space,\n",
    "    \"action_space\": env.action_space,\n",
    "}\n",
    "\n",
    "bc_trainer = BCTrainer(config=offline_rl_config)\n",
    "bc_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d181dd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# Let's train our new behavioral cloning Trainer for some iterations:\n",
    "for _ in range(5):\n",
    "    results = bc_trainer.train()\n",
    "    print(results[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8cb107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh no! What happened?\n",
    "# We don't have an environment! No way to measure rewards per episode.\n",
    "\n",
    "# A quick fix would be:\n",
    "# We cheat! Let's use our environment from above to run some separate evaluation workers on while we train:\n",
    "\n",
    "offline_rl_config.update({\n",
    "    # \n",
    "    \"evaluation_interval\": 1,\n",
    "    \"evaluation_parallel_to_training\": True,\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"evaluation_duration\": 100,\n",
    "    \"evaluation_duration_unit\": \"episodes\",\n",
    "    \"evaluation_config\": {\n",
    "        \"env\": \"my_env\",\n",
    "        \"input\": \"sampler\",\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b9ca875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 10:17:24,104\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-02-14 10:17:24,208\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ray.rllib.evaluation.worker_set.WorkerSet object at 0x7fee031a0af0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 10:17:33,257\tWARNING trainer.py:1074 -- Worker crashed during call to `step_attempt()`. To try to continue training without the failed worker, set `ignore_worker_failures=True`.\n"
     ]
    },
    {
     "ename": "RayTaskError(ValueError)",
     "evalue": "\u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=72459, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f935eef41c0>)\n  File \"/Users/sven/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 789, in sample\n    raise ValueError(\nValueError: RolloutWorker has no `input_reader` object! Cannot call `sample()`. You can try setting `create_env_on_driver` to True.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(ValueError)\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#bc_trainer.evaluate()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Let's train our new behavioral cloning Trainer for some iterations:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mbc_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/tune/trainable.py:315\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m\"\"\"Runs one logical iteration of training.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03mCalls ``step()`` internally. Subclasses should override ``step()``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m    A dict that describes training progress.\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    314\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 315\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:1079\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;66;03m# Error out.\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1074\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1075\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorker crashed during call to `step_attempt()`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1076\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo try to continue training without the failed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1077\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker, set `ignore_worker_failures=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1078\u001b[0m         )\n\u001b[0;32m-> 1079\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Any other exception.\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# Allow logs messages to propagate.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:1065\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step_ctx\u001b[38;5;241m.\u001b[39mshould_stop(step_attempt_results):\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1065\u001b[0m         step_attempt_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_attempt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;66;03m# @ray.remote RolloutWorker failure.\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# Try to recover w/o the failed worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:1175\u001b[0m, in \u001b[0;36mTrainer.step_attempt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     step_results\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m   1166\u001b[0m             duration_fn\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         )\n\u001b[1;32m   1173\u001b[0m     )\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1175\u001b[0m     step_results\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;66;03m# Collect the training results from the future.\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m step_results\u001b[38;5;241m.\u001b[39mupdate(train_future\u001b[38;5;241m.\u001b[39mresult())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:1343\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, episodes_left_fn, duration_fn)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m round_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1343\u001b[0m batches \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_workers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munits_left_to_do\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;66;03m# 1 episode per returned batch.\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisodes\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/worker.py:1740\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(ValueError)\u001b[0m: \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=72459, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f935eef41c0>)\n  File \"/Users/sven/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 789, in sample\n    raise ValueError(\nValueError: RolloutWorker has no `input_reader` object! Cannot call `sample()`. You can try setting `create_env_on_driver` to True."
     ]
    }
   ],
   "source": [
    "bc_trainer = BCTrainer(config=offline_rl_config)\n",
    "print(bc_trainer.evaluation_workers)\n",
    "#bc_trainer.evaluate()\n",
    "\n",
    "# Let's train our new behavioral cloning Trainer for some iterations:\n",
    "for _ in range(5):\n",
    "    results = bc_trainer.train()\n",
    "    print(results[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd66c3-f07a-4795-84ea-6b232ba6a047",
   "metadata": {},
   "source": [
    "### Saving and restoring a trained Trainer.\n",
    "Currently, `rllib_trainer` is in an already trained state.\n",
    "It holds optimized weights in its Q-value/Policy's models that allow it to act\n",
    "already somewhat smart in our environment when given an observation.\n",
    "\n",
    "However, if we closed this notebook right now, all the effort would have been for nothing.\n",
    "Let's therefore save the state of our trainer to disk for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57eae1e4-3cc4-4282-9a83-bc374bdad978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer (at iteration 31 was saved in '/Users/sven/ray_results/SlateQTrainer_my_env_2022-02-13_18-01-519mi4r8x8/checkpoint_000031/checkpoint-31'!\n",
      "The checkpoint directory contains the following files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['checkpoint-31.tune_metadata', '.is_checkpoint', 'checkpoint-31']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the `Trainer.save()` method to create a checkpoint.\n",
    "checkpoint_file = rllib_trainer.save()\n",
    "print(f\"Trainer (at iteration {rllib_trainer.iteration} was saved in '{checkpoint_file}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "os.listdir(os.path.dirname(checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e0ab-2c10-469a-97b1-4aadf1a1ec97",
   "metadata": {},
   "source": [
    "### Restoring and evaluating a Trainer\n",
    "In the following cell, we'll learn how to restore a saved Trainer from a checkpoint file.\n",
    "\n",
    "We'll also evaluate a completely new Trainer (should act more or less randomly) vs an already trained one (the one we just restored from the created checkpoint file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74ceedb9-c225-46f2-ad1d-f902c81d3256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 18:10:11,788\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-02-13 18:10:11,935\tINFO trainable.py:472 -- Restored on 127.0.0.1 from checkpoint: /Users/sven/ray_results/SlateQTrainer_my_env_2022-02-13_18-01-519mi4r8x8/checkpoint_000031/checkpoint-31\n",
      "2022-02-13 18:10:11,936\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 31, '_timesteps_total': 992, '_time_total': 112.33620977401733, '_episodes_total': 347}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating new trainer: R=nan\n",
      "Before restoring: Trainer is at iteration=0\n",
      "After restoring: Trainer is at iteration=31\n",
      "Evaluating restored trainer: R=nan\n"
     ]
    }
   ],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_trainer = SlateQTrainer(config=config)\n",
    "# Evaluate the new trainer (this should yield random results).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "print(f\"Before restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "new_trainer.restore(checkpoint_file)\n",
    "print(f\"After restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "\n",
    "# Evaluate again (this should yield results we saw after having trained our saved agent).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating restored trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {},
   "source": [
    "## Time for Q&A\n",
    "\n",
    "...\n",
    "\n",
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful.\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials/tree/main/rl_conference_2022\">github repo of this tutorial</a>.\n",
    "- <a href=\"https://docs.ray.io/en/latest/rllib/index.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on Ray and its libraries.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d097a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
