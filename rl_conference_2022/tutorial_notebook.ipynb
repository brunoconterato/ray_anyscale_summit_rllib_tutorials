{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa06051",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Recommender Systems\n",
    "## From Contextual Bandits to Slate-Q\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"images/youtube.png\" style=\"width: 230px;\"/> </td>\n",
    "    <td> <img src=\"images/dota2.jpg\" style=\"width: 213px;\"/> </td>\n",
    "    <td> <img src=\"images/forklifts.jpg\" style=\"width: 169px;\"/> </td>\n",
    "    <td> <img src=\"images/spotify.jpg\" style=\"width: 254px;\"/> </td>\n",
    "    <td> <img src=\"images/robots.jpg\" style=\"width: 252px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Overview\n",
    "“Industry-grade, hands-on RL with Ray RLlib” is a tutorial for industry researchers, domain-experts, and ML-engineers, showcasing ...\n",
    "\n",
    "1) .. how you can use RLlib to build a recommender system simulator for your industry applications and run a slate-capable algorithm against this simulator.\n",
    "\n",
    "2) .. how RLlib's offline algorithms pose solutions in case you don't have a simulator of your problem environment at hand.\n",
    "\n",
    "We will further explore how to deploy one or more trained models to production using Ray Serve and how RLlib's bandit algorithms could be used to select the best model from some set of candidates for that purpose.\n",
    "\n",
    "During the live-coding phases, we will build a recommender system simulating environment with RLlib and google's RecSim, choose, configure, and run an RLlib algorithm, and experiment and tune hyperparameters with Ray Tune.\n",
    "\n",
    "RLlib offers industry-grade scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL?) before proceeding to RLlib (recommender system) environments, neural network models, offline RL, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "### Intended Audience\n",
    "* Python programmers who are interested in using RL to solve their specific industry decision making problems and who want to get started with RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "To get this very notebook up and running on your local machine, you can follow these steps here:\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Linux):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install cmake \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "$ pip install grpcio # <- extra install only on apple M1 mac\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "$ pip install pywin32 # <- extra install only on Win10.\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials/rl_conference_2022\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and RLlib?\n",
    "* How do recommender systems work? How do we build our own?\n",
    "* How do we train RLlib's different algorithms on a recommender system problem?\n",
    "* What's offline RL and how can I use it with RLlib?\n",
    "\n",
    "\n",
    "\n",
    "### Tutorial Outline\n",
    "\n",
    "1. RL and RLlib in a nutshell.\n",
    "1. Defining a simple, RLlib-ready recommender system environment.\n",
    "1. Testing our environment.\n",
    "\n",
    "(7min break)\n",
    "\n",
    "1. What are contextual bandits?\n",
    "1. How to use contextual Bandits with RLlib and start our first training run.\n",
    "1. What if the environment becomes more difficult? Intro to google's RecSim and RLlib's Slate-Q algorithm.\n",
    "1. Starting a Slate-Q training run using Ray Tune.\n",
    "\n",
    "(7min break)\n",
    "\n",
    "1. Intro to Offline RL.\n",
    "1. What if we don't have an environment? Pretending the output of our previous experiments is historic data with which we can train an offline RL agent.\n",
    "1. BC and MARWIL: Quick how-to and setup instructions.\n",
    "1. Off policy evaluation (OPE) as a means to estimate how well an offline-RL trained policy will perform in production.\n",
    "1. Ray Serve example: How can we deploy a trained policy into our production environment?\n",
    "\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n",
    "\n",
    "<img src=\"images/unity3d_blog_post.png\" width=400>\n",
    "\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-yorkshire",
   "metadata": {},
   "source": [
    "## The RL cycle\n",
    "\n",
    "<img src=\"images/rl-cycle.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62744730",
   "metadata": {},
   "source": [
    "### Coding/defining our \"problem\" via an RL environment.\n",
    "\n",
    "We will use the following recommender system simulating environment (based on google's RecSim package)\n",
    "throughout this tutorial to demonstrate a large fraction of RLlib's\n",
    "APIs, features, and customization options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb35116-efda-4799-8bae-e96d7775a0d1",
   "metadata": {},
   "source": [
    "<img src=\"images/environment.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe753-d7e0-4de1-b937-160507f75ed8",
   "metadata": {},
   "source": [
    "#### A word or two on Spaces:\n",
    "\n",
    "Spaces are used in ML to describe what valid values the in- and outputs of a neural network can have.\n",
    "\n",
    "RL environments also use them to describe what their valid observations and actions are.\n",
    "\n",
    "Spaces are usually defined by their shape (e.g. 84x84x3 RGB images) and datatype (e.g. uint8 for RGB values between 0 and 255).\n",
    "However, spaces could also be composed of other spaces (see Tuple or Dict spaces below) or could be simply discrete with n fixed possible values\n",
    "(represented by integers). For example, in our recommender system env, where our agent has to suggest a k-slate of items, the action space would be `MultiDiscrete([num-items] * k)`. Our observation space will be a more complex `Dict` space containing user, item (document) and response information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e4135-98ed-4e65-9e26-66f340747529",
   "metadata": {},
   "source": [
    "<img src=\"images/spaces.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7e21f-f3de-4bad-a3a7-4bbd0b015559",
   "metadata": {},
   "source": [
    "# Diving in - Let's start coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71fcdc-3c95-4f9c-8e47-19e42dbc20ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get started with some basic imports.\n",
    "\n",
    "import ray  # .. of course\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "import pprint\n",
    "import re\n",
    "import requests\n",
    "from scipy.stats import sem  # standard error of the mean\n",
    "import tree  # dm_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6925507-0210-49d2-9e68-5d1e1157ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from ray.rllib.utils.numpy import softmax\n",
    "\n",
    "\n",
    "class RecommSys001(gym.Env):\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "\n",
    "        config = config or {}\n",
    "\n",
    "        # E (embedding size)\n",
    "        self.num_features = config[\"num_features\"]\n",
    "        # D\n",
    "        self.num_items_to_select_from = config[\"num_items_to_select_from\"]\n",
    "        # k\n",
    "        self.slate_size = config[\"slate_size\"]\n",
    "\n",
    "        self.num_items_in_db = config.get(\"num_items_in_db\")\n",
    "        self.items_db = None\n",
    "        # Generate an items-DB containing n items, once.\n",
    "        if self.num_items_in_db is not None:\n",
    "            self.items_db = [np.random.uniform(0.0, 1.0, size=(self.num_features,))\n",
    "                            for _ in range(self.num_items_in_db)]\n",
    "\n",
    "        self.num_users_in_db = config.get(\"num_users_in_db\")\n",
    "        self.users_db = None\n",
    "        # Store the user that's currently undergoing the episode/session.\n",
    "        self.current_user = None\n",
    "\n",
    "        # How much time does the user have to consume \n",
    "        self.user_time_budget = config.get(\"user_time_budget\", 1.0)\n",
    "        self.current_user_budget = self.user_time_budget\n",
    "\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            # The D items our agent sees at each timestep. It has to select a k-slate\n",
    "            # out of these.\n",
    "            \"doc\": gym.spaces.Dict({\n",
    "                str(idx):\n",
    "                    gym.spaces.Box(0.0, 1.0, shape=(self.num_features,), dtype=np.float32)\n",
    "                    for idx in range(self.num_items_to_select_from)\n",
    "            }),\n",
    "            # The user engaging in this timestep/episode.\n",
    "            \"user\": gym.spaces.Box(0.0, 1.0, shape=(self.num_features,), dtype=np.float32),\n",
    "            # For each item in the previous slate, was it clicked? If yes, how\n",
    "            # long was it being engaged with (e.g. watched)?\n",
    "            \"response\": gym.spaces.Tuple([\n",
    "                gym.spaces.Dict({\n",
    "                    # Clicked or not?\n",
    "                    \"click\": gym.spaces.Discrete(2),\n",
    "                    # Engagement time (how many minutes watched?).\n",
    "                    \"watch_time\": gym.spaces.Box(-np.inf, np.inf, shape=(), dtype=np.float32),\n",
    "                }) for _ in range(self.slate_size)\n",
    "            ]),\n",
    "        })\n",
    "        # Our action space is\n",
    "        self.action_space = gym.spaces.MultiDiscrete([\n",
    "            self.num_items_to_select_from for _ in range(self.slate_size)\n",
    "        ])\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the current user's time budget.\n",
    "        self.current_user_budget = self.user_time_budget\n",
    "\n",
    "        # Sample a user for the next episode/session.\n",
    "        # Pick from a only-once-sampled user DB.\n",
    "        if self.num_users_in_db is not None:\n",
    "            if self.users_db is None:\n",
    "                self.users_db = [np.random.uniform(0.0, 1.0, size=(self.num_features,))\n",
    "                                 for _ in range(self.num_users_in_db)]\n",
    "            self.current_user = self.users_db[np.random.choice(self.num_users_in_db)]\n",
    "        # Pick from an infinite pool of users.\n",
    "        else:\n",
    "            self.current_user = np.random.uniform(0.0, 1, size=(self.num_features,))\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Action is the suggested slate (indices of the items in the suggested ones).\n",
    "\n",
    "        scores = [np.dot(self.current_user, item)\n",
    "                  for item in self.currently_suggested_items]\n",
    "        best_reward = np.max(scores)\n",
    "\n",
    "        # User choice model: User picks an item stochastically,\n",
    "        # where probs are dot products between user- and item feature\n",
    "        # vectors.\n",
    "        # There is also a no-click item whose weight is 1.0.\n",
    "        user_item_overlaps = np.array([scores[a] for a in action] + [1.0])\n",
    "        which_clicked = np.random.choice(\n",
    "            np.arange(self.slate_size + 1), p=softmax(user_item_overlaps))\n",
    "\n",
    "        # Reward is the overlap, if clicked. 0.0 if nothing clicked.\n",
    "        reward = 0.0\n",
    "        # If anything clicked, deduct from the current user's time budget and compute\n",
    "        # reward.\n",
    "        if which_clicked < self.slate_size:\n",
    "            regret = best_reward - user_item_overlaps[which_clicked]\n",
    "            reward = 1.0 - regret\n",
    "            self.current_user_budget -= 1.0\n",
    "        done = self.current_user_budget <= 0.0\n",
    "\n",
    "        # Compile response.\n",
    "        response = tuple({\n",
    "            \"click\": int(idx == which_clicked),\n",
    "            \"watch_time\": reward if idx == which_clicked else 0.0,\n",
    "        } for idx in range(len(user_item_overlaps) - 1))\n",
    "\n",
    "        # Return 4-tuple: Next-observation, reward, done (True if episode has terminated), info dict (empty; not used here).\n",
    "        return self._get_obs(response=response), reward, done, {}\n",
    "\n",
    "    def _get_obs(self, response=None):\n",
    "        # Sample D items from infinity or our pre-existing items.\n",
    "        # Pick from a only-once-sampled items DB.\n",
    "        if self.num_items_in_db is not None:\n",
    "            self.currently_suggested_items = [\n",
    "                self.items_db[item_idx].astype(np.float32)\n",
    "                for item_idx in np.random.choice(self.num_items_in_db,\n",
    "                                                size=(self.num_items_to_select_from,),\n",
    "                                                replace=False)\n",
    "            ]\n",
    "        # Pick from an infinite pool of itemsdocs.\n",
    "        else:\n",
    "            self.currently_suggested_items = [\n",
    "                np.random.uniform(0.0, 1, size=(self.num_features,)).astype(np.float32)\n",
    "                for _ in range(self.num_items_to_select_from)\n",
    "            ]\n",
    "\n",
    "        return {\n",
    "            \"user\": self.current_user.astype(np.float32),\n",
    "            \"doc\": {\n",
    "                str(idx): item for idx, item in enumerate(self.currently_suggested_items)\n",
    "            },\n",
    "            \"response\": response if response else self.observation_space[\"response\"].sample()\n",
    "        }\n",
    "\n",
    "env = RecommSys001(config={\n",
    "    \"num_features\": 20,  # E (embedding size)\n",
    "    \"num_items_in_db\": 100,  # total number of items in our database\n",
    "    \"num_items_to_select_from\": 10,  # number of items to present to the agent to pick a k-slate from\n",
    "    \"slate_size\": 1,  # k\n",
    "    \"num_users_in_db\": 1,  # total number  of users in our database\n",
    "})\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-sussex",
   "metadata": {},
   "source": [
    "## Testing our environment\n",
    "\n",
    "In the cell above, we created a new environment instance. In order to start \"walking\" through a recommender system episode, we need to perform `reset()` and then several `step()` calls (with different actions) until the returned `done` flag is True.\n",
    "\n",
    "Let's follow these instructions here to get this done:\n",
    "\n",
    "1. `reset` the already created environment (variable `env`) to get the first (initial) observation.\n",
    "1. Enter an infinite while loop.\n",
    "1. Compute the next action for our agent by calling `env.action_space.sample()`.\n",
    "1. Pass this computed action into the env's `step()` method.\n",
    "1. Check the returned `done` for True (episode is terminated) and if True, break out of the loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !LIVE CODING!\n",
    "\n",
    "def test_env(env, episodes=1000, verbose=False):\n",
    "\n",
    "    # 1) Reset the env.\n",
    "    obs = env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "\n",
    "    # 2) Enter an infinite while loop (to step through the episode).\n",
    "    while num_episodes < episodes:\n",
    "        # 3) Calculate agent's action, using random sampling via the environment's action space.\n",
    "        action = env.action_space.sample()\n",
    "        # action = trainer.compute_single_action([obs])\n",
    "\n",
    "        # 4) Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # 5) Check, whether the episde is done, if yes, break out of the while loop.\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            num_episodes += 1\n",
    "            env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # 6) Print out mean episode reward!\n",
    "    env_mean_random_reward = np.mean(episode_rewards)\n",
    "    print(f\"Mean episode reward when acting randomly: {env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
    "\n",
    "    return env_mean_random_reward, sem(episode_rewards)\n",
    "\n",
    "env_mean_random_reward, env_sem_random_reward = test_env(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a3d658",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 7 min break :)\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
   "metadata": {},
   "source": [
    "# Plugging in RLlib!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd830b90-5762-4d22-8fa9-0abf0777a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new instance of Ray (when running this tutorial locally) or\n",
    "# connect to an already running one (when running this tutorial through Anyscale).\n",
    "\n",
    "ray.init()  # Hear the engine humming? ;)\n",
    "\n",
    "# In case you encounter the following error during our tutorial: `RuntimeError: Maybe you called ray.init twice by accident?`\n",
    "# Try: `ray.shutdown() + ray.init()` or `ray.init(ignore_reinit_error=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
   "metadata": {},
   "source": [
    "## Picking an RLlib algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_algorithms.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4656220",
   "metadata": {},
   "source": [
    "### Trying a contextual Bandit on our environment\n",
    "<img src=\"images/contextual_bandit.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98622c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "# We start our endeavor with the Bandit algorithms here b/c they are specialized in solving\n",
    "# n-arm/recommendation problems.\n",
    "from ray.rllib.agents.bandit import BanditLinUCBTrainer\n",
    "\n",
    "# Environment wrapping tools for:\n",
    "# a) Converting MultiDiscrete action space (k-slate recommendations) down to Discrete action space (we only have k=1 for now anyways).\n",
    "# b) Making sure our google RecSim-style environment is understood by RLlib's Bandit Trainers.\n",
    "from ray.rllib.env.wrappers.recsim import MultiDiscreteToDiscreteActionWrapper, \\\n",
    "    RecSimObservationBanditWrapper\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "tune.register_env(\n",
    "    \"recomm-sys-001-for-bandits\",\n",
    "    lambda config: RecSimObservationBanditWrapper(MultiDiscreteToDiscreteActionWrapper(RecommSys001(config))))\n",
    "\n",
    "bandit_config = {\n",
    "    # Use our tune-registered \"RecommSys001\" class.\n",
    "    \"env\": \"recomm-sys-001-for-bandits\",\n",
    "    \"env_config\": {\n",
    "        \"num_features\": 20,  # E\n",
    "        \"num_items_in_db\": 100,\n",
    "        \"num_items_to_select_from\": 10,  # D\n",
    "        \"slate_size\": 1,  # k=1\n",
    "        \"num_users_in_db\": 1,\n",
    "    },\n",
    "    #\"evaluation_duration_unit\": \"episodes\",\n",
    "    \"timesteps_per_iteration\": 1,\n",
    "}\n",
    "\n",
    "# Create the RLlib Trainer using above config.\n",
    "bandit_trainer = BanditLinUCBTrainer(config=bandit_config)\n",
    "\n",
    "# Train for n iterations (timesteps) and collect n-arm rewards.\n",
    "rewards = []\n",
    "for _ in range(300):\n",
    "    result = bandit_trainer.train()\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(list(range(10, len(rewards))), rewards[10:])  #x=[i for i in range(len(rewards))], y=rewards, xerr=None, yerr=[sem(rewards) for i in range(len(rewards))])\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528eab5f-548e-4bfc-9651-6865e1ee6573",
   "metadata": {},
   "source": [
    "## Trying Bandits on a tougher environment\n",
    "\n",
    "So far, we have trained against our simple recommender system environment.\n",
    "This environment has certain practical limitations, which you would probably like to avoid in a real recommender systems in production.\n",
    "\n",
    "In particular:\n",
    "\n",
    "1. An episode was always only one timestep long (via the config.user_time_budget setting of the env).\n",
    "1. Our slate size (k) was 1 (the algo only had to recommend a single item from the list of suggested ones).\n",
    "1. We were only dealing with a single user (the underlying user vector never changes and is only sampled once upon environment startup)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d1b10-7bb0-4b2c-be88-8cc69e3711d8",
   "metadata": {},
   "source": [
    "## Introducing google RecSim\n",
    "\n",
    "<a href=\"https://github.com/google-research/recsim\">Google's RecSim package</a> offers a flexible way for you to <a href=\"https://github.com/google-research/recsim/blob/master/recsim/colab/RecSim_Developing_an_Environment.ipynb\">define the different building blocks of a recommender system</a>:\n",
    "\n",
    "- User model (how do users change their preferences when having faced with, selected, and consumed certain items?).\n",
    "- Document model: Features of documents and how do documents get sampled.\n",
    "- Reward functions.\n",
    "\n",
    "RLlib comes with 3 off-the-shelf RecSim environments that are ready for training (with RLlib):\n",
    "* Interest Evolution (the one we'll use in this tutorial)\n",
    "* Long Term Satisfaction\n",
    "* Interest Exploration\n",
    "\n",
    "Let's take a quick look at a pre-configured RecSim environment: \"Intereset Evolution\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b07660b-5ea3-43c0-94e5-c90c1aa84c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import google's recsim package.\n",
    "import recsim\n",
    "\n",
    "# Import a built-in RecSim environment, ready to be trained by RLlib.\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import InterestEvolutionRecSimEnv\n",
    "\n",
    "# Create a RecSim instance using the following config parameters (very similar to what we used above in our own recommender system env):\n",
    "interest_evolution_env = InterestEvolutionRecSimEnv({\n",
    "    \"num_candidates\": 10,\n",
    "    \"resample_documents\": True,\n",
    "    \"slate_size\": 2,\n",
    "})\n",
    "obs = interest_evolution_env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87194f7f-4cdb-46d8-985c-a0be9248d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Bandits, we have to add the following config setting:\n",
    "interest_evolution_env_for_bandits = InterestEvolutionRecSimEnv({\n",
    "    \"num_candidates\": 10,\n",
    "    \"resample_documents\": True,\n",
    "    \"slate_size\": 2,\n",
    "    # Bandits: Add these two config keys here\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "    \"wrap_for_bandits\": True,\n",
    "})\n",
    "obs = interest_evolution_env_for_bandits.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c40aca3-bdfa-4a8f-9d98-7fb4bcd2a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update our env_config: Making things harder.\n",
    "bandit_config.update({\n",
    "    \"env\": InterestEvolutionRecSimEnv,\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 10,\n",
    "        \"resample_documents\": True,\n",
    "        \"slate_size\": 2,\n",
    "        # We need to set the following keys to make this env work with RLlib's Bandits.\n",
    "        \"wrap_for_bandits\": True,  # obs-space wrapping (some keys in the observation dict must be different for Bandits, e.g. \"item\" instead of \"doc\")\n",
    "        \"convert_to_discrete_action_space\": True,  # MultiDiscrete (slate) action space -> Discrete (flattened slate)\n",
    "    },\n",
    "})\n",
    "\n",
    "# Re-computing our random baseline.\n",
    "harder_env_mean_random_reward, _ = test_env(interest_evolution_env_for_bandits, episodes=150)\n",
    "\n",
    "\n",
    "# Create the RLlib Trainer using above config.\n",
    "bandit_trainer = BanditLinUCBTrainer(config=bandit_config)\n",
    "\n",
    "# Train for n iterations (timesteps) and collect n-arm rewards.\n",
    "rewards = []\n",
    "for _ in range(3000):\n",
    "    result = bandit_trainer.train()\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot([rewards[i-500:i] for i in range(500, len(rewards))]) #x=[i for i in range(len(rewards))], y=rewards, xerr=None, yerr=[sem(rewards) for i in range(len(rewards))])\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=harder_env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79746db-8abc-426d-b71f-f4f68eeb886b",
   "metadata": {},
   "source": [
    "#### Well, that doesn't look so great anymore.\n",
    "\n",
    "Bandits are able to learn recommender-system envs, but are having a harder time when we increase the number of users, the slate size, or the episode/session length.\n",
    "\n",
    "Luckily, RLlib offers another algorithm - Slate-Q - designed for k-slate and long-time horizon (user journey) recommendations problems.\n",
    "\n",
    "### Switching to Slate-Q\n",
    "<img src=\"images/slateq.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897c2ee-d0c8-4d06-b580-3c5da033c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "# We use the SlateQ algorithm here b/c it is specialized in solving slate recommendation problems\n",
    "# and works well with RLlib's RecSim environment adapter.\n",
    "\n",
    "from ray.rllib.agents.slateq import SlateQTrainer\n",
    "\n",
    "slateq_config = {\n",
    "    \"env\": InterestEvolutionRecSimEnv,\n",
    "    \"env_config\": bandit_config[\"env_config\"],  # <- use exact same env config as above for direct comparison.\n",
    "    \"exploration_config\": {\n",
    "        \"warmup_timesteps\": 10000,\n",
    "        \"epsilon_timesteps\": 25000,\n",
    "    },\n",
    "    \"replay_buffer_config\": {\n",
    "        \"capacity\": 100000,\n",
    "    },\n",
    "    \"learning_starts\": 10000,\n",
    "    \"target_network_update_freq\": 3200,\n",
    "\n",
    "    \"metrics_num_episodes_for_smoothing\": 200,\n",
    "}\n",
    "# But switch off bandit wrapping and use MultiDiscrete (slate) action space.\n",
    "slateq_config[\"env_config\"].update({\n",
    "    \"wrap_for_bandits\": False,  # SlateQ != Bandit\n",
    "    \"convert_to_discrete_action_space\": False,  # SlateQ handles MultiDiscrete action spaces (slate recommendations).\n",
    "})\n",
    "\n",
    "# Instantiate the Trainer object using the exact same config as in our last (harder-to-solve env) Bandit experiment above.\n",
    "slateq_trainer = SlateQTrainer(config=slateq_config)\n",
    "slateq_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ae150-c0a3-477f-8d78-d0a34f147958",
   "metadata": {},
   "source": [
    "### Ready to train with RLlib's SlateQ algorithm\n",
    "\n",
    "That's it, we are ready to train.\n",
    "Calling `Trainer.train()` will execute a single \"training iteration\".\n",
    "\n",
    "One iteration for most algos involves:\n",
    "\n",
    "1. Sampling from the environment(s)\n",
    "1. Using the sampled data (observations, actions taken, rewards) to update the policy model (neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c94d4-6871-4d20-81af-3d4081f05f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = slateq_trainer.train()\n",
    "\n",
    "# Delete the config from the results for clarity.\n",
    "# Only the stats will remain, then.\n",
    "del results[\"config\"]\n",
    "# Pretty print the stats.\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {},
   "source": [
    "Now that we have confirmed we have setup the Trainer correctly, let's call `train()` on it several times (what about 10 times?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae724d-71cc-422b-96cb-3dc9faa2d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "# Move on once you see episode rewards of 1050.0 or more.\n",
    "for _ in range(10):\n",
    "    results = slateq_trainer.train()\n",
    "    print(f\"Iteration={slateq_trainer.iteration}: R(\\\"return\\\")={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK!\n",
    "\n",
    "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using the SlateQ Policy and its NN models inside our Trainer object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff679e8-74b4-4603-9d5c-4cc0c6ebe45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the policy inside the Trainer, use `Trainer.get_policy([policy ID]=\"default_policy\")`:\n",
    "policy = slateq_trainer.get_policy()\n",
    "print(f\"Our Policy right now is: {policy}\")\n",
    "\n",
    "# To get to the model inside any policy, do:\n",
    "model = policy.model\n",
    "#print(f\"Our Policy's model is: {model}\")\n",
    "\n",
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\\n\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\\n\")\n",
    "\n",
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = env.observation_space.sample()\n",
    "\n",
    "# tf-specific code: Use tf1.Session().\n",
    "sess = policy.get_session()\n",
    "\n",
    "# Get the action logits (as torch tensor).\n",
    "with sess.graph.as_default():\n",
    "    q_values_per_candidate = model.q_value_head([\n",
    "        np.expand_dims(obs[\"user\"], 0),\n",
    "        np.expand_dims(np.concatenate([value for value in obs[\"doc\"].values()]), 0),\n",
    "    ])\n",
    "print(f\"q_values_per_candidate={sess.run(q_values_per_candidate)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de603d14-f0cb-4363-a72b-8f147c094071",
   "metadata": {},
   "source": [
    "In order to release all resources from a Trainer, you can use a Trainer's `stop()` method.\n",
    "You should definitley run this cell as it frees resources that we'll need later in this tutorial, when we'll do parallel hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737dca4f-942f-4fda-abcc-0052263a103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to release resources that a Trainer uses, you can call its `stop()` method:\n",
    "slateq_trainer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c1e4c-cb02-4719-ac5a-0106172a6c6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Moving stuff to the professional level: RLlib in connection w/ Ray Tune\n",
    "\n",
    "Running any experiments through Ray Tune is the recommended way of doing things with RLlib. If you look at our\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/rllib/examples\">examples scripts folder</a>, you will see that almost all of the scripts use Ray Tune to run the particular RLlib workload demonstrated in each script.\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdacebb-d27f-4174-9002-35c5657f146c",
   "metadata": {
    "tags": []
   },
   "source": [
    "When setting up hyperparameter sweeps for Tune, we'll do this in our already familiar config dict.\n",
    "\n",
    "So let's take a quick look at our SlateQ algo's default config to understand, which hyperparameters we may want to play around with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b32582-52bd-4585-9009-2f877a0723a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration dicts and Ray Tune.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# SlateQ algorithm:\n",
    "from ray.rllib.agents.slateq import DEFAULT_CONFIG as SLATEQ_DEFAULT_CONFIG\n",
    "print(f\"SlateQ's default config is:\")\n",
    "pprint.pprint(SLATEQ_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint.pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint.pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded886cc-436e-46cd-8fea-d68af8b41236",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's do a very simple grid-search over two learning rates with tune.run().\n",
    "\n",
    "In particular, we will try the learning rates (\"lr\") 0.00025 and 0.001 using `tune.grid_search([...])`\n",
    "inside our config dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063991e-173b-49be-a4e7-467e2e18321a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plugging in Ray Tune.\n",
    "# Note that this is the recommended way to run any experiments with RLlib.\n",
    "# Reasons:\n",
    "# - Tune allows you to do hyperparameter tuning in a user-friendly way\n",
    "#   and at large scale!\n",
    "# - Tune automatically allocates needed resources for the different\n",
    "#   hyperparam trials and experiment runs on a cluster.\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Running stuff with tune, we can re-use the exact\n",
    "# same config that we used when working with RLlib directly!\n",
    "slateq_tune_config = slateq_config.copy()\n",
    "\n",
    "# Let's add our first hyperparameter search via our config.\n",
    "slateq_tune_config[\"lr\"] = tune.grid_search([0.00025, 0.001])\n",
    "\n",
    "# We will configure an \"output\" location here to make sure we record all environment interactions.\n",
    "# This for the second part of this tutorial, in which we will explore offline RL.\n",
    "slateq_tune_config[\"output\"] = \"logdir\"\n",
    "\n",
    "# Set max. output file size to 256Mb.\n",
    "slateq_tune_config[\"output_max_file_size\"] = 256 * 1024 * 1024  # 256 Mb\n",
    "\n",
    "# Now that we will run things \"automatically\" through tune, we have to\n",
    "# define one or more stopping criteria.\n",
    "# Tune will stop the run, once any single one of the criteria is matched (not all of them!).\n",
    "stop = {\n",
    "    # Note that the keys used here can be anything present in the above `rllib_trainer.train()` output dict.\n",
    "    \"training_iteration\": 50,\n",
    "    \"episode_reward_mean\": 163.0,\n",
    "}\n",
    "\n",
    "# \"SlateQ\" is a registered name that points to RLlib's SlateQTrainer.\n",
    "# See `ray/rllib/agents/registry.py`\n",
    "\n",
    "# Run a simple experiment until one of the stopping criteria is met.\n",
    "results = tune.run(\n",
    "    \"SlateQ\",\n",
    "    config=slateq_tune_config,\n",
    "    stop=stop,\n",
    "    verbose=2,\n",
    "    # Note that no trainers will be returned from this call here.\n",
    "    # Tune will create n Trainers internally, run them in parallel and destroy them at the end.\n",
    "    # However, you can ...\n",
    "    checkpoint_at_end=True,  # ... create a checkpoint when done.\n",
    "    checkpoint_freq=10,  # ... create a checkpoint every 10 training iterations.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d282a-4ad1-4d5f-9dec-00afb8154048",
   "metadata": {
    "tags": []
   },
   "source": [
    "------------------\n",
    "## 7 min break :)\n",
    "\n",
    "(while the above experiment is running (and hopefully learning))\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82057-6b4c-4075-bd32-93c3426a1700",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction to Offline RL\n",
    "\n",
    "<img src=\"images/offline_rl.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5114691-b74f-4b4e-8bdb-5df704bee067",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The previous tune.run (the one we did before the break) produced \"historic data\" output.\n",
    "# We will use this output in the following as input to a newly initialized, untrained offline RL algorithm.\n",
    "\n",
    "# Let's take a look at the generated file(s) first:\n",
    "output_dir = results.get_best_logdir(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "print(output_dir)\n",
    "\n",
    "# Here is what the best log directory contains:\n",
    "print(\"\\n\\nThe logdir contains the following files:\")\n",
    "all_output_files = os.listdir(os.path.dirname(output_dir + \"/\"))\n",
    "pprint.pprint(all_output_files)\n",
    "\n",
    "json_output_file = os.path.join(output_dir, [f for f in all_output_files if re.match(\"^.*worker.*\\.json$\", f)][0])\n",
    "print(\"\\n\\nThe JSON file with all sampled trajectories is:\")\n",
    "print(json_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4230f",
   "metadata": {},
   "source": [
    "### Using an (offline) input file with an offline RL algorithm.\n",
    "\n",
    "We will now pretend that we don't have a simulator for our problem (same recommender system problem as above) available, however, let's assume we possess a lot of pre-recorded, historic data from some legacy (non-RL) system.\n",
    "\n",
    "Assuming that this legacy system wrote some data into a JSON file (we'll simply use the same JSON file that our SlateQ algo produced above), how can we use this historic data to do RL either way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a0aaea-b811-41e1-9e6c-532d9ce1b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the output file first:\n",
    "dataframe = pandas.read_json(json_output_file, lines=True)  # don't forget lines=True -> Each line in the json is one \"rollout\" of 4 timesteps.\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d33782-b4f1-4160-aa7b-657879bc1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's configure a new RLlib Trainer, one that's capable of reading the JSON input described\n",
    "# above and able to learn from this input.\n",
    "\n",
    "# For simplicity, we'll start with a behavioral cloning (BC) trainer:\n",
    "from ray.rllib.agents.marwil.bc import BCTrainer\n",
    "\n",
    "offline_rl_config = {\n",
    "    # Specify your offline RL algo's historic (JSON) inputs:\n",
    "    \"input\": [json_output_file],\n",
    "    # Note: For non-offline RL algos, this is set to \"sampler\" by default.\n",
    "    #\"input\": \"sampler\",\n",
    "    \"observation_space\": interest_evolution_env.observation_space,\n",
    "    \"action_space\": interest_evolution_env.action_space,\n",
    "    \"_disable_preprocessor_api\": True,\n",
    "}\n",
    "\n",
    "bc_trainer = BCTrainer(config=offline_rl_config)\n",
    "bc_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train our new behavioral cloning Trainer for some iterations:\n",
    "for _ in range(5):\n",
    "    results = bc_trainer.train()\n",
    "    print(results[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh no! What happened?\n",
    "# We don't have an environment! No way to measure rewards per episode.\n",
    "\n",
    "# A quick fix would be:\n",
    "# We cheat! Let's use our environment from above to run some separate evaluation workers on while we train:\n",
    "\n",
    "offline_rl_config.update({\n",
    "    # Add an evaluation track\n",
    "    \"evaluation_interval\": 1,\n",
    "    \"evaluation_parallel_to_training\": True,\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"evaluation_duration\": 100,\n",
    "    \"evaluation_duration_unit\": \"episodes\",\n",
    "    \"evaluation_config\": {\n",
    "        \"env\": InterestEvolutionRecSimEnv,\n",
    "        \"env_config\": slateq_config[\"env_config\"],\n",
    "        \"input\": \"sampler\",\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ca875",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer = BCTrainer(config=offline_rl_config)\n",
    "print(bc_trainer.evaluation_workers)\n",
    "#bc_trainer.evaluate()\n",
    "\n",
    "# Let's train our new behavioral cloning Trainer for some iterations:\n",
    "for _ in range(5):\n",
    "    results = bc_trainer.train()\n",
    "    print(results[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd66c3-f07a-4795-84ea-6b232ba6a047",
   "metadata": {},
   "source": [
    "### Saving and restoring a trained Trainer.\n",
    "Currently, `rllib_trainer` is in an already trained state.\n",
    "It holds optimized weights in its Q-value/Policy's models that allow it to act\n",
    "already somewhat smart in our environment when given an observation.\n",
    "\n",
    "However, if we closed this notebook right now, all the effort would have been for nothing.\n",
    "Let's therefore save the state of our trainer to disk for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eae1e4-3cc4-4282-9a83-bc374bdad978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the `Trainer.save()` method to create a checkpoint.\n",
    "checkpoint_file = bc_trainer.save()\n",
    "print(f\"Trainer (at iteration {bc_trainer.iteration} was saved in '{checkpoint_file}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "os.listdir(os.path.dirname(checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e0ab-2c10-469a-97b1-4aadf1a1ec97",
   "metadata": {},
   "source": [
    "### Restoring and evaluating a Trainer\n",
    "In the following cell, we'll learn how to restore a saved Trainer from a checkpoint file.\n",
    "\n",
    "We'll also evaluate a completely new Trainer (should act more or less randomly) vs an already trained one (the one we just restored from the created checkpoint file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ceedb9-c225-46f2-ad1d-f902c81d3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_trainer = BCTrainer(config=offline_rl_config)\n",
    "# Evaluate the new trainer (this should yield random results).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "print(f\"Before restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "new_trainer.restore(checkpoint_file)\n",
    "print(f\"After restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "\n",
    "# Evaluate again (this should yield results we saw after having trained our saved agent).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating restored trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03c572-b98f-47e5-b0b4-404391be9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "serve.start()\n",
    "\n",
    "from starlette.requests import Request\n",
    "\n",
    "\n",
    "\n",
    "@serve.deployment(route_prefix=\"/interest-evolution\")\n",
    "class ServeModel:\n",
    "    def __init__(self, checkpoint_path) -> None:\n",
    "        self.trainer = BCTrainer(\n",
    "            config=offline_rl_config,\n",
    "        )\n",
    "        self.trainer.restore(checkpoint_path)\n",
    "\n",
    "    async def __call__(self, request: Request):\n",
    "        json_input = await request.json()\n",
    "        obs = json_input[\"observation\"]\n",
    "\n",
    "        action = self.trainer.compute_single_action(obs)\n",
    "        return {\"action\": int(action)}\n",
    "\n",
    "    \n",
    "ServeModel.deploy(checkpoint_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082340d-51f4-42a2-853a-508413a3d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    obs = interest_evolution_env.reset()\n",
    "\n",
    "    print(f\"-> Sending observation {obs}\")\n",
    "    resp = requests.get(\n",
    "        \"http://localhost:8000/interest-evolution\", json={\"observation\": tree.map_structure(lambda s: s.tolist() if isinstance(s, np.ndarray) else s, obs)}\n",
    "    )\n",
    "    print(f\"<- Received response {resp.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {},
   "source": [
    "## Time for Q&A\n",
    "\n",
    "...\n",
    "\n",
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful.\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials/tree/main/rl_conference_2022\">github repo of this tutorial</a>.\n",
    "- <a href=\"https://docs.ray.io/en/latest/rllib/index.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on Ray and its libraries.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d097a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
