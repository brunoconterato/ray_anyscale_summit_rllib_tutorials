{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa06051",
   "metadata": {},
   "source": [
    "# Industry-grade, hands-on RL with Ray RLlib\n",
    "## Recommender systems, offline RL, and policy serving\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"images/youtube.png\" style=\"width: 230px;\"/> </td>\n",
    "    <td> <img src=\"images/dota2.jpg\" style=\"width: 213px;\"/> </td>\n",
    "    <td> <img src=\"images/forklifts.jpg\" style=\"width: 169px;\"/> </td>\n",
    "    <td> <img src=\"images/spotify.jpg\" style=\"width: 254px;\"/> </td>\n",
    "    <td> <img src=\"images/robots.jpg\" style=\"width: 252px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Overview\n",
    "“Industry-grade, hands-on RL with Ray RLlib” is a tutorial for industry researchers, domain-experts, and ML-engineers, showcasing ...\n",
    "\n",
    "1) .. how you can use RLlib to build a recommender system simulator for your industry applications and run a slate-capable algorithm against this simulator.\n",
    "\n",
    "2) .. how RLlib's offline algorithms pose solutions in case you don't have a simulator of your problem environment at hand.\n",
    "\n",
    "We will further explore how to deploy one or more trained models to production using Ray Serve and how to use RLlib's bandit algorithms to select a best model from some set of candidates for that purpose.\n",
    "\n",
    "During the live-coding phases, we will build a recommender system simulating environment with RLlib and google's RecSim, choose, configure, and run an RLlib algorithm, and experiment and tune hyperparameters with Ray Tune.\n",
    "\n",
    "RLlib offers industry-grade scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL?) before proceeding to RLlib (recommender system) environments, neural network models, offline RL, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "### Intended Audience\n",
    "* Python programmers who are interested in using RL to solve their specific industry decision making problems and who want to get started with RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "To get this very notebook up and running on your local machine, you can follow these steps here:\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Linux):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab\n",
    "$ pip install tensorflow  # <- either one works!\n",
    "$ pip install torch  # <- either one works!\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac):\n",
    "```\n",
    "$ conda create -n rllib python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install cmake \"ray[rllib,serve]\" recsim jupyterlab\n",
    "$ pip install grpcio # <- extra install only on apple M1 mac\n",
    "$ pip install tensorflow  # <- either one works!\n",
    "$ pip install torch  # <- either one works!\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab pywin32\n",
    "$ pip install tensorflow  # <- either one works!\n",
    "$ pip install torch  # <- either one works!\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials/rl_conference_2022\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and why RLlib?\n",
    "* How do recommender systems work? How do we build our own?\n",
    "* How to configure and hyperparameter-tune an RLlib algorithm that learns how to best recommend items.\n",
    "* RLlib debugging best practices.\n",
    "\n",
    "\n",
    "### Tutorial Outline\n",
    "\n",
    "1. RL and RLlib in a nutshell.\n",
    "1. Defining an RLlib-ready recommender system emulator with google's RecSim.\n",
    "1. Picking an algorithm and training our first RLlib Trainer.\n",
    "1. Configurations and hyperparameters - Easy tuning with Ray Tune.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. Intro to Offline RL.\n",
    "1. What if we don't have an environment? Pretending the output of our previous experiments is historic data with which we can train an offline RL agent.\n",
    "1. BC and MARWIL: Quick how-to and setup instructions.\n",
    "1. Off policy evaluation (OPE) as a means to estimate how well an offline-RL trained policy will perform in production.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. Checkpointing and restoring a Trainer.\n",
    "1. Ray Serve example: How can we deploy a trained policy into our production environment?\n",
    "1. Quick Ray Serve example.\n",
    "1. TODO: Bandits for training how to select the best policy in production.\n",
    "\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n",
    "\n",
    "<img src=\"images/unity3d_blog_post.png\" width=400>\n",
    "\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-yorkshire",
   "metadata": {},
   "source": [
    "## The RL cycle\n",
    "\n",
    "<img src=\"images/rl-cycle.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62744730",
   "metadata": {},
   "source": [
    "### Coding/defining our \"problem\" via an RL environment.\n",
    "\n",
    "We will use the following recommender system simulating environment (based on google's RecSim package)\n",
    "throughout this tutorial to demonstrate a large fraction of RLlib's\n",
    "APIs, features, and customization options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb35116-efda-4799-8bae-e96d7775a0d1",
   "metadata": {},
   "source": [
    "<img src=\"images/environment.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe753-d7e0-4de1-b937-160507f75ed8",
   "metadata": {},
   "source": [
    "#### A word or two on Spaces:\n",
    "\n",
    "Spaces are used in ML to describe what valid values the in- and outputs of a neural network can have.\n",
    "\n",
    "RL environments also use them to describe what their valid observations and actions are.\n",
    "\n",
    "Spaces are usually defined by their shape (e.g. 84x84x3 RGB images) and datatype (e.g. uint8 for RGB values between 0 and 255).\n",
    "However, spaces could also be composed of other spaces (see Tuple or Dict spaces below) or could be simply discrete with n fixed possible values\n",
    "(represented by integers). For example, in our recommender system env, where our agent has to suggest a k-slate of items, the action space would be `MultiDiscrete([num-items] * k)`. Our observation space will be a more complex `Dict` space containing user, item (document) and response information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e4135-98ed-4e65-9e26-66f340747529",
   "metadata": {},
   "source": [
    "<img src=\"images/spaces.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7e21f-f3de-4bad-a3a7-4bbd0b015559",
   "metadata": {},
   "source": [
    "# Diving in - Let's start coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b71fcdc-3c95-4f9c-8e47-19e42dbc20ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get started with some basic imports.\n",
    "\n",
    "import ray  # .. of course\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from scipy.stats import sem  # standard error of the mean\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f6925507-0210-49d2-9e68-5d1e1157ccd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RecommSys001 at 0x7f856e4f8c40>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from ray.rllib.utils.numpy import softmax\n",
    "\n",
    "\n",
    "class RecommSys001(gym.Env):\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "\n",
    "        config = config or {}\n",
    "\n",
    "        # E (embedding size)\n",
    "        self.num_features = config[\"num_features\"]\n",
    "        # D\n",
    "        self.num_items_to_select_from = config[\"num_items_to_select_from\"]\n",
    "        # k\n",
    "        self.slate_size = config[\"slate_size\"]\n",
    "\n",
    "        self.num_items_in_db = config.get(\"num_items_in_db\")\n",
    "        self.items_db = None\n",
    "        # Generate an items-DB containing n items, once.\n",
    "        if self.num_items_in_db is not None:\n",
    "            self.items_db = [np.random.uniform(-1, 1, size=(self.num_features,))\n",
    "                            for _ in range(self.num_items_in_db)]\n",
    "\n",
    "        self.num_users_in_db = config.get(\"num_users_in_db\")\n",
    "        self.users_db = None\n",
    "        # Store the user that's currently undergoing the episode/session.\n",
    "        self.current_user = None\n",
    "\n",
    "        # How much time does the user have to consume \n",
    "        self.user_time_budget = config.get(\"user_time_budget\", 1.0)\n",
    "        self.current_user_budget = self.user_time_budget\n",
    "\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            # The D items our agent sees at each timestep. It has to select a k-slate\n",
    "            # out of these.\n",
    "            \"doc\": gym.spaces.Dict({\n",
    "                str(idx):\n",
    "                    gym.spaces.Box(-1.0, 1.0, shape=(self.num_features,), dtype=np.float32)\n",
    "                    for idx in range(self.num_items_to_select_from)\n",
    "            }),\n",
    "            # The user engaging in this timestep/episode.\n",
    "            \"user\": gym.spaces.Box(-1.0, 1.0, shape=(self.num_features,), dtype=np.float32),\n",
    "            # For each item in the previous slate, was it clicked? If yes, how\n",
    "            # long was it being engaged with (e.g. watched)?\n",
    "            \"response\": gym.spaces.Tuple([\n",
    "                gym.spaces.Dict({\n",
    "                    # Clicked or not?\n",
    "                    \"click\": gym.spaces.Discrete(2),\n",
    "                    # Engagement time (how many minutes watched?).\n",
    "                    \"engagement\": gym.spaces.Box(-np.inf, np.inf, shape=(), dtype=np.float32),\n",
    "                }) for _ in range(self.slate_size)\n",
    "            ]),\n",
    "        })\n",
    "        # Our action space is\n",
    "        self.action_space = gym.spaces.MultiDiscrete([\n",
    "            self.num_items_to_select_from for _ in range(self.slate_size)\n",
    "        ])\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the current user's time budget.\n",
    "        self.current_user_budget = self.user_time_budget\n",
    "\n",
    "        # Sample a user for the next episode/session.\n",
    "        # Pick from a only-once-sampled user DB.\n",
    "        if self.num_users_in_db is not None:\n",
    "            if self.users_db is None:\n",
    "                self.users_db = [np.random.uniform(-1, 1, size=(self.num_features,))\n",
    "                                 for _ in range(self.num_users_in_db)]\n",
    "            self.current_user = self.users_db[np.random.choice(self.num_users_in_db)]\n",
    "        # Pick from an infinite pool of users.\n",
    "        else:\n",
    "            self.current_user = np.random.uniform(-1, 1, size=(self.num_features,))\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Action is the suggested slate (indices of the items in the suggested ones).\n",
    "\n",
    "        scores = [np.dot(self.current_user, item)\n",
    "                  for item in self.currently_suggested_items]\n",
    "        best_reward = np.max(scores)\n",
    "\n",
    "        # User choice model: User picks an item stochastically,\n",
    "        # where probs are dot products between user- and item feature\n",
    "        # vectors.\n",
    "        # There is also a no-click item whose weight is 0.0.\n",
    "        user_item_overlaps = np.array([scores[a] for a in action] + [0.0])\n",
    "        which_clicked = np.random.choice(\n",
    "            np.arange(self.slate_size + 1), p=softmax(user_item_overlaps))\n",
    "\n",
    "        # Reward is the overlap, if clicked. 0.0 if nothing clicked.\n",
    "        reward = 0.0\n",
    "        # If anything clicked, deduct from the current user's time budget and compute\n",
    "        # reward.\n",
    "        if which_clicked < self.slate_size:\n",
    "            regret = best_reward - user_item_overlaps[which_clicked]\n",
    "            reward = 1.0 - regret\n",
    "            self.current_user_budget -= 1.0\n",
    "        done = self.current_user_budget <= 0.0\n",
    "\n",
    "        # Compile response.\n",
    "        response = tuple({\n",
    "            \"click\": int(idx == which_clicked),\n",
    "            \"engagement\": reward if idx == which_clicked else 0.0,\n",
    "        } for idx in range(len(user_item_overlaps) - 1))\n",
    "\n",
    "        # Return 4-tuple: Next-observation, reward, done (True if episode has terminated), info dict (empty; not used here).\n",
    "        return self._get_obs(response=response), reward, done, {}\n",
    "\n",
    "    def _get_obs(self, response=None):\n",
    "        # Sample D items from infinity or our pre-existing items.\n",
    "        # Pick from a only-once-sampled items DB.\n",
    "        if self.num_items_in_db is not None:\n",
    "            self.currently_suggested_items = [\n",
    "                self.items_db[item_idx].astype(np.float32)\n",
    "                for item_idx in np.random.choice(self.num_items_in_db,\n",
    "                                                size=(self.num_items_to_select_from,),\n",
    "                                                replace=False)\n",
    "            ]\n",
    "        # Pick from an infinite pool of itemsdocs.\n",
    "        else:\n",
    "            self.currently_suggested_items = [\n",
    "                np.random.uniform(-1, 1, size=(self.num_features,)).astype(np.float32)\n",
    "                for _ in range(self.num_items_to_select_from)\n",
    "            ]\n",
    "\n",
    "        return {\n",
    "            \"user\": self.current_user.astype(np.float32),\n",
    "            \"doc\": {\n",
    "                str(idx): item for idx, item in enumerate(self.currently_suggested_items)\n",
    "            },\n",
    "            \"response\": response if response else self.observation_space[\"response\"].sample()\n",
    "        }\n",
    "\n",
    "env = RecommSys001(config={\n",
    "    \"num_features\": 20,  # E (embedding size)\n",
    "\n",
    "    \"num_items_in_db\": 100,  # total number of items in our database\n",
    "    \"num_items_to_select_from\": 10,  # number of items to present to the agent to pick a k-slate from\n",
    "    \"slate_size\": 1,  # k\n",
    "    \"num_users_in_db\": 1,  # total number  of users in our database\n",
    "})\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-sussex",
   "metadata": {},
   "source": [
    "## Testing our environment\n",
    "\n",
    "In the cell above, we created a new environment instance. In order to start \"walking\" through a recommender system episode, we need to perform `reset()` and then several `step()` calls (with different actions) until the returned `done` flag is True.\n",
    "\n",
    "Let's follow these instructions here to get this done:\n",
    "\n",
    "1. `reset` the already created (variable `env`) environment to get the first (initial) observation.\n",
    "1. Enter an infinite while loop.\n",
    "1. Compute the next action for our agent by calling `env.action_space.sample()`.\n",
    "1. Pass this computed action into the env's `step()` method.\n",
    "1. Check the returned `done` for True (episode is terminated) and if True, break out of the loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean episode reward when acting randomly: -0.52+/-0.04\n"
     ]
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "\n",
    "def test_env(env):\n",
    "\n",
    "    # 1) Reset the env.\n",
    "    obs = env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "\n",
    "    # 2) Enter an infinite while loop (to step through the episode).\n",
    "    while num_episodes < 1000:\n",
    "        # 3) Calculate agent's action, using random sampling via the environment's action space.\n",
    "        action = env.action_space.sample()\n",
    "        # action = trainer.compute_single_action([obs])\n",
    "\n",
    "        # 4) Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # 5) Check, whether the episde is done, if yes, break out of the while loop.\n",
    "        if done:\n",
    "            #print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            num_episodes += 1\n",
    "            env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # 6) Print out mean episode reward!\n",
    "    env_mean_random_reward = np.mean(episode_rewards)\n",
    "    print(f\"Mean episode reward when acting randomly: {env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
    "\n",
    "    return env_mean_random_reward, sem(episode_rewards)\n",
    "\n",
    "env_mean_random_reward, env_sem_random_reward = test_env(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a3d658",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 10 min break :)\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
   "metadata": {},
   "source": [
    "# Plugging in RLlib!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd830b90-5762-4d22-8fa9-0abf0777a240",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start a new instance of Ray (when running this tutorial locally) or\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# connect to an already running one (when running this tutorial through Anyscale).\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorials_2/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib_tutorials_2/lib/python3.9/site-packages/ray/worker.py:996\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, _enable_object_reconstruction, _redis_max_memory, _plasma_directory, _node_ip_address, _driver_object_store_memory, _memory, _redis_password, _temp_dir, _metrics_export_port, _system_config, _tracing_startup_hook, **kwargs)\u001b[0m\n\u001b[1;32m    994\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RayContext(\u001b[38;5;28mdict\u001b[39m(_global_node\u001b[38;5;241m.\u001b[39maddress_info, node_id\u001b[38;5;241m=\u001b[39mnode_id\u001b[38;5;241m.\u001b[39mhex()))\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 996\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    997\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaybe you called ray.init twice by accident? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    998\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis error can be suppressed by passing in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore_reinit_error=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1000\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mray.shutdown()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m prior to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mray.init()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1001\u001b[0m         )\n\u001b[1;32m   1003\u001b[0m _system_config \u001b[38;5;241m=\u001b[39m _system_config \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_system_config, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'."
     ]
    }
   ],
   "source": [
    "# Start a new instance of Ray (when running this tutorial locally) or\n",
    "# connect to an already running one (when running this tutorial through Anyscale).\n",
    "\n",
    "ray.init()  # Hear the engine humming? ;)\n",
    "\n",
    "# In case you encounter the following error during our tutorial: `RuntimeError: Maybe you called ray.init twice by accident?`\n",
    "# Try: `ray.shutdown() + ray.init()` or `ray.init(ignore_reinit_error=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
   "metadata": {},
   "source": [
    "## Picking an RLlib algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_algorithms.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4656220",
   "metadata": {},
   "source": [
    "### Trying a contextual Bandit on our environment\n",
    "<img src=\"images/contextual_bandit.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d98622c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 14:41:32,764\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................................................................................................................................................................................................................................"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "# We start our endeavor with the Bandit algorithms here b/c they are specialized in solving\n",
    "# n-arm/recommendation problems.\n",
    "from ray.rllib.agents.bandit import BanditLinUCBTrainer\n",
    "\n",
    "# Environment wrapping tools for:\n",
    "# a) Converting MultiDiscrete action space (k-slate recommendations) down to Discrete action space (we only have k=1 for now anyways).\n",
    "# b) Making sure our google RecSim-style environment is understood by RLlib's Bandit Trainers.\n",
    "from ray.rllib.env.wrappers.recsim import MultiDiscreteToDiscreteActionWrapper, \\\n",
    "    RecSimObservationBanditWrapper\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "tune.register_env(\n",
    "    \"recomm-sys-001-for-bandits\",\n",
    "    lambda config: RecSimObservationBanditWrapper(MultiDiscreteToDiscreteActionWrapper(RecommSys001(config))))\n",
    "\n",
    "bandit_config = {\n",
    "    # Use our tune-registered \"RecommSys001\" class.\n",
    "    \"env\": \"recomm-sys-001-for-bandits\",\n",
    "    \"env_config\": {\n",
    "        \"num_features\": 20,  # E\n",
    "\n",
    "        \"num_items_in_db\": 100,\n",
    "        \"num_items_to_select_from\": 10,  # D\n",
    "        \"slate_size\": 1,  # k=1\n",
    "\n",
    "        \"num_users_in_db\": 1,\n",
    "    },\n",
    "    #\"evaluation_duration_unit\": \"episodes\",\n",
    "    \"timesteps_per_iteration\": 1,\n",
    "}\n",
    "\n",
    "# Create the RLlib Trainer using above config.\n",
    "bandit_trainer = BanditLinUCBTrainer(config=bandit_config)\n",
    "\n",
    "# Train for n iterations (timesteps) and collect n-arm rewards.\n",
    "rewards = []\n",
    "for _ in range(300):\n",
    "    result = bandit_trainer.train()\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(rewards)#x=[i for i in range(len(rewards))], y=rewards, xerr=None, yerr=[sem(rewards) for i in range(len(rewards))])\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236aafae-2660-4894-9193-6176f7cc6d03",
   "metadata": {},
   "source": [
    "## Trying Slate-Q on a harder version of the same environment.\n",
    "\n",
    "So far, we have dumbed down our environment via the following contraints:\n",
    "\n",
    "1. An episode was always only one timestep long (via the config.user_time_budget setting of the env).\n",
    "1. Our slate size (k) was 1 (the algo only had to recommend a single item from the list of suggested ones).\n",
    "1. We were only dealing with a single user (the underlying user vector never changes and is only sampled once upon environment startup).\n",
    "\n",
    "Let's try relaxing all of these restrictions and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d5251d23-5be5-44de-a550-87ace027f990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 14:46:54,747\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean episode reward when acting randomly: -3.58+/-0.13\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Update our env_config: Making things harder.\n",
    "bandit_config.update({\n",
    "    \"env_config\": {\n",
    "        \"num_features\": 20,  # E (no change)\n",
    "\n",
    "        \"num_items_in_db\": 100,  # (no change)\n",
    "        \"num_items_to_select_from\": 10,  # D (no change)\n",
    "        \"slate_size\": 2,  # k=2\n",
    "\n",
    "        \"num_users_in_db\": None,  # More users!\n",
    "        \"user_time_budget\": 10.0,  # Longer episodes.\n",
    "    },\n",
    "})\n",
    "\n",
    "# Re-computing our random baseline.\n",
    "harder_env = RecommSys001(config=bandit_config[\"env_config\"])\n",
    "harder_env_mean_random_reward, _ = test_env(harder_env)\n",
    "\n",
    "\n",
    "# Create the RLlib Trainer using above config.\n",
    "bandit_trainer = BanditLinUCBTrainer(config=bandit_config)\n",
    "\n",
    "# Train for n iterations (timesteps) and collect n-arm rewards.\n",
    "rewards = []\n",
    "for _ in range(1200):\n",
    "    result = bandit_trainer.train()\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(rewards)#x=[i for i in range(len(rewards))], y=rewards, xerr=None, yerr=[sem(rewards) for i in range(len(rewards))])\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=harder_env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79746db-8abc-426d-b71f-f4f68eeb886b",
   "metadata": {},
   "source": [
    "#### Well, that doesn't look so well anymore.\n",
    "\n",
    "Bandits can learn in harder recommender-style envs, but are having a harder time when we increase the number of users, the slate size, or the episode/session length.\n",
    "\n",
    "Let's try the Slate-Q algorithm, which was designed for k-slate and long-time horizon (user journey) recommendations problems.\n",
    "\n",
    "### Switching to Slate-Q\n",
    "<img src=\"images/slateq.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5897c2ee-d0c8-4d06-b580-3c5da033c381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 14:47:25,466\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SlateQTrainer"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "# We use the SlateQ algorithm here b/c it is specialized in solving slate recommendation problems\n",
    "# and works well with RLlib's RecSim environment adapter.\n",
    "\n",
    "from ray.rllib.agents.slateq import SlateQTrainer\n",
    "\n",
    "tune.register_env(\n",
    "    \"recomm-sys-001-for-slateq\",\n",
    "    lambda config: RecommSys001(config))\n",
    "\n",
    "slateq_config = {\n",
    "    \"env\": \"recomm-sys-001-for-slateq\",\n",
    "    \"env_config\": bandit_config[\"env_config\"],  # <- use exact same env config as above for direct comparison.\n",
    "}\n",
    "\n",
    "# Instantiate the Trainer object using the exact same config as in our last (harder-to-solve env) Bandit experiment above.\n",
    "slateq_trainer = SlateQTrainer(config=slateq_config)\n",
    "slateq_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ae150-c0a3-477f-8d78-d0a34f147958",
   "metadata": {},
   "source": [
    "### Ready to train with RLlib's SlateQ algorithm\n",
    "\n",
    "That's it, we are ready to train.\n",
    "Calling `Trainer.train()` will execute a single \"training iteration\".\n",
    "\n",
    "One iteration for most algos involves:\n",
    "\n",
    "1. Sampling from the environment(s)\n",
    "1. Using the sampled data (observations, actions taken, rewards) to update the policy model (neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0f6c94d4-6871-4d20-81af-3d4081f05f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_timesteps_total': 1007,\n",
      " 'custom_metrics': {},\n",
      " 'date': '2022-02-16_14-47-28',\n",
      " 'done': False,\n",
      " 'episode_len_mean': 13.794520547945206,\n",
      " 'episode_media': {},\n",
      " 'episode_reward_max': 7.871986030092429,\n",
      " 'episode_reward_mean': -3.6675469289760576,\n",
      " 'episode_reward_min': -27.86451471069936,\n",
      " 'episodes_this_iter': 73,\n",
      " 'episodes_total': 73,\n",
      " 'experiment_id': '6da08033be854ec188615751404eae57',\n",
      " 'hist_stats': {'episode_lengths': [18,\n",
      "                                    12,\n",
      "                                    12,\n",
      "                                    14,\n",
      "                                    17,\n",
      "                                    11,\n",
      "                                    13,\n",
      "                                    17,\n",
      "                                    12,\n",
      "                                    17,\n",
      "                                    12,\n",
      "                                    14,\n",
      "                                    12,\n",
      "                                    13,\n",
      "                                    10,\n",
      "                                    19,\n",
      "                                    12,\n",
      "                                    18,\n",
      "                                    13,\n",
      "                                    13,\n",
      "                                    10,\n",
      "                                    15,\n",
      "                                    13,\n",
      "                                    15,\n",
      "                                    12,\n",
      "                                    15,\n",
      "                                    15,\n",
      "                                    10,\n",
      "                                    12,\n",
      "                                    13,\n",
      "                                    16,\n",
      "                                    10,\n",
      "                                    14,\n",
      "                                    10,\n",
      "                                    13,\n",
      "                                    11,\n",
      "                                    12,\n",
      "                                    16,\n",
      "                                    14,\n",
      "                                    13,\n",
      "                                    12,\n",
      "                                    13,\n",
      "                                    12,\n",
      "                                    14,\n",
      "                                    11,\n",
      "                                    13,\n",
      "                                    15,\n",
      "                                    16,\n",
      "                                    22,\n",
      "                                    11,\n",
      "                                    16,\n",
      "                                    14,\n",
      "                                    16,\n",
      "                                    19,\n",
      "                                    11,\n",
      "                                    14,\n",
      "                                    13,\n",
      "                                    15,\n",
      "                                    13,\n",
      "                                    13,\n",
      "                                    16,\n",
      "                                    13,\n",
      "                                    17,\n",
      "                                    19,\n",
      "                                    13,\n",
      "                                    12,\n",
      "                                    16,\n",
      "                                    19,\n",
      "                                    11,\n",
      "                                    11,\n",
      "                                    15,\n",
      "                                    13,\n",
      "                                    11],\n",
      "                'episode_reward': [5.5736720080138635,\n",
      "                                   -8.433975443557111,\n",
      "                                   -8.873504246103543,\n",
      "                                   -3.258493523217652,\n",
      "                                   -0.3694155039266871,\n",
      "                                   -1.0357872087395168,\n",
      "                                   -5.1348373492984685,\n",
      "                                   2.9135220904115333,\n",
      "                                   -5.636871886850658,\n",
      "                                   -7.899130723290677,\n",
      "                                   7.871986030092429,\n",
      "                                   -7.903055969075982,\n",
      "                                   -2.6067095473432036,\n",
      "                                   -10.121423555777444,\n",
      "                                   -1.2189556652101872,\n",
      "                                   -2.389249646783972,\n",
      "                                   -5.710507670464855,\n",
      "                                   -2.950014201693725,\n",
      "                                   -6.186422501410425,\n",
      "                                   -9.304755434007184,\n",
      "                                   -4.170995770016054,\n",
      "                                   -3.9947660237361373,\n",
      "                                   -4.505640430781303,\n",
      "                                   -5.816500445143371,\n",
      "                                   0.923766852396573,\n",
      "                                   1.6153639625402425,\n",
      "                                   -4.238679853306815,\n",
      "                                   -5.898580273201076,\n",
      "                                   -2.530273235790793,\n",
      "                                   -6.224152604745138,\n",
      "                                   -4.268308428120331,\n",
      "                                   -0.8264524162620344,\n",
      "                                   -1.4630055723664852,\n",
      "                                   -11.190183415990218,\n",
      "                                   -4.539428395405075,\n",
      "                                   -3.0769261710891125,\n",
      "                                   -11.33339991002048,\n",
      "                                   -6.458857382238872,\n",
      "                                   -2.706629806882085,\n",
      "                                   0.9213861417698752,\n",
      "                                   -27.86451471069936,\n",
      "                                   -0.23597524038158757,\n",
      "                                   -7.730886510818403,\n",
      "                                   -4.906979550803747,\n",
      "                                   -11.801117592920773,\n",
      "                                   7.577103459862755,\n",
      "                                   -7.674764297799028,\n",
      "                                   -0.6159009984247688,\n",
      "                                   -10.586681544882026,\n",
      "                                   -6.280747254831778,\n",
      "                                   -8.662101429601815,\n",
      "                                   0.46744931159600855,\n",
      "                                   1.9199274123811336,\n",
      "                                   6.625072814020506,\n",
      "                                   -1.0480039033724737,\n",
      "                                   1.4234881852933008,\n",
      "                                   -2.6773795272471737,\n",
      "                                   0.550307541879407,\n",
      "                                   -9.49865214028868,\n",
      "                                   -3.610042497970462,\n",
      "                                   -15.128597425539274,\n",
      "                                   3.0236308872232156,\n",
      "                                   -0.3188707358086532,\n",
      "                                   1.894190443519903,\n",
      "                                   -1.5136026699166236,\n",
      "                                   -2.4814476526686597,\n",
      "                                   -1.9332978592300203,\n",
      "                                   -2.0163087366442434,\n",
      "                                   7.449308802207973,\n",
      "                                   -4.6185493850605965,\n",
      "                                   -5.0302746410109425,\n",
      "                                   -2.6753708462000154,\n",
      "                                   -7.295144394493173]},\n",
      " 'hostname': 'Svens-MBP',\n",
      " 'info': {'last_target_update_ts': 1007,\n",
      "          'learner': {'default_policy': {'custom_metrics': {},\n",
      "                                         'learner_stats': {'allreduce_latency': 0.0,\n",
      "                                                           'choice_loss': 1.0986121892929077,\n",
      "                                                           'choice_model.beta': 0.0,\n",
      "                                                           'choice_model.score_no_click': 0.0,\n",
      "                                                           'grad_gnorm': array(0.11424741, dtype=float32),\n",
      "                                                           'next_q_minus_q': 0.012928486801683903,\n",
      "                                                           'next_q_values': 0.06548737734556198,\n",
      "                                                           'q_loss': 0.4131934642791748,\n",
      "                                                           'q_model.layers.0.bias': -0.0033349660225212574,\n",
      "                                                           'q_model.layers.0.weight': 0.0008225464262068272,\n",
      "                                                           'q_model.layers.2.bias': -0.0064122481271624565,\n",
      "                                                           'q_model.layers.2.weight': -1.2376054655760527e-05,\n",
      "                                                           'q_model.layers.4.bias': 0.02637946605682373,\n",
      "                                                           'q_model.layers.4.weight': 0.0005452538607642055,\n",
      "                                                           'q_model.layers.6.bias': 0.04230514168739319,\n",
      "                                                           'q_model.layers.6.weight': 0.015219779685139656,\n",
      "                                                           'q_values': 0.05255889520049095,\n",
      "                                                           'raw_scores': 0.0,\n",
      "                                                           'target_q_values': -0.24209019541740417,\n",
      "                                                           'td_error': 0.7348557114601135},\n",
      "                                         'model': {},\n",
      "                                         'num_agent_steps_trained': 32}},\n",
      "          'num_agent_steps_sampled': 1007,\n",
      "          'num_agent_steps_trained': 32,\n",
      "          'num_steps_sampled': 1007,\n",
      "          'num_steps_trained': 32,\n",
      "          'num_steps_trained_this_iter': 32,\n",
      "          'num_target_updates': 1},\n",
      " 'iterations_since_restore': 1,\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_healthy_workers': 0,\n",
      " 'off_policy_estimator': {},\n",
      " 'perf': {'cpu_util_percent': 1.8800000000000001, 'ram_util_percent': 69.3},\n",
      " 'pid': 11293,\n",
      " 'policy_reward_max': {},\n",
      " 'policy_reward_mean': {},\n",
      " 'policy_reward_min': {},\n",
      " 'sampler_perf': {'mean_action_processing_ms': 0.03942184978061252,\n",
      "                  'mean_env_render_ms': 0.0,\n",
      "                  'mean_env_wait_ms': 0.18419325351715088,\n",
      "                  'mean_inference_ms': 0.9251291316653054,\n",
      "                  'mean_raw_obs_processing_ms': 0.148856450640966},\n",
      " 'time_since_restore': 1.375519037246704,\n",
      " 'time_this_iter_s': 1.375519037246704,\n",
      " 'time_total_s': 1.375519037246704,\n",
      " 'timers': {'learn_throughput': 6861.145, 'learn_time_ms': 4.664},\n",
      " 'timestamp': 1645019248,\n",
      " 'timesteps_since_restore': 32,\n",
      " 'timesteps_this_iter': 32,\n",
      " 'timesteps_total': 1007,\n",
      " 'training_iteration': 1,\n",
      " 'trial_id': 'default'}\n"
     ]
    }
   ],
   "source": [
    "results = slateq_trainer.train()\n",
    "\n",
    "# Delete the config from the results for clarity.\n",
    "# Only the stats will remain, then.\n",
    "del results[\"config\"]\n",
    "# Pretty print the stats.\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {},
   "source": [
    "Now that we have confirmed we have setup the Trainer correctly, let's call `train()` on it several times (what about 10 times?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "17ae724d-71cc-422b-96cb-3dc9faa2d111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=2: R(\"return\")=-3.6058821107118524\n",
      "Iteration=3: R(\"return\")=-3.342824598976804\n",
      "Iteration=4: R(\"return\")=-4.4703072611561305\n",
      "Iteration=5: R(\"return\")=-3.7655925905477403\n",
      "Iteration=6: R(\"return\")=-4.009908508895739\n",
      "Iteration=7: R(\"return\")=-3.830771392351125\n",
      "Iteration=8: R(\"return\")=-4.266252861654902\n",
      "Iteration=9: R(\"return\")=-4.380105068394669\n",
      "Iteration=10: R(\"return\")=-4.9723363739664315\n",
      "Iteration=11: R(\"return\")=-6.422665332905927\n"
     ]
    }
   ],
   "source": [
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "# Move on once you see episode rewards of 1050.0 or more.\n",
    "for _ in range(10):\n",
    "    results = slateq_trainer.train()\n",
    "    print(f\"Iteration={slateq_trainer.iteration}: R(\\\"return\\\")={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK!\n",
    "\n",
    "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using the SlateQ Policy and its NN models inside our Trainer object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "aff679e8-74b4-4603-9d5c-4cc0c6ebe45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Policy right now is: SlateQTorchPolicy\n",
      "Our Policy's observation space is: Box([-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], (226,), float32)\n",
      "Our Policy's action space is: MultiDiscrete([10 10])\n",
      "tensor([[ 0.0369, -0.7328,  0.2318, -0.0213,  0.4157,  0.0108,  0.1607, -0.8525,\n",
      "          0.0408, -0.5265,  0.2277,  0.0209, -0.8565,  0.5269,  0.3624,  0.2247,\n",
      "          0.4007, -0.5717, -0.7300,  0.9204],\n",
      "        [ 0.0603,  0.7443,  0.7493,  0.1182, -0.3066, -0.3555,  0.5466, -0.5694,\n",
      "          0.0151, -0.8083, -0.6318, -0.5378, -0.1343,  0.4991, -0.4888,  0.7792,\n",
      "         -0.5604,  0.6788,  0.0951, -0.3856],\n",
      "        [-0.1120, -0.9404,  0.7245,  0.7172,  0.3595, -0.8697, -0.5139,  0.5998,\n",
      "         -0.8103,  0.6347, -0.2953, -0.3958,  0.7144,  0.0481, -0.0306,  0.1504,\n",
      "         -0.2934,  0.4241, -0.2351, -0.5776],\n",
      "        [ 0.0515,  0.8679,  0.8673,  0.4754,  0.6960, -0.7548, -0.3858,  0.3569,\n",
      "          0.2571, -0.4564, -0.1485,  0.1008, -0.6165,  0.0960, -0.9298, -0.8846,\n",
      "          0.6260, -0.8550,  0.9738,  0.5112],\n",
      "        [ 0.4646,  0.0126, -0.6726,  0.7177, -0.3718,  0.1515, -0.9630, -0.3908,\n",
      "         -0.3341, -0.5312,  0.9716,  0.5616,  0.5635,  0.1644,  0.0071,  0.7456,\n",
      "         -0.8407,  0.9531, -0.0848,  0.9985],\n",
      "        [-0.1587, -0.4213, -0.6101,  0.1707, -0.2977, -0.0291, -0.3622, -0.4860,\n",
      "          0.7884, -0.5476, -0.4267,  0.0580,  0.7892, -0.3977,  0.9853, -0.5475,\n",
      "          0.4050, -0.8081,  0.5732, -0.1350],\n",
      "        [ 0.3874, -0.7062, -0.9368,  0.4729,  0.0740, -0.3912,  0.4755, -0.7710,\n",
      "         -0.0617, -0.4726,  0.9900,  0.6077, -0.5745,  0.8804, -0.1211,  0.0357,\n",
      "         -0.1424,  0.2924, -0.0993,  0.6856],\n",
      "        [ 0.5602,  0.6407, -0.8154, -0.5250,  0.7554,  0.8834, -0.1456, -0.3503,\n",
      "          0.7087, -0.7691, -0.7765, -0.0037,  0.6648,  0.5469, -0.3137,  0.5649,\n",
      "          0.2359,  0.4563,  0.0104, -0.4107],\n",
      "        [-0.6938, -0.8534,  0.5343,  0.6872, -0.2094, -0.4909,  0.3277,  0.5361,\n",
      "          0.5404, -0.0212, -0.8911, -0.7208, -0.0615,  0.3932,  0.2236, -0.8177,\n",
      "          0.0969, -0.2216,  0.7907,  0.1226],\n",
      "        [ 0.3701,  0.2433,  0.0954, -0.9201,  0.1255, -0.4606, -0.6833, -0.3283,\n",
      "         -0.6333,  0.7815,  0.1540,  0.9369, -0.3495, -0.7510,  0.6867, -0.8857,\n",
      "         -0.7663, -0.9601, -0.0521, -0.7896]])\n",
      "per_slate_q_values=[[-0.30163366 -0.30261642 -0.316326   -0.31592008 -0.29794025 -0.29856676\n",
      "  -0.31456894 -0.2987798  -0.3079966  -0.29761258 -0.31114239 -0.3106993\n",
      "  -0.29290026 -0.29357034 -0.3093618  -0.29381344 -0.30286574 -0.31208697\n",
      "  -0.31165275 -0.29391104 -0.29456902 -0.31031948 -0.294805   -0.30384234\n",
      "  -0.32533082 -0.3076459  -0.30816615 -0.32400146 -0.3083138  -0.31751707\n",
      "  -0.3071668  -0.30769622 -0.32365993 -0.30784816 -0.31712204 -0.2897726\n",
      "  -0.30580586 -0.2900448  -0.29920265 -0.30634952 -0.2907432  -0.2998144\n",
      "  -0.3065104  -0.3157754  -0.30001858]]\n"
     ]
    }
   ],
   "source": [
    "# To get the policy inside the Trainer, use `Trainer.get_policy([policy ID]=\"default_policy\")`:\n",
    "policy = slateq_trainer.get_policy()\n",
    "print(f\"Our Policy right now is: {policy}\")\n",
    "\n",
    "# To get to the model inside any policy, do:\n",
    "model = policy.model\n",
    "#print(f\"Our Policy's model is: {model}\")\n",
    "\n",
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\")\n",
    "\n",
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = env.observation_space.sample()\n",
    "print(torch.stack([torch.from_numpy(v) for k, v in obs[\"doc\"].items()]))\n",
    "\n",
    "# Get the action logits (as torch tensor).\n",
    "per_slate_q_values = model.get_per_slate_q_values(user=torch.from_numpy(obs[\"user\"]).unsqueeze(0), doc=torch.stack([torch.from_numpy(v) for k, v in obs[\"doc\"].items()]).unsqueeze(0))\n",
    "per_slate_q_values = per_slate_q_values.detach().cpu().numpy()\n",
    "print(f\"per_slate_q_values={per_slate_q_values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de603d14-f0cb-4363-a72b-8f147c094071",
   "metadata": {},
   "source": [
    "In order to release all resources from a Trainer, you can use a Trainer's `stop()` method.\n",
    "You should definitley run this cell as it frees resources that we'll need later in this tutorial, when we'll do parallel hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "737dca4f-942f-4fda-abcc-0052263a103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slateq_trainer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c1e4c-cb02-4719-ac5a-0106172a6c6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Moving stuff to the professional level: RLlib in connection w/ Ray Tune\n",
    "\n",
    "Running any experiments through Ray Tune is the recommended way of doing things with RLlib. If you look at our\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/rllib/examples\">examples scripts folder</a>, you will see that almost all of the scripts use Ray Tune to run the particular RLlib workload demonstrated in each script.\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdacebb-d27f-4174-9002-35c5657f146c",
   "metadata": {
    "tags": []
   },
   "source": [
    "When setting up hyperparameter sweeps for Tune, we'll do this in our already familiar config dict.\n",
    "\n",
    "So let's take a quick look at our SlateQ algo's default config to understand, which hyperparameters we may want to play around with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1b32582-52bd-4585-9009-2f877a0723a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlateQ's default config is:\n",
      "{'_disable_action_flattening': False,\n",
      " '_disable_execution_plan_api': False,\n",
      " '_disable_preprocessor_api': False,\n",
      " '_fake_gpus': False,\n",
      " '_tf_policy_handles_more_than_one_loss': False,\n",
      " 'action_space': None,\n",
      " 'actions_in_input_normalized': False,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'always_attach_evaluation_results': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'buffer_size': -1,\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': False,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': -1,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'double_q': True,\n",
      " 'eager_max_retraces': 20,\n",
      " 'eager_tracing': False,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'env_task_fn': None,\n",
      " 'evaluation_config': {'explore': False},\n",
      " 'evaluation_duration': 10,\n",
      " 'evaluation_duration_unit': 'episodes',\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': -1,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'evaluation_parallel_to_training': False,\n",
      " 'exploration_config': {'type': 'SlateSoftQ'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'torch',\n",
      " 'gamma': 0.99,\n",
      " 'grad_clip': 40,\n",
      " 'hiddens': [256, 64, 16],\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_config': {},\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'keep_per_episode_custom_metrics': False,\n",
      " 'learning_starts': 1000,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'lr': 0.0001,\n",
      " 'lr_choice_model': 0.001,\n",
      " 'lr_q_model': 0.001,\n",
      " 'metrics_episode_collection_timeout_s': 180,\n",
      " 'metrics_num_episodes_for_smoothing': 100,\n",
      " 'metrics_smoothing_episodes': -1,\n",
      " 'min_iter_time_s': -1,\n",
      " 'min_sample_timesteps_per_reporting': None,\n",
      " 'min_time_s_per_reporting': 1,\n",
      " 'min_train_timesteps_per_reporting': None,\n",
      " 'model': {'_disable_action_flattening': False,\n",
      "           '_disable_preprocessor_api': False,\n",
      "           '_time_major': False,\n",
      "           '_use_default_native_models': False,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': True,\n",
      "           'zero_mean': True},\n",
      " 'monitor': -1,\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_map_cache': None,\n",
      "                'policy_map_capacity': 100,\n",
      "                'policy_mapping_fn': None,\n",
      "                'replay_mode': 'independent'},\n",
      " 'no_done_at_end': False,\n",
      " 'normalize_actions': True,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_workers': 0,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'observation_space': None,\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_config': {},\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'record_env': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'replay_buffer_config': {'capacity': 100000, 'type': 'MultiAgentReplayBuffer'},\n",
      " 'replay_sequence_length': 1,\n",
      " 'rollout_fragment_length': 4,\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'seed': None,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'simple_optimizer': -1,\n",
      " 'slateq_strategy': 'QL',\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'target_network_update_freq': 1,\n",
      " 'tau': 0.005,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 1000,\n",
      " 'train_batch_size': 32,\n",
      " 'training_intensity': None,\n",
      " 'worker_side_prioritization': False}\n"
     ]
    }
   ],
   "source": [
    "# Configuration dicts and Ray Tune.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# SlateQ algorithm:\n",
    "from ray.rllib.agents.slateq import DEFAULT_CONFIG as SLATEQ_DEFAULT_CONFIG\n",
    "print(f\"SlateQ's default config is:\")\n",
    "pprint.pprint(SLATEQ_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint.pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint.pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded886cc-436e-46cd-8fea-d68af8b41236",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's do a very simple grid-search over two learning rates with tune.run().\n",
    "\n",
    "In particular, we will try the train_batch_sizes 32 and 64 using `tune.grid_search([...])`\n",
    "inside our config dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5063991e-173b-49be-a4e7-467e2e18321a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:25:42 (running for 00:00:00.12)<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (2 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                32</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72456)\u001b[0m 2022-02-13 18:25:51,048\tINFO trainer.py:861 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72458)\u001b[0m 2022-02-13 18:25:51,048\tINFO trainer.py:861 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:25:51 (running for 00:00:08.90)<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>RUNNING </td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72456)\u001b[0m 2022-02-13 18:25:51,155\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72458)\u001b[0m 2022-02-13 18:25:51,154\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72456)\u001b[0m /Users/sven/opt/anaconda3/envs/ray/lib/python3.8/site-packages/gym/spaces/box.py:142: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72456)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72458)\u001b[0m /Users/sven/opt/anaconda3/envs/ray/lib/python3.8/site-packages/gym/spaces/box.py:142: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(SlateQTrainer pid=72458)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:25:53 (running for 00:00:10.91)<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>RUNNING </td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SlateQ_my_env_f8560_00001:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-13_18-25-53\n",
      "  done: true\n",
      "  episode_len_mean: 87.54545454545455\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.277315709038163\n",
      "  episode_reward_mean: 9.182349776607046\n",
      "  episode_reward_min: 6.221774288301231\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 11\n",
      "  experiment_id: 2854ee45d5e04d1692c62dbbf2cd328a\n",
      "  hostname: Svens-MBP\n",
      "  info:\n",
      "    last_target_update_ts: 1000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          choice_beta: 10.0\n",
      "          choice_loss: 1.3261504173278809\n",
      "          choice_model.beta: 10.0\n",
      "          choice_model.score_no_click: 0.0\n",
      "          choice_score_no_click: 0.0\n",
      "          grad_gnorm: 0.10184044390916824\n",
      "          next_q_minus_q: 0.011721569113433361\n",
      "          next_q_values: 0.07172474265098572\n",
      "          q_loss: 0.01740223728120327\n",
      "          q_model.layers.0.bias: -0.005163680762052536\n",
      "          q_model.layers.0.weight: 0.0023021039087325335\n",
      "          q_model.layers.2.bias: -0.0011576698161661625\n",
      "          q_model.layers.2.weight: -0.00015765760326758027\n",
      "          q_model.layers.4.bias: -0.018162351101636887\n",
      "          q_model.layers.4.weight: 0.0009697921923361719\n",
      "          q_model.layers.6.bias: 0.08257558941841125\n",
      "          q_model.layers.6.weight: 0.01841333508491516\n",
      "          q_values: 0.06000317633152008\n",
      "          raw_scores: 0.6047173142433167\n",
      "          target_q_values: 0.17137761414051056\n",
      "          td_error: 0.11271998286247253\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 64\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 64\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 64\n",
      "    num_steps_trained_this_iter: 64\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.733333333333334\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 72458\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04264905855253145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20202461417976555\n",
      "    mean_inference_ms: 1.0549050349217433\n",
      "    mean_raw_obs_processing_ms: 0.2390135537375223\n",
      "  time_since_restore: 2.0409727096557617\n",
      "  time_this_iter_s: 2.0409727096557617\n",
      "  time_total_s: 2.0409727096557617\n",
      "  timers:\n",
      "    learn_throughput: 4559.413\n",
      "    learn_time_ms: 14.037\n",
      "  timestamp: 1644773153\n",
      "  timesteps_since_restore: 64\n",
      "  timesteps_this_iter: 64\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: f8560_00001\n",
      "  \n",
      "Result for SlateQ_my_env_f8560_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-13_18-25-53\n",
      "  done: false\n",
      "  episode_len_mean: 88.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.496796226342333\n",
      "  episode_reward_mean: 8.97666416090417\n",
      "  episode_reward_min: 6.2187911669965406\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 11\n",
      "  experiment_id: 7f16debe21064adeb50c9cea5966a4ab\n",
      "  hostname: Svens-MBP\n",
      "  info:\n",
      "    last_target_update_ts: 1000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          choice_beta: 10.0\n",
      "          choice_loss: 1.4960806369781494\n",
      "          choice_model.beta: 10.0\n",
      "          choice_model.score_no_click: 0.0\n",
      "          choice_score_no_click: 0.0\n",
      "          grad_gnorm: 0.10409796983003616\n",
      "          next_q_minus_q: 0.029564648866653442\n",
      "          next_q_values: 0.21553686261177063\n",
      "          q_loss: 0.009436151944100857\n",
      "          q_model.layers.0.bias: 0.001102454960346222\n",
      "          q_model.layers.0.weight: -0.0011577063705772161\n",
      "          q_model.layers.2.bias: -0.0010439902544021606\n",
      "          q_model.layers.2.weight: -0.00012651048018597066\n",
      "          q_model.layers.4.bias: 0.010950950905680656\n",
      "          q_model.layers.4.weight: -0.0013435980072245002\n",
      "          q_model.layers.6.bias: 0.18343889713287354\n",
      "          q_model.layers.6.weight: 0.02172801084816456\n",
      "          q_values: 0.185972198843956\n",
      "          raw_scores: 0.7186157703399658\n",
      "          target_q_values: 0.29751530289649963\n",
      "          td_error: 0.1116337850689888\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 32\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 32\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.733333333333334\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 72456\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04428273790723437\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20213608260635849\n",
      "    mean_inference_ms: 1.0659456491232155\n",
      "    mean_raw_obs_processing_ms: 0.23718313737349075\n",
      "  time_since_restore: 2.052877187728882\n",
      "  time_this_iter_s: 2.052877187728882\n",
      "  time_total_s: 2.052877187728882\n",
      "  timers:\n",
      "    learn_throughput: 2636.321\n",
      "    learn_time_ms: 12.138\n",
      "  timestamp: 1644773153\n",
      "  timesteps_since_restore: 32\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: f8560_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:25:59 (running for 00:00:16.88)<br>Memory usage on this node: 10.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING   </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         5.84984</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> 8.69381</td><td style=\"text-align: right;\">             13.4968</td><td style=\"text-align: right;\">             5.9909 </td><td style=\"text-align: right;\">           87.9091</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SlateQ_my_env_f8560_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-13_18-26-01\n",
      "  done: false\n",
      "  episode_len_mean: 87.8529411764706\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.496796226342333\n",
      "  episode_reward_mean: 8.542674939247744\n",
      "  episode_reward_min: 5.9908987127629265\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 34\n",
      "  experiment_id: 7f16debe21064adeb50c9cea5966a4ab\n",
      "  hostname: Svens-MBP\n",
      "  info:\n",
      "    last_target_update_ts: 3000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          choice_beta: 10.710765838623047\n",
      "          choice_loss: 2.6868820190429688\n",
      "          choice_model.beta: 10.710765838623047\n",
      "          choice_model.score_no_click: -0.7926892638206482\n",
      "          choice_score_no_click: -0.7926892638206482\n",
      "          grad_gnorm: 438.3589172363281\n",
      "          next_q_minus_q: -23.313079833984375\n",
      "          next_q_values: 9057.2119140625\n",
      "          q_loss: 521.5323486328125\n",
      "          q_model.layers.0.bias: 0.3147527575492859\n",
      "          q_model.layers.0.weight: 0.22903656959533691\n",
      "          q_model.layers.2.bias: 0.02270541898906231\n",
      "          q_model.layers.2.weight: 0.019072968512773514\n",
      "          q_model.layers.4.bias: 0.08097691833972931\n",
      "          q_model.layers.4.weight: -0.001361580565571785\n",
      "          q_model.layers.6.bias: 0.3352315127849579\n",
      "          q_model.layers.6.weight: 0.13700604438781738\n",
      "          q_values: 9080.5244140625\n",
      "          raw_scores: 1.4197367429733276\n",
      "          target_q_values: 8966.720703125\n",
      "          td_error: 522.0323486328125\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 16032\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 16032\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 501\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.933333333333332\n",
      "    ram_util_percent: 67.16666666666667\n",
      "  pid: 72456\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042279534494568366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19546526437953005\n",
      "    mean_inference_ms: 1.0207993432420064\n",
      "    mean_raw_obs_processing_ms: 0.22904998317152425\n",
      "  time_since_restore: 9.76020336151123\n",
      "  time_this_iter_s: 3.9103620052337646\n",
      "  time_total_s: 9.76020336151123\n",
      "  timers:\n",
      "    learn_throughput: 4097.238\n",
      "    learn_time_ms: 7.81\n",
      "  timestamp: 1644773161\n",
      "  timesteps_since_restore: 96\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: f8560_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:26:04 (running for 00:00:22.00)<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING   </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         9.7602 </td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\"> 8.54267</td><td style=\"text-align: right;\">             13.4968</td><td style=\"text-align: right;\">             5.9909 </td><td style=\"text-align: right;\">           87.8529</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:26:09 (running for 00:00:27.25)<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING   </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        14.0904 </td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 8.5577 </td><td style=\"text-align: right;\">             13.4968</td><td style=\"text-align: right;\">             5.9909 </td><td style=\"text-align: right;\">           87.9111</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SlateQ_my_env_f8560_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-13_18-26-09\n",
      "  done: false\n",
      "  episode_len_mean: 88.48214285714286\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.496796226342333\n",
      "  episode_reward_mean: 8.480204960215547\n",
      "  episode_reward_min: 5.9908987127629265\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 56\n",
      "  experiment_id: 7f16debe21064adeb50c9cea5966a4ab\n",
      "  hostname: Svens-MBP\n",
      "  info:\n",
      "    last_target_update_ts: 5000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          choice_beta: 10.850476264953613\n",
      "          choice_loss: 2.6736936569213867\n",
      "          choice_model.beta: 10.850476264953613\n",
      "          choice_model.score_no_click: -1.012082576751709\n",
      "          choice_score_no_click: -1.012082576751709\n",
      "          grad_gnorm: 5.253176689147949\n",
      "          next_q_minus_q: 1.8787908554077148\n",
      "          next_q_values: 130.59974670410156\n",
      "          q_loss: 1.602462649345398\n",
      "          q_model.layers.0.bias: 0.11360964179039001\n",
      "          q_model.layers.0.weight: 0.17807967960834503\n",
      "          q_model.layers.2.bias: 0.015433719381690025\n",
      "          q_model.layers.2.weight: 0.006932763382792473\n",
      "          q_model.layers.4.bias: 0.02636750414967537\n",
      "          q_model.layers.4.weight: -0.005369562655687332\n",
      "          q_model.layers.6.bias: 0.24597123265266418\n",
      "          q_model.layers.6.weight: 0.05647430568933487\n",
      "          q_values: 128.720947265625\n",
      "          raw_scores: 1.2273144721984863\n",
      "          target_q_values: 129.38209533691406\n",
      "          td_error: 2.032569646835327\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 32032\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 32032\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 1001\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.366666666666665\n",
      "    ram_util_percent: 67.5\n",
      "  pid: 72456\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042281323353341616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19650953151085118\n",
      "    mean_inference_ms: 1.0213568236493216\n",
      "    mean_raw_obs_processing_ms: 0.2293473218779732\n",
      "  time_since_restore: 18.41587257385254\n",
      "  time_this_iter_s: 4.325501203536987\n",
      "  time_total_s: 18.41587257385254\n",
      "  timers:\n",
      "    learn_throughput: 4333.099\n",
      "    learn_time_ms: 7.385\n",
      "  timestamp: 1644773169\n",
      "  timesteps_since_restore: 160\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: f8560_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:26:15 (running for 00:00:33.14)<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING   </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">        22.7947 </td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\"> 8.51982</td><td style=\"text-align: right;\">             14.3484</td><td style=\"text-align: right;\">             5.34808</td><td style=\"text-align: right;\">           88.0882</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SlateQ_my_env_f8560_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-13_18-26-18\n",
      "  done: false\n",
      "  episode_len_mean: 88.31645569620254\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.348397316568608\n",
      "  episode_reward_mean: 8.471780915961952\n",
      "  episode_reward_min: 5.348079643947841\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 79\n",
      "  experiment_id: 7f16debe21064adeb50c9cea5966a4ab\n",
      "  hostname: Svens-MBP\n",
      "  info:\n",
      "    last_target_update_ts: 7000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          choice_beta: 10.843669891357422\n",
      "          choice_loss: 2.3360838890075684\n",
      "          choice_model.beta: 10.843669891357422\n",
      "          choice_model.score_no_click: -1.0141793489456177\n",
      "          choice_score_no_click: -1.0141793489456177\n",
      "          grad_gnorm: 2.5946853160858154\n",
      "          next_q_minus_q: 0.9468803405761719\n",
      "          next_q_values: 67.03594207763672\n",
      "          q_loss: 0.7521615624427795\n",
      "          q_model.layers.0.bias: 0.10560129582881927\n",
      "          q_model.layers.0.weight: 0.17562110722064972\n",
      "          q_model.layers.2.bias: 0.014242280274629593\n",
      "          q_model.layers.2.weight: 0.005855854135006666\n",
      "          q_model.layers.4.bias: 0.01642632856965065\n",
      "          q_model.layers.4.weight: -0.005849548615515232\n",
      "          q_model.layers.6.bias: 0.2236071228981018\n",
      "          q_model.layers.6.weight: 0.052395839244127274\n",
      "          q_values: 66.08906555175781\n",
      "          raw_scores: 1.2217755317687988\n",
      "          target_q_values: 66.4686050415039\n",
      "          td_error: 1.138209581375122\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 48032\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 48032\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 1501\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.785714285714286\n",
      "    ram_util_percent: 67.85714285714286\n",
      "  pid: 72456\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042495367338140584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19832518890269377\n",
      "    mean_inference_ms: 1.026992598488568\n",
      "    mean_raw_obs_processing_ms: 0.23065104757530014\n",
      "  time_since_restore: 27.20004153251648\n",
      "  time_this_iter_s: 4.405339956283569\n",
      "  time_total_s: 27.20004153251648\n",
      "  timers:\n",
      "    learn_throughput: 4418.342\n",
      "    learn_time_ms: 7.243\n",
      "  timestamp: 1644773178\n",
      "  timesteps_since_restore: 224\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: f8560_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:26:20 (running for 00:00:38.53)<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING   </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">        27.2    </td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\"> 8.47178</td><td style=\"text-align: right;\">             14.3484</td><td style=\"text-align: right;\">             5.34808</td><td style=\"text-align: right;\">           88.3165</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:26:26 (running for 00:00:43.94)<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING   </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        31.4886 </td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> 8.53312</td><td style=\"text-align: right;\">             14.3484</td><td style=\"text-align: right;\">             5.34808</td><td style=\"text-align: right;\">           88.5222</td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SlateQ_my_env_f8560_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-13_18-26-27\n",
      "  done: false\n",
      "  episode_len_mean: 88.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.348397316568608\n",
      "  episode_reward_mean: 8.488203978658104\n",
      "  episode_reward_min: 5.348079643947841\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 101\n",
      "  experiment_id: 7f16debe21064adeb50c9cea5966a4ab\n",
      "  hostname: Svens-MBP\n",
      "  info:\n",
      "    last_target_update_ts: 9000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          choice_beta: 10.835955619812012\n",
      "          choice_loss: 2.959062337875366\n",
      "          choice_model.beta: 10.835955619812012\n",
      "          choice_model.score_no_click: -1.0179862976074219\n",
      "          choice_score_no_click: -1.0179862976074219\n",
      "          grad_gnorm: 11.886569023132324\n",
      "          next_q_minus_q: 1.3588831424713135\n",
      "          next_q_values: 89.31866455078125\n",
      "          q_loss: 0.9463427662849426\n",
      "          q_model.layers.0.bias: 0.10830353200435638\n",
      "          q_model.layers.0.weight: 0.1764192134141922\n",
      "          q_model.layers.2.bias: 0.0146125303581357\n",
      "          q_model.layers.2.weight: 0.006143838167190552\n",
      "          q_model.layers.4.bias: 0.020733069628477097\n",
      "          q_model.layers.4.weight: -0.00559161277487874\n",
      "          q_model.layers.6.bias: 0.23773539066314697\n",
      "          q_model.layers.6.weight: 0.05373510718345642\n",
      "          q_values: 87.95977783203125\n",
      "          raw_scores: 1.0845980644226074\n",
      "          target_q_values: 88.50926971435547\n",
      "          td_error: 1.3341495990753174\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 64032\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 64032\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 2001\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.75\n",
      "    ram_util_percent: 67.89999999999999\n",
      "  pid: 72456\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042658434715001727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19960969027865105\n",
      "    mean_inference_ms: 1.031034859101174\n",
      "    mean_raw_obs_processing_ms: 0.23157041855572197\n",
      "  time_since_restore: 35.668009757995605\n",
      "  time_this_iter_s: 4.179455041885376\n",
      "  time_total_s: 35.668009757995605\n",
      "  timers:\n",
      "    learn_throughput: 4756.475\n",
      "    learn_time_ms: 6.728\n",
      "  timestamp: 1644773187\n",
      "  timesteps_since_restore: 288\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: f8560_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:26:31 (running for 00:00:49.12)<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>RUNNING   </td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">        35.668  </td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\"> 8.4882 </td><td style=\"text-align: right;\">             14.3484</td><td style=\"text-align: right;\">             5.34808</td><td style=\"text-align: right;\">           88.61  </td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SlateQ_my_env_f8560_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-13_18-26-31\n",
      "  done: true\n",
      "  episode_len_mean: 88.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.348397316568608\n",
      "  episode_reward_mean: 8.43597573897418\n",
      "  episode_reward_min: 5.348079643947841\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 112\n",
      "  experiment_id: 7f16debe21064adeb50c9cea5966a4ab\n",
      "  hostname: Svens-MBP\n",
      "  info:\n",
      "    last_target_update_ts: 10000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          choice_beta: 10.830251693725586\n",
      "          choice_loss: 2.4941964149475098\n",
      "          choice_model.beta: 10.830251693725586\n",
      "          choice_model.score_no_click: -1.0198209285736084\n",
      "          choice_score_no_click: -1.0198209285736084\n",
      "          grad_gnorm: 13.080735206604004\n",
      "          next_q_minus_q: -1.5250234603881836\n",
      "          next_q_values: 76.68445587158203\n",
      "          q_loss: 3.1740760803222656\n",
      "          q_model.layers.0.bias: 0.1064179465174675\n",
      "          q_model.layers.0.weight: 0.1758025586605072\n",
      "          q_model.layers.2.bias: 0.014357289299368858\n",
      "          q_model.layers.2.weight: 0.005947076715528965\n",
      "          q_model.layers.4.bias: 0.01974213495850563\n",
      "          q_model.layers.4.weight: -0.005652374122291803\n",
      "          q_model.layers.6.bias: 0.23484385013580322\n",
      "          q_model.layers.6.weight: 0.05281691253185272\n",
      "          q_values: 78.20948028564453\n",
      "          raw_scores: 1.2586359977722168\n",
      "          target_q_values: 76.05487823486328\n",
      "          td_error: 3.538743495941162\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 32\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 72032\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 72032\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 2251\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.683333333333332\n",
      "    ram_util_percent: 67.86666666666666\n",
      "  pid: 72456\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042595305270069356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20004319682853997\n",
      "    mean_inference_ms: 1.0298484764294724\n",
      "    mean_raw_obs_processing_ms: 0.23155246404025653\n",
      "  time_since_restore: 40.178202867507935\n",
      "  time_this_iter_s: 4.510193109512329\n",
      "  time_total_s: 40.178202867507935\n",
      "  timers:\n",
      "    learn_throughput: 4886.028\n",
      "    learn_time_ms: 6.549\n",
      "  timestamp: 1644773191\n",
      "  timesteps_since_restore: 320\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: f8560_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-13 18:26:32 (running for 00:00:49.78)<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.88 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /Users/sven/ray_results/SlateQ<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SlateQ_my_env_f8560_00000</td><td>TERMINATED</td><td>127.0.0.1:72456</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        40.1782 </td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\"> 8.43598</td><td style=\"text-align: right;\">             14.3484</td><td style=\"text-align: right;\">             5.34808</td><td style=\"text-align: right;\">           88.76  </td></tr>\n",
       "<tr><td>SlateQ_my_env_f8560_00001</td><td>TERMINATED</td><td>127.0.0.1:72458</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04097</td><td style=\"text-align: right;\"> 1000</td><td style=\"text-align: right;\"> 9.18235</td><td style=\"text-align: right;\">             12.2773</td><td style=\"text-align: right;\">             6.22177</td><td style=\"text-align: right;\">           87.5455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 18:26:32,759\tINFO tune.py:636 -- Total run time: 50.46 seconds (49.70 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# Plugging in Ray Tune.\n",
    "# Note that this is the recommended way to run any experiments with RLlib.\n",
    "# Reasons:\n",
    "# - Tune allows you to do hyperparameter tuning in a user-friendly way\n",
    "#   and at large scale!\n",
    "# - Tune automatically allocates needed resources for the different\n",
    "#   hyperparam trials and experiment runs on a cluster.\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Running stuff with tune, we can re-use the exact\n",
    "# same config that we used when working with RLlib directly!\n",
    "tune_config = config.copy()\n",
    "\n",
    "# Let's add our first hyperparameter search via our config.\n",
    "tune_config[\"train_batch_size\"] = tune.grid_search([32, 64])\n",
    "\n",
    "# We will configure an \"output\" location here to make sure we record all environment interactions.\n",
    "# This for the second part of this tutorial, in which we will explore offline RL.\n",
    "tune_config[\"output\"] = \"logdir\"\n",
    "\n",
    "# Now that we will run things \"automatically\" through tune, we have to\n",
    "# define one or more stopping criteria.\n",
    "# Tune will stop the run, once any single one of the criteria is matched (not all of them!).\n",
    "stop = {\n",
    "    # Note that the keys used here can be anything present in the above `rllib_trainer.train()` output dict.\n",
    "    \"training_iteration\": 10,\n",
    "    \"episode_reward_mean\": 9.0,\n",
    "}\n",
    "\n",
    "# \"SlateQ\" is a registered name that points to RLlib's SlateQTrainer.\n",
    "# See `ray/rllib/agents/registry.py`\n",
    "\n",
    "# Run a simple experiment until one of the stopping criteria is met.\n",
    "results = tune.run(\n",
    "    \"SlateQ\",\n",
    "    config=tune_config,\n",
    "    stop=stop,\n",
    "\n",
    "    # Note that no trainers will be returned from this call here.\n",
    "    # Tune will create n Trainers internally, run them in parallel and destroy them at the end.\n",
    "    # However, you can ...\n",
    "    checkpoint_at_end=True,  # ... create a checkpoint when done.\n",
    "    checkpoint_freq=10,  # ... create a checkpoint every 10 training iterations.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b886fb8-6ccd-4be2-80bb-fc0936808d11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Why did we use 2 CPUs in the tune run above (1 CPU per trial)?\n",
    "\n",
    "PPO - by default - SlateQ uses 0 \"rollout\" workers (`num_workers=0`), meaning all sampling happens on the \"local worker\". SlateQ uses a replay buffer, which allows it to ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d282a-4ad1-4d5f-9dec-00afb8154048",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 10 min break :)\n",
    "------------------\n",
    "\n",
    "\n",
    "(while the above experiment is running (and hopefully learning))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82057-6b4c-4075-bd32-93c3426a1700",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction to Offline RL\n",
    "\n",
    "<img src=\"images/offline_rl.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5160e4d0-8feb-411d-a457-dfc10d50e909",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sven/ray_results/SlateQ/SlateQ_my_env_f8560_00001_1_train_batch_size=64_2022-02-13_18-25-42\n",
      "The logdir contains the following files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['result.json',\n",
       " 'params.pkl',\n",
       " 'params.json',\n",
       " 'output-2022-02-13_18-25-51_worker-0_0.json',\n",
       " 'checkpoint_000001',\n",
       " 'events.out.tfevents.1644773142.Svens-MBP',\n",
       " 'progress.csv']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The previous tune.run (the one we did before the break) produced \"historic data\" output.\n",
    "# We will use this output in the following as input to a newly initialized, untrained offline RL algorithm.\n",
    "\n",
    "# Let's take a look at the generated file(s) first:\n",
    "output_dir = results.get_best_logdir(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "print(output_dir)\n",
    "\n",
    "# Here is what the best log directory contains:\n",
    "print(\"The logdir contains the following files:\")\n",
    "os.listdir(os.path.dirname(output_dir + \"/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4230f",
   "metadata": {},
   "source": [
    "### Using an (offline) input file with an offline RL algorithm.\n",
    "\n",
    "We will now pretend that we don't have a simulator for our problem (same recommender system problem as above) available, however, let's assume we possess a lot of pre-recorded, historic data from some legacy (non-RL) system.\n",
    "\n",
    "Assuming that this legacy system wrote some data into a JSON file (we'll simply use the same JSON file that our SlateQ algo produced above), how can we use this historic data to do RL either way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98f129c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 18:58:50,981\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BCTrainer"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's configure a new RLlib Trainer, one that's capable of reading the JSON input described\n",
    "# above and able to learn from this input.\n",
    "\n",
    "# For simplicity, we'll start with a behavioral cloning (BC) trainer:\n",
    "from ray.rllib.agents.marwil import BCTrainer\n",
    "\n",
    "offline_rl_config = {\n",
    "    # Specify your offline RL algo's historic (JSON) input:\n",
    "    \"input\": output_dir + \"/output-2022-02-13_18-25-51_worker-0_0.json\",\n",
    "    # Note: For non-offline RL algos, this is set to \"sampler\" by default.\n",
    "    #\"input\": \"sampler\",\n",
    "    \"observation_space\": env.observation_space,\n",
    "    \"action_space\": env.action_space,\n",
    "}\n",
    "\n",
    "bc_trainer = BCTrainer(config=offline_rl_config)\n",
    "bc_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d181dd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# Let's train our new behavioral cloning Trainer for some iterations:\n",
    "for _ in range(5):\n",
    "    results = bc_trainer.train()\n",
    "    print(results[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8cb107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh no! What happened?\n",
    "# We don't have an environment! No way to measure rewards per episode.\n",
    "\n",
    "# A quick fix would be:\n",
    "# We cheat! Let's use our environment from above to run some separate evaluation workers on while we train:\n",
    "\n",
    "offline_rl_config.update({\n",
    "    # \n",
    "    \"evaluation_interval\": 1,\n",
    "    \"evaluation_parallel_to_training\": True,\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"evaluation_duration\": 100,\n",
    "    \"evaluation_duration_unit\": \"episodes\",\n",
    "    \"evaluation_config\": {\n",
    "        \"env\": \"my_env\",\n",
    "        \"input\": \"sampler\",\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b9ca875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 10:17:24,104\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-02-14 10:17:24,208\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ray.rllib.evaluation.worker_set.WorkerSet object at 0x7fee031a0af0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 10:17:33,257\tWARNING trainer.py:1074 -- Worker crashed during call to `step_attempt()`. To try to continue training without the failed worker, set `ignore_worker_failures=True`.\n"
     ]
    },
    {
     "ename": "RayTaskError(ValueError)",
     "evalue": "\u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=72459, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f935eef41c0>)\n  File \"/Users/sven/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 789, in sample\n    raise ValueError(\nValueError: RolloutWorker has no `input_reader` object! Cannot call `sample()`. You can try setting `create_env_on_driver` to True.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(ValueError)\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#bc_trainer.evaluate()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Let's train our new behavioral cloning Trainer for some iterations:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mbc_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/tune/trainable.py:315\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m\"\"\"Runs one logical iteration of training.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03mCalls ``step()`` internally. Subclasses should override ``step()``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m    A dict that describes training progress.\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    314\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 315\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:1079\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;66;03m# Error out.\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1074\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1075\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorker crashed during call to `step_attempt()`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1076\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo try to continue training without the failed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1077\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker, set `ignore_worker_failures=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1078\u001b[0m         )\n\u001b[0;32m-> 1079\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Any other exception.\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# Allow logs messages to propagate.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:1065\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step_ctx\u001b[38;5;241m.\u001b[39mshould_stop(step_attempt_results):\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1065\u001b[0m         step_attempt_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_attempt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;66;03m# @ray.remote RolloutWorker failure.\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# Try to recover w/o the failed worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:1175\u001b[0m, in \u001b[0;36mTrainer.step_attempt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     step_results\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m   1166\u001b[0m             duration_fn\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         )\n\u001b[1;32m   1173\u001b[0m     )\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1175\u001b[0m     step_results\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;66;03m# Collect the training results from the future.\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m step_results\u001b[38;5;241m.\u001b[39mupdate(train_future\u001b[38;5;241m.\u001b[39mresult())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:1343\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, episodes_left_fn, duration_fn)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m round_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1343\u001b[0m batches \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_workers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munits_left_to_do\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;66;03m# 1 episode per returned batch.\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisodes\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/worker.py:1740\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(ValueError)\u001b[0m: \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=72459, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f935eef41c0>)\n  File \"/Users/sven/opt/anaconda3/envs/ray/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 789, in sample\n    raise ValueError(\nValueError: RolloutWorker has no `input_reader` object! Cannot call `sample()`. You can try setting `create_env_on_driver` to True."
     ]
    }
   ],
   "source": [
    "bc_trainer = BCTrainer(config=offline_rl_config)\n",
    "print(bc_trainer.evaluation_workers)\n",
    "#bc_trainer.evaluate()\n",
    "\n",
    "# Let's train our new behavioral cloning Trainer for some iterations:\n",
    "for _ in range(5):\n",
    "    results = bc_trainer.train()\n",
    "    print(results[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd66c3-f07a-4795-84ea-6b232ba6a047",
   "metadata": {},
   "source": [
    "### Saving and restoring a trained Trainer.\n",
    "Currently, `rllib_trainer` is in an already trained state.\n",
    "It holds optimized weights in its Q-value/Policy's models that allow it to act\n",
    "already somewhat smart in our environment when given an observation.\n",
    "\n",
    "However, if we closed this notebook right now, all the effort would have been for nothing.\n",
    "Let's therefore save the state of our trainer to disk for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57eae1e4-3cc4-4282-9a83-bc374bdad978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer (at iteration 31 was saved in '/Users/sven/ray_results/SlateQTrainer_my_env_2022-02-13_18-01-519mi4r8x8/checkpoint_000031/checkpoint-31'!\n",
      "The checkpoint directory contains the following files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['checkpoint-31.tune_metadata', '.is_checkpoint', 'checkpoint-31']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the `Trainer.save()` method to create a checkpoint.\n",
    "checkpoint_file = rllib_trainer.save()\n",
    "print(f\"Trainer (at iteration {rllib_trainer.iteration} was saved in '{checkpoint_file}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "os.listdir(os.path.dirname(checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e0ab-2c10-469a-97b1-4aadf1a1ec97",
   "metadata": {},
   "source": [
    "### Restoring and evaluating a Trainer\n",
    "In the following cell, we'll learn how to restore a saved Trainer from a checkpoint file.\n",
    "\n",
    "We'll also evaluate a completely new Trainer (should act more or less randomly) vs an already trained one (the one we just restored from the created checkpoint file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74ceedb9-c225-46f2-ad1d-f902c81d3256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 18:10:11,788\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-02-13 18:10:11,935\tINFO trainable.py:472 -- Restored on 127.0.0.1 from checkpoint: /Users/sven/ray_results/SlateQTrainer_my_env_2022-02-13_18-01-519mi4r8x8/checkpoint_000031/checkpoint-31\n",
      "2022-02-13 18:10:11,936\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 31, '_timesteps_total': 992, '_time_total': 112.33620977401733, '_episodes_total': 347}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating new trainer: R=nan\n",
      "Before restoring: Trainer is at iteration=0\n",
      "After restoring: Trainer is at iteration=31\n",
      "Evaluating restored trainer: R=nan\n"
     ]
    }
   ],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_trainer = SlateQTrainer(config=config)\n",
    "# Evaluate the new trainer (this should yield random results).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "print(f\"Before restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "new_trainer.restore(checkpoint_file)\n",
    "print(f\"After restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "\n",
    "# Evaluate again (this should yield results we saw after having trained our saved agent).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating restored trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {},
   "source": [
    "## Time for Q&A\n",
    "\n",
    "...\n",
    "\n",
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful.\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials/tree/main/rl_conference_2022\">github repo of this tutorial</a>.\n",
    "- <a href=\"https://docs.ray.io/en/latest/rllib/index.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on Ray and its libraries.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d097a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
